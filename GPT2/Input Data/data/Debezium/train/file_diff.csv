file_diff,commit_time,file_diff_id
"@@ -12,7 +12,7 @@ August XX, 2016 - [Detailed release notes](https://issues.jboss.org/browse/DBZ/f
 
 ### Backwards-incompatible changes since 0.3.0
 
-None
+* MySQL connector now properly decodes string values from the binlog based upon the column's character set encoding as read by the DDL statement. Upon upgrade and restart, the connector will re-read the recorded database history and now associate the columns with their the character sets, and any newly processed events will use properly encoded strings values. As expected, previously generated events are never altered. Force a snapshot to regenerate events for the servers. [DBZ-102](https://issues.jboss.org/projects/DBZ/issues/DBZ-102)
 
 ### Fixes since 0.3.0
 ",2016-08-29T17:19:24Z,86
"@@ -114,11 +114,13 @@ public void start(Map<String, String> props) {
                     // full history of the database.
                     logger.info(""Found no existing offset and snapshots disallowed, so starting at beginning of binlog"");
                     source.setBinlogStartPoint("""", 0L);// start from the beginning of the binlog
+                    taskContext.initializeHistory();
                 } else {
                     // We are allowed to use snapshots, and that is the best way to start ...
                     startWithSnapshot = true;
                     // The snapshot will determine if GTIDs are set
                     logger.info(""Found no existing offset, so preparing to perform a snapshot"");
+                    // The snapshot will also initialize history ...
                 }
             }
 ",2016-08-29T17:19:24Z,10
"@@ -5,9 +5,14 @@
  */
 package io.debezium.connector.mysql;
 
+import java.nio.charset.StandardCharsets;
 import java.sql.SQLException;
+import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
+import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.slf4j.Logger;
@@ -18,6 +23,7 @@
 import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
+import io.debezium.util.Strings;
 
 /**
  * A context for a JDBC connection to MySQL.
@@ -32,19 +38,21 @@ public class MySqlJdbcContext implements AutoCloseable {
     protected final Logger logger = LoggerFactory.getLogger(getClass());
     protected final Configuration config;
     protected final JdbcConnection jdbc;
-    private final Map<String,String> originalSystemProperties = new HashMap<>();
+    private final Map<String, String> originalSystemProperties = new HashMap<>();
 
     public MySqlJdbcContext(Configuration config) {
         this.config = config; // must be set before most methods are used
 
         // Set up the JDBC connection without actually connecting, with extra MySQL-specific properties
-        // to give us better JDBC database metadata behavior ...
+        // to give us better JDBC database metadata behavior, including using UTF-8 for the client-side character encoding
+        // per https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-reference-charsets.html
         boolean useSSL = sslModeEnabled();
         Configuration jdbcConfig = config.subset(""database."", true)
                                          .edit()
                                          .with(""useInformationSchema"", ""true"")
                                          .with(""nullCatalogMeansCurrent"", ""false"")
                                          .with(""useSSL"", Boolean.toString(useSSL))
+                                         .with(""characterEncoding"", StandardCharsets.UTF_8.name())
                                          .build();
         this.jdbc = new JdbcConnection(jdbcConfig, FACTORY);
     }
@@ -81,11 +89,11 @@ public SecureConnectionMode sslMode() {
         String mode = config.getString(MySqlConnectorConfig.SSL_MODE);
         return SecureConnectionMode.parse(mode);
     }
-    
+
     public boolean sslModeEnabled() {
         return sslMode() != SecureConnectionMode.DISABLED;
     }
-    
+
     public void start() {
         if (sslModeEnabled()) {
             originalSystemProperties.clear();
@@ -104,9 +112,9 @@ public void shutdown() {
             logger.error(""Unexpected error shutting down the database connection"", e);
         } finally {
             // Reset the system properties to their original value ...
-            originalSystemProperties.forEach((name,value)->{
-                if ( value != null ) {
-                    System.setProperty(name,value);
+            originalSystemProperties.forEach((name, value) -> {
+                if (value != null) {
+                    System.setProperty(name, value);
                 } else {
                     System.clearProperty(name);
                 }
@@ -123,6 +131,59 @@ protected String connectionString() {
         return jdbc.connectionString(MYSQL_CONNECTION_URL);
     }
 
+    /**
+     * Read the MySQL charset-related system variables.
+     * 
+     * @param sql the reference that should be set to the SQL statement; may be null if not needed
+     * @return the system variables that are related to server character sets; never null
+     */
+    protected Map<String, String> readMySqlCharsetSystemVariables(AtomicReference<String> sql) {
+        // Read the system variables from the MySQL instance and get the current database name ...
+        Map<String, String> variables = new HashMap<>();
+        try (JdbcConnection mysql = jdbc.connect()) {
+            logger.debug(""Reading MySQL charset-related system variables before parsing DDL history."");
+            String statement = ""SHOW VARIABLES WHERE Variable_name IN ('character_set_server','collation_server')"";
+            if (sql != null) sql.set(statement);
+            mysql.query(statement, rs -> {
+                while (rs.next()) {
+                    String varName = rs.getString(1);
+                    String value = rs.getString(2);
+                    if (varName != null && value != null) {
+                        variables.put(varName, value);
+                        logger.debug(""\t{} = {}"",
+                                     Strings.pad(varName, 45, ' '),
+                                     Strings.pad(value, 45, ' '));
+                    }
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Error reading MySQL variables: "" + e.getMessage(), e);
+        }
+        return variables;
+    }
+
+    protected String setStatementFor(Map<String, String> variables) {
+        StringBuilder sb = new StringBuilder(""SET "");
+        boolean first = true;
+        List<String> varNames = new ArrayList<>(variables.keySet());
+        Collections.sort(varNames);
+        for (String varName : varNames) {
+            if (first) {
+                first = false;
+            } else {
+                sb.append("", "");
+            }
+            sb.append(varName).append(""="");
+            String value = variables.get(varName);
+            if (value == null) value = """";
+            if (value.contains("","") || value.contains("";"")) {
+                value = ""'"" + value + ""'"";
+            }
+            sb.append(value);
+        }
+        return sb.append("";"").toString();
+    }
+
     protected void setSystemProperty(String property, Field field, boolean showValueInError) {
         String value = config.getString(field);
         if (value != null) {",2016-08-29T17:19:24Z,29
"@@ -24,6 +24,7 @@
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.TemporalPrecisionMode;
+import io.debezium.connector.mysql.MySqlSystemVariables.Scope;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.TimeZoneAdapter;
 import io.debezium.relational.Table;
@@ -106,7 +107,7 @@ public MySqlSchema(Configuration config, String serverName) {
         boolean adaptiveTimePrecision = TemporalPrecisionMode.ADAPTIVE.equals(timePrecisionMode);
         MySqlValueConverters valueConverters = new MySqlValueConverters(adaptiveTimePrecision);
         this.schemaBuilder = new TableSchemaBuilder(valueConverters, schemaNameValidator::validate);
-        
+
         // Set up the server name and schema prefix ...
         if (serverName != null) serverName = serverName.trim();
         this.serverName = serverName;
@@ -197,6 +198,26 @@ public String historyLocation() {
         return dbHistory.toString();
     }
 
+    /**
+     * Set the system variables on the DDL parser.
+     * 
+     * @param variables the system variables; may not be null but may be empty
+     */
+    public void setSystemVariables(Map<String, String> variables) {
+        variables.forEach((varName, value) -> {
+            ddlParser.systemVariables().setVariable(Scope.SESSION, varName, value);
+        });
+    }
+    
+    /**
+     * Get the system variables as known by the DDL parser.
+     * 
+     * @return the system variables; never null
+     */
+    public MySqlSystemVariables systemVariables() {
+        return ddlParser.systemVariables();
+    }
+    
     /**
      * Load the schema for the databases using JDBC database metadata. If there are changes relative to any
      * table definitions that existed when this method is called, those changes are recorded in the database history
@@ -335,12 +356,12 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
                     // the same order they were read for each _affected_ database, grouped together if multiple apply
                     // to the same _affected_ database...
                     ddlChanges.groupStatementStringsByDatabase((dbName, ddl) -> {
-                        if (filters.databaseFilter().test(dbName)) {
+                        if (filters.databaseFilter().test(dbName) || dbName == null || """".equals(dbName)) {
                             if (dbName == null) dbName = """";
                             statementConsumer.consume(dbName, ddlStatements);
                         }
                     });
-                } else if (filters.databaseFilter().test(databaseName)) {
+                } else if (filters.databaseFilter().test(databaseName) || databaseName == null || """".equals(databaseName)) {
                     if (databaseName == null) databaseName = """";
                     statementConsumer.consume(databaseName, ddlStatements);
                 }",2016-08-29T17:19:24Z,16
"@@ -5,11 +5,14 @@
  */
 package io.debezium.connector.mysql;
 
+import java.util.Map;
+
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.util.Clock;
 import io.debezium.util.LoggingContext;
 import io.debezium.util.LoggingContext.PreviousContext;
+import io.debezium.util.Strings;
 
 /**
  * A Kafka Connect source task reads the MySQL binary log and generate the corresponding data change events.
@@ -59,6 +62,19 @@ public RecordMakers makeRecord() {
         return recordProcessor;
     }
 
+    /**
+     * Initialize the database history with any server-specific information. This should be done only upon connector startup
+     * when the connector has no prior history.
+     */
+    public void initializeHistory() {
+        // Read the system variables from the MySQL instance and get the current database name ...
+        Map<String, String> variables = readMySqlCharsetSystemVariables(null);
+        String ddlStatement = setStatementFor(variables);
+
+        // And write them into the database history ...
+        dbSchema.applyDdl(source, """", ddlStatement, null);
+    }
+
     /**
      * Load the database schema information using the previously-recorded history, and stop reading the history when the
      * the history reaches the supplied starting point.
@@ -67,7 +83,23 @@ public RecordMakers makeRecord() {
      *            offset} at which the database schemas are to reflect; may not be null
      */
     public void loadHistory(SourceInfo startingPoint) {
+        // Read the system variables from the MySQL instance and load them into the DDL parser as defaults ...
+        Map<String, String> variables = readMySqlCharsetSystemVariables(null);
+        dbSchema.setSystemVariables(variables);
+
+        // And then load the history ...
         dbSchema.loadHistory(startingPoint);
+
+        // The server's default character set may have changed since we last recorded it in the history,
+        // so we need to see if the history's state does not match ...
+        String systemCharsetName = variables.get(MySqlSystemVariables.CHARSET_NAME_SERVER);
+        String systemCharsetNameFromHistory = dbSchema.systemVariables().getVariable(MySqlSystemVariables.CHARSET_NAME_SERVER);
+        if (!Strings.equalsIgnoreCase(systemCharsetName, systemCharsetNameFromHistory)) {
+            // The history's server character set is NOT the same as the server's current default,
+            // so record the change in the history ...
+            String ddlStatement = setStatementFor(variables);
+            dbSchema.applyDdl(source, """", ddlStatement, null);
+        }
         recordProcessor.regenerate();
     }
 
@@ -81,7 +113,7 @@ public long serverId() {
 
     public String serverName() {
         String serverName = config.getString(MySqlConnectorConfig.SERVER_NAME);
-        if ( serverName == null ) {
+        if (serverName == null) {
             serverName = hostname() + "":"" + port();
         }
         return serverName;
@@ -102,7 +134,7 @@ public long timeoutInMilliseconds() {
     public long pollIntervalInMillseconds() {
         return config.getLong(MySqlConnectorConfig.POLL_INTERVAL_MS);
     }
-    
+
     public long rowCountForLargeTable() {
         return config.getLong(MySqlConnectorConfig.ROW_COUNT_FOR_STREAMING_RESULT_SETS);
     }
@@ -150,14 +182,15 @@ public void shutdown() {
 
     /**
      * Configure the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * 
      * @param contextName the name of the context; may not be null
      * @return the previous MDC context; never null
      * @throws IllegalArgumentException if {@code contextName} is null
      */
     public PreviousContext configureLoggingContext(String contextName) {
         return LoggingContext.forConnector(""MySQL"", serverName(), contextName);
     }
-    
+
     /**
      * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
      * state before this method was called.",2016-08-29T17:19:24Z,87
"@@ -5,14 +5,20 @@
  */
 package io.debezium.connector.mysql;
 
+import java.nio.charset.Charset;
+import java.nio.charset.IllegalCharsetNameException;
+import java.nio.charset.StandardCharsets;
 import java.sql.Types;
 import java.time.OffsetDateTime;
 import java.time.ZoneOffset;
 
 import org.apache.kafka.connect.data.Field;
+import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaBuilder;
+import org.apache.kafka.connect.source.SourceRecord;
 
 import com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer;
+import com.mysql.jdbc.CharsetMapping;
 
 import io.debezium.annotation.Immutable;
 import io.debezium.jdbc.JdbcValueConverters;
@@ -73,11 +79,11 @@ public SchemaBuilder schemaBuilder(Column column) {
             return Year.builder();
         }
         if (matches(typeName, ""ENUM"")) {
-            String commaSeparatedOptions = extractEnumAndSetOptions(column,true);
+            String commaSeparatedOptions = extractEnumAndSetOptions(column, true);
             return io.debezium.data.Enum.builder(commaSeparatedOptions);
         }
         if (matches(typeName, ""SET"")) {
-            String commaSeparatedOptions = extractEnumAndSetOptions(column,true);
+            String commaSeparatedOptions = extractEnumAndSetOptions(column, true);
             return io.debezium.data.EnumSet.builder(commaSeparatedOptions);
         }
         // Otherwise, let the base class handle it ...
@@ -93,18 +99,87 @@ public ValueConverter converter(Column column, Field fieldDefn) {
         }
         if (matches(typeName, ""ENUM"")) {
             // Build up the character array based upon the column's type ...
-            String options = extractEnumAndSetOptions(column,false);
+            String options = extractEnumAndSetOptions(column, false);
             return (data) -> convertEnumToString(options, column, fieldDefn, data);
         }
         if (matches(typeName, ""SET"")) {
             // Build up the character array based upon the column's type ...
-            String options = extractEnumAndSetOptions(column,false);
+            String options = extractEnumAndSetOptions(column, false);
             return (data) -> convertSetToString(options, column, fieldDefn, data);
         }
+        
+        // We have to convert bytes encoded in the column's character set ...
+        switch (column.jdbcType()) {
+            case Types.CHAR: // variable-length
+            case Types.VARCHAR: // variable-length
+            case Types.LONGVARCHAR: // variable-length
+            case Types.CLOB: // variable-length
+            case Types.NCHAR: // fixed-length
+            case Types.NVARCHAR: // fixed-length
+            case Types.LONGNVARCHAR: // fixed-length
+            case Types.NCLOB: // fixed-length
+            case Types.DATALINK:
+            case Types.SQLXML:
+                Charset charset = charsetFor(column);
+                if (charset != null) {
+                    return (data) -> convertString(column, fieldDefn, charset, data);
+                }
+                logger.warn(""Using UTF-8 charset by default for column without charset: {}"", column);
+                return (data) -> convertString(column, fieldDefn, StandardCharsets.UTF_8, data);
+            default:
+                break;
+        }
+
         // Otherwise, let the base class handle it ...
         return super.converter(column, fieldDefn);
     }
 
+    /**
+     * Return the {@link Charset} instance with the MySQL-specific character set name used by the given column.
+     * 
+     * @param column the column in which the character set is used; never null
+     * @return the Java {@link Charset}, or null if there is no mapping
+     */
+    protected Charset charsetFor(Column column) {
+        String mySqlCharsetName = column.charsetName();
+        if (mySqlCharsetName == null) {
+            logger.warn(""Column is missing a character set: {}"", column);
+            return null;
+        }
+        String encoding = CharsetMapping.getJavaEncodingForMysqlCharset(mySqlCharsetName);
+        if (encoding == null) {
+            logger.warn(""Column uses MySQL character set '{}', which has no mapping to a Java character set"", mySqlCharsetName);
+        } else {
+            try {
+                return Charset.forName(encoding);
+            } catch (IllegalCharsetNameException e) {
+                logger.error(""Unable to load Java charset '{}' for column with MySQL character set '{}'"", encoding, mySqlCharsetName);
+            }
+        }
+        return null;
+    }
+
+    /**
+     * Convert the {@link String} or {@code byte[]} value to a string value used in a {@link SourceRecord}.
+     * 
+     * @param column the column in which the value appears
+     * @param fieldDefn the field definition for the {@link SourceRecord}'s {@link Schema}; never null
+     * @param columnCharset the Java character set in which column byte[] values are encoded; may not be null
+     * @param data the data; may be null
+     * @return the string value; may be null if the value is null or is an unknown input type
+     */
+    protected Object convertString(Column column, Field fieldDefn, Charset columnCharset, Object data) {
+        if (data == null) return null;
+        if (data instanceof byte[]) {
+            // Decode the binary representation using the given character encoding ...
+            return new String((byte[]) data, columnCharset);
+        }
+        if (data instanceof String) {
+            return data;
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
     /**
      * Converts a value object for a MySQL {@code YEAR}, which appear in the binlog as an integer though returns from
      * the MySQL JDBC driver as either a short or a {@link java.sql.Date}.
@@ -151,7 +226,7 @@ protected Object convertEnumToString(String options, Column column, Field fieldD
         }
         if (data instanceof Integer) {
             // The binlog will contain an int with the 1-based index of the option in the enum value ...
-            int index = ((Integer) data).intValue() - 1;    // 'options' is 0-based
+            int index = ((Integer) data).intValue() - 1; // 'options' is 0-based
             if (index < options.length()) {
                 return options.substring(index, index + 1);
             }
@@ -180,7 +255,7 @@ protected Object convertSetToString(String options, Column column, Field fieldDe
         if (data instanceof Long) {
             // The binlog will contain a long with the indexes of the options in the set value ...
             long indexes = ((Long) data).longValue();
-            return convertSetValue(indexes,options);
+            return convertSetValue(indexes, options);
         }
         return handleUnknownData(column, fieldDefn, data);
     }
@@ -200,25 +275,29 @@ protected boolean matches(String upperCaseTypeName, String upperCaseMatch) {
 
     protected String extractEnumAndSetOptions(Column column, boolean commaSeparated) {
         String options = MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
-        if ( !commaSeparated ) return options;
+        if (!commaSeparated) return options;
         StringBuilder sb = new StringBuilder();
         boolean first = true;
-        for ( int i=0; i!=options.length(); ++i ) {
-            if ( first ) first = false;
-            else sb.append(',');
+        for (int i = 0; i != options.length(); ++i) {
+            if (first)
+                first = false;
+            else
+                sb.append(',');
             sb.append(options.charAt(i));
         }
         return sb.toString();
     }
-    
-    protected String convertSetValue( long indexes, String options ) {
+
+    protected String convertSetValue(long indexes, String options) {
         StringBuilder sb = new StringBuilder();
         int index = 0;
         boolean first = true;
         while (indexes != 0L) {
             if (indexes % 2L != 0) {
-                if ( first ) first = false;
-                else sb.append(',');
+                if (first)
+                    first = false;
+                else
+                    sb.append(',');
                 sb.append(options.substring(index, index + 1));
             }
             ++index;",2016-08-29T17:19:24Z,70
"@@ -56,6 +56,16 @@ public static class DeleteRowsDeserializer extends DeleteRowsEventDataDeserializ
         public DeleteRowsDeserializer(Map<Long, TableMapEventData> tableMapEventByTableId) {
             super(tableMapEventByTableId);
         }
+        
+        @Override
+        protected Serializable deserializeString(int length, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeString(length, inputStream);
+        }
+        
+        @Override
+        protected Serializable deserializeVarString(int meta, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeVarString(meta, inputStream);
+        }
 
         @Override
         protected Serializable deserializeDate(ByteArrayInputStream inputStream) throws IOException {
@@ -104,6 +114,16 @@ public UpdateRowsDeserializer(Map<Long, TableMapEventData> tableMapEventByTableI
             super(tableMapEventByTableId);
         }
 
+        @Override
+        protected Serializable deserializeString(int length, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeString(length, inputStream);
+        }
+        
+        @Override
+        protected Serializable deserializeVarString(int meta, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeVarString(meta, inputStream);
+        }
+
         @Override
         protected Serializable deserializeDate(ByteArrayInputStream inputStream) throws IOException {
             return RowDeserializers.deserializeDate(inputStream);
@@ -151,6 +171,16 @@ public WriteRowsDeserializer(Map<Long, TableMapEventData> tableMapEventByTableId
             super(tableMapEventByTableId);
         }
 
+        @Override
+        protected Serializable deserializeString(int length, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeString(length, inputStream);
+        }
+        
+        @Override
+        protected Serializable deserializeVarString(int meta, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeVarString(meta, inputStream);
+        }
+
         @Override
         protected Serializable deserializeDate(ByteArrayInputStream inputStream) throws IOException {
             return RowDeserializers.deserializeDate(inputStream);
@@ -192,6 +222,34 @@ protected Serializable deserializeYear(ByteArrayInputStream inputStream) throws
         }
     }
 
+    /**
+     * Converts a MySQL string to a {@code byte[]}.
+     * 
+     * @param length the number of bytes used to store the length of the string
+     * @param inputStream the binary stream containing the raw binlog event data for the value
+     * @return the {@code byte[]} object
+     * @throws IOException if there is an error reading from the binlog event data
+     */
+    protected static Serializable deserializeString(int length, ByteArrayInputStream inputStream) throws IOException {
+        // charset is not present in the binary log (meaning there is no way to distinguish between CHAR / BINARY)
+        // as a result - return byte[] instead of an actual String
+        int stringLength = length < 256 ? inputStream.readInteger(1) : inputStream.readInteger(2);
+        return inputStream.read(stringLength);
+    }
+
+    /**
+     * Converts a MySQL string to a {@code byte[]}.
+     * 
+     * @param meta the {@code meta} value containing the number of bytes in the length field
+     * @param inputStream the binary stream containing the raw binlog event data for the value
+     * @return the {@code byte[]} object
+     * @throws IOException if there is an error reading from the binlog event data
+     */
+    protected static Serializable deserializeVarString(int meta, ByteArrayInputStream inputStream) throws IOException {
+        int varcharLength = meta < 256 ? inputStream.readInteger(1) : inputStream.readInteger(2);
+        return inputStream.read(varcharLength);
+    }
+
     /**
      * Converts a MySQL {@code DATE} value to a {@link LocalDate}.
      * ",2016-08-29T17:19:24Z,116
"@@ -19,6 +19,7 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
+
 import org.apache.kafka.connect.source.SourceRecord;
 
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
@@ -168,6 +169,10 @@ protected void execute() {
             sql.set(""SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"");
             mysql.execute(sql.get());
 
+            // Generate the DDL statements that set the charset-related system variables ...
+            Map<String, String> systemVariables = context.readMySqlCharsetSystemVariables(sql);
+            String setSystemVariablesStatement = context.setStatementFor(systemVariables);
+
             // ------
             // STEP 1
             // ------
@@ -206,13 +211,14 @@ protected void execute() {
                         String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
                         source.setGtidSet(gtidSet);
                         logger.info(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
-                                     gtidSet);
+                                    gtidSet);
                     } else {
                         logger.info(""\t using binlog '{}' at position '{}'"", binlogFilename, binlogPosition);
                     }
                     source.startSnapshot();
                 } else {
-                    throw new IllegalStateException(""Cannot read the binlog filename and position via '"" + showMasterStmt + ""'. Make sure your server is correctly configured"");    
+                    throw new IllegalStateException(""Cannot read the binlog filename and position via '"" + showMasterStmt
+                            + ""'. Make sure your server is correctly configured"");
                 }
             });
 
@@ -264,14 +270,18 @@ protected void execute() {
             // Transform the current schema so that it reflects the *current* state of the MySQL server's contents.
             // First, get the DROP TABLE and CREATE TABLE statement (with keys and constraint definitions) for our tables ...
             logger.info(""Step 6: generating DROP and CREATE statements to reflect current database schemas:"");
+            schema.applyDdl(source, null, setSystemVariablesStatement, this::enqueueSchemaChanges);
+
             // Add DROP TABLE statements for all tables that we knew about AND those tables found in the databases ...
             Set<TableId> allTableIds = new HashSet<>(schema.tables().tableIds());
             allTableIds.addAll(tableIds);
-            allTableIds.forEach(tableId -> schema.applyDdl(source, tableId.schema(), ""DROP TABLE IF EXISTS "" + tableId, this::enqueueSchemaChanges));
+            allTableIds.forEach(tableId -> schema.applyDdl(source, tableId.schema(), ""DROP TABLE IF EXISTS "" + tableId,
+                                                           this::enqueueSchemaChanges));
             // Add a DROP DATABASE statement for each database that we no longer know about ...
             schema.tables().tableIds().stream().map(TableId::catalog)
                   .filter(Predicates.not(databaseNames::contains))
-                  .forEach(missingDbName -> schema.applyDdl(source, missingDbName, ""DROP DATABASE IF EXISTS "" + missingDbName, this::enqueueSchemaChanges));
+                  .forEach(missingDbName -> schema.applyDdl(source, missingDbName, ""DROP DATABASE IF EXISTS "" + missingDbName,
+                                                            this::enqueueSchemaChanges));
             // Now process all of our tables for each database ...
             for (Map.Entry<String, List<TableId>> entry : tableIdsByDbName.entrySet()) {
                 String dbName = entry.getKey();
@@ -472,7 +482,7 @@ private Statement createStatement(Connection connection) throws SQLException {
     private void logServerInformation(JdbcConnection mysql) {
         try {
             logger.info(""MySQL server variables related to change data capture:"");
-            mysql.query(""SHOW VARIABLES WHERE Variable_name REGEXP 'version|binlog|tx_|gtid'"", rs -> {
+            mysql.query(""SHOW VARIABLES WHERE Variable_name REGEXP 'version|binlog|tx_|gtid|character_set|collation'"", rs -> {
                 while (rs.next()) {
                     logger.info(""\t{} = {}"",
                                 Strings.pad(rs.getString(1), 45, ' '),",2016-08-29T17:19:24Z,15
"@@ -116,6 +116,7 @@ public void shouldCreateSnapshotOfSingleDatabase() throws Exception {
         context = new MySqlTaskContext(config);
         context.start();
         context.source().setBinlogStartPoint("""",0L); // start from beginning
+        context.initializeHistory();
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...
@@ -175,6 +176,7 @@ public void shouldCreateSnapshotOfSingleDatabaseWithSchemaChanges() throws Excep
         context = new MySqlTaskContext(config);
         context.start();
         context.source().setBinlogStartPoint("""",0L); // start from beginning
+        context.initializeHistory();
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...",2016-08-29T17:19:24Z,24
"@@ -286,16 +286,17 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // ---------------------------------------------------------------------------------------------------------------
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
-        SourceRecords records = consumeRecordsByTopic(5 + 9 + 9 + 4 + 11); // 11 schema change records
-        assertThat(records.recordsForTopic(""myServer"").size()).isEqualTo(11);
+        SourceRecords records = consumeRecordsByTopic(5 + 9 + 9 + 4 + 11 + 1); // 11 schema change records + 1 SET statement
+        assertThat(records.recordsForTopic(""myServer"").size()).isEqualTo(12);
         assertThat(records.recordsForTopic(""myServer.connector_test.products"").size()).isEqualTo(9);
         assertThat(records.recordsForTopic(""myServer.connector_test.products_on_hand"").size()).isEqualTo(9);
         assertThat(records.recordsForTopic(""myServer.connector_test.customers"").size()).isEqualTo(4);
         assertThat(records.recordsForTopic(""myServer.connector_test.orders"").size()).isEqualTo(5);
         assertThat(records.topics().size()).isEqualTo(5);
-        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.databaseNames().size()).isEqualTo(2);
         assertThat(records.ddlRecordsForDatabase(""connector_test"").size()).isEqualTo(11);
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase("""").size()).isEqualTo(1);
         records.ddlRecordsForDatabase(""connector_test"").forEach(this::print);
 
         // Check that all records are valid, can be serialized and deserialized ...",2016-08-29T17:19:24Z,88
"@@ -109,11 +109,11 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
                 String c1 = after.getString(""c1"");
                 String c2 = after.getString(""c2"");
-                if ( c1.equals(""a"") ) {
+                if (c1.equals(""a"")) {
                     assertThat(c2).isEqualTo(""a,b,c"");
-                } else if ( c1.equals(""b"") ) {
+                } else if (c1.equals(""b"")) {
                     assertThat(c2).isEqualTo(""a,b"");
-                } else if ( c1.equals(""c"") ) {
+                } else if (c1.equals(""c"")) {
                     assertThat(c2).isEqualTo(""a"");
                 } else {
                     fail(""c1 didn't match expected value"");
@@ -230,11 +230,11 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
                 String c1 = after.getString(""c1"");
                 String c2 = after.getString(""c2"");
-                if ( c1.equals(""a"") ) {
+                if (c1.equals(""a"")) {
                     assertThat(c2).isEqualTo(""a,b,c"");
-                } else if ( c1.equals(""b"") ) {
+                } else if (c1.equals(""b"")) {
                     assertThat(c2).isEqualTo(""a,b"");
-                } else if ( c1.equals(""c"") ) {
+                } else if (c1.equals(""c"")) {
                     assertThat(c2).isEqualTo(""a"");
                 } else {
                     fail(""c1 didn't match expected value"");
@@ -325,20 +325,23 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
         // Testing.Debug.enable();
-        // 11 schema change records = 5 drop tables, 1 drop database, 1 create database, 1 use database, 5 create tables
-        SourceRecords records = consumeRecordsByTopic(11 + 6); // plus 6 data records ...
+        // 12 schema change records = 1 set variables, 5 drop tables, 1 drop database, 1 create database, 1 use database, 5 create
+        // tables
+        SourceRecords records = consumeRecordsByTopic(12 + 6); // plus 6 data records ...
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(11);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(12);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_100_enumsettest"").size()).isEqualTo(3);
         assertThat(records.topics().size()).isEqualTo(5);
-        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.databaseNames().size()).isEqualTo(2);
+        assertThat(records.databaseNames()).containsOnly(""regression_test"","""");
         assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(11);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase("""").size()).isEqualTo(1); // SET statement
         records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
 
         // Check that all records are valid, can be serialized and deserialized ...
@@ -349,11 +352,11 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
                 String c1 = after.getString(""c1"");
                 String c2 = after.getString(""c2"");
-                if ( c1.equals(""a"") ) {
+                if (c1.equals(""a"")) {
                     assertThat(c2).isEqualTo(""a,b,c"");
-                } else if ( c1.equals(""b"") ) {
+                } else if (c1.equals(""b"")) {
                     assertThat(c2).isEqualTo(""a,b"");
-                } else if ( c1.equals(""c"") ) {
+                } else if (c1.equals(""c"")) {
                     assertThat(c2).isEqualTo(""a"");
                 } else {
                     fail(""c1 didn't match expected value"");",2016-08-29T17:19:24Z,71
"@@ -61,7 +61,10 @@ public void shouldApplyDdlStatementsAndRecover() {
         mysql.start();
 
         // Testing.Print.enable();
+
+        // Set up the server ...
         source.setBinlogStartPoint(""binlog-001"",400);
+        mysql.applyDdl(source, ""db1"", ""SET "" + MySqlSystemVariables.CHARSET_NAME_SERVER + ""=utf8mb4"", this::printStatements);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -82,6 +85,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
         mysql.start();
 
         source.setBinlogStartPoint(""binlog-001"",400);
+        mysql.applyDdl(source, ""mysql"", ""SET "" + MySqlSystemVariables.CHARSET_NAME_SERVER + ""=utf8mb4"", this::printStatements);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
         source.setBinlogStartPoint(""binlog-001"",1000);
@@ -107,6 +111,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
         mysql.start();
 
         source.setBinlogStartPoint(""binlog-001"",400);
+        mysql.applyDdl(source, ""mysql"", ""SET "" + MySqlSystemVariables.CHARSET_NAME_SERVER + ""=utf8mb4"", this::printStatements);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
         source.setBinlogStartPoint(""binlog-001"",1000);",2016-08-29T17:19:24Z,16
"@@ -268,10 +268,10 @@ public void shouldCreateSnapshotOfSingleDatabaseWithSchemaChanges() throws Excep
         // The last poll should always return null ...
         assertThat(records).isNull();
         
-        // There should be 11 schema changes ...
-        assertThat(schemaChanges.recordCount()).isEqualTo(11);
-        assertThat(schemaChanges.databaseCount()).isEqualTo(1);
-        assertThat(schemaChanges.databases()).containsOnly(DB_NAME);
+        // There should be 11 schema changes plus 1 SET statement ...
+        assertThat(schemaChanges.recordCount()).isEqualTo(12);
+        assertThat(schemaChanges.databaseCount()).isEqualTo(2);
+        assertThat(schemaChanges.databases()).containsOnly(DB_NAME,"""");
         
         // Check the records via the store ...
         assertThat(store.collectionCount()).isEqualTo(4);",2016-08-29T17:19:24Z,15
"@@ -61,7 +61,7 @@ public class JdbcValueConverters implements ValueConverterProvider {
     private static final Double DOUBLE_TRUE = new Double(1.0d);
     private static final Double DOUBLE_FALSE = new Double(0.0d);
 
-    private final Logger logger = LoggerFactory.getLogger(getClass());
+    protected final Logger logger = LoggerFactory.getLogger(getClass());
     private final ZoneOffset defaultOffset;
     private final boolean adaptiveTimePrecision;
 ",2016-08-29T17:19:24Z,91
"@@ -16,7 +16,6 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicLong;
-import java.util.function.Predicate;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -155,28 +154,21 @@ protected void doStart() {
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
 
-        // The 'source' object holds the starting point in the binlog where we should start reading,
-        // set set the client to start from that point ...
-        String gtidSetStr = source.gtidSet();
-        if (gtidSetStr != null) {
+        // Get the current GtidSet from MySQL so we can get a filtered/merged GtidSet based off of the last Debezium checkpoint.
+        String availableServerGtidStr = context.knownGtidSet();
+        GtidSet availableServerGtidSet = new GtidSet(availableServerGtidStr);
+        GtidSet filteredGtidSet = context.getFilteredGtidSet(availableServerGtidSet);
+        if (filteredGtidSet != null) {
             // Register the event handler ...
             eventHandlers.put(EventType.GTID, this::handleGtidEvent);
-
-            logger.info(""GTID set from previous recorded offset: {}"", gtidSetStr);
-            // Remove any of the GTID sources that are not required/acceptable ...
-            Predicate<String> gtidSourceFilter = context.gtidSourceFilter();
-            if (gtidSourceFilter != null) {
-                GtidSet gtidSet = new GtidSet(gtidSetStr).retainAll(gtidSourceFilter);
-                gtidSetStr = gtidSet.toString();
-                logger.info(""GTID set after applying GTID source includes/excludes: {}"", gtidSetStr);
-                source.setGtidSet(gtidSetStr);
-            }
-            client.setGtidSet(gtidSetStr);
-            gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(gtidSetStr);
+            logger.info(""Registering binlog reader with GTID set: {}"", filteredGtidSet);
+            String filteredGtidSetStr = filteredGtidSet.toString();
+            client.setGtidSet(filteredGtidSetStr);
+            source.setGtidSet(filteredGtidSetStr);
+            gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(filteredGtidSetStr);
         } else {
             client.setBinlogFilename(source.binlogFilename());
             client.setBinlogPosition(source.nextBinlogPosition());
-            gtidSet = null;
         }
 
         // Set the starting row number, which is the next row number to be read ...",2016-11-03T17:14:54Z,24
"@@ -8,6 +8,7 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
@@ -100,6 +101,19 @@ public boolean isContainedWithin(GtidSet other) {
         return true;
     }
 
+    /**
+     * Obtain a copy of this {@link GtidSet} except overwritten with all of the GTID ranges in the supplied {@link GtidSet}.
+     * @param other the other {@link GtidSet} with ranges to add/overwrite on top of those in this set;
+     * @return the new GtidSet, or this object if {@code other} is null or empty; never null
+     */
+    public GtidSet with(GtidSet other) {
+        if (other == null || other.uuidSetsByServerId.isEmpty()) return this;
+        Map<String, UUIDSet> newSet = new HashMap<>();
+        newSet.putAll(this.uuidSetsByServerId);
+        newSet.putAll(other.uuidSetsByServerId);
+        return new GtidSet(newSet);
+    }
+
     @Override
     public int hashCode() {
         return uuidSetsByServerId.keySet().hashCode();",2016-11-03T17:14:54Z,101
"@@ -241,7 +241,7 @@ protected boolean isBinlogAvailable() {
         String gtidStr = taskContext.source().gtidSet();
         if (gtidStr != null) {
             if (gtidStr.trim().isEmpty()) return true; // start at beginning ...
-            String availableGtidStr = knownGtidSet();
+            String availableGtidStr = taskContext.knownGtidSet();
             if (availableGtidStr == null || availableGtidStr.trim().isEmpty()) {
                 // Last offsets had GTIDs but the server does not use them ...
                 logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
@@ -250,7 +250,7 @@ protected boolean isBinlogAvailable() {
             // GTIDs are enabled, and we used them previously, but retain only those GTID ranges for the allowed source UUIDs ...
             GtidSet gtidSet = new GtidSet(gtidStr).retainAll(taskContext.gtidSourceFilter());
             // Get the GTID set that is available in the server ...
-            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            GtidSet availableGtidSet = new GtidSet(availableGtidStr);
             if (gtidSet.isContainedWithin(availableGtidSet)) {
                 logger.info(""MySQL current GTID set {} does contain the GTID set required by the connector {}"", availableGtidSet, gtidSet);
                 return true;
@@ -328,26 +328,6 @@ protected boolean isGtidModeEnabled() {
         return !""OFF"".equalsIgnoreCase(mode.get());
     }
 
-    /**
-     * Determine the available GTID set for MySQL.
-     * 
-     * @return the string representation of MySQL's GTID sets.
-     */
-    protected String knownGtidSet() {
-        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
-        try {
-            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
-                if (rs.next()) {
-                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
-                }
-            });
-        } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
-        }
-
-        return gtidSetStr.get();
-    }
-
     /**
      * Determine whether the MySQL server has the row-level binlog enabled.
      * ",2016-11-03T17:14:54Z,10
"@@ -123,6 +123,26 @@ public void close() {
         shutdown();
     }
 
+    /**
+     * Determine the available GTID set for MySQL.
+     *
+     * @return the string representation of MySQL's GTID sets.
+     */
+    public String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            jdbc.query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
+        }
+
+        return gtidSetStr.get();
+    }
+
     protected String connectionString() {
         return jdbc.connectionString(MYSQL_CONNECTION_URL);
     }",2016-11-03T17:14:54Z,29
"@@ -246,4 +246,40 @@ public ObjectName metricName(String contextName) throws MalformedObjectNameExcep
         return new ObjectName(""debezium.mysql:type=connector-metrics,context="" + contextName + "",server="" + serverName());
     }
 
+    /**
+     * Retrieve GTID set after applying include/exclude filters on the source. Also, merges the server GTID set with the
+     * filtered client (Debezium) set.
+     *
+     * The merging behavior of this method might seem a bit strange at first. It's required in order for Debezium to consume a
+     * MySQL binlog that has multi-source replication enabled, if a failover has to occur. In such a case, the server that
+     * Debezium is failed over to might have a different set of sources, but still include the sources required for Debezium
+     * to continue to function. MySQL does not allow downstream replicas to connect if the GTID set does not contain GTIDs for
+     * all channels that the server is replicating from, even if the server does have the data needed by the client. To get
+     * around this, we can have Debezium merge its GTID set with whatever is on the server, so that MySQL will allow it to
+     * connect. See <a href=""https://issues.jboss.org/browse/DBZ-143"">DBZ-143</a> for details.
+     *
+     * This method does not mutate any state in the context.
+     *
+     * @return A GTID set meant for consuming from a MySQL binlog; may return null if the SourceInfo has no GTIDs and therefore
+     *         none were filtered
+     */
+    public GtidSet getFilteredGtidSet(GtidSet availableServerGtidSet) {
+        logger.info(""Attempting to generate a filtered GTID set"");
+        String gtidStr = source.gtidSet();
+        if (gtidStr == null) {
+            return null;
+        }
+        logger.info(""GTID set from previous recorded offset: {}"", gtidStr);
+        GtidSet filteredGtidSet = new GtidSet(gtidStr);
+        Predicate<String> gtidSourceFilter = gtidSourceFilter();
+        if (gtidSourceFilter != null) {
+            filteredGtidSet = filteredGtidSet.retainAll(gtidSourceFilter);
+            logger.info(""GTID set after applying GTID source includes/excludes to previous recorded offset: {}"", filteredGtidSet);
+        }
+        logger.info(""GTID set available on server: {}"", availableServerGtidSet);
+        GtidSet mergedGtidSet = availableServerGtidSet.with(filteredGtidSet);
+        logger.info(""Final merged GTID set to use when connecting to MySQL: {}"", mergedGtidSet);
+        return mergedGtidSet;
+    }
+
 }",2016-11-03T17:14:54Z,87
"@@ -6,6 +6,7 @@
 package io.debezium.connector.mysql;
 
 import java.nio.file.Path;
+import java.util.Arrays;
 import java.util.function.Predicate;
 
 import org.junit.After;
@@ -201,4 +202,31 @@ public void shouldNotAllowBothGtidSetIncludesAndExcludes() throws Exception {
         boolean valid = config.validateAndRecord(MySqlConnectorConfig.ALL_FIELDS,msg->{});
         assertThat(valid).isFalse();
     }
+
+    @Test
+    public void shouldFilterAndMergeGtidSet() throws Exception {
+        String gtidStr = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,""
+          + ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41"";
+        String availableServerGtidStr = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-20,""
+          + ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3200,""
+          + ""123e4567-e89b-12d3-a456-426655440000:1-41"";
+        config = simpleConfig().with(MySqlConnectorConfig.GTID_SOURCE_INCLUDES,
+                                     ""036d85a9-64e5-11e6-9b48-42010af0000c"")
+                               .build();
+        context = new MySqlTaskContext(config);
+        context.start();
+        context.source().setGtidSet(gtidStr);
+
+        GtidSet mergedGtidSet = context.getFilteredGtidSet(new GtidSet(availableServerGtidStr));
+        assertThat(mergedGtidSet).isNotNull();
+        GtidSet.UUIDSet uuidSet1 = mergedGtidSet.forServerWithId(""036d85a9-64e5-11e6-9b48-42010af0000c"");
+        GtidSet.UUIDSet uuidSet2 = mergedGtidSet.forServerWithId(""7145bf69-d1ca-11e5-a588-0242ac110004"");
+        GtidSet.UUIDSet uuidSet3 = mergedGtidSet.forServerWithId(""123e4567-e89b-12d3-a456-426655440000"");
+        GtidSet.UUIDSet uuidSet4 = mergedGtidSet.forServerWithId(""7c1de3f2-3fd2-11e6-9cdc-42010af000bc"");
+
+        assertThat(uuidSet1.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 2)));
+        assertThat(uuidSet2.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 3200)));
+        assertThat(uuidSet3.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 41)));
+        assertThat(uuidSet4).isNull();
+    }
 }",2016-11-03T17:14:54Z,32
"@@ -146,6 +146,53 @@
                       </assembly>
                     </build>
                   </image>
+                  <image>
+                    <!-- A Docker image using a partial MySQL installation maintained by MySQL team. -->
+                    <name>debezium/mysql-server-gtids-test-databases</name>
+                    <alias>database</alias>
+                    <run>
+                      <namingStrategy>alias</namingStrategy>
+                      <env>
+                        <MYSQL_ROOT_PASSWORD>debezium-rocks</MYSQL_ROOT_PASSWORD>
+                        <MYSQL_DATABASE>mysql</MYSQL_DATABASE> <!-- database created upon init -->
+                        <MYSQL_USER>${database.user}</MYSQL_USER>
+                        <MYSQL_PASSWORD>${database.password}</MYSQL_PASSWORD>
+                      </env>
+                      <ports>
+                        <port>${database.port}:3306</port>
+                      </ports>
+                      <log>
+                        <prefix>mysql</prefix>
+                        <enabled>true</enabled>
+                        <color>yellow</color>
+                      </log>
+                      <wait>
+                        <log>MySQL init process done. Ready for start up.</log>
+                        <time>30000</time> <!-- 30 seconds max -->
+                      </wait>
+                    </run>
+                    <build>
+                      <from>mysql/mysql-server:${version.mysql.server}</from>
+                      <assembly>
+                        <inline>
+                          <fileSets>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/server-gtids</directory>
+                              <includes>
+                                <include>my.cnf</include>
+                              </includes>
+                              <outputDirectory>etc/mysql</outputDirectory>
+                            </fileSet>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/init</directory>
+                              <outputDirectory>docker-entrypoint-initdb.d</outputDirectory>
+                            </fileSet>
+                          </fileSets>
+                        </inline>
+                        <basedir>/</basedir>
+                      </assembly>
+                    </build>
+                  </image>
                   <image>
                     <!-- A Docker image using a complete MySQL installation maintained by Docker team. -->
                     <name>debezium/mysql-test--databases</name>
@@ -350,5 +397,22 @@
           <docker.skip>false</docker.skip>
         </properties>
       </profile>
+      <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+            Use the Docker image for MySQL configured to use GTIDs.
+            To use, specify ""-Dgtid-mysql"" or -Pgtid-mysql on the Maven command line.
+            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+      <profile>
+        <id>gtid-mysql</id>
+        <activation>
+          <activeByDefault>false</activeByDefault>
+          <property>
+            <name>gtid-mysql</name>
+          </property>
+        </activation>
+        <properties>
+          <docker.image>debezium/mysql-server-gtids-test-databases</docker.image>
+          <docker.skip>false</docker.skip>
+        </properties>
+      </profile>
     </profiles>
 </project>",2016-06-07T17:01:51Z,97
"@@ -88,6 +88,7 @@ protected void doStart() {
         eventHandlers.put(EventType.ROTATE, this::handleRotateLogsEvent);
         eventHandlers.put(EventType.TABLE_MAP, this::handleUpdateTableMetadata);
         eventHandlers.put(EventType.QUERY, this::handleQueryEvent);
+        eventHandlers.put(EventType.GTID, this::handleGtidEvent);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, this::handleInsert);
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
@@ -142,9 +143,10 @@ protected void ignoreEvent(Event event) {
     protected void handleEvent(Event event) {
         if (event == null) return;
 
-        // Update the source offset info ...
+        // Update the source offset info. Note that the client returns the value in *milliseconds*, even though the binlog
+        // contains only *seconds* precision ...
         EventHeader eventHeader = event.getHeader();
-        source.setBinlogTimestamp(eventHeader.getTimestamp());
+        source.setBinlogTimestampSeconds(eventHeader.getTimestamp()/1000L); // client returns milliseconds, we record seconds
         source.setBinlogServerId(eventHeader.getServerId());
         EventType eventType = eventHeader.getEventType();
         if (eventType == EventType.ROTATE) {
@@ -166,16 +168,6 @@ protected void handleEvent(Event event) {
                 source.setRowInEvent(0);
             }
         }
-        if (eventType == EventType.GTID) {
-            EventData eventData = event.getData();
-            GtidEventData gtidEventData;
-            if (eventData instanceof EventDeserializer.EventDataWrapper) {
-                gtidEventData = (GtidEventData) ((EventDeserializer.EventDataWrapper) eventData).getInternal();
-            } else {
-                gtidEventData = (GtidEventData) eventData;
-            }
-            source.setGtids(gtidEventData.getGtid());
-        }
 
         // If there is a handler for this event, forward the event to it ...
         try {
@@ -188,6 +180,15 @@ protected void handleEvent(Event event) {
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    protected <T extends EventData> T unwrapData(Event event) {
+        EventData eventData = event.getData();
+        if (eventData instanceof EventDeserializer.EventDataWrapper) {
+            eventData = ((EventDeserializer.EventDataWrapper) eventData).getInternal();
+        }
+        return (T)eventData;
+    }
+
     /**
      * Handle the supplied event that signals that mysqld has stopped.
      * 
@@ -226,19 +227,31 @@ protected void handleServerIncident(Event event) {
      */
     protected void handleRotateLogsEvent(Event event) {
         logger.debug(""Rotating logs: {}"", event);
-        RotateEventData command = event.getData();
+        RotateEventData command = unwrapData(event);
         assert command != null;
         recordMakers.clear();
     }
 
+    /**
+     * Handle the supplied event with a {@link GtidEventData} that signals the beginning of a GTID transaction.
+     * 
+     * @param event the GTID event to be processed; may not be null
+     */
+    protected void handleGtidEvent(Event event) {
+        logger.debug(""GTID transaction: {}"", event);
+        GtidEventData gtidEvent = unwrapData(event);
+        source.setGtid(gtidEvent.getGtid());
+        source.setGtidSet(client.getGtidSet());
+    }
+
     /**
      * Handle the supplied event with an {@link QueryEventData} by possibly recording the DDL statements as changes in the
      * MySQL schemas.
      * 
      * @param event the database change data event to be processed; may not be null
      */
     protected void handleQueryEvent(Event event) {
-        QueryEventData command = event.getData();
+        QueryEventData command = unwrapData(event);
         logger.debug(""Received update table command: {}"", event);
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords && recordMakers.schemaChanges(dbName, statements, super::enqueueRecord) > 0) {
@@ -262,7 +275,7 @@ protected void handleQueryEvent(Event event) {
      * @param event the update event; never null
      */
     protected void handleUpdateTableMetadata(Event event) {
-        TableMapEventData metadata = event.getData();
+        TableMapEventData metadata = unwrapData(event);
         long tableNumber = metadata.getTableId();
         String databaseName = metadata.getDatabase();
         String tableName = metadata.getTable();
@@ -281,7 +294,7 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
-        WriteRowsEventData write = event.getData();
+        WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);
@@ -302,7 +315,7 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
-        UpdateRowsEventData update = event.getData();
+        UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
         // BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
@@ -330,7 +343,7 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
-        DeleteRowsEventData deleted = event.getData();
+        DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);",2016-06-07T17:01:51Z,24
"@@ -7,6 +7,7 @@
 
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.TreeMap;
@@ -111,11 +112,17 @@ protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
                 intervals.add(new Interval(interval.getStart(), interval.getEnd()));
             });
             Collections.sort(this.intervals);
-        }
-
-        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
-            this.uuid = uuid;
-            this.intervals = intervals;
+            if ( this.intervals.size() > 1 ) {
+                // Collapse adjacent intervals ...
+                for ( int i=intervals.size()-1; i!=0; --i) {
+                    Interval before = this.intervals.get(i-1);
+                    Interval after = this.intervals.get(i);
+                    if ( (before.getEnd() + 1) == after.getStart() ) {
+                        this.intervals.set(i-1,new Interval(before.getStart(),after.getEnd()));
+                        this.intervals.remove(i);
+                    }
+                }
+            }
         }
 
         /**
@@ -218,17 +225,41 @@ public String toString() {
             StringBuilder sb = new StringBuilder();
             if (sb.length() != 0) sb.append(',');
             sb.append(uuid).append(':');
-            sb.append(intervals.getFirst().getStart());
-            sb.append(intervals.getLast().getEnd());
+            Iterator<Interval> iter = intervals.iterator();
+            if ( iter.hasNext() ) sb.append(iter.next());
+            while ( iter.hasNext() ) {
+                sb.append(':');
+                sb.append(iter.next());
+            }
             return sb.toString();
         }
     }
 
     @Immutable
-    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+    public static class Interval implements Comparable<Interval> {
+
+        private final long start;
+        private final long end;
 
         public Interval(long start, long end) {
-            super(start, end);
+            this.start = start;
+            this.end = end;
+        }
+
+        /**
+         * Get the starting transaction number in this interval.
+         * @return this interval's first transaction number
+         */
+        public long getStart() {
+            return start;
+        }
+
+        /**
+         * Get the ending transaction number in this interval.
+         * @return this interval's last transaction number
+         */
+        public long getEnd() {
+            return end;
         }
 
         /**
@@ -244,6 +275,15 @@ public boolean isSubsetOf(Interval other) {
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
         }
+        
+        @Override
+        public int compareTo(Interval that) {
+            if ( that == this ) return 0;
+            long diff = this.start - that.start;
+            if ( diff > Integer.MAX_VALUE ) return Integer.MAX_VALUE;
+            if ( diff < Integer.MIN_VALUE ) return Integer.MIN_VALUE;
+            return (int) diff;
+        }
 
         @Override
         public int hashCode() {",2016-06-07T17:01:51Z,101
"@@ -9,6 +9,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -110,9 +111,16 @@ public void start(Map<String, String> props) {
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;
+                // The snapshot will determine if GTIDs are set
             }
         }
 
+        if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
+            // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
+            // previously used but the MySQL server has them enabled ...
+            source.setGtidSet("""");
+        }
+
         // Set up the readers ...
         this.binlogReader = new BinlogReader(taskContext);
         if (startWithSnapshot) {
@@ -174,13 +182,33 @@ protected void transitionToReadBinlog() {
      * @return {@code true} if the server has the binlog coordinates, or {@code false} otherwise
      */
     protected boolean isBinlogAvailable() {
+        String gtidStr = taskContext.source().gtidSet();
+        if ( gtidStr != null) {
+            if ( gtidStr.trim().isEmpty() ) return true; // start at beginning ...
+            String availableGtidStr = knownGtidSet();
+            if ( availableGtidStr == null || availableGtidStr.trim().isEmpty() ) {
+                // Last offsets had GTIDs but the server does not use them ...
+                logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
+                return false;
+            }
+            // GTIDs are enabled, and we used them previously ...
+            GtidSet gtidSet = new GtidSet(gtidStr);
+            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+                return true;
+            }
+            logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);
+            return false;
+        }
+        
         String binlogFilename = taskContext.source().binlogFilename();
         if (binlogFilename == null) return true; // start at current position
         if (binlogFilename.equals("""")) return true; // start at beginning
 
         // Accumulate the available binlog filenames ...
         List<String> logNames = new ArrayList<>();
         try {
+            logger.info(""Stop 0: Get all known binlogs from MySQL"");
             taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
                 while (rs.next()) {
                     logNames.add(rs.getString(1));
@@ -191,6 +219,50 @@ protected boolean isBinlogAvailable() {
         }
 
         // And compare with the one we're supposed to use ...
-        return logNames.stream().anyMatch(binlogFilename::equals);
+        boolean found = logNames.stream().anyMatch(binlogFilename::equals);
+        if ( !found ) {
+            logger.info(""Connector requires binlog file '{}', but MySQL only has {}"",binlogFilename,String.join("", "",logNames));
+        }
+        return found;
+    }
+
+    /**
+     * Determine whether the MySQL server has GTIDs enabled.
+     * 
+     * @return {@code false} if the server's {@code gtid_mode} is set and is {@code OFF}, or {@code true} otherwise
+     */
+    protected boolean isGtidModeEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>(""off"");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'GTID_MODE'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return !""OFF"".equalsIgnoreCase(mode.get());
+    }
+
+    /**
+     * Determine the available GTID set for MySQL.
+     * 
+     * @return the string representation of MySQL's GTID sets.
+     */
+    protected String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return gtidSetStr.get();
     }
 }",2016-06-07T17:01:51Z,10
"@@ -188,7 +188,7 @@ protected void execute() {
                 if (rs.next()) {
                     source.setBinlogFilename(rs.getString(1));
                     source.setBinlogPosition(rs.getLong(2));
-                    source.setGtids(rs.getString(5));// GTIDs
+                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
                     source.startSnapshot();
                 }
             });",2016-06-07T17:01:51Z,15
"@@ -14,6 +14,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.data.Envelope;
 import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
@@ -26,7 +27,7 @@
  * 
  * <pre>
  * {
- *     ""db"" : ""myDatabase""
+ *     ""server"" : ""production-server""
  * }
  * </pre>
  * 
@@ -36,14 +37,40 @@
  * 
  * <pre>
  * {
- *         ""gtids"" = ""3E11FA47-71CA-11E1-9E33-C80AA9429562:1-5"",
- *         ""file"" = ""mysql-bin.000003"",
- *         ""pos"" = 105586,
- *         ""row"" = 0
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
+ *     ""file"" = ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"" = 0,
+ *     ""snapshot"": true
  * }
  * </pre>
  * 
- * Note that the ""{@code gtid}"" field is present only when GTIDs are enabled.
+ * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
+ * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
+ * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
+ * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
+ * 
+ * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
+ * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
+ * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
+ * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
+ * an event that corresponds to the above partition and offset:
+ * 
+ * <pre>
+ * {
+ *     ""name"": ""production-server"",
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"": 0,
+ *     ""snapshot"": true
+ * }
+ * </pre>
  * 
  * @author Randall Hauch
  */
@@ -56,11 +83,12 @@ final class SourceInfo {
 
     public static final String SERVER_NAME_KEY = ""name"";
     public static final String SERVER_PARTITION_KEY = ""server"";
-    public static final String GTID_KEY = ""gtids"";
+    public static final String GTID_SET_KEY = ""gtids"";
+    public static final String GTID_KEY = ""gtid"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
     public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
-    public static final String TIMESTAMP_KEY = ""ts"";
+    public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
     /**
@@ -78,13 +106,14 @@ final class SourceInfo {
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private GtidSet binlogGtids;
+    private String gtidSet;
+    private String binlogGtid;
     private String binlogFilename;
     private long binlogPosition = 4;
     private int eventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
-    private long binlogTs = 0;
+    private long binlogTimestampSeconds = 0;
     private Map<String, String> sourcePartition;
     private boolean snapshot = false;
 
@@ -122,11 +151,11 @@ public Map<String, String> partition() {
      */
     public Map<String, ?> offset() {
         Map<String, Object> map = new HashMap<>();
-        if (binlogGtids != null) {
-            map.put(GTID_KEY, binlogGtids.toString());
+        if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
+        if (gtidSet != null) {
+            map.put(GTID_SET_KEY, gtidSet);
         }
-        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTs != 0 ) map.put(TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -158,13 +187,14 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        if (binlogGtids != null) {
-            result.put(GTID_KEY, binlogGtids.toString());
+        // Don't put the GTID Set into the struct; only the current GTID is fine ...
+        if (binlogGtid != null) {
+            result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
-        result.put(TIMESTAMP_KEY, binlogTs);
+        result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
@@ -193,12 +223,23 @@ public boolean isSnapshotInEffect() {
     }
 
     /**
-     * Set the GTID range for the MySQL binary log file.
+     * Set the latest GTID from the MySQL binary log file.
+     * 
+     * @param gtid the string representation of a specific GTID; may not be null
+     */
+    public void setGtid(String gtid) {
+        this.binlogGtid = gtid;
+    }
+
+    /**
+     * Set the set of GTIDs known to the MySQL server.
      * 
-     * @param gtids the string representation of the binlog GTIDs; may not be null
+     * @param gtidSet the string representation of GTID set; may not be null
      */
-    public void setGtids(String gtids) {
-        this.binlogGtids = gtids != null && !gtids.trim().isEmpty() ? new GtidSet(gtids) : null;
+    public void setGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            this.gtidSet = gtidSet;
+        }
     }
 
     /**
@@ -239,12 +280,12 @@ public void setBinlogServerId(long serverId) {
     }
 
     /**
-     * Set the timestamp as found within the MySQL binary log file.
+     * Set the number of <em>seconds</em> since Unix epoch (January 1, 1970) as found within the MySQL binary log file.
      * 
-     * @param timestamp the timestamp found within the binary log file
+     * @param timestampInSeconds the timestamp in <em>seconds</em> found within the binary log file
      */
-    public void setBinlogTimestamp(long timestamp) {
-        this.binlogTs = timestamp;
+    public void setBinlogTimestampSeconds(long timestampInSeconds) {
+        this.binlogTimestampSeconds = timestampInSeconds / 1000;
     }
 
     /**
@@ -270,7 +311,7 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtids((String) sourceOffset.get(GTID_KEY)); // may be null
+            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
             binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
@@ -297,7 +338,7 @@ private long longOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.binlogGtids != null ? this.binlogGtids.toString() : null;
+        return this.gtidSet != null ? this.gtidSet.toString() : null;
     }
 
     /**
@@ -340,9 +381,9 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (binlogGtids != null) {
+        if (gtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(binlogGtids);
+            sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
             sb.append("", pos="").append(binlogPosition());
             sb.append("", row="").append(eventRowNumber());
@@ -379,17 +420,17 @@ public String toString() {
      * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
      */
     public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
-        String recordedGtidSetStr = recorded.getString(GTID_KEY);
-        String desiredGtidSetStr = desired.getString(GTID_KEY);
+        String recordedGtidSetStr = recorded.getString(GTID_SET_KEY);
+        String desiredGtidSetStr = desired.getString(GTID_SET_KEY);
         if (desiredGtidSetStr != null) {
             // The desired position uses GTIDs, so we ideally compare using GTIDs ...
             if (recordedGtidSetStr != null) {
                 // Both have GTIDs, so base the comparison entirely on the GTID sets.
                 GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
                 GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
-                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                if (recordedGtidSet.equals(desiredGtidSet)) {
                     // They are exactly the same, which means the recorded position exactly matches the desired ...
-                    if ( !recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
+                    if (!recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
@@ -414,35 +455,35 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         }
 
         // Both positions are missing GTIDs. Look at the servers ...
-        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        if ( recordedServerId != desiredServerId ) {
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        if (recordedServerId != desiredServerId) {
             // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
             // is compare timestamps, and we have to assume that the server timestamps can be compared ...
-            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
-            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
+            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
+            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
             return recordedTimestamp <= desiredTimestamp;
         }
-        
+
         // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
         // comparable ...
         String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
         String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
-        if ( diff > 0 ) return false;
-        
+        if (diff > 0) return false;
+
         // The positions are the same, so compare the row number ...
         int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The binlog coordinates are the same ...
         return true;",2016-06-07T17:01:51Z,11
"@@ -0,0 +1,52 @@
+# For advice on how to change settings please see
+# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
+
+[mysqld]
+#
+# Remove leading # and set to the amount of RAM for the most important data
+# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
+# innodb_buffer_pool_size = 128M
+#
+# Remove leading # to turn on a very important data integrity option: logging
+# changes to the binary log between backups.
+# log_bin
+#
+# Remove leading # to set options mainly useful for reporting servers.
+# The server defaults are faster for transactions and fast SELECTs.
+# Adjust sizes as needed, experiment to find the optimal values.
+# join_buffer_size = 128M
+# sort_buffer_size = 2M
+# read_rnd_buffer_size = 2M
+skip-host-cache
+skip-name-resolve
+datadir=/var/lib/mysql
+socket=/var/lib/mysql/mysql.sock
+secure-file-priv=/var/lib/mysql-files
+user=mysql
+
+# Disabling symbolic-links is recommended to prevent assorted security risks
+symbolic-links=0
+
+log-error=/var/log/mysqld.log
+pid-file=/var/run/mysqld/mysqld.pid
+
+# ----------------------------------------------
+# Enable GTIDs on this master
+# ----------------------------------------------
+gtid_mode                 = on
+enforce_gtid_consistency  = on
+
+# ----------------------------------------------
+# Debezium ingest
+# ----------------------------------------------
+
+# Enable binary replication log and set the prefix, expiration, and log format.
+# The prefix is arbitrary, expiration can be short for integration tests but would
+# be longer on a production system. Row-level info is required for ingest to work.
+# Server ID is required, but this will vary on production systems
+server-id         = 112233
+log_bin           = mysql-bin
+expire_logs_days  = 1
+binlog_format     = row
+
+",2016-06-07T17:01:51Z,117
"@@ -0,0 +1,107 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.connector.mysql.GtidSet.Interval;
+import io.debezium.connector.mysql.GtidSet.UUIDSet;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class GtidSetTest {
+    
+    private static final String UUID1 = ""24bc7850-2c16-11e6-a073-0242ac110002"";
+
+    private GtidSet gtids;
+    
+    @Test
+    public void shouldCreateSetWithSingleInterval() {
+        gtids = new GtidSet(UUID1 + "":1-191"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,191);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,1,191);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191"");
+    }
+    
+    @Test
+    public void shouldCollapseAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,199);
+        asertFirstInterval(UUID1,1,199);
+        asertLastInterval(UUID1,1,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199"");
+    }
+
+    
+    @Test
+    public void shouldNotCollapseNonAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199"");
+        asertIntervalCount(UUID1,2);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,193,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199:1000-1033"");
+        asertIntervalCount(UUID1,3);
+        asertFirstInterval(UUID1,1,191);
+        asertIntervalExists(UUID1,193,199);
+        asertLastInterval(UUID1,1000,1033);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199:1000-1033"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervalsThatMayBeAdjacent() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199:1000-1033:1035-1036:1038-1039"");
+        asertIntervalCount(UUID1, 4);
+        asertFirstInterval(UUID1, 1, 199);
+        asertIntervalExists(UUID1, 1000, 1033);
+        asertIntervalExists(UUID1, 1035, 1036);
+        asertLastInterval(UUID1, 1038, 1039);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199:1000-1033:1035-1036:1038-1039""); // ??
+    }
+    
+    protected void asertIntervalCount( String uuid, int count) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        assertThat(set.getIntervals().size()).isEqualTo(count);
+    }
+    
+    protected void asertIntervalExists( String uuid, int start, int end) {
+        assertThat(hasInterval(uuid,start,end)).isTrue();
+    }
+    
+    protected void asertFirstInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getFirstInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected void asertLastInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getLastInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected boolean hasInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        for ( Interval interval : set.getIntervals() ) {
+            if ( interval.getStart() == start && interval.getEnd() == end ) return true;
+        }
+        return false;
+    }
+
+}",2016-06-07T17:01:51Z,118
"@@ -86,9 +86,9 @@ protected Document positionWithGtids(String gtids) {
 
     protected Document positionWithGtids(String gtids, boolean snapshot) {
         if (snapshot) {
-            return Document.create(SourceInfo.GTID_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
+            return Document.create(SourceInfo.GTID_SET_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
         }
-        return Document.create(SourceInfo.GTID_KEY, gtids);
+        return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
     protected Document positionWithoutGtids(String filename, int position, int row) {",2016-06-07T17:01:51Z,11
"@@ -45,15 +45,18 @@ public static enum Operation {
          */
         DELETE(""d"");
         private final String code;
+
         private Operation(String code) {
             this.code = code;
         }
-        public static Operation forCode( String code ) {
-            for ( Operation op : Operation.values()) {
-                if ( op.code().equalsIgnoreCase(code)) return op;
+
+        public static Operation forCode(String code) {
+            for (Operation op : Operation.values()) {
+                if (op.code().equalsIgnoreCase(code)) return op;
             }
             return null;
         }
+
         public String code() {
             return code;
         }
@@ -81,11 +84,13 @@ public static final class FieldName {
          */
         public static final String SOURCE = ""source"";
         /**
-         * The {@code ts} field is used to store the information about the local time at which the connector processed/generated
-         * the event. Note that the accuracy of the timestamp is not defined, and the values may not always be monotonically
-         * increasing.
+         * The {@code ts_ms} field is used to store the information about the local time at which the connector
+         * processed/generated the event. The timestamp values are the number of milliseconds past epoch (January 1, 1970), and
+         * determined by the {@link System#currentTimeMillis() JVM current time in milliseconds}. Note that the <em>accuracy</em>
+         * of the timestamp value depends on the JVM's system clock and all of its assumptions, limitations, conditions, and
+         * variations.
          */
-        public static final String TIMESTAMP = ""ts"";
+        public static final String TIMESTAMP = ""ts_ms"";
     }
 
     /**
@@ -304,6 +309,7 @@ public Struct delete(Struct before, Struct source, Long timestamp) {
 
     /**
      * Obtain the operation for the given source record.
+     * 
      * @param record the source record; may not be null
      * @return the operation, or null if no valid operation was found in the record
      */",2016-06-07T17:01:51Z,119
"@@ -91,10 +91,27 @@ public static TemporalPrecisionMode parse(String value, String defaultValue) {
      * The set of predefined SnapshotMode options or aliases.
      */
     public static enum SnapshotMode {
+
+        /**
+         * Perform a snapshot when it is needed.
+         */
+        WHEN_NEEDED(""when_needed""),
+        
+        /**
+         * Perform a snapshot only upon initial startup of a connector.
+         */
+        INITIAL(""initial""),
+
+        /**
+         * Never perform a snapshot and only read the binlog. This assumes the binlog contains all the history of those
+         * databases and tables that will be captured.
+         */
+        NEVER(""never""),
+
         /**
-         * Forwards each event as a structured Kafka Connect message.
+         * Perform a snapshot and then stop before attempting to read the binlog.
          */
-        WHEN_NEEDED(""when_needed""), INITIAL(""initial""), NEVER(""never"");
+        INITIAL_ONLY(""initial_only"");
 
         private final String value;
 
@@ -474,7 +491,8 @@ public static SecureConnectionMode parse(String value, String defaultValue) {
                                                    .withDescription(""The criteria for running a snapshot upon startup of the connector. ""
                                                            + ""Options include: ""
                                                            + ""'when_needed' to specify that the connector run a snapshot upon startup whenever it deems it necessary; ""
-                                                           + ""'initial' (the default) to specify the connector can run a snapshot only when no offsets are available for the logical server name; and ""
+                                                           + ""'initial' (the default) to specify the connector can run a snapshot only when no offsets are available for the logical server name; ""
+                                                           + ""'initial_only' same as 'initial' except the connector should stop after completing the snapshot and before it would normally read the binlog; and""
                                                            + ""'never' to specify the connector should never run a snapshot and that upon first startup the connector should read from the beginning of the binlog. ""
                                                            + ""The 'never' mode should be used with care, and only when the binlog is known to contain all history."");
 ",2016-09-22T22:09:11Z,65
"@@ -115,6 +115,15 @@ public void start(Map<String, String> props) {
                     logger.info(""Found no existing offset and snapshots disallowed, so starting at beginning of binlog"");
                     source.setBinlogStartPoint("""", 0L);// start from the beginning of the binlog
                     taskContext.initializeHistory();
+
+                    // Look to see what the first available binlog file is called, and whether it looks like binlog files have
+                    // been purged. If so, then output a warning ...
+                    String earliestBinlogFilename = earliestBinlogFilename();
+                    if (earliestBinlogFilename == null) {
+                        logger.warn(""No binlog appears to be available. Ensure that the MySQL row-level binlog is enabled."");
+                    } else if (!earliestBinlogFilename.endsWith(""00001"")) {
+                        logger.warn(""It is possible the server has purged some binlogs. If this is the case, then using snapshot mode may be required."");
+                    }
                 } else {
                     // We are allowed to use snapshots, and that is the best way to start ...
                     startWithSnapshot = true;
@@ -130,22 +139,44 @@ public void start(Map<String, String> props) {
                 source.setGtidSet("""");
             }
 
+            // Check whether the row-level binlog is enabled ...
+            final boolean rowBinlogEnabled = isRowBinlogEnabled();
+
             // Set up the readers ...
             this.binlogReader = new BinlogReader(taskContext);
             if (startWithSnapshot) {
                 // We're supposed to start with a snapshot, so set that up ...
                 this.snapshotReader = new SnapshotReader(taskContext);
-                this.snapshotReader.onSuccessfulCompletion(this::transitionToReadBinlog);
+                if (!taskContext.isInitialSnapshotOnly()) {
+                    logger.warn(""This connector will only perform a snapshot, and will stop after that completes."");
+                    this.snapshotReader.onSuccessfulCompletion(this::skipReadBinlog);
+                } else if (rowBinlogEnabled) {
+                    // This is the normal mode ...
+                    this.snapshotReader.onSuccessfulCompletion(this::transitionToReadBinlog);
+                } else {
+                    assert !rowBinlogEnabled;
+                    assert !taskContext.isInitialSnapshotOnly();
+                    throw new ConnectException(""The MySQL server is not configured to use a row-level binlog, which is ""
+                            + ""required for this connector to work properly. Change the MySQL configuration to use a ""
+                            + ""row-level binlog and restart the connector."");
+                }
                 this.snapshotReader.useMinimalBlocking(taskContext.useMinimalSnapshotLocking());
                 if (snapshotEventsAreInserts) this.snapshotReader.generateInsertEvents();
                 this.currentReader = this.snapshotReader;
             } else {
+                if (!rowBinlogEnabled) {
+                    throw new ConnectException(
+                            ""The MySQL server does not appear to be using a row-level binlog, which is required for this connector to work properly. Enable this mode and restart the connector."");
+                }
                 // Just starting to read the binlog ...
                 this.currentReader = this.binlogReader;
             }
 
             // And start our first reader ...
             this.currentReader.start();
+        } catch (RuntimeException e) {
+            this.taskContext.shutdown();
+            throw e;
         } finally {
             prevLoggingContext.restore();
         }
@@ -198,6 +229,10 @@ protected void transitionToReadBinlog() {
         this.currentReader = this.binlogReader;
     }
 
+    protected void skipReadBinlog() {
+        logger.info(""Connector configured to only perform snapshot, and snapshot completed successfully. Connector will terminate."");
+    }
+
     /**
      * Determine whether the binlog position as set on the {@link MySqlTaskContext#source() SourceInfo} is available in the
      * server.
@@ -251,6 +286,29 @@ protected boolean isBinlogAvailable() {
         return found;
     }
 
+    /**
+     * Determine the earliest binlog filename that is still available in the server.
+     * 
+     * @return the name of the earliest binlog filename, or null if there are none.
+     */
+    protected String earliestBinlogFilename() {
+        // Accumulate the available binlog filenames ...
+        List<String> logNames = new ArrayList<>();
+        try {
+            logger.info(""Checking all known binlogs from MySQL"");
+            taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
+                while (rs.next()) {
+                    logNames.add(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking for binary logs: "", e);
+        }
+
+        if (logNames.isEmpty()) return null;
+        return logNames.get(0);
+    }
+
     /**
      * Determine whether the MySQL server has GTIDs enabled.
      * 
@@ -290,4 +348,25 @@ protected String knownGtidSet() {
 
         return gtidSetStr.get();
     }
+
+    /**
+     * Determine whether the MySQL server has the row-level binlog enabled.
+     * 
+     * @return {@code true} if the server's {@code binlog_format} is set to {@code ROW}, or {@code false} otherwise
+     */
+    protected boolean isRowBinlogEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>("""");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'binlog_format'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(2));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at BINLOG mode: "", e);
+        }
+
+        logger.info(""binlog_format={}"" + mode.get());
+        return ""ROW"".equalsIgnoreCase(mode.get());
+    }
 }",2016-09-22T22:09:11Z,10
"@@ -150,6 +150,10 @@ public boolean isSnapshotNeverAllowed() {
         return snapshotMode() == SnapshotMode.NEVER;
     }
 
+    public boolean isInitialSnapshotOnly() {
+        return snapshotMode() == SnapshotMode.INITIAL_ONLY;
+    }
+
     protected SnapshotMode snapshotMode() {
         String value = config.getString(MySqlConnectorConfig.SNAPSHOT_MODE);
         return SnapshotMode.parse(value, MySqlConnectorConfig.SNAPSHOT_MODE.defaultValueAsString());",2016-09-22T22:09:11Z,87
"@@ -20,6 +20,7 @@
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
 
+import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
 
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
@@ -130,8 +131,10 @@ protected void doCleanup() {
         try {
             // Call the completion function to say that we've successfully completed
             if (onSuccessfulCompletion != null) onSuccessfulCompletion.run();
+        } catch (RuntimeException e) {
+            throw e;
         } catch (Throwable e) {
-            logger.error(""Error calling completion function after completing snapshot"", e);
+            throw new ConnectException(""Error calling completion function after completing snapshot"", e);
         }
     }
 ",2016-09-22T22:09:11Z,15
"@@ -262,7 +262,7 @@ public void setGtid(String gtid) {
      */
     public void setGtidSet(String gtidSet) {
         if (gtidSet != null && !gtidSet.trim().isEmpty()) {
-            this.gtidSet = gtidSet;
+            this.gtidSet = gtidSet.replaceAll(""\n"", """"); // remove all of the newline chars if they exist
         }
     }
 ",2016-08-23T22:47:19Z,11
"@@ -19,6 +19,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.confluent.connect.avro.AvroData;
+import io.debezium.doc.FixFor;
 import io.debezium.document.Document;
 
 public class SourceInfoTest {
@@ -400,6 +401,33 @@ public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
         assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
     }
 
+    @FixFor(""DBZ-107"")
+    @Test
+    public void shouldRemoveNewlinesFromGtidSet() {
+        String gtidExecuted = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,\n"" +
+                ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3149,\n"" +
+                ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:1-39"";
+        String gtidCleaned = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,"" +
+                ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3149,"" +
+                ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:1-39"";
+        source.setGtidSet(gtidExecuted);
+        assertThat(source.gtidSet()).isEqualTo(gtidCleaned);
+    }
+
+    @FixFor(""DBZ-107"")
+    @Test
+    public void shouldNotSetBlankGtidSet() {
+        source.setGtidSet("""");
+        assertThat(source.gtidSet()).isNull();
+    }
+
+    @FixFor(""DBZ-107"")
+    @Test
+    public void shouldNotSetNullGtidSet() {
+        source.setGtidSet(null);
+        assertThat(source.gtidSet()).isNull();
+    }
+
     protected Document positionWithGtids(String gtids) {
         return positionWithGtids(gtids, false);
     }",2016-08-23T22:47:19Z,11
"@@ -53,3 +53,9 @@ This will use the `mysql:5.7` image to start a new container named `database` wh
 The second volume mount, namely `-v src/test/docker/init:/docker-entrypoint-initdb.d`, makes available all of our existing scripts inside the `src/test/docker/init` directory so that they are run upon server initialization.
 
 The command also defines the same `mysql` database and uses the same username and password(s) as our integration test MySQL container.
+
+### Use MySQL client
+
+The following command can be used to manually start up a Docker container to run the MySQL command line client:
+
+    $ docker run -it --link database:mysql --rm mysql:5.7 sh -c 'exec mysql -h""$MYSQL_PORT_3306_TCP_ADDR"" -P""$MYSQL_PORT_3306_TCP_PORT"" -uroot -p""$MYSQL_ENV_MYSQL_ROOT_PASSWORD""'",2016-05-26T20:58:58Z,120
"@@ -0,0 +1,114 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.Set;
+import java.util.function.Predicate;
+
+import io.debezium.annotation.Immutable;
+import io.debezium.config.Configuration;
+import io.debezium.relational.ColumnId;
+import io.debezium.relational.Selectors;
+import io.debezium.relational.TableId;
+import io.debezium.relational.mapping.ColumnMappers;
+import io.debezium.util.Collect;
+
+/**
+ * A utility that is contains various filters for acceptable database names, {@link TableId}s, and columns.
+ * 
+ * @author Randall Hauch
+ */
+@Immutable
+public class Filters {
+
+    protected static final Set<String> BUILT_IN_TABLE_NAMES = Collect.unmodifiableSet(""db"", ""user"", ""func"", ""plugin"", ""tables_priv"",
+                                                                                      ""columns_priv"", ""help_topic"", ""help_category"",
+                                                                                      ""help_relation"", ""help_keyword"",
+                                                                                      ""time_zone_name"", ""time_zone"", ""time_zone_transition"",
+                                                                                      ""time_zone_transition_type"", ""time_zone_leap_second"",
+                                                                                      ""proc"", ""procs_priv"", ""general_log"", ""event"",
+                                                                                      ""ndb_binlog_index"",
+                                                                                      ""innodb_table_stats"", ""innodb_index_stats"",
+                                                                                      ""slave_relay_log_info"", ""slave_master_info"",
+                                                                                      ""slave_worker_info"", ""gtid_executed"",
+                                                                                      ""server_cost"", ""engine_cost"");
+    protected static final Set<String> BUILT_IN_DB_NAMES = Collect.unmodifiableSet(""mysql"", ""performance_schema"");
+
+    private final Predicate<String> dbFilter;
+    private final Predicate<TableId> tableFilter;
+    private final Predicate<String> isBuiltInDb;
+    private final Predicate<TableId> isBuiltInTable;
+    private final Predicate<ColumnId> columnFilter;
+    private final ColumnMappers columnMappers;
+
+    /**
+     * @param config the configuration; may not be null
+     */
+    public Filters(Configuration config) {
+        this.isBuiltInDb = (dbName) -> {
+            return BUILT_IN_DB_NAMES.contains(dbName.toLowerCase());
+        };
+        this.isBuiltInTable = (id) -> {
+            return isBuiltInDb.test(id.catalog()) || BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase());
+        };
+
+        // Define the filter used for database names ...
+        Predicate<String> dbFilter = Selectors.databaseSelector()
+                                              .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
+                                              .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
+                                              .build();
+
+        // Define the filter using the whitelists and blacklists for tables and database names ...
+        Predicate<TableId> tableFilter = Selectors.tableSelector()
+                                                  .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
+                                                  .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
+                                                  .includeTables(config.getString(MySqlConnectorConfig.TABLE_WHITELIST))
+                                                  .excludeTables(config.getString(MySqlConnectorConfig.TABLE_BLACKLIST))
+                                                  .build();
+
+        // Ignore built-in databases and tables ...
+        if (config.getBoolean(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN)) {
+            this.tableFilter = tableFilter.and(isBuiltInTable.negate());
+            this.dbFilter = dbFilter.and(isBuiltInDb.negate());
+        } else {
+            this.tableFilter = tableFilter.or(isBuiltInTable);
+            this.dbFilter = dbFilter.or(isBuiltInDb);
+        }
+
+        // Define the filter that excludes blacklisted columns, truncated columns, and masked columns ...
+        this.columnFilter = Selectors.excludeColumns(config.getString(MySqlConnectorConfig.COLUMN_BLACKLIST));
+
+        // Define the truncated, masked, and mapped columns ...
+        ColumnMappers.Builder columnMapperBuilder = ColumnMappers.create();
+        config.forEachMatchingFieldNameWithInteger(""column\\.truncate\\.to\\.(\\d+)\\.chars"", columnMapperBuilder::truncateStrings);
+        config.forEachMatchingFieldNameWithInteger(""column\\.mask\\.with\\.(\\d+)\\.chars"", columnMapperBuilder::maskStrings);
+        this.columnMappers = columnMapperBuilder.build();
+    }
+
+    public Predicate<String> databaseFilter() {
+        return dbFilter;
+    }
+
+    public Predicate<TableId> tableFilter() {
+        return tableFilter;
+    }
+
+    public Predicate<TableId> builtInTableFilter() {
+        return isBuiltInTable;
+    }
+
+    public Predicate<String> builtInDatabaseFilter() {
+        return isBuiltInDb;
+    }
+
+    public Predicate<ColumnId> columnFilter() {
+        return columnFilter;
+    }
+
+    public ColumnMappers columnMappers() {
+        return columnMappers;
+    }
+}",2016-05-26T20:58:58Z,17
"@@ -12,14 +12,12 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Queue;
-import java.util.Set;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.LinkedBlockingDeque;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Consumer;
-import java.util.function.Predicate;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -40,15 +38,7 @@
 
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
-import io.debezium.relational.ColumnId;
-import io.debezium.relational.Selectors;
-import io.debezium.relational.TableId;
-import io.debezium.relational.Tables;
-import io.debezium.relational.ddl.DdlParser;
-import io.debezium.relational.history.DatabaseHistory;
-import io.debezium.relational.mapping.ColumnMappers;
 import io.debezium.util.Clock;
-import io.debezium.util.Collect;
 import io.debezium.util.Metronome;
 
 /**
@@ -60,35 +50,21 @@
 @NotThreadSafe
 public final class MySqlConnectorTask extends SourceTask {
 
-    private final Set<String> BUILT_IN_TABLE_NAMES = Collect.unmodifiableSet(""db"", ""user"", ""func"", ""plugin"", ""tables_priv"",
-                                                                             ""columns_priv"", ""help_topic"", ""help_category"",
-                                                                             ""help_relation"", ""help_keyword"",
-                                                                             ""time_zone_name"", ""time_zone"", ""time_zone_transition"",
-                                                                             ""time_zone_transition_type"", ""time_zone_leap_second"",
-                                                                             ""proc"", ""procs_priv"", ""general_log"", ""event"",
-                                                                             ""ndb_binlog_index"",
-                                                                             ""innodb_table_stats"", ""innodb_index_stats"",
-                                                                             ""slave_relay_log_info"", ""slave_master_info"",
-                                                                             ""slave_worker_info"", ""gtid_executed"",
-                                                                             ""server_cost"", ""engine_cost"");
-    private final Set<String> BUILT_IN_DB_NAMES = Collect.unmodifiableSet(""mysql"", ""performance_schema"");
-
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final TopicSelector topicSelector;
+    private final Clock clock = Clock.system();
+    private final AtomicBoolean running = new AtomicBoolean(false);
 
     // These are all effectively constants between start(...) and stop(...)
-    private DatabaseHistory dbHistory;
-    private EnumMap<EventType, EventHandler> eventHandlers = new EnumMap<>(EventType.class);
-    private Tables tables;
-    private TableConverters tableConverters;
-    private BinaryLogClient client;
+    private MySqlSchema dbSchema;
+    private String serverName;
+    private int maxBatchSize;
     private BlockingQueue<Event> events;
     private Queue<Event> batchEvents;
-    private int maxBatchSize;
-    private String serverName;
+    private EnumMap<EventType, EventHandler> eventHandlers = new EnumMap<>(EventType.class);
+    private TableConverters tableConverters;
     private Metronome metronome;
-    private final Clock clock = Clock.system();
-    private final AtomicBoolean running = new AtomicBoolean(false);
+    private BinaryLogClient client;
 
     // Used in the methods that process events ...
     private final SourceInfo source = new SourceInfo();
@@ -100,7 +76,7 @@ public final class MySqlConnectorTask extends SourceTask {
      */
     public MySqlConnectorTask() {
         this.topicSelector = TopicSelector.defaultSelector();
-        this.dbHistory = null; // delay creating the history until startup, which is only allowed by default constructor
+        this.dbSchema = null; // delay creating the history until startup, which is only allowed by default constructor
     }
 
     @Override
@@ -121,15 +97,8 @@ public void start(Map<String, String> props) {
         }
 
         // Create and configure the database history ...
-        this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
-        if (this.dbHistory == null) {
-            throw new ConnectException(""Unable to instantiate the database history class "" +
-                    config.getString(MySqlConnectorConfig.DATABASE_HISTORY));
-        }
-        Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false); // do not remove
-                                                                                                                 // prefix
-        this.dbHistory.configure(dbHistoryConfig); // validates
-        this.dbHistory.start();
+        this.dbSchema = new MySqlSchema(config);
+        this.dbSchema.start();
         this.running.set(true);
 
         // Read the configuration ...
@@ -148,43 +117,12 @@ public void start(Map<String, String> props) {
         maxBatchSize = config.getInteger(MySqlConnectorConfig.MAX_BATCH_SIZE);
         metronome = Metronome.parker(pollIntervalMs, TimeUnit.MILLISECONDS, Clock.SYSTEM);
 
-        // Define the filter used for database names ...
-        Predicate<String> dbFilter = Selectors.databaseSelector()
-                                              .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
-                                              .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
-                                              .build();
-
-        // Define the filter using the whitelists and blacklists for tables and database names ...
-        Predicate<TableId> tableFilter = Selectors.tableSelector()
-                                                  .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
-                                                  .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
-                                                  .includeTables(config.getString(MySqlConnectorConfig.TABLE_WHITELIST))
-                                                  .excludeTables(config.getString(MySqlConnectorConfig.TABLE_BLACKLIST))
-                                                  .build();
-        if (config.getBoolean(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN)) {
-            Predicate<TableId> isBuiltin = (id) -> {
-                return BUILT_IN_DB_NAMES.contains(id.catalog().toLowerCase()) || BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase());
-            };
-            tableFilter = tableFilter.and(isBuiltin.negate());
-        }
-
-        // Define the filter that excludes blacklisted columns, truncated columns, and masked columns ...
-        Predicate<ColumnId> columnFilter = Selectors.excludeColumns(config.getString(MySqlConnectorConfig.COLUMN_BLACKLIST));
-
-        // Define the truncated, masked, and mapped columns ...
-        ColumnMappers.Builder columnMapperBuilder = ColumnMappers.create();
-        config.forEachMatchingFieldNameWithInteger(""column\\.truncate\\.to\\.(\\d+)\\.chars"", columnMapperBuilder::truncateStrings);
-        config.forEachMatchingFieldNameWithInteger(""column\\.mask\\.with\\.(\\d+)\\.chars"", columnMapperBuilder::maskStrings);
-        ColumnMappers columnMappers = columnMapperBuilder.build();
-
         // Create the queue ...
         events = new LinkedBlockingDeque<>(maxQueueSize);
         batchEvents = new ArrayDeque<>(maxBatchSize);
 
         // Set up our handlers for specific kinds of events ...
-        tables = new Tables();
-        tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, clock,
-                                              dbFilter, tables, tableFilter, columnFilter, columnMappers);
+        tableConverters = new TableConverters(topicSelector, dbSchema, clock, includeSchemaChanges);
         eventHandlers.put(EventType.ROTATE, tableConverters::rotateLogs);
         eventHandlers.put(EventType.TABLE_MAP, tableConverters::updateTableMetadata);
         eventHandlers.put(EventType.QUERY, tableConverters::updateTableCommand);
@@ -207,6 +145,13 @@ public void start(Map<String, String> props) {
 
         // Check if we've already processed some of the log for this database ...
         source.setServerName(serverName);
+        
+        // We use the initial binlog filename configuration property to know whether to perform a snapshot
+        // or to start with that (or the previous) binlog position...
+        if ( initialBinLogFilename == null || initialBinLogFilename.trim().isEmpty() ) {
+            // No initial binlog filename was specified, so perform a snapshot ...
+        }
+        
         // Get the offsets for our partition ...
         Map<String, ?> offsets = context.offsetStorageReader().offset(source.partition());
         if (offsets != null) {
@@ -224,11 +169,9 @@ public void start(Map<String, String> props) {
             // to our Tables object. Each of those DDL messages is keyed by the database name, and contains a single string
             // of DDL. However, we should consume no further than offset we recovered above.
             try {
-                logger.info(""Recovering MySQL connector '{}' database schemas from history stored in {}"", serverName, dbHistory);
-                DdlParser ddlParser = new MySqlDdlParser();
-                dbHistory.recover(source.partition(), source.offset(), tables, ddlParser);
-                tableConverters.loadTables();
-                logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, tables.subset(tableFilter));
+                logger.info(""Recovering MySQL connector '{}' database schemas from history stored in {}"", serverName, dbSchema.historyLocation());
+                dbSchema.loadHistory(source);
+                logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, dbSchema.tables());
             } catch (Throwable t) {
                 throw new ConnectException(""Failure while recovering database schemas"", t);
             }
@@ -327,7 +270,7 @@ public void stop() {
 
             // Flush and stop the database history ...
             logger.debug(""Stopping database history for MySQL server '{}'"", serverName);
-            dbHistory.stop();
+            dbSchema.shutdown();
         } catch (Throwable e) {
             logger.error(""Unexpected error shutting down the database history"", e);
         } finally {",2016-05-26T20:58:58Z,10
"@@ -0,0 +1,250 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.errors.ConnectException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import io.debezium.annotation.NotThreadSafe;
+import io.debezium.config.Configuration;
+import io.debezium.relational.Table;
+import io.debezium.relational.TableId;
+import io.debezium.relational.TableSchema;
+import io.debezium.relational.TableSchemaBuilder;
+import io.debezium.relational.Tables;
+import io.debezium.relational.ddl.DdlChanges;
+import io.debezium.relational.ddl.DdlChanges.DatabaseStatementStringConsumer;
+import io.debezium.relational.history.DatabaseHistory;
+import io.debezium.text.ParsingException;
+import io.debezium.util.Collect;
+
+/**
+ * Component that records the schema history for databases hosted by a MySQL database server. The schema information includes
+ * the {@link Tables table definitions} and the Kafka Connect {@link #schemaFor(TableId) Schema}s for each table, where the
+ * {@link Schema} excludes any columns that have been {@link MySqlConnectorConfig#COLUMN_BLACKLIST specified} in the
+ * configuration.
+ * <p>
+ * The history is changed by {@link #applyDdl(SourceInfo, String, String, DatabaseStatementStringConsumer) applying DDL
+ * statements}, and every change is {@link DatabaseHistory persisted} as defined in the supplied {@link MySqlConnectorConfig MySQL
+ * connector configuration}. This component can be reconstructed (e.g., on connector restart) and the history
+ * {@link #loadHistory(SourceInfo) loaded} from persisted storage.
+ * <p>
+ * Note that when {@link #applyDdl(SourceInfo, String, String, DatabaseStatementStringConsumer) applying DDL statements}, the
+ * caller is able to supply a {@link DatabaseStatementStringConsumer consumer function} that will be called with the DDL
+ * statements and the database to which they apply, grouped by database names. However, these will only be called based when the
+ * databases are included by the database filters defined in the {@link MySqlConnectorConfig MySQL connector configuration}.
+ * 
+ * @author Randall Hauch
+ */
+@NotThreadSafe
+public class MySqlSchema {
+
+    private final Logger logger = LoggerFactory.getLogger(getClass());
+    private final Set<String> ignoredQueryStatements = Collect.unmodifiableSet(""BEGIN"", ""END"", ""FLUSH PRIVILEGES"");
+    private final MySqlDdlParser ddlParser;
+    private final Map<TableId, TableSchema> tableSchemaByTableId = new HashMap<>();
+    private final Filters filters;
+    private final DatabaseHistory dbHistory;
+    private final TableSchemaBuilder schemaBuilder;
+    private final DdlChanges ddlChanges;
+    private Tables tables;
+
+    /**
+     * Create a schema component given the supplied {@link MySqlConnectorConfig MySQL connector configuration}.
+     * 
+     * @param config the connector configuration, which is presumed to be valid
+     */
+    public MySqlSchema(Configuration config) {
+        this.filters = new Filters(config);
+        this.ddlParser = new MySqlDdlParser(false);
+        this.tables = new Tables();
+        this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
+        this.ddlParser.addListener(ddlChanges);
+        this.schemaBuilder = new TableSchemaBuilder();
+
+        // Create and configure the database history ...
+        this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
+        if (this.dbHistory == null) {
+            throw new ConnectException(""Unable to instantiate the database history class "" +
+                    config.getString(MySqlConnectorConfig.DATABASE_HISTORY));
+        }
+        // Do not remove the prefix from the subset of config properties ...
+        Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false);
+        this.dbHistory.configure(dbHistoryConfig); // validates
+    }
+
+    /**
+     * Start by acquiring resources needed to persist the database history
+     */
+    public void start() {
+        this.dbHistory.start();
+    }
+
+    /**
+     * Stop recording history and release any resources acquired since {@link #start()}.
+     */
+    public void shutdown() {
+        this.dbHistory.stop();
+    }
+
+    /**
+     * Get the {@link Filters database and table filters} defined by the configuration.
+     * 
+     * @return the filters; never null
+     */
+    public Filters filters() {
+        return filters;
+    }
+
+    /**
+     * Get all of the table definitions for all database tables as defined by
+     * {@link #applyDdl(SourceInfo, String, String, DatabaseStatementStringConsumer) applied DDL statements}, including those
+     * that have been excluded by the {@link #filters() filters}.
+     * 
+     * @return the table definitions; never null
+     */
+    public Tables tables() {
+        return tables.subset(filters.tableFilter());
+    }
+
+    /**
+     * Get the {@link TableSchema Schema information} for the table with the given identifier, if that table exists and is
+     * included by the {@link #filters() filter}.
+     * 
+     * @param id the fully-qualified table identifier; may be null
+     * @return the current table definition, or null if there is no table with the given identifier, if the identifier is null,
+     *         or if the table has been excluded by the filters
+     */
+    public Table tableFor(TableId id) {
+        return filters.tableFilter().test(id) ? tables.forTable(id) : null;
+    }
+
+    /**
+     * Get the {@link TableSchema Schema information} for the table with the given identifier, if that table exists and is
+     * included by the {@link #filters() filter}.
+     * <p>
+     * Note that the {@link Schema} will not contain any columns that have been {@link MySqlConnectorConfig#COLUMN_BLACKLIST
+     * filtered out}.
+     * 
+     * @param id the fully-qualified table identifier; may be null
+     * @return the schema information, or null if there is no table with the given identifier, if the identifier is null,
+     *         or if the table has been excluded by the filters
+     */
+    public TableSchema schemaFor(TableId id) {
+        return filters.tableFilter().test(id) ? tableSchemaByTableId.get(id) : null;
+    }
+
+    /**
+     * Get the information about where the DDL statement history is recorded.
+     * 
+     * @return the history description; never null
+     */
+    public String historyLocation() {
+        return dbHistory.toString();
+    }
+
+    /**
+     * Load the database schema information using the previously-recorded history, and stop reading the history when the
+     * the history reaches the supplied starting point.
+     * 
+     * @param startingPoint the source information with the current {@link SourceInfo#partition()} and {@link SourceInfo#offset()
+     *            offset} at which the database schemas are to reflect; may not be null
+     */
+    public void loadHistory(SourceInfo startingPoint) {
+        tables = new Tables();
+        dbHistory.recover(startingPoint.partition(), startingPoint.offset(), tables, ddlParser);
+        refreshSchemas();
+    }
+
+    /**
+     * Discard any currently-cached schemas and rebuild them using the filters.
+     */
+    protected void refreshSchemas() {
+        tableSchemaByTableId.clear();
+        // Create TableSchema instances for any existing table ...
+        this.tables.tableIds().forEach(id -> {
+            Table table = this.tables.forTable(id);
+            TableSchema schema = schemaBuilder.create(table, filters.columnFilter(), filters.columnMappers());
+            tableSchemaByTableId.put(id, schema);
+        });
+    }
+
+    /**
+     * Apply the supplied DDL statements to this database schema and record the history. If a {@code statementConsumer} is
+     * supplied, then call it for each sub-sequence of the DDL statements that all apply to the same database.
+     * <p>
+     * Typically DDL statements are applied using a connection to a single database, and unless the statements use fully-qualified
+     * names, the DDL statements apply to this database.
+     * 
+     * @param source the current {@link SourceInfo#partition()} and {@link SourceInfo#offset() offset} at which these changes are
+     *            found; may not be null
+     * @param databaseName the name of the default database under which these statements are applied; may not be null
+     * @param ddlStatements the {@code ;}-separated DDL statements; may be null or empty
+     * @param statementConsumer the consumer that should be called with each sub-sequence of DDL statements that apply to
+     *            a single database; may be null if no action is to be performed with the changes
+     * @return {@code true} if changes were made to the database schema, or {@code false} if the DDL statements had no
+     *         effect on the database schema
+     */
+    public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatements,
+                            DatabaseStatementStringConsumer statementConsumer) {
+        if (ignoredQueryStatements.contains(ddlStatements)) return false;
+        try {
+            this.ddlChanges.reset();
+            this.ddlParser.setCurrentSchema(databaseName);
+            this.ddlParser.parse(ddlStatements, tables);
+        } catch (ParsingException e) {
+            logger.error(""Error parsing DDL statement and updating tables: {}"", ddlStatements, e);
+        } finally {
+            if (statementConsumer != null) {
+
+                // We are supposed to _also_ record the schema changes as SourceRecords, but these need to be filtered
+                // by database. Unfortunately, the databaseName on the event might not be the same database as that
+                // being modified by the DDL statements (since the DDL statements can have fully-qualified names).
+                // Therefore, we have to look at each statement to figure out which database it applies and then
+                // record the DDL statements (still in the same order) to those databases.
+
+                if (!ddlChanges.isEmpty() && ddlChanges.applyToMoreDatabasesThan(databaseName)) {
+
+                    // We understood at least some of the DDL statements and can figure out to which database they apply.
+                    // They also apply to more databases than 'databaseName', so we need to apply the DDL statements in
+                    // the same order they were read for each _affected_ database, grouped together if multiple apply
+                    // to the same _affected_ database...
+                    ddlChanges.groupStatementStringsByDatabase((dbName, ddl) -> {
+                        if (filters.databaseFilter().test(dbName)) {
+                            statementConsumer.consume(databaseName, ddlStatements);
+                        }
+                    });
+                } else if (filters.databaseFilter().test(databaseName)) {
+                    statementConsumer.consume(databaseName, ddlStatements);
+                }
+            }
+
+            // Record the DDL statement so that we can later recover them if needed. We do this _after_ writing the
+            // schema change records so that failure recovery (which is based on of the history) won't lose
+            // schema change records.
+            dbHistory.record(source.partition(), source.offset(), databaseName, tables, ddlStatements);
+        }
+
+        // Figure out what changed ...
+        Set<TableId> changes = tables.drainChanges();
+        changes.forEach(tableId -> {
+            Table table = tables.forTable(tableId);
+            if (table == null) { // removed
+                tableSchemaByTableId.remove(tableId);
+            } else {
+                TableSchema schema = schemaBuilder.create(table, filters.columnFilter(), filters.columnMappers());
+                tableSchemaByTableId.put(tableId, schema);
+            }
+        });
+        return true;
+    }
+}",2016-05-26T20:58:58Z,16
"@@ -8,14 +8,11 @@
 import java.io.Serializable;
 import java.util.BitSet;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Objects;
-import java.util.Set;
 import java.util.function.Consumer;
-import java.util.function.Predicate;
 
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaBuilder;
@@ -34,19 +31,10 @@
 
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.data.Envelope;
-import io.debezium.relational.ColumnId;
-import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.relational.TableSchema;
-import io.debezium.relational.TableSchemaBuilder;
-import io.debezium.relational.Tables;
-import io.debezium.relational.ddl.DdlChanges;
-import io.debezium.relational.history.DatabaseHistory;
 import io.debezium.relational.history.HistoryRecord.Fields;
-import io.debezium.relational.mapping.ColumnMappers;
-import io.debezium.text.ParsingException;
 import io.debezium.util.Clock;
-import io.debezium.util.Collect;
 
 /**
  * @author Randall Hauch
@@ -82,54 +70,22 @@ public Struct schemaChangeRecordValue(SourceInfo source, String databaseName, St
     }
 
     private final Logger logger = LoggerFactory.getLogger(getClass());
-    private final DatabaseHistory dbHistory;
+    private final MySqlSchema dbSchema;
     private final TopicSelector topicSelector;
-    private final MySqlDdlParser ddlParser;
-    private final DdlChanges ddlChanges;
-    private final Tables tables;
-    private final TableSchemaBuilder schemaBuilder = new TableSchemaBuilder();
-    private final Map<TableId, TableSchema> tableSchemaByTableId = new HashMap<>();
     private final Map<Long, Converter> convertersByTableId = new HashMap<>();
     private final Map<String, Long> tableNumbersByTableName = new HashMap<>();
     private final boolean recordSchemaChangesInSourceRecords;
-    private final Predicate<String> dbFilter;
-    private final Predicate<TableId> tableFilter;
-    private final Predicate<ColumnId> columnFilter;
-    private final ColumnMappers columnMappers;
-    private final Set<String> ignoredQueryStatements = Collect.unmodifiableSet(""BEGIN"", ""END"", ""FLUSH PRIVILEGES"");
-    private final Set<TableId> unknownTableIds = new HashSet<>();
     private final Clock clock;
 
-    public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
-            boolean recordSchemaChangesInSourceRecords, Clock clock, Predicate<String> dbFilter, Tables tables,
-            Predicate<TableId> tableFilter, Predicate<ColumnId> columnFilter, ColumnMappers columnSelectors) {
+    public TableConverters(TopicSelector topicSelector, MySqlSchema dbSchema, Clock clock,
+            boolean recordSchemaChangesInSourceRecords) {
         Objects.requireNonNull(topicSelector, ""A topic selector is required"");
-        Objects.requireNonNull(dbHistory, ""Database history storage is required"");
-        Objects.requireNonNull(tables, ""A Tables object is required"");
         Objects.requireNonNull(clock, ""A Clock object is required"");
-        Objects.requireNonNull(dbFilter, ""A database filter object is required"");
+        Objects.requireNonNull(dbSchema, ""A DatabaseSchema object is required"");
         this.topicSelector = topicSelector;
-        this.dbHistory = dbHistory;
+        this.dbSchema = dbSchema;
         this.clock = clock;
-        this.dbFilter = dbFilter;
-        this.tables = tables;
-        this.columnFilter = columnFilter;
-        this.columnMappers = columnSelectors;
-        this.ddlParser = new MySqlDdlParser(false); // don't include views
-        this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
-        this.ddlParser.addListener(ddlChanges);
         this.recordSchemaChangesInSourceRecords = recordSchemaChangesInSourceRecords;
-        Predicate<TableId> knownTables = (id) -> !unknownTableIds.contains(id); // known if not unknown
-        this.tableFilter = tableFilter != null ? tableFilter.and(knownTables) : knownTables;
-    }
-
-    public void loadTables() {
-        // Create TableSchema instances for any existing table ...
-        this.tables.tableIds().forEach(id -> {
-            Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(table, columnFilter, columnMappers);
-            tableSchemaByTableId.put(id, schema);
-        });
     }
 
     public void rotateLogs(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
@@ -147,76 +103,19 @@ public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRe
         QueryEventData command = event.getData();
         // The command's database is the one that the client was using when submitting the DDL statements,
         // and that might not be the database(s) affected by the DDL statements ...
-        String databaseName = command.getDatabase();
-        String ddlStatements = command.getSql();
-        if (ignoredQueryStatements.contains(ddlStatements)) return;
         logger.debug(""Received update table command: {}"", event);
-        try {
-            this.ddlChanges.reset();
-            this.ddlParser.setCurrentSchema(databaseName);
-            this.ddlParser.parse(ddlStatements, tables);
-        } catch (ParsingException e) {
-            logger.error(""Error parsing DDL statement and updating tables: {}"", ddlStatements, e);
-        } finally {
+        dbSchema.applyDdl(source, command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords) {
-
-                // We are supposed to _also_ record the schema changes as SourceRecords, but these need to be filtered
-                // by database. Unfortunately, the databaseName on the event might not be the same database as that
-                // being modified by the DDL statements (since the DDL statements can have fully-qualified names).
-                // Therefore, we have to look at each statement to figure out which database it applies and then
-                // record the DDL statements (still in the same order) to those databases.
-                
-                if ( !ddlChanges.isEmpty() && ddlChanges.applyToMoreDatabasesThan(databaseName) ) {
-                    
-                    // We understood at least some of the DDL statements and can figure out to which database they apply.
-                    // They also apply to more databases than 'databaseName', so we need to apply the DDL statements in
-                    // the same order they were read for each _affected_ database, grouped together if multiple apply
-                    // to the same _affected_ database...
-                    ddlChanges.groupStatementStringsByDatabase((dbName, statements) -> {
-                        if (dbFilter.test(dbName)) {
-                            String serverName = source.serverName();
-                            String topicName = topicSelector.getTopic(serverName);
-                            Integer partition = 0;
-                            Struct key = schemaChangeRecordKey(databaseName);
-                            Struct value = schemaChangeRecordValue(source, dbName, statements);
-                            SourceRecord record = new SourceRecord(source.partition(), source.offset(),
-                                    topicName, partition,
-                                    SCHEMA_CHANGE_RECORD_KEY_SCHEMA, key,
-                                    SCHEMA_CHANGE_RECORD_VALUE_SCHEMA, value);
-                            recorder.accept(record);
-                        }
-                    });
-                } else if (dbFilter.test(databaseName)) {
-                    // Either all of the statements applied to 'databaseName', or we didn't understand any of the statements.
-                    // But the database filter includes 'databaseName' so we should forward all of the statements ...
-                    String serverName = source.serverName();
-                    String topicName = topicSelector.getTopic(serverName);
-                    Integer partition = 0;
-                    Struct key = schemaChangeRecordKey(databaseName);
-                    Struct value = schemaChangeRecordValue(source, databaseName, ddlStatements);
-                    SourceRecord record = new SourceRecord(source.partition(), source.offset(),
-                            topicName, partition,
-                            SCHEMA_CHANGE_RECORD_KEY_SCHEMA, key,
-                            SCHEMA_CHANGE_RECORD_VALUE_SCHEMA, value);
-                    recorder.accept(record);
-                }
-            }
-
-            // Record the DDL statement so that we can later recover them if needed. We do this _after_ writing the
-            // schema change records so that failure recovery (which is based on of the history) won't lose
-            // schema change records.
-            dbHistory.record(source.partition(), source.offset(), databaseName, tables, ddlStatements);
-        }
-
-        // Figure out what changed ...
-        Set<TableId> changes = tables.drainChanges();
-        changes.forEach(tableId -> {
-            Table table = tables.forTable(tableId);
-            if (table == null) { // removed
-                tableSchemaByTableId.remove(tableId);
-            } else {
-                TableSchema schema = schemaBuilder.create(table, columnFilter, columnMappers);
-                tableSchemaByTableId.put(tableId, schema);
+                String serverName = source.serverName();
+                String topicName = topicSelector.getTopic(serverName);
+                Integer partition = 0;
+                Struct key = schemaChangeRecordKey(dbName);
+                Struct value = schemaChangeRecordValue(source, dbName, statements);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(),
+                        topicName, partition,
+                        SCHEMA_CHANGE_RECORD_KEY_SCHEMA, key,
+                        SCHEMA_CHANGE_RECORD_VALUE_SCHEMA, value);
+                recorder.accept(record);
             }
         });
     }
@@ -250,15 +149,12 @@ public void updateTableMetadata(Event event, SourceInfo source, Consumer<SourceR
 
             // Just get the current schema, which should be up-to-date ...
             TableId tableId = new TableId(databaseName, null, tableName);
-            TableSchema tableSchema = tableSchemaByTableId.get(tableId);
+            TableSchema tableSchema = dbSchema.schemaFor(tableId);
             logger.debug(""Registering metadata for table {} with table #{}"", tableId, tableNumber);
             if (tableSchema == null) {
-                // We are seeing an event for a row that's in a table we don't know about, meaning the table
-                // was created before the binlog was enabled (or before the point we started reading it).
-                if (unknownTableIds.add(tableId)) {
-                    logger.warn(""Transaction affects rows in {}, for which no metadata exists. All subsequent changes to rows in this table will be ignored."",
-                                tableId);
-                }
+                // We are seeing an event for a row that's in a table we don't know about or that has been filtered out ...
+                logger.debug(""Skipping update table metadata event: {}"", event);
+                return;
             }
             // Specify the envelope structure for this table's messages ...
             Envelope envelope = Envelope.defineSchema()
@@ -328,32 +224,28 @@ public void handleInsert(Event event, SourceInfo source, Consumer<SourceRecord>
         Converter converter = convertersByTableId.get(tableNumber);
         if (converter != null) {
             TableId tableId = converter.tableId();
-            if (tableFilter.test(tableId)) {
-                logger.debug(""Processing insert row event for {}: {}"", tableId, event);
-                String topic = converter.topic();
-                Integer partitionNum = converter.partition();
-                List<Serializable[]> rows = write.getRows();
-                Long ts = clock.currentTimeInMillis();
-                for (int row = 0; row != rows.size(); ++row) {
-                    Serializable[] values = rows.get(row);
-                    Schema keySchema = converter.keySchema();
-                    Object key = converter.createKey(values, includedColumns);
-                    Struct value = converter.createValue(values, includedColumns);
-                    if (value != null || key != null) {
-                        Envelope envelope = converter.envelope();
-                        Map<String, ?> partition = source.partition();
-                        Map<String, ?> offset = source.offset(row);
-                        Struct origin = source.struct();
-                        SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
-                                keySchema, key, envelope.schema(), envelope.create(value, origin, ts));
-                        recorder.accept(record);
-                    }
+            logger.debug(""Processing insert row event for {}: {}"", tableId, event);
+            String topic = converter.topic();
+            Integer partitionNum = converter.partition();
+            List<Serializable[]> rows = write.getRows();
+            Long ts = clock.currentTimeInMillis();
+            for (int row = 0; row != rows.size(); ++row) {
+                Serializable[] values = rows.get(row);
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(values, includedColumns);
+                Struct value = converter.createValue(values, includedColumns);
+                if (value != null || key != null) {
+                    Envelope envelope = converter.envelope();
+                    Map<String, ?> partition = source.partition();
+                    Map<String, ?> offset = source.offset(row);
+                    Struct origin = source.struct();
+                    SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
+                            keySchema, key, envelope.schema(), envelope.create(value, origin, ts));
+                    recorder.accept(record);
                 }
-            } else if (logger.isDebugEnabled()) {
-                logger.debug(""Skipping insert row event: {}"", event);
             }
         } else {
-            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
+            logger.debug(""Skipping insert row event: {}"", event);
         }
     }
 
@@ -372,53 +264,49 @@ public void handleUpdate(Event event, SourceInfo source, Consumer<SourceRecord>
         Converter converter = convertersByTableId.get(tableNumber);
         if (converter != null) {
             TableId tableId = converter.tableId();
-            if (tableFilter.test(tableId)) {
-                logger.debug(""Processing update row event for {}: {}"", tableId, event);
-                String topic = converter.topic();
-                Integer partitionNum = converter.partition();
-                Long ts = clock.currentTimeInMillis();
-                List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
-                for (int row = 0; row != rows.size(); ++row) {
-                    Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
-                    Serializable[] before = changes.getKey();
-                    Serializable[] after = changes.getValue();
-                    Schema keySchema = converter.keySchema();
-                    Object key = converter.createKey(after, includedColumns);
-                    Object oldKey = converter.createKey(before, includedColumns);
-                    Struct valueBefore = converter.createValue(before, includedColumnsBefore);
-                    Struct valueAfter = converter.createValue(after, includedColumns);
-                    if (valueAfter != null || key != null) {
-                        Envelope envelope = converter.envelope();
-                        Map<String, ?> partition = source.partition();
-                        Map<String, ?> offset = source.offset(row);
-                        Struct origin = source.struct();
-                        if (key != null && !Objects.equals(key, oldKey)) {
-                            // The key has indeed changed, so first send a create event ...
-                            SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
-                                    keySchema, key, envelope.schema(), envelope.create(valueAfter, origin, ts));
-                            recorder.accept(record);
-
-                            // then send a delete event for the old key ...
-                            record = new SourceRecord(partition, offset, topic, partitionNum,
-                                    keySchema, oldKey, envelope.schema(), envelope.delete(valueBefore, origin, ts));
-                            recorder.accept(record);
-
-                            // Send a tombstone event for the old key ...
-                            record = new SourceRecord(partition, offset, topic, partitionNum, keySchema, oldKey, null, null);
-                            recorder.accept(record);
-                        } else {
-                            // The key has not changed, so a simple update is fine ...
-                            SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
-                                    keySchema, key, envelope.schema(), envelope.update(valueBefore, valueAfter, origin, ts));
-                            recorder.accept(record);
-                        }
+            logger.debug(""Processing update row event for {}: {}"", tableId, event);
+            String topic = converter.topic();
+            Integer partitionNum = converter.partition();
+            Long ts = clock.currentTimeInMillis();
+            List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
+            for (int row = 0; row != rows.size(); ++row) {
+                Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
+                Serializable[] before = changes.getKey();
+                Serializable[] after = changes.getValue();
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(after, includedColumns);
+                Object oldKey = converter.createKey(before, includedColumns);
+                Struct valueBefore = converter.createValue(before, includedColumnsBefore);
+                Struct valueAfter = converter.createValue(after, includedColumns);
+                if (valueAfter != null || key != null) {
+                    Envelope envelope = converter.envelope();
+                    Map<String, ?> partition = source.partition();
+                    Map<String, ?> offset = source.offset(row);
+                    Struct origin = source.struct();
+                    if (key != null && !Objects.equals(key, oldKey)) {
+                        // The key has indeed changed, so first send a create event ...
+                        SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
+                                keySchema, key, envelope.schema(), envelope.create(valueAfter, origin, ts));
+                        recorder.accept(record);
+
+                        // then send a delete event for the old key ...
+                        record = new SourceRecord(partition, offset, topic, partitionNum,
+                                keySchema, oldKey, envelope.schema(), envelope.delete(valueBefore, origin, ts));
+                        recorder.accept(record);
+
+                        // Send a tombstone event for the old key ...
+                        record = new SourceRecord(partition, offset, topic, partitionNum, keySchema, oldKey, null, null);
+                        recorder.accept(record);
+                    } else {
+                        // The key has not changed, so a simple update is fine ...
+                        SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
+                                keySchema, key, envelope.schema(), envelope.update(valueBefore, valueAfter, origin, ts));
+                        recorder.accept(record);
                     }
                 }
-            } else if (logger.isDebugEnabled()) {
-                logger.debug(""Skipping update row event: {}"", event);
             }
-        } else {
-            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
+        } else if (logger.isDebugEnabled()) {
+            logger.debug(""Skipping update row event: {}"", event);
         }
     }
 
@@ -429,36 +317,32 @@ public void handleDelete(Event event, SourceInfo source, Consumer<SourceRecord>
         Converter converter = convertersByTableId.get(tableNumber);
         if (converter != null) {
             TableId tableId = converter.tableId();
-            if (tableFilter.test(tableId)) {
-                logger.debug(""Processing delete row event for {}: {}"", tableId, event);
-                String topic = converter.topic();
-                Integer partitionNum = converter.partition();
-                Long ts = clock.currentTimeInMillis();
-                List<Serializable[]> rows = deleted.getRows();
-                for (int row = 0; row != rows.size(); ++row) {
-                    Serializable[] values = rows.get(row);
-                    Schema keySchema = converter.keySchema();
-                    Object key = converter.createKey(values, includedColumns);
-                    Struct value = converter.createValue(values, includedColumns);
-                    if (value != null || key != null) {
-                        Envelope envelope = converter.envelope();
-                        Map<String, ?> partition = source.partition();
-                        Map<String, ?> offset = source.offset(row);
-                        Struct origin = source.struct();
-                        SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
-                                keySchema, key, envelope.schema(), envelope.delete(value, origin, ts));
-                        recorder.accept(record);
-                        // And send a tombstone ...
-                        record = new SourceRecord(partition, offset, topic, partitionNum,
-                                keySchema, key, null, null);
-                        recorder.accept(record);
-                    }
+            logger.debug(""Processing delete row event for {}: {}"", tableId, event);
+            String topic = converter.topic();
+            Integer partitionNum = converter.partition();
+            Long ts = clock.currentTimeInMillis();
+            List<Serializable[]> rows = deleted.getRows();
+            for (int row = 0; row != rows.size(); ++row) {
+                Serializable[] values = rows.get(row);
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(values, includedColumns);
+                Struct value = converter.createValue(values, includedColumns);
+                if (value != null || key != null) {
+                    Envelope envelope = converter.envelope();
+                    Map<String, ?> partition = source.partition();
+                    Map<String, ?> offset = source.offset(row);
+                    Struct origin = source.struct();
+                    SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
+                            keySchema, key, envelope.schema(), envelope.delete(value, origin, ts));
+                    recorder.accept(record);
+                    // And send a tombstone ...
+                    record = new SourceRecord(partition, offset, topic, partitionNum,
+                            keySchema, key, null, null);
+                    recorder.accept(record);
                 }
-            } else if (logger.isDebugEnabled()) {
-                logger.debug(""Skipping delete row event: {}"", event);
             }
-        } else {
-            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
+        } else if (logger.isDebugEnabled()) {
+            logger.debug(""Skipping delete row event: {}"", event);
         }
     }
 ",2016-05-26T20:58:58Z,28
"@@ -0,0 +1,83 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.nio.file.Path;
+
+import io.debezium.config.Configuration;
+import io.debezium.config.Field;
+import io.debezium.relational.history.FileDatabaseHistory;
+
+/**
+ * A helper for easily building connector configurations for testing.
+ * 
+ * @author Randall Hauch
+ */
+public class Configurator {
+
+    private Configuration.Builder configBuilder = Configuration.create();
+
+    public Configurator with(Field field, String value) {
+        configBuilder.with(field, value);
+        return this;
+    }
+
+    public Configurator with(Field field, boolean value) {
+        configBuilder.with(field, value);
+        return this;
+    }
+
+    public Configurator includeDatabases(String regexList) {
+        return with(MySqlConnectorConfig.DATABASE_WHITELIST, regexList);
+    }
+
+    public Configurator excludeDatabases(String regexList) {
+        return with(MySqlConnectorConfig.DATABASE_BLACKLIST, regexList);
+    }
+
+    public Configurator includeTables(String regexList) {
+        return with(MySqlConnectorConfig.TABLE_WHITELIST, regexList);
+    }
+
+    public Configurator excludeTables(String regexList) {
+        return with(MySqlConnectorConfig.TABLE_BLACKLIST, regexList);
+    }
+
+    public Configurator excludeColumns(String regexList) {
+        return with(MySqlConnectorConfig.COLUMN_BLACKLIST, regexList);
+    }
+
+    public Configurator truncateColumns(int length, String fullyQualifiedTableNames) {
+        return with(MySqlConnectorConfig.TRUNCATE_COLUMN(length), fullyQualifiedTableNames);
+    }
+
+    public Configurator maskColumns(int length, String fullyQualifiedTableNames) {
+        return with(MySqlConnectorConfig.MASK_COLUMN(length), fullyQualifiedTableNames);
+    }
+
+    public Configurator excludeBuiltInTables() {
+        return with(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN, true);
+    }
+
+    public Configurator includeBuiltInTables() {
+        return with(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN, false);
+    }
+
+    public Configurator storeDatabaseHistoryInFile(Path path) {
+        with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class.getName());
+        with(FileDatabaseHistory.FILE_PATH,path.toAbsolutePath().toString());
+        return this;
+    }
+
+    public Filters createFilters() {
+        return new Filters(configBuilder.build());
+    }
+
+    public MySqlSchema createSchemas() {
+        return new MySqlSchema(configBuilder.build());
+    }
+
+}
\ No newline at end of file",2016-05-26T20:58:58Z,35
"@@ -0,0 +1,306 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.relational.TableId;
+
+/**
+ * @author Randall Hauch
+ */
+public class FiltersTest {
+
+    private Configurator build;
+    private Filters filters;
+
+    @Before
+    public void beforeEach() {
+        build = new Configurator();
+        filters = null;
+    }
+    
+    @Test
+    public void shouldAllowDatabaseListedWithLiteralInWhitelistAndNoDatabaseBlacklist() {
+        filters = build.includeDatabases(""connector_test"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseExcluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowDatabaseListedWithMultipleLiteralsInWhitelistAndNoDatabaseBlacklist() {
+        filters = build.includeDatabases(""connector_test,another_included"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""another_included"");
+        assertDatabaseExcluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowDatabaseListedWithMultipleRegexInWhitelistAndNoDatabaseBlacklist() {
+        filters = build.includeDatabases(""connector.*_test,another_{1}.*"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""another_included"");
+        assertDatabaseIncluded(""another__test"");
+        assertDatabaseExcluded(""conn_test"");
+        assertDatabaseExcluded(""connector-test"");
+        assertDatabaseExcluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowDatabaseListedWithWildcardInWhitelistAndNoDatabaseBlacklist() {
+        filters = build.includeDatabases("".*"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""another_included"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowAllDatabaseExceptSystemWhenWhitelistIsBlank() {
+        filters = build.includeDatabases("""").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+
+    @Test
+    public void shouldNotAllowDatabaseListedWithLiteralInBlacklistAndNoDatabaseWhitelist() {
+        filters = build.excludeDatabases(""connector_test"").createFilters();
+        assertDatabaseExcluded(""connector_test"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldNotAllowDatabaseListedWithMultipleLiteralsInBlacklistAndNoDatabaseWhitelist() {
+        filters = build.excludeDatabases(""connector_test,another_included"").createFilters();
+        assertDatabaseExcluded(""connector_test"");
+        assertDatabaseExcluded(""another_included"");
+        assertDatabaseIncluded(""other"");
+        assertDatabaseIncluded(""something-else"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldNotAllowDatabaseListedWithMultipleRegexInBlacklistAndNoDatabaseWhitelist() {
+        filters = build.excludeDatabases(""connector.*_test,another_{1}.*"").createFilters();
+        assertDatabaseExcluded(""connector_test"");
+        assertDatabaseExcluded(""another_included"");
+        assertDatabaseExcluded(""another__test"");
+        assertDatabaseIncluded(""conn_test"");
+        assertDatabaseIncluded(""connector-test"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldNotAllowDatabaseListedWithWildcardInBlacklistAndNoDatabaseWhitelist() {
+        filters = build.excludeDatabases("".*"").createFilters();
+        assertDatabaseExcluded(""connector_test"");
+        assertDatabaseExcluded(""another_included"");
+        assertDatabaseExcluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldRespectOnlyDatabaseWhitelistWithDatabaseBlacklistAlsoSpecified() {
+        filters = build.includeDatabases(""A,B,C,D.*"").excludeDatabases(""C,B,E"").createFilters();
+        assertDatabaseIncluded(""A"");
+        assertDatabaseIncluded(""B"");
+        assertDatabaseIncluded(""C"");
+        assertDatabaseIncluded(""D"");
+        assertDatabaseIncluded(""D1"");
+        assertDatabaseIncluded(""D_3"");
+        assertDatabaseExcluded(""E"");
+        assertDatabaseExcluded(""another__test"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowAllDatabaseWhenBlacklistIsBlank() {
+        filters = build.excludeDatabases("""").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+
+    @Test
+    public void shouldIgnoreDatabaseBlacklistWhenDatabaseWhitelistIsNonEmpty() {
+        filters = build.includeDatabases("".*"").excludeDatabases(""connector_test,other"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other"");
+        assertDatabaseIncluded(""something_else"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithLiteralInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test.table1"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableExcluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithLiteralWithEscapedPeriodInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test[.]table1"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableExcluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+
+        filters = build.includeTables(""connector_test\\.table1"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableExcluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithMultipleLiteralsInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test.table1,connector_test.table2"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableIncluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithMultipleRegexInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test.table[x]?1,connector_test[.](.*)2"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableIncluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithWildcardInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test[.](.*)"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableIncluded(""connector_test.table2"");
+        assertTableIncluded(""connector_test.table3"");
+        assertTableIncluded(""connector_test.ABC"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithLiteralInWhitelistAndNoTableBlacklistWhenDatabaseIncludedButSystemTablesIncluded() {
+        filters = build.includeTables(""connector_test.table1,connector_test.table2"").includeBuiltInTables().createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableIncluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertTableExcluded(""other.table1"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesIncluded();
+        assertSystemDatabasesIncluded();
+    }
+    
+    @Test
+    public void shouldNotAllowTableWhenNotIncludedInDatabaseWhitelist() {
+        filters = build.includeTables(""db1.table1,db2.table1,db3.*"").includeDatabases(""db1,db3"").createFilters();
+        assertTableIncluded(""db1.table1"");
+        assertTableExcluded(""db1.table2"");
+        assertTableExcluded(""db2.table1"");
+        assertTableExcluded(""db2.table2"");
+        assertTableIncluded(""db3.table1"");
+        assertTableIncluded(""db3.table2"");
+        assertTableExcluded(""db4.table1"");
+        assertTableExcluded(""db4.table2"");
+        assertDatabaseIncluded(""db1"");
+        assertDatabaseIncluded(""db3"");
+        assertDatabaseExcluded(""db2"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldNotAllowTableWhenExcludedInDatabaseWhitelist() {
+        filters = build.includeTables(""db1.table1,db2.table1,db3.*"").excludeDatabases(""db2"").createFilters();
+        assertTableIncluded(""db1.table1"");
+        assertTableExcluded(""db1.table2""); // not explicitly included in the tables
+        assertTableExcluded(""db2.table1"");
+        assertTableExcluded(""db2.table2"");
+        assertTableIncluded(""db3.table1"");
+        assertTableIncluded(""db3.table2"");
+        assertTableExcluded(""db4.table1""); // not explicitly included in the tables
+        assertTableExcluded(""db4.table2""); // not explicitly included in the tables
+        assertDatabaseIncluded(""db1"");
+        assertDatabaseIncluded(""db3"");
+        assertDatabaseExcluded(""db2"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    protected void assertDatabaseIncluded( String databaseName ) {
+        assertThat(filters.databaseFilter().test(databaseName)).isTrue();
+    }
+
+    protected void assertDatabaseExcluded( String databaseName ) {
+        assertThat(filters.databaseFilter().test(databaseName)).isFalse();
+    }
+
+    protected void assertSystemDatabasesExcluded() {
+        Filters.BUILT_IN_DB_NAMES.forEach(this::assertDatabaseExcluded);
+    }
+
+    protected void assertSystemDatabasesIncluded() {
+        Filters.BUILT_IN_DB_NAMES.forEach(this::assertDatabaseIncluded);
+    }
+
+    protected void assertSystemTablesExcluded() {
+        Filters.BUILT_IN_TABLE_NAMES.forEach(tableName->{
+            Filters.BUILT_IN_DB_NAMES.forEach(dbName->{
+                assertTableExcluded(dbName + ""."" + tableName);
+            });
+        });
+    }
+
+    protected void assertSystemTablesIncluded() {
+        Filters.BUILT_IN_TABLE_NAMES.forEach(tableName->{
+            Filters.BUILT_IN_DB_NAMES.forEach(dbName->{
+                assertTableIncluded(dbName + ""."" + tableName);
+            });
+        });
+    }
+
+    protected void assertTableIncluded( String fullyQualifiedTableName ) {
+        TableId id = TableId.parse(fullyQualifiedTableName);
+        assertThat(filters.tableFilter().test(id)).isTrue();
+    }
+
+    protected void assertTableExcluded( String fullyQualifiedTableName ) {
+        TableId id = TableId.parse(fullyQualifiedTableName);
+        assertThat(filters.tableFilter().test(id)).isFalse();
+    }
+
+}",2016-05-26T20:58:58Z,17
"@@ -0,0 +1,185 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Path;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.relational.Table;
+import io.debezium.relational.TableId;
+import io.debezium.relational.TableSchema;
+import io.debezium.util.IoUtil;
+import io.debezium.util.Testing;
+
+/**
+ * @author Randall Hauch
+ */
+public class MySqlSchemaTest {
+
+    private static final Path TEST_FILE_PATH = Testing.Files.createTestingPath(""dbHistory.log"");
+
+    private Configurator build;
+    private MySqlSchema mysql;
+    private SourceInfo source;
+
+    @Before
+    public void beforeEach() {
+        Testing.Files.delete(TEST_FILE_PATH);
+        build = new Configurator();
+        mysql = null;
+        source = new SourceInfo();
+    }
+
+    @After
+    public void afterEach() {
+        if (mysql != null) {
+            try {
+                mysql.shutdown();
+            } finally {
+                mysql = null;
+            }
+        }
+    }
+
+    @Test
+    public void shouldApplyDdlStatementsAndRecover() {
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        mysql.start();
+
+        // Testing.Print.enable();
+        source.setBinlogFilename(""binlog-001"");
+        source.setBinlogPosition(400);
+        mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
+
+        // Check that we have tables ...
+        assertTableIncluded(""connector_test.products"");
+        assertTableIncluded(""connector_test.products_on_hand"");
+        assertTableIncluded(""connector_test.customers"");
+        assertTableIncluded(""connector_test.orders"");
+        assertHistoryRecorded();
+    }
+
+    @Test
+    public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases() {
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .includeDatabases(""connector_test"")
+                     .excludeBuiltInTables()
+                     .createSchemas();
+        mysql.start();
+
+        source.setBinlogFilename(""binlog-001"");
+        source.setBinlogPosition(400);
+        mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
+
+        source.setBinlogPosition(1000);
+        mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
+
+        // Check that we have tables ...
+        assertTableIncluded(""connector_test.products"");
+        assertTableIncluded(""connector_test.products_on_hand"");
+        assertTableIncluded(""connector_test.customers"");
+        assertTableIncluded(""connector_test.orders"");
+        assertTableExcluded(""mysql.columns_priv"");
+        assertNoTablesExistForDatabase(""mysql"");
+        assertHistoryRecorded();
+    }
+
+    @Test
+    public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .includeDatabases(""connector_test"")
+                     .includeBuiltInTables()
+                     .createSchemas();
+        mysql.start();
+
+        source.setBinlogFilename(""binlog-001"");
+        source.setBinlogPosition(400);
+        mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
+
+        source.setBinlogPosition(1000);
+        mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
+
+        // Check that we have tables ...
+        assertTableIncluded(""connector_test.products"");
+        assertTableIncluded(""connector_test.products_on_hand"");
+        assertTableIncluded(""connector_test.customers"");
+        assertTableIncluded(""connector_test.orders"");
+        assertTableIncluded(""mysql.columns_priv"");
+        assertTablesExistForDatabase(""mysql"");
+        assertHistoryRecorded();
+    }
+
+    protected void assertTableIncluded(String fullyQualifiedTableName) {
+        TableId tableId = TableId.parse(fullyQualifiedTableName);
+        assertThat(mysql.tables().forTable(tableId)).isNotNull();
+        assertThat(mysql.schemaFor(tableId)).isNotNull();
+    }
+
+    protected void assertTableExcluded(String fullyQualifiedTableName) {
+        TableId tableId = TableId.parse(fullyQualifiedTableName);
+        assertThat(mysql.tables().forTable(tableId)).isNull();
+        assertThat(mysql.schemaFor(tableId)).isNull();
+    }
+    
+    protected void assertNoTablesExistForDatabase(String dbName) {
+        assertThat(mysql.tables().tableIds().stream().filter(id->id.catalog().equals(dbName)).count()).isEqualTo(0);
+    }
+    protected void assertTablesExistForDatabase(String dbName) {
+        assertThat(mysql.tables().tableIds().stream().filter(id->id.catalog().equals(dbName)).count()).isGreaterThan(0);
+    }
+
+    protected void assertHistoryRecorded() {
+        MySqlSchema duplicate = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        duplicate.loadHistory(source);
+
+        // Make sure table is defined in each ...
+        assertThat(duplicate.tables()).isEqualTo(mysql.tables());
+        for (int i = 0; i != 2; ++i) {
+            duplicate.tables().tableIds().forEach(tableId -> {
+                TableSchema dupSchema = duplicate.schemaFor(tableId);
+                TableSchema schema = mysql.schemaFor(tableId);
+                assertThat(schema).isEqualTo(dupSchema);
+                Table dupTable = duplicate.tables().forTable(tableId);
+                Table table = mysql.tables().forTable(tableId);
+                assertThat(table).isEqualTo(dupTable);
+            });
+            mysql.tables().tableIds().forEach(tableId -> {
+                TableSchema dupSchema = duplicate.schemaFor(tableId);
+                TableSchema schema = mysql.schemaFor(tableId);
+                assertThat(schema).isEqualTo(dupSchema);
+                Table dupTable = duplicate.tables().forTable(tableId);
+                Table table = mysql.tables().forTable(tableId);
+                assertThat(table).isEqualTo(dupTable);
+            });
+            duplicate.refreshSchemas();
+        }
+    }
+
+    protected void printStatements(String dbName, String ddlStatements) {
+        Testing.print(""Running DDL for '"" + dbName + ""': "" + ddlStatements);
+    }
+
+    protected String readFile(String classpathResource) {
+        try (InputStream stream = getClass().getClassLoader().getResourceAsStream(classpathResource);) {
+            assertThat(stream).isNotNull();
+            return IoUtil.read(stream);
+        } catch (IOException e) {
+            fail(""Unable to read '"" + classpathResource + ""'"");
+        }
+        assert false : ""should never get here"";
+        return null;
+    }
+
+}",2016-05-26T20:58:58Z,16
"@@ -0,0 +1,37 @@
+# Create the database that we'll use to populate data and watch the effect in the binlog
+CREATE DATABASE connector_test;
+
+# Create and populate our products using a single insert with many rows
+CREATE TABLE connector_test.products (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  name VARCHAR(255) NOT NULL,
+  description VARCHAR(512),
+  weight FLOAT
+);
+ALTER TABLE connector_test.products AUTO_INCREMENT = 101;
+
+# Create and populate the products on hand using multiple inserts
+CREATE TABLE connector_test.products_on_hand (
+  product_id INTEGER NOT NULL PRIMARY KEY,
+  quantity INTEGER NOT NULL,
+  FOREIGN KEY (product_id) REFERENCES products(id)
+);
+
+# Create some customers ...
+CREATE TABLE connector_test.customers (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  first_name VARCHAR(255) NOT NULL,
+  last_name VARCHAR(255) NOT NULL,
+  email VARCHAR(255) NOT NULL UNIQUE KEY
+) AUTO_INCREMENT=1001;
+
+# Create some veyr simple orders
+CREATE TABLE connector_test.orders (
+  order_number INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  order_date DATE NOT NULL,
+  purchaser INTEGER NOT NULL,
+  quantity INTEGER NOT NULL,
+  product_id INTEGER NOT NULL,
+  FOREIGN KEY order_customer (purchaser) REFERENCES customers(id),
+  FOREIGN KEY ordered_product (product_id) REFERENCES products(id)
+) AUTO_INCREMENT = 10001;
\ No newline at end of file",2016-05-26T20:58:58Z,121
"@@ -54,8 +54,12 @@ public static <T> Predicate<T> includes(String regexPatterns, Function<T, String
         Set<Pattern> patterns = Strings.listOfRegex(regexPatterns,Pattern.CASE_INSENSITIVE);
         return (t) -> {
             String str = conversion.apply(t);
-            for ( Pattern p : patterns ) {
-                if ( p.matcher(str).matches()) return true;
+            if ( str != null ) {
+                for ( Pattern p : patterns ) {
+                    if ( p.matcher(str).matches()) return true;
+                }
+            } else {
+                int x =0;
             }
             return false;
         };",2016-05-26T20:58:58Z,122
"@@ -15,6 +15,18 @@
 @Immutable
 public final class TableId implements Comparable<TableId> {
 
+    /**
+     * Parse the supplied string, extracting up to the first 3 parts into a TableID.
+     * 
+     * @param str the string representation of the table identifier; may not be null
+     * @return the table ID, or null if it could not be parsed
+     */
+    public static TableId parse(String str) {
+        String[] parts = str.split(""[\\"" + '.' + ""]"");
+        if ( parts.length < 0 ) return null;
+        return TableId.parse(parts, parts.length, true);
+    }
+
     /**
      * Parse the supplied string, extracting up to the first 3 parts into a TableID.
      * ",2016-05-26T20:58:58Z,123
"@@ -5,12 +5,14 @@
  */
 package io.debezium.relational;
 
+import java.util.Objects;
 import java.util.function.Function;
 
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.Struct;
 
 import io.debezium.annotation.Immutable;
+import io.debezium.data.SchemaUtil;
 
 /**
  * Defines the Kafka Connect {@link Schema} functionality associated with a given {@link Table table definition}, and which can
@@ -108,4 +110,24 @@ public Object keyFromColumnData(Object[] columnData) {
     public Struct valueFromColumnData(Object[] columnData) {
         return columnData == null ? null : valueGenerator.apply(columnData);
     }
+    
+    @Override
+    public int hashCode() {
+        return valueSchema().hashCode();
+    }
+    
+    @Override
+    public boolean equals(Object obj) {
+        if ( obj == this ) return true;
+        if ( obj instanceof TableSchema ) {
+            TableSchema that = (TableSchema)obj;
+            return Objects.equals(this.keySchema(),that.keySchema()) && Objects.equals(this.valueSchema(),that.valueSchema());
+        }
+        return false;
+    }
+    
+    @Override
+    public String toString() {
+        return ""{ key : "" + SchemaUtil.asString(keySchema()) + "", value : "" + SchemaUtil.asString(valueSchema()) + "" }"";
+    }
 }",2016-05-26T20:58:58Z,124
"@@ -19,15 +19,16 @@
  * @author Randall Hauch
  */
 public interface DatabaseHistory {
-    
+
     public static final String CONFIGURATION_FIELD_PREFIX_STRING = ""database.history."";
-    
+
     /**
      * Configure this instance.
+     * 
      * @param config the configuration for this history store
      */
     void configure(Configuration config);
-    
+
     /**
      * Start the history.
      */
@@ -54,11 +55,12 @@ public interface DatabaseHistory {
      * 
      * @param source the information about the source database; may not be null
      * @param position the point in history at which the {@link Tables database schema} should be recovered; may not be null
-     * @param schema the definition of the schema for the named {@code database}; may not be null
+     * @param schema the table definitions that should be changed to reflect the database schema at the desired point in history;
+     *            may not be null
      * @param ddlParser the DDL parser that can be used to apply DDL statements to the given {@code schema}; may not be null
      */
     void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser);
-    
+
     /**
      * Stop recording history and release any resources acquired since {@link #configure(Configuration)}.
      */",2016-05-26T20:58:58Z,14
"@@ -150,10 +150,10 @@ public void start(Map<String, String> props) {
                                                         config.getString(MySqlConnectorConfig.TABLE_WHITELIST),
                                                         config.getString(MySqlConnectorConfig.TABLE_BLACKLIST));
         if (config.getBoolean(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN)) {
-            Predicate<TableId> ignoreBuiltins = (id) -> {
-                return !BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase()) && !BUILT_IN_DB_NAMES.contains(id.catalog().toLowerCase());
+            Predicate<TableId> isBuiltin = (id) -> {
+                return BUILT_IN_DB_NAMES.contains(id.catalog().toLowerCase()) || BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase());
             };
-            tableFilter = ignoreBuiltins.or(tableFilter);
+            tableFilter = tableFilter.and(isBuiltin.negate());
         }
 
         // Create the queue ...
@@ -163,6 +163,7 @@ public void start(Map<String, String> props) {
         // Set up our handlers for specific kinds of events ...
         tables = new Tables();
         tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, tables, tableFilter);
+        eventHandlers.put(EventType.ROTATE, tableConverters::rotateLogs);
         eventHandlers.put(EventType.TABLE_MAP, tableConverters::updateTableMetadata);
         eventHandlers.put(EventType.QUERY, tableConverters::updateTableCommand);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, tableConverters::handleInsert);",2016-03-17T16:03:28Z,10
"@@ -26,6 +26,7 @@
 import com.github.shyiko.mysql.binlog.event.DeleteRowsEventData;
 import com.github.shyiko.mysql.binlog.event.Event;
 import com.github.shyiko.mysql.binlog.event.QueryEventData;
+import com.github.shyiko.mysql.binlog.event.RotateEventData;
 import com.github.shyiko.mysql.binlog.event.TableMapEventData;
 import com.github.shyiko.mysql.binlog.event.UpdateRowsEventData;
 import com.github.shyiko.mysql.binlog.event.WriteRowsEventData;
@@ -86,6 +87,17 @@ public void loadTables() {
         });
     }
 
+    public void rotateLogs(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
+        logger.debug(""Rotating logs: {}"", event);
+        RotateEventData command = event.getData();
+        if (command != null) {
+            // The logs are being rotated, which means the server was either restarted, or the binlog has transitioned to a new
+            // file. In either case, the table numbers will change, so we need to discard the cache of converters by the table IDs
+            // (e.g., the Map<Long,Converter>). Note, however, that we're NOT clearing out the Map<TableId,TableSchema>.
+            convertersByTableId.clear();
+        }
+    }
+
     public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
         QueryEventData command = event.getData();
         String databaseName = command.getDatabase();
@@ -142,8 +154,8 @@ public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRe
     public void updateTableMetadata(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
         TableMapEventData metadata = event.getData();
         long tableNumber = metadata.getTableId();
+        logger.debug(""Received update table metadata event: {}"", event);
         if (!convertersByTableId.containsKey(tableNumber)) {
-            logger.debug(""Received update table metadata event: {}"", event);
             // We haven't seen this table ID, so we need to rebuild our converter functions ...
             String serverName = source.serverName();
             String databaseName = metadata.getDatabase();
@@ -153,6 +165,7 @@ public void updateTableMetadata(Event event, SourceInfo source, Consumer<SourceR
             // Just get the current schema, which should be up-to-date ...
             TableId tableId = new TableId(databaseName, null, tableName);
             TableSchema tableSchema = tableSchemaByTableId.get(tableId);
+            logger.debug(""Registering metadata for table {} with table #{}"", tableId, tableNumber);
             if (tableSchema == null) {
                 // We are seeing an event for a row that's in a table we don't know about, meaning the table
                 // was created before the binlog was enabled (or before the point we started reading it).
@@ -201,7 +214,7 @@ public Struct inserted(Serializable[] row, BitSet includedColumns) {
                 }
 
                 @Override
-                public Struct updated(Serializable[] after, BitSet includedColumns, Serializable[] before,
+                public Struct updated(Serializable[] before, BitSet includedColumns, Serializable[] after,
                                       BitSet includedColumnsBeforeUpdate) {
                     // assume all columns in the table are included, and we'll write out only the after state ...
                     return tableSchema.valueFromColumnData(after);
@@ -218,6 +231,8 @@ public Struct deleted(Serializable[] deleted, BitSet includedColumns) {
             if (previousTableNumber != null) {
                 convertersByTableId.remove(previousTableNumber);
             }
+        } else if (logger.isDebugEnabled()) {
+            logger.debug(""Skipping update table metadata event: {}"", event);
         }
     }
 
@@ -226,25 +241,30 @@ public void handleInsert(Event event, SourceInfo source, Consumer<SourceRecord>
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         Converter converter = convertersByTableId.get(tableNumber);
-        if (tableFilter.test(converter.tableId())) {
-            logger.debug(""Received insert row event: {}"", event);
-            String topic = converter.topic();
-            Integer partition = converter.partition();
-            List<Serializable[]> rows = write.getRows();
-            for (int row = 0; row != rows.size(); ++row) {
-                Serializable[] values = rows.get(row);
-                Schema keySchema = converter.keySchema();
-                Object key = converter.createKey(values, includedColumns);
-                Schema valueSchema = converter.valueSchema();
-                Struct value = converter.inserted(values, includedColumns);
-                if (value != null || key != null) {
-                    SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                            keySchema, key, valueSchema, value);
-                    recorder.accept(record);
+        if (converter != null) {
+            TableId tableId = converter.tableId();
+            if (tableFilter.test(tableId)) {
+                logger.debug(""Processing insert row event for {}: {}"", tableId, event);
+                String topic = converter.topic();
+                Integer partition = converter.partition();
+                List<Serializable[]> rows = write.getRows();
+                for (int row = 0; row != rows.size(); ++row) {
+                    Serializable[] values = rows.get(row);
+                    Schema keySchema = converter.keySchema();
+                    Object key = converter.createKey(values, includedColumns);
+                    Schema valueSchema = converter.valueSchema();
+                    Struct value = converter.inserted(values, includedColumns);
+                    if (value != null || key != null) {
+                        SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                                keySchema, key, valueSchema, value);
+                        recorder.accept(record);
+                    }
                 }
+            } else if (logger.isDebugEnabled()) {
+                logger.debug(""Skipping insert row event: {}"", event);
             }
-        } else if (logger.isDebugEnabled()) {
-            logger.debug(""Skipping insert row event: {}"", event);
+        } else {
+            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
         }
     }
 
@@ -261,27 +281,32 @@ public void handleUpdate(Event event, SourceInfo source, Consumer<SourceRecord>
         BitSet includedColumns = update.getIncludedColumns();
         BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
         Converter converter = convertersByTableId.get(tableNumber);
-        if (tableFilter.test(converter.tableId())) {
-            logger.debug(""Received update row event: {}"", event);
-            String topic = converter.topic();
-            Integer partition = converter.partition();
-            List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
-            for (int row = 0; row != rows.size(); ++row) {
-                Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
-                Serializable[] before = changes.getKey();
-                Serializable[] after = changes.getValue();
-                Schema keySchema = converter.keySchema();
-                Object key = converter.createKey(after, includedColumns);
-                Schema valueSchema = converter.valueSchema();
-                Struct value = converter.updated(before, includedColumnsBefore, after, includedColumns);
-                if (value != null || key != null) {
-                    SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                            keySchema, key, valueSchema, value);
-                    recorder.accept(record);
+        if (converter != null) {
+            TableId tableId = converter.tableId();
+            if (tableFilter.test(tableId)) {
+                logger.debug(""Processing update row event for {}: {}"", tableId, event);
+                String topic = converter.topic();
+                Integer partition = converter.partition();
+                List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
+                for (int row = 0; row != rows.size(); ++row) {
+                    Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
+                    Serializable[] before = changes.getKey();
+                    Serializable[] after = changes.getValue();
+                    Schema keySchema = converter.keySchema();
+                    Object key = converter.createKey(after, includedColumns);
+                    Schema valueSchema = converter.valueSchema();
+                    Struct value = converter.updated(before, includedColumnsBefore, after, includedColumns);
+                    if (value != null || key != null) {
+                        SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                                keySchema, key, valueSchema, value);
+                        recorder.accept(record);
+                    }
                 }
+            } else if (logger.isDebugEnabled()) {
+                logger.debug(""Skipping update row event: {}"", event);
             }
-        } else if (logger.isDebugEnabled()) {
-            logger.debug(""Skipping update row event: {}"", event);
+        } else {
+            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
         }
     }
 
@@ -290,25 +315,30 @@ public void handleDelete(Event event, SourceInfo source, Consumer<SourceRecord>
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         Converter converter = convertersByTableId.get(tableNumber);
-        if (tableFilter.test(converter.tableId())) {
-            logger.debug(""Received delete row event: {}"", event);
-            String topic = converter.topic();
-            Integer partition = converter.partition();
-            List<Serializable[]> rows = deleted.getRows();
-            for (int row = 0; row != rows.size(); ++row) {
-                Serializable[] values = rows.get(row);
-                Schema keySchema = converter.keySchema();
-                Object key = converter.createKey(values, includedColumns);
-                Schema valueSchema = converter.valueSchema();
-                Struct value = converter.deleted(values, includedColumns);
-                if (value != null || key != null) {
-                    SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                            keySchema, key, valueSchema, value);
-                    recorder.accept(record);
+        if (converter != null) {
+            TableId tableId = converter.tableId();
+            if (tableFilter.test(tableId)) {
+                logger.debug(""Processing delete row event for {}: {}"", tableId, event);
+                String topic = converter.topic();
+                Integer partition = converter.partition();
+                List<Serializable[]> rows = deleted.getRows();
+                for (int row = 0; row != rows.size(); ++row) {
+                    Serializable[] values = rows.get(row);
+                    Schema keySchema = converter.keySchema();
+                    Object key = converter.createKey(values, includedColumns);
+                    Schema valueSchema = converter.valueSchema();
+                    Struct value = converter.deleted(values, includedColumns);
+                    if (value != null || key != null) {
+                        SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                                keySchema, key, valueSchema, value);
+                        recorder.accept(record);
+                    }
                 }
+            } else if (logger.isDebugEnabled()) {
+                logger.debug(""Skipping delete row event: {}"", event);
             }
-        } else if (logger.isDebugEnabled()) {
-            logger.debug(""Skipping delete row event: {}"", event);
+        } else {
+            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
         }
     }
 
@@ -327,7 +357,7 @@ protected static interface Converter {
 
         Struct inserted(Serializable[] row, BitSet includedColumns);
 
-        Struct updated(Serializable[] after, BitSet includedColumns, Serializable[] before, BitSet includedColumnsBeforeUpdate);
+        Struct updated(Serializable[] before, BitSet includedColumns, Serializable[] after, BitSet includedColumnsBeforeUpdate);
 
         Struct deleted(Serializable[] deleted, BitSet includedColumns);
     }",2016-03-17T16:03:28Z,28
"@@ -70,6 +70,9 @@ public class KafkaDatabaseHistory extends AbstractDatabaseHistory {
     public static Collection<Field> ALL_FIELDS = Collect.arrayListOf(TOPIC, BOOTSTRAP_SERVERS,
                                                                      RECOVERY_POLL_INTERVAL_MS, RECOVERY_POLL_ATTEMPTS);
 
+    private static final String CONSUMER_PREFIX = CONFIGURATION_FIELD_PREFIX_STRING + ""consumer."";
+    private static final String PRODUCER_PREFIX = CONFIGURATION_FIELD_PREFIX_STRING + ""producer."";
+    
     private final DocumentReader reader = DocumentReader.defaultReader();
     private final Integer partition = new Integer(0);
     private String topicName;
@@ -92,7 +95,7 @@ public void configure(Configuration config) {
         String bootstrapServers = config.getString(BOOTSTRAP_SERVERS);
         // Copy the relevant portions of the configuration and add useful defaults ...
         String clientAndGroupId = UUID.randomUUID().toString();
-        this.consumerConfig = config.subset(""consumer."", true).edit()
+        this.consumerConfig = config.subset(CONSUMER_PREFIX, true).edit()
                                     .withDefault(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
                                     .withDefault(ConsumerConfig.CLIENT_ID_CONFIG, clientAndGroupId)
                                     .withDefault(ConsumerConfig.GROUP_ID_CONFIG, clientAndGroupId)
@@ -104,7 +107,7 @@ public void configure(Configuration config) {
                                     .withDefault(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class)
                                     .withDefault(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class)
                                     .build();
-        this.producerConfig = config.subset(""producer."", true).edit()
+        this.producerConfig = config.subset(PRODUCER_PREFIX, true).edit()
                                     .withDefault(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
                                     .withDefault(ProducerConfig.CLIENT_ID_CONFIG, UUID.randomUUID().toString())
                                     .withDefault(ProducerConfig.ACKS_CONFIG, 1)
@@ -116,6 +119,8 @@ public void configure(Configuration config) {
                                     .withDefault(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class)
                                     .withDefault(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class)
                                     .build();
+        logger.info(""KafkaDatabaseHistory Consumer config: "" + consumerConfig);
+        logger.info(""KafkaDatabaseHistory Producer config: "" + producerConfig);
     }
 
     @Override",2016-03-17T16:03:28Z,30
"@@ -5,7 +5,6 @@
  */
 package io.debezium.connector.mysql;
 
-import java.nio.charset.StandardCharsets;
 import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -32,7 +31,7 @@
  */
 public class MySqlJdbcContext implements AutoCloseable {
 
-    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false&useSSL=${useSSL}"";
+    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false&useSSL=${useSSL}&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8"";
     protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(MYSQL_CONNECTION_URL);
 
     protected final Logger logger = LoggerFactory.getLogger(getClass());
@@ -49,10 +48,7 @@ public MySqlJdbcContext(Configuration config) {
         boolean useSSL = sslModeEnabled();
         Configuration jdbcConfig = config.subset(""database."", true)
                                          .edit()
-                                         .with(""useInformationSchema"", ""true"")
-                                         .with(""nullCatalogMeansCurrent"", ""false"")
                                          .with(""useSSL"", Boolean.toString(useSSL))
-                                         .with(""characterEncoding"", StandardCharsets.UTF_8.name())
                                          .build();
         this.jdbc = new JdbcConnection(jdbcConfig, FACTORY);
     }",2016-08-29T20:14:25Z,29
"@@ -122,6 +122,7 @@ public ValueConverter converter(Column column, Field fieldDefn) {
             case Types.SQLXML:
                 Charset charset = charsetFor(column);
                 if (charset != null) {
+                    logger.debug(""Using {} charset by default for column: {}"", charset, column);
                     return (data) -> convertString(column, fieldDefn, charset, data);
                 }
                 logger.warn(""Using UTF-8 charset by default for column without charset: {}"", column);",2016-08-29T20:14:25Z,70
"@@ -237,4 +237,16 @@ CREATE TABLE dbz_100_enumsettest (
 );
 INSERT INTO dbz_100_enumsettest VALUES ('a', 'a,b,c');
 INSERT INTO dbz_100_enumsettest VALUES ('b', 'b,a');
-INSERT INTO dbz_100_enumsettest VALUES ('c', 'a');
\ No newline at end of file
+INSERT INTO dbz_100_enumsettest VALUES ('c', 'a');
+
+-- DBZ-102 handle character sets
+-- Use session variables to dictate the character sets used by the client running these commands so
+-- the literal value is interpretted correctly...
+set character_set_client=utf8;
+set character_set_connection=utf8;
+CREATE TABLE dbz_102_charsettest (
+  id INT(11) NOT NULL AUTO_INCREMENT,
+  text VARCHAR(255) DEFAULT NULL,
+  PRIMARY KEY (`id`)
+) ENGINE=InnoDB AUTO_INCREMENT=2001 DEFAULT CHARSET=utf8;
+INSERT INTO dbz_102_charsettest VALUES (default, """");
\ No newline at end of file",2016-08-29T20:14:25Z,93
"@@ -86,17 +86,18 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
         // Testing.Debug.enable();
-        SourceRecords records = consumeRecordsByTopic(5 + 6); // 5 schema change record, 6 inserts
+        SourceRecords records = consumeRecordsByTopic(6 + 7); // 5 schema change record, 7 inserts
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(6);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_100_enumsettest"").size()).isEqualTo(3);
-        assertThat(records.topics().size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_102_charsettest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(6);
         assertThat(records.databaseNames().size()).isEqualTo(1);
-        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(5);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(6);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
@@ -118,6 +119,10 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 } else {
                     fail(""c1 didn't match expected value"");
                 }
+            } else if (record.topic().endsWith(""dbz_102_charsettest"")) {
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                String text = after.getString(""text"");
+                assertThat(text).isEqualTo("""");
             } else if (record.topic().endsWith(""dbz_85_fractest"")) {
                 // The microseconds of all three should be exactly 780
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
@@ -207,17 +212,18 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
         // Testing.Debug.enable();
-        SourceRecords records = consumeRecordsByTopic(5 + 6); // 5 schema change record, 6 inserts
+        SourceRecords records = consumeRecordsByTopic(6 + 7); // 6 schema change record, 7 inserts
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(6);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_100_enumsettest"").size()).isEqualTo(3);
-        assertThat(records.topics().size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_102_charsettest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(6);
         assertThat(records.databaseNames().size()).isEqualTo(1);
-        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(5);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(6);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
@@ -239,6 +245,10 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                 } else {
                     fail(""c1 didn't match expected value"");
                 }
+            } else if (record.topic().endsWith(""dbz_102_charsettest"")) {
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                String text = after.getString(""text"");
+                assertThat(text).isEqualTo("""");
             } else if (record.topic().endsWith(""dbz_85_fractest"")) {
                 // The microseconds of all three should be exactly 780
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
@@ -324,21 +334,27 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // ---------------------------------------------------------------------------------------------------------------
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
-        // Testing.Debug.enable();
-        // 12 schema change records = 1 set variables, 5 drop tables, 1 drop database, 1 create database, 1 use database, 5 create
-        // tables
-        SourceRecords records = consumeRecordsByTopic(12 + 6); // plus 6 data records ...
+        Testing.Debug.enable();
+        // We expect a total of 14 schema change records:
+        // 1 set variables
+        // 6 drop tables
+        // 1 drop database
+        // 1 create database
+        // 1 use database
+        // 6 create tables
+        SourceRecords records = consumeRecordsByTopic(14 + 7); // plus 7 data records ...
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(12);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(14);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_100_enumsettest"").size()).isEqualTo(3);
-        assertThat(records.topics().size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_102_charsettest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(6);
         assertThat(records.databaseNames().size()).isEqualTo(2);
         assertThat(records.databaseNames()).containsOnly(""regression_test"","""");
-        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(11);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(13);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase("""").size()).isEqualTo(1); // SET statement
@@ -361,6 +377,10 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                 } else {
                     fail(""c1 didn't match expected value"");
                 }
+            } else if (record.topic().endsWith(""dbz_102_charsettest"")) {
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                String text = after.getString(""text"");
+                assertThat(text).isEqualTo("""");
             } else if (record.topic().endsWith(""dbz_85_fractest"")) {
                 // The microseconds of all three should be exactly 780
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);",2016-08-29T20:14:25Z,71
"@@ -36,7 +36,9 @@
 import com.github.shyiko.mysql.binlog.event.deserialization.GtidEventDataDeserializer;
 import com.github.shyiko.mysql.binlog.io.ByteArrayInputStream;
 import com.github.shyiko.mysql.binlog.network.AuthenticationException;
+import com.github.shyiko.mysql.binlog.network.SSLMode;
 
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
 import io.debezium.function.BlockingConsumer;
 import io.debezium.relational.TableId;
@@ -70,6 +72,7 @@ public BinlogReader(MySqlTaskContext context) {
         // Set up the log reader ...
         client = new BinaryLogClient(context.hostname(), context.port(), context.username(), context.password());
         client.setServerId(context.serverId());
+        client.setSSLMode(sslModeFor(context.sslMode()));
         client.setKeepAlive(context.config().getBoolean(MySqlConnectorConfig.KEEP_ALIVE));
         client.registerEventListener(this::handleEvent);
         client.registerLifecycleListener(new ReaderThreadLifecycleListener());
@@ -407,6 +410,22 @@ protected void handleDelete(Event event) throws InterruptedException {
             logger.debug(""Skipping delete row event: {}"", event);
         }
     }
+    
+    protected SSLMode sslModeFor( SecureConnectionMode mode ) {
+        switch(mode) {
+            case DISABLED:
+                return SSLMode.DISABLED;
+            case PREFERRED:
+                return SSLMode.PREFERRED;
+            case REQUIRED:
+                return SSLMode.REQUIRED;
+            case VERIFY_CA:
+                return SSLMode.VERIFY_CA;
+            case VERIFY_IDENTITY:
+                return SSLMode.VERIFY_IDENTITY;
+        }
+        return null;
+    }
 
     protected final class ReaderThreadLifecycleListener implements LifecycleListener {
         @Override",2016-08-24T18:27:35Z,24
"@@ -17,6 +17,8 @@
 import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.connect.connector.Task;
 import org.apache.kafka.connect.source.SourceConnector;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import io.debezium.config.Configuration;
 import io.debezium.jdbc.JdbcConnection;
@@ -32,7 +34,8 @@
  * @author Randall Hauch
  */
 public class MySqlConnector extends SourceConnector {
-
+    
+    private Logger logger = LoggerFactory.getLogger(getClass());
     private Map<String, String> props;
 
     public MySqlConnector() {
@@ -88,10 +91,17 @@ public Config validate(Map<String, String> connectorConfigs) {
                 && passwordValue.errorMessages().isEmpty()) {
             // Try to connect to the database ...
             try (MySqlJdbcContext jdbcContext = new MySqlJdbcContext(config)) {
+                jdbcContext.start();
                 JdbcConnection mysql = jdbcContext.jdbc();
-                mysql.execute(""SELECT version()"");
-            } catch (SQLException e) {
-                hostnameValue.addErrorMessage(""Unable to connect: "" + e.getMessage());
+                try {
+                    mysql.execute(""SELECT version()"");
+                    logger.info(""Successfully tested connection for {} with user '{}'"", jdbcContext.connectionString(), mysql.username());
+                } catch (SQLException e) {
+                    logger.info(""Failed testing connection for {} with user '{}'"", jdbcContext.connectionString(), mysql.username());
+                    hostnameValue.addErrorMessage(""Unable to connect: "" + e.getMessage());
+                } finally {
+                    jdbcContext.shutdown();
+                }
             }
         }
         return new Config(new ArrayList<>(results.values()));",2016-08-24T18:27:35Z,125
"@@ -135,7 +135,75 @@ public static SnapshotMode parse(String value, String defaultValue) {
         }
     }
 
-    private static final String DATABASE_LIST_NAME = ""database.list"";
+    /**
+     * The set of predefined SecureConnectionMode options or aliases.
+     */
+    public static enum SecureConnectionMode {
+        /**
+         * Establish an unencrypted connection.
+         */
+        DISABLED(""disabled""),
+        
+        /**
+         * Establish a secure (encrypted) connection if the server supports secure connections.
+         * Fall back to an unencrypted connection otherwise.
+         */
+        PREFERRED(""preferred""),
+        /**
+         * Establish a secure connection if the server supports secure connections.
+         * The connection attempt fails if a secure connection cannot be established.
+         */
+        REQUIRED(""required""),
+        /**
+         * Like REQUIRED, but additionally verify the server TLS certificate against the configured Certificate Authority
+         * (CA) certificates. The connection attempt fails if no valid matching CA certificates are found.
+         */
+        VERIFY_CA(""verify_ca""),
+        /**
+         * Like VERIFY_CA, but additionally verify that the server certificate matches the host to which the connection is
+         * attempted.
+         */
+        VERIFY_IDENTITY(""verify_identity"");
+        
+        private final String value;
+
+        private SecureConnectionMode(String value) {
+            this.value = value;
+        }
+
+        public String getValue() {
+            return value;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         * 
+         * @param value the configuration property value; may not be null
+         * @return the matching option, or null if no match is found
+         */
+        public static SecureConnectionMode parse(String value) {
+            if (value == null) return null;
+            value = value.trim();
+            for (SecureConnectionMode option : SecureConnectionMode.values()) {
+                if (option.getValue().equalsIgnoreCase(value)) return option;
+            }
+            return null;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         * 
+         * @param value the configuration property value; may not be null
+         * @param defaultValue the default value; may be null
+         * @return the matching option, or null if no match is found and the non-null default is invalid
+         */
+        public static SecureConnectionMode parse(String value, String defaultValue) {
+            SecureConnectionMode mode = parse(value);
+            if (mode == null && defaultValue != null) mode = parse(defaultValue);
+            return mode;
+        }
+    }
+
     private static final String DATABASE_WHITELIST_NAME = ""database.whitelist"";
     private static final String TABLE_WHITELIST_NAME = ""table.whitelist"";
     private static final String TABLE_IGNORE_BUILTIN_NAME = ""table.ignore.builtin"";
@@ -147,7 +215,6 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                               .withType(Type.STRING)
                                               .withWidth(Width.MEDIUM)
                                               .withImportance(Importance.HIGH)
-                                              .withDependents(DATABASE_LIST_NAME)
                                               .withValidation(Field::isRequired)
                                               .withDescription(""Resolvable hostname or IP address of the MySQL database server."");
 
@@ -157,7 +224,6 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                           .withWidth(Width.SHORT)
                                           .withDefault(3306)
                                           .withImportance(Importance.HIGH)
-                                          .withDependents(DATABASE_LIST_NAME)
                                           .withValidation(Field::isInteger)
                                           .withDescription(""Port of the MySQL database server."");
 
@@ -166,7 +232,6 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                           .withType(Type.STRING)
                                           .withWidth(Width.SHORT)
                                           .withImportance(Importance.HIGH)
-                                          .withDependents(DATABASE_LIST_NAME)
                                           .withValidation(Field::isRequired)
                                           .withDescription(""Name of the MySQL database user to be used when connecting to the database."");
 
@@ -175,7 +240,6 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                               .withType(Type.PASSWORD)
                                               .withWidth(Width.SHORT)
                                               .withImportance(Importance.HIGH)
-                                              .withDependents(DATABASE_LIST_NAME)
                                               .withValidation(Field::isRequired)
                                               .withDescription(""Password of the MySQL database user to be used when connecting to the database."");
 
@@ -202,6 +266,49 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                                        + ""MySQL database cluster as another server (with this unique ID) so it can read ""
                                                        + ""the binlog. By default, a random number is generated between 5400 and 6400."");
 
+    public static final Field SSL_MODE = Field.create(""database.ssl.mode"")
+                                              .withDisplayName(""SSL mode"")
+                                              .withEnum(SecureConnectionMode.class, SecureConnectionMode.DISABLED)
+                                              .withWidth(Width.MEDIUM)
+                                              .withImportance(Importance.MEDIUM)
+                                              .withDescription(""Whether to use an encrypted connection to MySQL. Options include""
+                                                      + ""'disabled' (the default) to use an unencrypted connection; ""
+                                                      + ""'preferred' to establish a secure (encrypted) connection if the server supports secure connections, ""
+                                                      + ""but fall back to an unencrypted connection otherwise; ""
+                                                      + ""'required' to use a secure (encrypted) connection, and fail if one cannot be established; ""
+                                                      + ""'verify_ca' like 'required' but additionally verify the server TLS certificate against the configured Certificate Authority ""
+                                                      + ""(CA) certificates, or fail if no valid matching CA certificates are found; or""
+                                                      + ""'verify_identity' like 'verify_ca' but additionally verify that the server certificate matches the host to which the connection is attempted."");
+
+    public static final Field SSL_KEYSTORE = Field.create(""database.ssl.keystore"")
+                                                  .withDisplayName(""SSL Keystore"")
+                                                  .withType(Type.STRING)
+                                                  .withWidth(Width.LONG)
+                                                  .withImportance(Importance.MEDIUM)
+                                                  .withDescription(""Location of the Java keystore file containing an application process's own certificate and private key."");
+
+    public static final Field SSL_KEYSTORE_PASSWORD = Field.create(""database.ssl.keystore.password"")
+                                                           .withDisplayName(""SSL Keystore Password"")
+                                                           .withType(Type.PASSWORD)
+                                                           .withWidth(Width.MEDIUM)
+                                                           .withImportance(Importance.MEDIUM)
+                                                           .withDescription(""Password to access the private key from the keystore file specified by 'ssl.keystore' configuration property or the 'javax.net.ssl.keyStore' system or JVM property. ""
+                                                                   + ""This password is used to unlock the keystore file (store password), and to decrypt the private key stored in the keystore (key password)."");
+
+    public static final Field SSL_TRUSTSTORE = Field.create(""database.ssl.truststore"")
+                                                    .withDisplayName(""SSL Truststore"")
+                                                    .withType(Type.STRING)
+                                                    .withWidth(Width.LONG)
+                                                    .withImportance(Importance.MEDIUM)
+                                                    .withDescription(""Location of the Java truststore file containing the collection of CA certificates trusted by this application process (trust store)."");
+
+    public static final Field SSL_TRUSTSTORE_PASSWORD = Field.create(""database.ssl.truststore.password"")
+                                                             .withDisplayName(""SSL Truststore Password"")
+                                                             .withType(Type.PASSWORD)
+                                                             .withWidth(Width.MEDIUM)
+                                                             .withImportance(Importance.MEDIUM)
+                                                             .withDescription(""Password to unlock the keystore file (store password) specified by 'ssl.trustore' configuration property or the 'javax.net.ssl.trustStore' system or JVM property."");
+
     public static final Field TABLES_IGNORE_BUILTIN = Field.create(TABLE_IGNORE_BUILTIN_NAME)
                                                            .withDisplayName(""Ignore system databases"")
                                                            .withType(Type.BOOLEAN)
@@ -361,16 +468,15 @@ public static SnapshotMode parse(String value, String defaultValue) {
 
     public static final Field SNAPSHOT_MODE = Field.create(""snapshot.mode"")
                                                    .withDisplayName(""Snapshot mode"")
-                                                   .withEnum(SnapshotMode.class)
+                                                   .withEnum(SnapshotMode.class, SnapshotMode.INITIAL)
                                                    .withWidth(Width.SHORT)
                                                    .withImportance(Importance.LOW)
                                                    .withDescription(""The criteria for running a snapshot upon startup of the connector. ""
                                                            + ""Options include: ""
                                                            + ""'when_needed' to specify that the connector run a snapshot upon startup whenever it deems it necessary; ""
                                                            + ""'initial' (the default) to specify the connector can run a snapshot only when no offsets are available for the logical server name; and ""
                                                            + ""'never' to specify the connector should never run a snapshot and that upon first startup the connector should read from the beginning of the binlog. ""
-                                                           + ""The 'never' mode should be used with care, and only when the binlog is known to contain all history."")
-                                                   .withDefault(SnapshotMode.INITIAL.getValue());
+                                                           + ""The 'never' mode should be used with care, and only when the binlog is known to contain all history."");
 
     public static final Field SNAPSHOT_MINIMAL_LOCKING = Field.create(""snapshot.minimal.locks"")
                                                               .withDisplayName(""Use shortest database locking for snapshots"")
@@ -387,14 +493,13 @@ public static SnapshotMode parse(String value, String defaultValue) {
 
     public static final Field TIME_PRECISION_MODE = Field.create(""time.precision.mode"")
                                                          .withDisplayName(""Time Precision"")
-                                                         .withEnum(TemporalPrecisionMode.class)
+                                                         .withEnum(TemporalPrecisionMode.class, TemporalPrecisionMode.ADAPTIVE)
                                                          .withWidth(Width.SHORT)
                                                          .withImportance(Importance.MEDIUM)
                                                          .withDescription(""Time, date, and timestamps can be represented with different kinds of precisions, including:""
                                                                  + ""'adaptive' (the default) bases the precision of time, date, and timestamp values on the database column's precision; ""
                                                                  + ""'connect' always represents time, date, and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, ""
-                                                                 + ""which uses millisecond precision regardless of the database columns' precision ."")
-                                                         .withDefault(TemporalPrecisionMode.ADAPTIVE.getValue());
+                                                                 + ""which uses millisecond precision regardless of the database columns' precision ."");
 
     /**
      * Method that generates a Field for specifying that string columns whose names match a set of regular expressions should
@@ -438,7 +543,9 @@ public static final Field MASK_COLUMN(int length) {
                                                      TABLE_WHITELIST, TABLE_BLACKLIST, TABLES_IGNORE_BUILTIN,
                                                      DATABASE_WHITELIST, DATABASE_BLACKLIST,
                                                      COLUMN_BLACKLIST, SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING,
-                                                     TIME_PRECISION_MODE);
+                                                     TIME_PRECISION_MODE,
+                                                     SSL_MODE, SSL_KEYSTORE, SSL_KEYSTORE_PASSWORD,
+                                                     SSL_TRUSTSTORE, SSL_TRUSTSTORE_PASSWORD);
 
     /**
      * The set of {@link Field}s that are included in the {@link #configDef() configuration definition}. This includes
@@ -453,10 +560,11 @@ public static final Field MASK_COLUMN(int length) {
 
     protected static ConfigDef configDef() {
         ConfigDef config = new ConfigDef();
-        Field.group(config, ""MySQL"", HOSTNAME, PORT, USER, PASSWORD, SERVER_NAME, SERVER_ID);
-        Field.group(config, ""History Storage"", KafkaDatabaseHistory.BOOTSTRAP_SERVERS, KafkaDatabaseHistory.TOPIC,
-                    KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS, KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS,
-                    DATABASE_HISTORY);
+        Field.group(config, ""MySQL"", HOSTNAME, PORT, USER, PASSWORD, SERVER_NAME, SERVER_ID,
+                    SSL_MODE, SSL_KEYSTORE, SSL_KEYSTORE_PASSWORD, SSL_TRUSTSTORE, SSL_TRUSTSTORE_PASSWORD);
+        Field.group(config, ""History Storage"", KafkaDatabaseHistory.BOOTSTRAP_SERVERS,
+                    KafkaDatabaseHistory.TOPIC, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS,
+                    KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS, DATABASE_HISTORY);
         Field.group(config, ""Events"", INCLUDE_SCHEMA_CHANGES, TABLES_IGNORE_BUILTIN, DATABASE_WHITELIST, TABLE_WHITELIST,
                     COLUMN_BLACKLIST, TABLE_BLACKLIST, DATABASE_BLACKLIST);
         Field.group(config, ""Connector"", CONNECTION_TIMEOUT_MS, KEEP_ALIVE, MAX_QUEUE_SIZE, MAX_BATCH_SIZE, POLL_INTERVAL_MS,",2016-08-24T18:27:35Z,65
"@@ -6,9 +6,16 @@
 package io.debezium.connector.mysql;
 
 import java.sql.SQLException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.kafka.connect.errors.ConnectException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import io.debezium.config.Configuration;
+import io.debezium.config.Field;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
 
@@ -19,22 +26,25 @@
  */
 public class MySqlJdbcContext implements AutoCloseable {
 
-    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false"";
+    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false&useSSL=${useSSL}"";
     protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(MYSQL_CONNECTION_URL);
 
     protected final Logger logger = LoggerFactory.getLogger(getClass());
     protected final Configuration config;
     protected final JdbcConnection jdbc;
+    private final Map<String,String> originalSystemProperties = new HashMap<>();
 
     public MySqlJdbcContext(Configuration config) {
         this.config = config; // must be set before most methods are used
 
         // Set up the JDBC connection without actually connecting, with extra MySQL-specific properties
         // to give us better JDBC database metadata behavior ...
+        boolean useSSL = sslModeEnabled();
         Configuration jdbcConfig = config.subset(""database."", true)
                                          .edit()
                                          .with(""useInformationSchema"", ""true"")
                                          .with(""nullCatalogMeansCurrent"", ""false"")
+                                         .with(""useSSL"", Boolean.toString(useSSL))
                                          .build();
         this.jdbc = new JdbcConnection(jdbcConfig, FACTORY);
     }
@@ -67,23 +77,68 @@ public int port() {
         return config.getInteger(MySqlConnectorConfig.PORT);
     }
 
+    public SecureConnectionMode sslMode() {
+        String mode = config.getString(MySqlConnectorConfig.SSL_MODE);
+        return SecureConnectionMode.parse(mode);
+    }
+    
+    public boolean sslModeEnabled() {
+        return sslMode() != SecureConnectionMode.DISABLED;
+    }
+    
     public void start() {
+        if (sslModeEnabled()) {
+            originalSystemProperties.clear();
+            // Set the System properties for SSL for the MySQL driver ...
+            setSystemProperty(""javax.net.ssl.keyStore"", MySqlConnectorConfig.SSL_KEYSTORE, true);
+            setSystemProperty(""javax.net.ssl.keyStorePassword"", MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD, false);
+            setSystemProperty(""javax.net.ssl.trustStore"", MySqlConnectorConfig.SSL_TRUSTSTORE, true);
+            setSystemProperty(""javax.net.ssl.trustStorePassword"", MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD, false);
+        }
     }
 
     public void shutdown() {
         try {
             jdbc.close();
         } catch (SQLException e) {
             logger.error(""Unexpected error shutting down the database connection"", e);
+        } finally {
+            // Reset the system properties to their original value ...
+            originalSystemProperties.forEach((name,value)->{
+                if ( value != null ) {
+                    System.setProperty(name,value);
+                } else {
+                    System.clearProperty(name);
+                }
+            });
         }
     }
 
     @Override
     public void close() {
         shutdown();
     }
-    
+
     protected String connectionString() {
         return jdbc.connectionString(MYSQL_CONNECTION_URL);
     }
+
+    protected void setSystemProperty(String property, Field field, boolean showValueInError) {
+        String value = config.getString(field);
+        if (value != null) {
+            String existingValue = System.getProperty(property);
+            if (existingValue != null && existingValue.equalsIgnoreCase(value)) {
+                String msg = ""System or JVM property '"" + property + ""' is already defined, but the configuration property '"" + field.name()
+                        + ""' defines a different value"";
+                if (showValueInError) {
+                    msg = ""System or JVM property '"" + property + ""' is already defined as "" + existingValue
+                            + "", but the configuration property '"" + field.name() + ""' defines a different value '"" + value + ""'"";
+                }
+                throw new ConnectException(msg);
+            } else {
+                String existing = System.setProperty(property, value);
+                originalSystemProperties.put(property, existing); // the existing value may be null
+            }
+        }
+    }
 }",2016-08-24T18:27:35Z,29
"@@ -18,6 +18,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.data.KeyValueStore;
 import io.debezium.data.KeyValueStore.Collection;
 import io.debezium.data.SchemaChangeHistory;
@@ -99,14 +100,14 @@ protected Configuration.Builder simpleConfig() {
                             .with(MySqlConnectorConfig.PORT, port)
                             .with(MySqlConnectorConfig.USER, ""replicator"")
                             .with(MySqlConnectorConfig.PASSWORD, ""replpass"")
+                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                             .with(MySqlConnectorConfig.SERVER_ID, 18911)
                             .with(MySqlConnectorConfig.SERVER_NAME, LOGICAL_NAME)
                             .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
                             .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, false)
                             .with(MySqlConnectorConfig.DATABASE_WHITELIST, DB_NAME)
                             .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                            .with(""database.useSSL"", false); // eliminates MySQL driver warning about SSL connections
+                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH);
     }
 
     @Test",2016-08-24T18:27:35Z,24
"@@ -26,6 +26,7 @@ public class MySQLConnection extends JdbcConnection {
     public static MySQLConnection forTestDatabase(String databaseName) {
         return new MySQLConnection(JdbcConfiguration.copy(Configuration.fromSystemProperties(""database.""))
                                                     .withDatabase(databaseName)
+                                                    .with(""useSSL"", false)
                                                     .build());
     }
 
@@ -42,6 +43,7 @@ public static MySQLConnection forTestDatabase(String databaseName, String userna
                                                     .withDatabase(databaseName)
                                                     .withUser(username)
                                                     .withPassword(password)
+                                                    .with(""useSSL"", false)
                                                     .build());
     }
 ",2016-08-24T18:27:35Z,21
"@@ -24,6 +24,7 @@
 
 import io.debezium.config.Configuration;
 import io.debezium.config.Field.Recommender;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
 import io.debezium.embedded.AbstractConnectorTest;
@@ -106,26 +107,85 @@ public void shouldFailToValidateInvalidConfiguration() {
         assertNoConfigurationErrors(result, MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES);
         assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MODE);
         assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_MODE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE_PASSWORD);
         assertConfigurationErrors(result, KafkaDatabaseHistory.BOOTSTRAP_SERVERS);
         assertConfigurationErrors(result, KafkaDatabaseHistory.TOPIC);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS);
     }
 
+    @Test
+    public void shouldValidateValidConfigurationWithSSL() {
+        Configuration config = Configuration.create()
+                                            .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                                            .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                                            .with(MySqlConnectorConfig.USER, ""snapper"")
+                                            .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.REQUIRED.name().toLowerCase())
+                                            .with(MySqlConnectorConfig.SSL_KEYSTORE, ""/some/path/to/keystore"")
+                                            .with(MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD, ""keystore1234"")
+                                            .with(MySqlConnectorConfig.SSL_TRUSTSTORE, ""/some/path/to/truststore"")
+                                            .with(MySqlConnectorConfig.SSL_TRUSTSTORE_PASSWORD, ""truststore1234"")
+                                            .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                                            .with(MySqlConnectorConfig.SERVER_NAME, ""myServer"")
+                                            .with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS, ""some.host.com"")
+                                            .with(KafkaDatabaseHistory.TOPIC, ""my.db.history.topic"")
+                                            .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                                            .build();
+        MySqlConnector connector = new MySqlConnector();
+        Config result = connector.validate(config.asMap());
+
+        // Can't connect to MySQL using SSL on a container using the 'mysql/mysql-server' image maintained by MySQL team,
+        // but can actually connect to MySQL using SSL on a container using the 'mysql' image maintained by Docker, Inc.
+        assertConfigurationErrors(result, MySqlConnectorConfig.HOSTNAME, 0, 1);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.PORT);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.USER);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.PASSWORD);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SERVER_NAME);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SERVER_ID);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.TABLES_IGNORE_BUILTIN);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.DATABASE_WHITELIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.DATABASE_BLACKLIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.TABLE_WHITELIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.TABLE_BLACKLIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.COLUMN_BLACKLIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.CONNECTION_TIMEOUT_MS);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.KEEP_ALIVE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.MAX_QUEUE_SIZE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.MAX_BATCH_SIZE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.POLL_INTERVAL_MS);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.DATABASE_HISTORY);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MODE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_MODE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE_PASSWORD);
+        assertNoConfigurationErrors(result, KafkaDatabaseHistory.BOOTSTRAP_SERVERS);
+        assertNoConfigurationErrors(result, KafkaDatabaseHistory.TOPIC);
+        assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS);
+        assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS);
+    }
+
     @Test
     public void shouldValidateAcceptableConfiguration() {
         Configuration config = Configuration.create()
                                             .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
                                             .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                                             .with(MySqlConnectorConfig.USER, ""snapper"")
                                             .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                                             .with(MySqlConnectorConfig.SERVER_ID, 18765)
                                             .with(MySqlConnectorConfig.SERVER_NAME, ""myServer"")
                                             .with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS, ""some.host.com"")
                                             .with(KafkaDatabaseHistory.TOPIC, ""my.db.history.topic"")
                                             .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
-                                            .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL
-                                                                            // connections
                                             .build();
         MySqlConnector connector = new MySqlConnector();
         Config result = connector.validate(config.asMap());
@@ -151,6 +211,11 @@ public void shouldValidateAcceptableConfiguration() {
         assertNoConfigurationErrors(result, MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES);
         assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MODE);
         assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_MODE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE_PASSWORD);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.BOOTSTRAP_SERVERS);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.TOPIC);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS);
@@ -206,12 +271,12 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
                               .with(MySqlConnectorConfig.SERVER_ID, 18765)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""myServer"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
                               .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test"")
                               .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
@@ -430,6 +495,7 @@ public void shouldConsumeEventsWithNoSnapshot() throws SQLException, Interrupted
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18780)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""myServer1"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -438,7 +504,6 @@ public void shouldConsumeEventsWithNoSnapshot() throws SQLException, Interrupted
                               .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.name().toLowerCase())
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
 
         // Start the connector ...
@@ -479,6 +544,7 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18780)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""myServer2"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -488,7 +554,6 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
                               .with(MySqlConnectorConfig.MASK_COLUMN(12), ""connector_test_ro.customers.email"")
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, false)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
 
         // Start the connector ...",2016-08-24T18:27:35Z,88
"@@ -25,6 +25,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.connector.mysql.MySqlConnectorConfig.TemporalPrecisionMode;
 import io.debezium.data.Envelope;
@@ -68,6 +69,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18765)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -76,7 +78,6 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
@@ -188,6 +189,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18765)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -197,7 +199,6 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                               .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
                               .with(MySqlConnectorConfig.TIME_PRECISION_MODE, TemporalPrecisionMode.CONNECT.toString())
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
@@ -307,6 +308,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18765)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -315,7 +317,6 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.INITIAL.toString())
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);",2016-08-24T18:27:35Z,71
"@@ -16,6 +16,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.util.Testing;
@@ -69,12 +70,12 @@ protected Configuration.Builder simpleConfig() {
                             .with(MySqlConnectorConfig.PORT, port)
                             .with(MySqlConnectorConfig.USER, username)
                             .with(MySqlConnectorConfig.PASSWORD, password)
+                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                             .with(MySqlConnectorConfig.SERVER_ID, serverId)
                             .with(MySqlConnectorConfig.SERVER_NAME, serverName)
                             .with(MySqlConnectorConfig.DATABASE_WHITELIST, databaseName)
                             .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                            .with(""database.useSSL"",false); // eliminates MySQL driver warning about SSL connections
+                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH);
     }
 
     @Test",2016-08-24T18:27:35Z,25
"@@ -44,6 +44,7 @@
 import com.github.shyiko.mysql.binlog.event.WriteRowsEventData;
 import com.github.shyiko.mysql.binlog.event.XidEventData;
 import com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer;
+import com.github.shyiko.mysql.binlog.network.SSLMode;
 import com.github.shyiko.mysql.binlog.network.ServerException;
 
 import static org.fest.assertions.Assertions.assertThat;
@@ -105,6 +106,7 @@ protected void startClient(Consumer<BinaryLogClient> preConnect) throws IOExcept
         client = new BinaryLogClient(config.getHostname(), config.getPort(), ""replicator"", ""replpass"");
         client.setServerId(client.getServerId() - 1); // avoid clashes between BinaryLogClient instances
         client.setKeepAlive(false);
+        client.setSSLMode(SSLMode.DISABLED);
         client.registerEventListener(counters);
         client.registerEventListener(this::recordEvent);
         client.registerLifecycleListener(new TraceLifecycleListener());",2016-08-24T18:27:35Z,12
"@@ -20,6 +20,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.data.KeyValueStore;
 import io.debezium.data.KeyValueStore.Collection;
 import io.debezium.data.SchemaChangeHistory;
@@ -70,14 +71,14 @@ protected Configuration.Builder simpleConfig() {
                             .with(MySqlConnectorConfig.PORT, port)
                             .with(MySqlConnectorConfig.USER, ""snapper"")
                             .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.toString().toLowerCase())
                             .with(MySqlConnectorConfig.SERVER_ID, 18911)
                             .with(MySqlConnectorConfig.SERVER_NAME, LOGICAL_NAME)
                             .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
                             .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, false)
                             .with(MySqlConnectorConfig.DATABASE_WHITELIST, DB_NAME)
                             .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                            .with(""database.useSSL"",false); // eliminates MySQL driver warning about SSL connections
+                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH);
     }
 
     @Test",2016-08-24T18:27:35Z,15
"@@ -639,8 +639,26 @@ public Field withType(Type type) {
      * @return the new field; never null
      */
     public <T extends Enum<T>> Field withEnum(Class<T> enumType) {
+        return withEnum(enumType,null);
+    }
+
+    /**
+     * Create and return a new Field instance that is a copy of this field but has a {@link #withType(Type) type} of
+     * {@link org.apache.kafka.connect.data.Schema.Type#STRING}, a {@link #withRecommender(Recommender) recommender}
+     * that returns a list of {@link Enum#name() Enum names} as valid values, and a validator that verifies values are valid
+     * enumeration names.
+     * 
+     * @param enumType the enumeration type for the field
+     * @param defaultOption the default enumeration value; may be null
+     * @return the new field; never null
+     */
+    public <T extends Enum<T>> Field withEnum(Class<T> enumType, T defaultOption) {
         EnumRecommender<T> recommendator = new EnumRecommender<>(enumType);
-        return withType(Type.STRING).withRecommender(recommendator).withValidation(recommendator);
+        Field result = withType(Type.STRING).withRecommender(recommendator).withValidation(recommendator);
+        if ( defaultOption != null ) {
+            result = result.withDefault(defaultOption.name().toLowerCase());
+        }
+        return result;
     }
 
     /**",2016-08-24T18:27:35Z,5
"@@ -134,15 +134,22 @@ private static Field[] combineVariables(Field[] overriddenVariables,
 
     private static String findAndReplace(String url, Properties props, Field... variables) {
         for (Field field : variables) {
-            String variable = field.name();
-            if (variable != null && url.contains(""${"" + variable + ""}"")) {
-                // Otherwise, we have to remove it from the properties ...
-                String value = props.getProperty(variable);
-                if (value != null) {
-                    props.remove(variable);
-                    // And replace the variable ...
-                    url = url.replaceAll(""\\$\\{"" + variable + ""\\}"", value);
-                }
+            if ( field != null ) url = findAndReplace(url, field.name(), props);
+        }
+        for (Object key : new HashSet<>(props.keySet())) {
+            if (key != null ) url = findAndReplace(url, key.toString(), props);
+        }
+        return url;
+    }
+    
+    private static String findAndReplace(String url, String name, Properties props) {
+        if (name != null && url.contains(""${"" + name + ""}"")) {
+            // Otherwise, we have to remove it from the properties ...
+            String value = props.getProperty(name);
+            if (value != null) {
+                props.remove(name);
+                // And replace the variable ...
+                url = url.replaceAll(""\\$\\{"" + name + ""\\}"", value);
             }
         }
         return url;",2016-08-24T18:27:35Z,46
"@@ -5,6 +5,8 @@
  */
 package io.debezium.embedded;
 
+import static org.junit.Assert.fail;
+
 import java.nio.file.Path;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -510,6 +512,12 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
         assertThat(value.errorMessages().size()).isEqualTo(numErrors);
     }
 
+    protected void assertConfigurationErrors(Config config, io.debezium.config.Field field, int minErrorsInclusive, int maxErrorsInclusive) {
+        ConfigValue value = configValue(config, field.name());
+        assertThat(value.errorMessages().size()).isGreaterThanOrEqualTo(minErrorsInclusive);
+        assertThat(value.errorMessages().size()).isLessThanOrEqualTo(maxErrorsInclusive);
+    }
+
     protected void assertConfigurationErrors(Config config, io.debezium.config.Field field) {
         ConfigValue value = configValue(config, field.name());
         assertThat(value.errorMessages().size()).isGreaterThan(0);
@@ -518,7 +526,11 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
     protected void assertNoConfigurationErrors(Config config, io.debezium.config.Field... fields) {
         for (io.debezium.config.Field field : fields) {
             ConfigValue value = configValue(config, field.name());
-            assertThat(value.errorMessages().size()).isEqualTo(0);
+            if ( value != null ) {
+                if ( !value.errorMessages().isEmpty() ) {
+                    fail(""Error messages on field '"" + field.name() + ""': "" + value.errorMessages());
+                }
+            }
         }
     }
 ",2016-08-24T18:27:35Z,61
"@@ -64,7 +64,7 @@
         <version.postgresql.server>9.4</version.postgresql.server>
         <version.mysql.server>5.7</version.mysql.server>
         <version.mysql.driver>5.1.39</version.mysql.driver>
-        <version.mysql.binlog>0.3.3</version.mysql.binlog>
+        <version.mysql.binlog>0.4.0</version.mysql.binlog>
         <version.mongo.server>3.2.6</version.mongo.server>
         <version.mongo.driver>3.2.2</version.mongo.driver>
 ",2016-08-24T18:27:35Z,82
"@@ -146,6 +146,53 @@
                       </assembly>
                     </build>
                   </image>
+                  <image>
+                    <!-- A Docker image using a partial MySQL installation maintained by MySQL team. -->
+                    <name>debezium/mysql-server-gtids-test-databases</name>
+                    <alias>database</alias>
+                    <run>
+                      <namingStrategy>alias</namingStrategy>
+                      <env>
+                        <MYSQL_ROOT_PASSWORD>debezium-rocks</MYSQL_ROOT_PASSWORD>
+                        <MYSQL_DATABASE>mysql</MYSQL_DATABASE> <!-- database created upon init -->
+                        <MYSQL_USER>${database.user}</MYSQL_USER>
+                        <MYSQL_PASSWORD>${database.password}</MYSQL_PASSWORD>
+                      </env>
+                      <ports>
+                        <port>${database.port}:3306</port>
+                      </ports>
+                      <log>
+                        <prefix>mysql</prefix>
+                        <enabled>true</enabled>
+                        <color>yellow</color>
+                      </log>
+                      <wait>
+                        <log>MySQL init process done. Ready for start up.</log>
+                        <time>30000</time> <!-- 30 seconds max -->
+                      </wait>
+                    </run>
+                    <build>
+                      <from>mysql/mysql-server:${version.mysql.server}</from>
+                      <assembly>
+                        <inline>
+                          <fileSets>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/server-gtids</directory>
+                              <includes>
+                                <include>my.cnf</include>
+                              </includes>
+                              <outputDirectory>etc/mysql</outputDirectory>
+                            </fileSet>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/init</directory>
+                              <outputDirectory>docker-entrypoint-initdb.d</outputDirectory>
+                            </fileSet>
+                          </fileSets>
+                        </inline>
+                        <basedir>/</basedir>
+                      </assembly>
+                    </build>
+                  </image>
                   <image>
                     <!-- A Docker image using a complete MySQL installation maintained by Docker team. -->
                     <name>debezium/mysql-test--databases</name>
@@ -350,5 +397,22 @@
           <docker.skip>false</docker.skip>
         </properties>
       </profile>
+      <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+            Use the Docker image for MySQL configured to use GTIDs.
+            To use, specify ""-Dgtid-mysql"" or -Pgtid-mysql on the Maven command line.
+            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+      <profile>
+        <id>gtid-mysql</id>
+        <activation>
+          <activeByDefault>false</activeByDefault>
+          <property>
+            <name>gtid-mysql</name>
+          </property>
+        </activation>
+        <properties>
+          <docker.image>debezium/mysql-server-gtids-test-databases</docker.image>
+          <docker.skip>false</docker.skip>
+        </properties>
+      </profile>
     </profiles>
 </project>",2016-06-07T17:01:51Z,97
"@@ -88,6 +88,7 @@ protected void doStart() {
         eventHandlers.put(EventType.ROTATE, this::handleRotateLogsEvent);
         eventHandlers.put(EventType.TABLE_MAP, this::handleUpdateTableMetadata);
         eventHandlers.put(EventType.QUERY, this::handleQueryEvent);
+        eventHandlers.put(EventType.GTID, this::handleGtidEvent);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, this::handleInsert);
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
@@ -142,9 +143,10 @@ protected void ignoreEvent(Event event) {
     protected void handleEvent(Event event) {
         if (event == null) return;
 
-        // Update the source offset info ...
+        // Update the source offset info. Note that the client returns the value in *milliseconds*, even though the binlog
+        // contains only *seconds* precision ...
         EventHeader eventHeader = event.getHeader();
-        source.setBinlogTimestamp(eventHeader.getTimestamp());
+        source.setBinlogTimestampSeconds(eventHeader.getTimestamp()/1000L); // client returns milliseconds, we record seconds
         source.setBinlogServerId(eventHeader.getServerId());
         EventType eventType = eventHeader.getEventType();
         if (eventType == EventType.ROTATE) {
@@ -166,16 +168,6 @@ protected void handleEvent(Event event) {
                 source.setRowInEvent(0);
             }
         }
-        if (eventType == EventType.GTID) {
-            EventData eventData = event.getData();
-            GtidEventData gtidEventData;
-            if (eventData instanceof EventDeserializer.EventDataWrapper) {
-                gtidEventData = (GtidEventData) ((EventDeserializer.EventDataWrapper) eventData).getInternal();
-            } else {
-                gtidEventData = (GtidEventData) eventData;
-            }
-            source.setGtids(gtidEventData.getGtid());
-        }
 
         // If there is a handler for this event, forward the event to it ...
         try {
@@ -188,6 +180,15 @@ protected void handleEvent(Event event) {
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    protected <T extends EventData> T unwrapData(Event event) {
+        EventData eventData = event.getData();
+        if (eventData instanceof EventDeserializer.EventDataWrapper) {
+            eventData = ((EventDeserializer.EventDataWrapper) eventData).getInternal();
+        }
+        return (T)eventData;
+    }
+
     /**
      * Handle the supplied event that signals that mysqld has stopped.
      * 
@@ -226,19 +227,31 @@ protected void handleServerIncident(Event event) {
      */
     protected void handleRotateLogsEvent(Event event) {
         logger.debug(""Rotating logs: {}"", event);
-        RotateEventData command = event.getData();
+        RotateEventData command = unwrapData(event);
         assert command != null;
         recordMakers.clear();
     }
 
+    /**
+     * Handle the supplied event with a {@link GtidEventData} that signals the beginning of a GTID transaction.
+     * 
+     * @param event the GTID event to be processed; may not be null
+     */
+    protected void handleGtidEvent(Event event) {
+        logger.debug(""GTID transaction: {}"", event);
+        GtidEventData gtidEvent = unwrapData(event);
+        source.setGtid(gtidEvent.getGtid());
+        source.setGtidSet(client.getGtidSet());
+    }
+
     /**
      * Handle the supplied event with an {@link QueryEventData} by possibly recording the DDL statements as changes in the
      * MySQL schemas.
      * 
      * @param event the database change data event to be processed; may not be null
      */
     protected void handleQueryEvent(Event event) {
-        QueryEventData command = event.getData();
+        QueryEventData command = unwrapData(event);
         logger.debug(""Received update table command: {}"", event);
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords && recordMakers.schemaChanges(dbName, statements, super::enqueueRecord) > 0) {
@@ -262,7 +275,7 @@ protected void handleQueryEvent(Event event) {
      * @param event the update event; never null
      */
     protected void handleUpdateTableMetadata(Event event) {
-        TableMapEventData metadata = event.getData();
+        TableMapEventData metadata = unwrapData(event);
         long tableNumber = metadata.getTableId();
         String databaseName = metadata.getDatabase();
         String tableName = metadata.getTable();
@@ -281,7 +294,7 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
-        WriteRowsEventData write = event.getData();
+        WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);
@@ -302,7 +315,7 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
-        UpdateRowsEventData update = event.getData();
+        UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
         // BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
@@ -330,7 +343,7 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
-        DeleteRowsEventData deleted = event.getData();
+        DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);",2016-06-07T17:01:51Z,24
"@@ -7,6 +7,7 @@
 
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.TreeMap;
@@ -111,11 +112,17 @@ protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
                 intervals.add(new Interval(interval.getStart(), interval.getEnd()));
             });
             Collections.sort(this.intervals);
-        }
-
-        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
-            this.uuid = uuid;
-            this.intervals = intervals;
+            if ( this.intervals.size() > 1 ) {
+                // Collapse adjacent intervals ...
+                for ( int i=intervals.size()-1; i!=0; --i) {
+                    Interval before = this.intervals.get(i-1);
+                    Interval after = this.intervals.get(i);
+                    if ( (before.getEnd() + 1) == after.getStart() ) {
+                        this.intervals.set(i-1,new Interval(before.getStart(),after.getEnd()));
+                        this.intervals.remove(i);
+                    }
+                }
+            }
         }
 
         /**
@@ -218,17 +225,41 @@ public String toString() {
             StringBuilder sb = new StringBuilder();
             if (sb.length() != 0) sb.append(',');
             sb.append(uuid).append(':');
-            sb.append(intervals.getFirst().getStart());
-            sb.append(intervals.getLast().getEnd());
+            Iterator<Interval> iter = intervals.iterator();
+            if ( iter.hasNext() ) sb.append(iter.next());
+            while ( iter.hasNext() ) {
+                sb.append(':');
+                sb.append(iter.next());
+            }
             return sb.toString();
         }
     }
 
     @Immutable
-    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+    public static class Interval implements Comparable<Interval> {
+
+        private final long start;
+        private final long end;
 
         public Interval(long start, long end) {
-            super(start, end);
+            this.start = start;
+            this.end = end;
+        }
+
+        /**
+         * Get the starting transaction number in this interval.
+         * @return this interval's first transaction number
+         */
+        public long getStart() {
+            return start;
+        }
+
+        /**
+         * Get the ending transaction number in this interval.
+         * @return this interval's last transaction number
+         */
+        public long getEnd() {
+            return end;
         }
 
         /**
@@ -244,6 +275,15 @@ public boolean isSubsetOf(Interval other) {
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
         }
+        
+        @Override
+        public int compareTo(Interval that) {
+            if ( that == this ) return 0;
+            long diff = this.start - that.start;
+            if ( diff > Integer.MAX_VALUE ) return Integer.MAX_VALUE;
+            if ( diff < Integer.MIN_VALUE ) return Integer.MIN_VALUE;
+            return (int) diff;
+        }
 
         @Override
         public int hashCode() {",2016-06-07T17:01:51Z,101
"@@ -9,6 +9,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -110,9 +111,16 @@ public void start(Map<String, String> props) {
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;
+                // The snapshot will determine if GTIDs are set
             }
         }
 
+        if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
+            // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
+            // previously used but the MySQL server has them enabled ...
+            source.setGtidSet("""");
+        }
+
         // Set up the readers ...
         this.binlogReader = new BinlogReader(taskContext);
         if (startWithSnapshot) {
@@ -174,13 +182,33 @@ protected void transitionToReadBinlog() {
      * @return {@code true} if the server has the binlog coordinates, or {@code false} otherwise
      */
     protected boolean isBinlogAvailable() {
+        String gtidStr = taskContext.source().gtidSet();
+        if ( gtidStr != null) {
+            if ( gtidStr.trim().isEmpty() ) return true; // start at beginning ...
+            String availableGtidStr = knownGtidSet();
+            if ( availableGtidStr == null || availableGtidStr.trim().isEmpty() ) {
+                // Last offsets had GTIDs but the server does not use them ...
+                logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
+                return false;
+            }
+            // GTIDs are enabled, and we used them previously ...
+            GtidSet gtidSet = new GtidSet(gtidStr);
+            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+                return true;
+            }
+            logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);
+            return false;
+        }
+        
         String binlogFilename = taskContext.source().binlogFilename();
         if (binlogFilename == null) return true; // start at current position
         if (binlogFilename.equals("""")) return true; // start at beginning
 
         // Accumulate the available binlog filenames ...
         List<String> logNames = new ArrayList<>();
         try {
+            logger.info(""Stop 0: Get all known binlogs from MySQL"");
             taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
                 while (rs.next()) {
                     logNames.add(rs.getString(1));
@@ -191,6 +219,50 @@ protected boolean isBinlogAvailable() {
         }
 
         // And compare with the one we're supposed to use ...
-        return logNames.stream().anyMatch(binlogFilename::equals);
+        boolean found = logNames.stream().anyMatch(binlogFilename::equals);
+        if ( !found ) {
+            logger.info(""Connector requires binlog file '{}', but MySQL only has {}"",binlogFilename,String.join("", "",logNames));
+        }
+        return found;
+    }
+
+    /**
+     * Determine whether the MySQL server has GTIDs enabled.
+     * 
+     * @return {@code false} if the server's {@code gtid_mode} is set and is {@code OFF}, or {@code true} otherwise
+     */
+    protected boolean isGtidModeEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>(""off"");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'GTID_MODE'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return !""OFF"".equalsIgnoreCase(mode.get());
+    }
+
+    /**
+     * Determine the available GTID set for MySQL.
+     * 
+     * @return the string representation of MySQL's GTID sets.
+     */
+    protected String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return gtidSetStr.get();
     }
 }",2016-06-07T17:01:51Z,10
"@@ -188,7 +188,7 @@ protected void execute() {
                 if (rs.next()) {
                     source.setBinlogFilename(rs.getString(1));
                     source.setBinlogPosition(rs.getLong(2));
-                    source.setGtids(rs.getString(5));// GTIDs
+                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
                     source.startSnapshot();
                 }
             });",2016-06-07T17:01:51Z,15
"@@ -14,6 +14,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.data.Envelope;
 import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
@@ -26,7 +27,7 @@
  * 
  * <pre>
  * {
- *     ""db"" : ""myDatabase""
+ *     ""server"" : ""production-server""
  * }
  * </pre>
  * 
@@ -36,14 +37,40 @@
  * 
  * <pre>
  * {
- *         ""gtids"" = ""3E11FA47-71CA-11E1-9E33-C80AA9429562:1-5"",
- *         ""file"" = ""mysql-bin.000003"",
- *         ""pos"" = 105586,
- *         ""row"" = 0
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
+ *     ""file"" = ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"" = 0,
+ *     ""snapshot"": true
  * }
  * </pre>
  * 
- * Note that the ""{@code gtid}"" field is present only when GTIDs are enabled.
+ * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
+ * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
+ * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
+ * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
+ * 
+ * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
+ * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
+ * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
+ * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
+ * an event that corresponds to the above partition and offset:
+ * 
+ * <pre>
+ * {
+ *     ""name"": ""production-server"",
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"": 0,
+ *     ""snapshot"": true
+ * }
+ * </pre>
  * 
  * @author Randall Hauch
  */
@@ -56,11 +83,12 @@ final class SourceInfo {
 
     public static final String SERVER_NAME_KEY = ""name"";
     public static final String SERVER_PARTITION_KEY = ""server"";
-    public static final String GTID_KEY = ""gtids"";
+    public static final String GTID_SET_KEY = ""gtids"";
+    public static final String GTID_KEY = ""gtid"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
     public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
-    public static final String TIMESTAMP_KEY = ""ts"";
+    public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
     /**
@@ -78,13 +106,14 @@ final class SourceInfo {
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private GtidSet binlogGtids;
+    private String gtidSet;
+    private String binlogGtid;
     private String binlogFilename;
     private long binlogPosition = 4;
     private int eventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
-    private long binlogTs = 0;
+    private long binlogTimestampSeconds = 0;
     private Map<String, String> sourcePartition;
     private boolean snapshot = false;
 
@@ -122,11 +151,11 @@ public Map<String, String> partition() {
      */
     public Map<String, ?> offset() {
         Map<String, Object> map = new HashMap<>();
-        if (binlogGtids != null) {
-            map.put(GTID_KEY, binlogGtids.toString());
+        if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
+        if (gtidSet != null) {
+            map.put(GTID_SET_KEY, gtidSet);
         }
-        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTs != 0 ) map.put(TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -158,13 +187,14 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        if (binlogGtids != null) {
-            result.put(GTID_KEY, binlogGtids.toString());
+        // Don't put the GTID Set into the struct; only the current GTID is fine ...
+        if (binlogGtid != null) {
+            result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
-        result.put(TIMESTAMP_KEY, binlogTs);
+        result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
@@ -193,12 +223,23 @@ public boolean isSnapshotInEffect() {
     }
 
     /**
-     * Set the GTID range for the MySQL binary log file.
+     * Set the latest GTID from the MySQL binary log file.
+     * 
+     * @param gtid the string representation of a specific GTID; may not be null
+     */
+    public void setGtid(String gtid) {
+        this.binlogGtid = gtid;
+    }
+
+    /**
+     * Set the set of GTIDs known to the MySQL server.
      * 
-     * @param gtids the string representation of the binlog GTIDs; may not be null
+     * @param gtidSet the string representation of GTID set; may not be null
      */
-    public void setGtids(String gtids) {
-        this.binlogGtids = gtids != null && !gtids.trim().isEmpty() ? new GtidSet(gtids) : null;
+    public void setGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            this.gtidSet = gtidSet;
+        }
     }
 
     /**
@@ -239,12 +280,12 @@ public void setBinlogServerId(long serverId) {
     }
 
     /**
-     * Set the timestamp as found within the MySQL binary log file.
+     * Set the number of <em>seconds</em> since Unix epoch (January 1, 1970) as found within the MySQL binary log file.
      * 
-     * @param timestamp the timestamp found within the binary log file
+     * @param timestampInSeconds the timestamp in <em>seconds</em> found within the binary log file
      */
-    public void setBinlogTimestamp(long timestamp) {
-        this.binlogTs = timestamp;
+    public void setBinlogTimestampSeconds(long timestampInSeconds) {
+        this.binlogTimestampSeconds = timestampInSeconds / 1000;
     }
 
     /**
@@ -270,7 +311,7 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtids((String) sourceOffset.get(GTID_KEY)); // may be null
+            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
             binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
@@ -297,7 +338,7 @@ private long longOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.binlogGtids != null ? this.binlogGtids.toString() : null;
+        return this.gtidSet != null ? this.gtidSet.toString() : null;
     }
 
     /**
@@ -340,9 +381,9 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (binlogGtids != null) {
+        if (gtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(binlogGtids);
+            sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
             sb.append("", pos="").append(binlogPosition());
             sb.append("", row="").append(eventRowNumber());
@@ -379,17 +420,17 @@ public String toString() {
      * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
      */
     public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
-        String recordedGtidSetStr = recorded.getString(GTID_KEY);
-        String desiredGtidSetStr = desired.getString(GTID_KEY);
+        String recordedGtidSetStr = recorded.getString(GTID_SET_KEY);
+        String desiredGtidSetStr = desired.getString(GTID_SET_KEY);
         if (desiredGtidSetStr != null) {
             // The desired position uses GTIDs, so we ideally compare using GTIDs ...
             if (recordedGtidSetStr != null) {
                 // Both have GTIDs, so base the comparison entirely on the GTID sets.
                 GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
                 GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
-                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                if (recordedGtidSet.equals(desiredGtidSet)) {
                     // They are exactly the same, which means the recorded position exactly matches the desired ...
-                    if ( !recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
+                    if (!recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
@@ -414,35 +455,35 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         }
 
         // Both positions are missing GTIDs. Look at the servers ...
-        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        if ( recordedServerId != desiredServerId ) {
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        if (recordedServerId != desiredServerId) {
             // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
             // is compare timestamps, and we have to assume that the server timestamps can be compared ...
-            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
-            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
+            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
+            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
             return recordedTimestamp <= desiredTimestamp;
         }
-        
+
         // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
         // comparable ...
         String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
         String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
-        if ( diff > 0 ) return false;
-        
+        if (diff > 0) return false;
+
         // The positions are the same, so compare the row number ...
         int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The binlog coordinates are the same ...
         return true;",2016-06-07T17:01:51Z,11
"@@ -0,0 +1,52 @@
+# For advice on how to change settings please see
+# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
+
+[mysqld]
+#
+# Remove leading # and set to the amount of RAM for the most important data
+# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
+# innodb_buffer_pool_size = 128M
+#
+# Remove leading # to turn on a very important data integrity option: logging
+# changes to the binary log between backups.
+# log_bin
+#
+# Remove leading # to set options mainly useful for reporting servers.
+# The server defaults are faster for transactions and fast SELECTs.
+# Adjust sizes as needed, experiment to find the optimal values.
+# join_buffer_size = 128M
+# sort_buffer_size = 2M
+# read_rnd_buffer_size = 2M
+skip-host-cache
+skip-name-resolve
+datadir=/var/lib/mysql
+socket=/var/lib/mysql/mysql.sock
+secure-file-priv=/var/lib/mysql-files
+user=mysql
+
+# Disabling symbolic-links is recommended to prevent assorted security risks
+symbolic-links=0
+
+log-error=/var/log/mysqld.log
+pid-file=/var/run/mysqld/mysqld.pid
+
+# ----------------------------------------------
+# Enable GTIDs on this master
+# ----------------------------------------------
+gtid_mode                 = on
+enforce_gtid_consistency  = on
+
+# ----------------------------------------------
+# Debezium ingest
+# ----------------------------------------------
+
+# Enable binary replication log and set the prefix, expiration, and log format.
+# The prefix is arbitrary, expiration can be short for integration tests but would
+# be longer on a production system. Row-level info is required for ingest to work.
+# Server ID is required, but this will vary on production systems
+server-id         = 112233
+log_bin           = mysql-bin
+expire_logs_days  = 1
+binlog_format     = row
+
+",2016-06-07T17:01:51Z,117
"@@ -0,0 +1,107 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.connector.mysql.GtidSet.Interval;
+import io.debezium.connector.mysql.GtidSet.UUIDSet;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class GtidSetTest {
+    
+    private static final String UUID1 = ""24bc7850-2c16-11e6-a073-0242ac110002"";
+
+    private GtidSet gtids;
+    
+    @Test
+    public void shouldCreateSetWithSingleInterval() {
+        gtids = new GtidSet(UUID1 + "":1-191"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,191);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,1,191);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191"");
+    }
+    
+    @Test
+    public void shouldCollapseAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,199);
+        asertFirstInterval(UUID1,1,199);
+        asertLastInterval(UUID1,1,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199"");
+    }
+
+    
+    @Test
+    public void shouldNotCollapseNonAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199"");
+        asertIntervalCount(UUID1,2);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,193,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199:1000-1033"");
+        asertIntervalCount(UUID1,3);
+        asertFirstInterval(UUID1,1,191);
+        asertIntervalExists(UUID1,193,199);
+        asertLastInterval(UUID1,1000,1033);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199:1000-1033"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervalsThatMayBeAdjacent() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199:1000-1033:1035-1036:1038-1039"");
+        asertIntervalCount(UUID1, 4);
+        asertFirstInterval(UUID1, 1, 199);
+        asertIntervalExists(UUID1, 1000, 1033);
+        asertIntervalExists(UUID1, 1035, 1036);
+        asertLastInterval(UUID1, 1038, 1039);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199:1000-1033:1035-1036:1038-1039""); // ??
+    }
+    
+    protected void asertIntervalCount( String uuid, int count) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        assertThat(set.getIntervals().size()).isEqualTo(count);
+    }
+    
+    protected void asertIntervalExists( String uuid, int start, int end) {
+        assertThat(hasInterval(uuid,start,end)).isTrue();
+    }
+    
+    protected void asertFirstInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getFirstInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected void asertLastInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getLastInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected boolean hasInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        for ( Interval interval : set.getIntervals() ) {
+            if ( interval.getStart() == start && interval.getEnd() == end ) return true;
+        }
+        return false;
+    }
+
+}",2016-06-07T17:01:51Z,118
"@@ -86,9 +86,9 @@ protected Document positionWithGtids(String gtids) {
 
     protected Document positionWithGtids(String gtids, boolean snapshot) {
         if (snapshot) {
-            return Document.create(SourceInfo.GTID_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
+            return Document.create(SourceInfo.GTID_SET_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
         }
-        return Document.create(SourceInfo.GTID_KEY, gtids);
+        return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
     protected Document positionWithoutGtids(String filename, int position, int row) {",2016-06-07T17:01:51Z,11
"@@ -45,15 +45,18 @@ public static enum Operation {
          */
         DELETE(""d"");
         private final String code;
+
         private Operation(String code) {
             this.code = code;
         }
-        public static Operation forCode( String code ) {
-            for ( Operation op : Operation.values()) {
-                if ( op.code().equalsIgnoreCase(code)) return op;
+
+        public static Operation forCode(String code) {
+            for (Operation op : Operation.values()) {
+                if (op.code().equalsIgnoreCase(code)) return op;
             }
             return null;
         }
+
         public String code() {
             return code;
         }
@@ -81,11 +84,13 @@ public static final class FieldName {
          */
         public static final String SOURCE = ""source"";
         /**
-         * The {@code ts} field is used to store the information about the local time at which the connector processed/generated
-         * the event. Note that the accuracy of the timestamp is not defined, and the values may not always be monotonically
-         * increasing.
+         * The {@code ts_ms} field is used to store the information about the local time at which the connector
+         * processed/generated the event. The timestamp values are the number of milliseconds past epoch (January 1, 1970), and
+         * determined by the {@link System#currentTimeMillis() JVM current time in milliseconds}. Note that the <em>accuracy</em>
+         * of the timestamp value depends on the JVM's system clock and all of its assumptions, limitations, conditions, and
+         * variations.
          */
-        public static final String TIMESTAMP = ""ts"";
+        public static final String TIMESTAMP = ""ts_ms"";
     }
 
     /**
@@ -304,6 +309,7 @@ public Struct delete(Struct before, Struct source, Long timestamp) {
 
     /**
      * Obtain the operation for the given source record.
+     * 
      * @param record the source record; may not be null
      * @return the operation, or null if no valid operation was found in the record
      */",2016-06-07T17:01:51Z,119
"@@ -5,10 +5,12 @@
  */
 package io.debezium.jdbc;
 
+import java.time.Instant;
 import java.time.LocalDate;
 import java.time.LocalTime;
 import java.time.Month;
 import java.time.ZonedDateTime;
+import java.time.temporal.ChronoField;
 import java.util.Calendar;
 
 import org.junit.Before;
@@ -38,12 +40,13 @@ public void beforeEach() {
     public void shouldAdaptSqlDate() {
         // '2014-09-08', '17:51:04.777', '2014-09-08 17:51:04.777', '2014-09-08 17:51:04.777'
         java.sql.Date sqlDate = createSqlDate(2014, Month.SEPTEMBER, 8);
+        ZonedDateTime expectedDateInTargetTZ = ZonedDateTime.ofInstant(Instant.ofEpochMilli(sqlDate.getTime()), adapter.targetZoneId());
         ZonedDateTime zdt = adapter.toZonedDateTime(sqlDate);
         // The date should match ...
         LocalDate date = zdt.toLocalDate();
         assertThat(date.getYear()).isEqualTo(2014);
         assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
-        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        assertThat(date.getDayOfMonth()).isEqualTo(expectedDateInTargetTZ.get(ChronoField.DAY_OF_MONTH));
         // There should be no time component ...
         LocalTime time = zdt.toLocalTime();
         assertThat(time.getHour()).isEqualTo(0);",2016-07-26T11:17:31Z,126
"@@ -41,6 +41,7 @@ public class ZookeeperServer {
     private volatile File dataDir;
     private volatile File snapshotDir;
     private volatile File logDir;
+    private volatile ZooKeeperServer server;
 
     /**
      * Create a new server instance.
@@ -75,7 +76,8 @@ public synchronized ZookeeperServer startup() throws IOException {
         this.logDir.mkdirs();
 
         try {
-            factory.startup(new ZooKeeperServer(snapshotDir, logDir, tickTime));
+            server = new ZooKeeperServer(snapshotDir, logDir, tickTime); 
+            factory.startup(server);
             return this;
         } catch (InterruptedException e) {
             factory = null;
@@ -100,6 +102,12 @@ public synchronized void shutdown(boolean deleteData) {
         if (factory != null) {
             try {
                 factory.shutdown();
+                try {
+                    // Zookeeper 3.4.6 does not close the ZK DB during shutdown, so we must do this here to avoid file locks and open handles...
+                    server.getZKDatabase().close();
+                } catch (IOException e) {
+                    LOGGER.error(""Unable to close zookeeper DB"", e);
+                }
             } finally {
                 factory = null;
                 if (deleteData) {",2016-07-26T11:17:31Z,127
"@@ -146,6 +146,53 @@
                       </assembly>
                     </build>
                   </image>
+                  <image>
+                    <!-- A Docker image using a partial MySQL installation maintained by MySQL team. -->
+                    <name>debezium/mysql-server-gtids-test-databases</name>
+                    <alias>database</alias>
+                    <run>
+                      <namingStrategy>alias</namingStrategy>
+                      <env>
+                        <MYSQL_ROOT_PASSWORD>debezium-rocks</MYSQL_ROOT_PASSWORD>
+                        <MYSQL_DATABASE>mysql</MYSQL_DATABASE> <!-- database created upon init -->
+                        <MYSQL_USER>${database.user}</MYSQL_USER>
+                        <MYSQL_PASSWORD>${database.password}</MYSQL_PASSWORD>
+                      </env>
+                      <ports>
+                        <port>${database.port}:3306</port>
+                      </ports>
+                      <log>
+                        <prefix>mysql</prefix>
+                        <enabled>true</enabled>
+                        <color>yellow</color>
+                      </log>
+                      <wait>
+                        <log>MySQL init process done. Ready for start up.</log>
+                        <time>30000</time> <!-- 30 seconds max -->
+                      </wait>
+                    </run>
+                    <build>
+                      <from>mysql/mysql-server:${version.mysql.server}</from>
+                      <assembly>
+                        <inline>
+                          <fileSets>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/server-gtids</directory>
+                              <includes>
+                                <include>my.cnf</include>
+                              </includes>
+                              <outputDirectory>etc/mysql</outputDirectory>
+                            </fileSet>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/init</directory>
+                              <outputDirectory>docker-entrypoint-initdb.d</outputDirectory>
+                            </fileSet>
+                          </fileSets>
+                        </inline>
+                        <basedir>/</basedir>
+                      </assembly>
+                    </build>
+                  </image>
                   <image>
                     <!-- A Docker image using a complete MySQL installation maintained by Docker team. -->
                     <name>debezium/mysql-test--databases</name>
@@ -350,5 +397,22 @@
           <docker.skip>false</docker.skip>
         </properties>
       </profile>
+      <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+            Use the Docker image for MySQL configured to use GTIDs.
+            To use, specify ""-Dgtid-mysql"" or -Pgtid-mysql on the Maven command line.
+            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+      <profile>
+        <id>gtid-mysql</id>
+        <activation>
+          <activeByDefault>false</activeByDefault>
+          <property>
+            <name>gtid-mysql</name>
+          </property>
+        </activation>
+        <properties>
+          <docker.image>debezium/mysql-server-gtids-test-databases</docker.image>
+          <docker.skip>false</docker.skip>
+        </properties>
+      </profile>
     </profiles>
 </project>",2016-06-07T17:01:51Z,97
"@@ -88,6 +88,7 @@ protected void doStart() {
         eventHandlers.put(EventType.ROTATE, this::handleRotateLogsEvent);
         eventHandlers.put(EventType.TABLE_MAP, this::handleUpdateTableMetadata);
         eventHandlers.put(EventType.QUERY, this::handleQueryEvent);
+        eventHandlers.put(EventType.GTID, this::handleGtidEvent);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, this::handleInsert);
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
@@ -142,9 +143,10 @@ protected void ignoreEvent(Event event) {
     protected void handleEvent(Event event) {
         if (event == null) return;
 
-        // Update the source offset info ...
+        // Update the source offset info. Note that the client returns the value in *milliseconds*, even though the binlog
+        // contains only *seconds* precision ...
         EventHeader eventHeader = event.getHeader();
-        source.setBinlogTimestamp(eventHeader.getTimestamp());
+        source.setBinlogTimestampSeconds(eventHeader.getTimestamp()/1000L); // client returns milliseconds, we record seconds
         source.setBinlogServerId(eventHeader.getServerId());
         EventType eventType = eventHeader.getEventType();
         if (eventType == EventType.ROTATE) {
@@ -166,16 +168,6 @@ protected void handleEvent(Event event) {
                 source.setRowInEvent(0);
             }
         }
-        if (eventType == EventType.GTID) {
-            EventData eventData = event.getData();
-            GtidEventData gtidEventData;
-            if (eventData instanceof EventDeserializer.EventDataWrapper) {
-                gtidEventData = (GtidEventData) ((EventDeserializer.EventDataWrapper) eventData).getInternal();
-            } else {
-                gtidEventData = (GtidEventData) eventData;
-            }
-            source.setGtids(gtidEventData.getGtid());
-        }
 
         // If there is a handler for this event, forward the event to it ...
         try {
@@ -188,6 +180,15 @@ protected void handleEvent(Event event) {
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    protected <T extends EventData> T unwrapData(Event event) {
+        EventData eventData = event.getData();
+        if (eventData instanceof EventDeserializer.EventDataWrapper) {
+            eventData = ((EventDeserializer.EventDataWrapper) eventData).getInternal();
+        }
+        return (T)eventData;
+    }
+
     /**
      * Handle the supplied event that signals that mysqld has stopped.
      * 
@@ -226,19 +227,31 @@ protected void handleServerIncident(Event event) {
      */
     protected void handleRotateLogsEvent(Event event) {
         logger.debug(""Rotating logs: {}"", event);
-        RotateEventData command = event.getData();
+        RotateEventData command = unwrapData(event);
         assert command != null;
         recordMakers.clear();
     }
 
+    /**
+     * Handle the supplied event with a {@link GtidEventData} that signals the beginning of a GTID transaction.
+     * 
+     * @param event the GTID event to be processed; may not be null
+     */
+    protected void handleGtidEvent(Event event) {
+        logger.debug(""GTID transaction: {}"", event);
+        GtidEventData gtidEvent = unwrapData(event);
+        source.setGtid(gtidEvent.getGtid());
+        source.setGtidSet(client.getGtidSet());
+    }
+
     /**
      * Handle the supplied event with an {@link QueryEventData} by possibly recording the DDL statements as changes in the
      * MySQL schemas.
      * 
      * @param event the database change data event to be processed; may not be null
      */
     protected void handleQueryEvent(Event event) {
-        QueryEventData command = event.getData();
+        QueryEventData command = unwrapData(event);
         logger.debug(""Received update table command: {}"", event);
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords && recordMakers.schemaChanges(dbName, statements, super::enqueueRecord) > 0) {
@@ -262,7 +275,7 @@ protected void handleQueryEvent(Event event) {
      * @param event the update event; never null
      */
     protected void handleUpdateTableMetadata(Event event) {
-        TableMapEventData metadata = event.getData();
+        TableMapEventData metadata = unwrapData(event);
         long tableNumber = metadata.getTableId();
         String databaseName = metadata.getDatabase();
         String tableName = metadata.getTable();
@@ -281,7 +294,7 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
-        WriteRowsEventData write = event.getData();
+        WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);
@@ -302,7 +315,7 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
-        UpdateRowsEventData update = event.getData();
+        UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
         // BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
@@ -330,7 +343,7 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
-        DeleteRowsEventData deleted = event.getData();
+        DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);",2016-06-07T17:01:51Z,24
"@@ -7,6 +7,7 @@
 
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.TreeMap;
@@ -111,11 +112,17 @@ protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
                 intervals.add(new Interval(interval.getStart(), interval.getEnd()));
             });
             Collections.sort(this.intervals);
-        }
-
-        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
-            this.uuid = uuid;
-            this.intervals = intervals;
+            if ( this.intervals.size() > 1 ) {
+                // Collapse adjacent intervals ...
+                for ( int i=intervals.size()-1; i!=0; --i) {
+                    Interval before = this.intervals.get(i-1);
+                    Interval after = this.intervals.get(i);
+                    if ( (before.getEnd() + 1) == after.getStart() ) {
+                        this.intervals.set(i-1,new Interval(before.getStart(),after.getEnd()));
+                        this.intervals.remove(i);
+                    }
+                }
+            }
         }
 
         /**
@@ -218,17 +225,41 @@ public String toString() {
             StringBuilder sb = new StringBuilder();
             if (sb.length() != 0) sb.append(',');
             sb.append(uuid).append(':');
-            sb.append(intervals.getFirst().getStart());
-            sb.append(intervals.getLast().getEnd());
+            Iterator<Interval> iter = intervals.iterator();
+            if ( iter.hasNext() ) sb.append(iter.next());
+            while ( iter.hasNext() ) {
+                sb.append(':');
+                sb.append(iter.next());
+            }
             return sb.toString();
         }
     }
 
     @Immutable
-    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+    public static class Interval implements Comparable<Interval> {
+
+        private final long start;
+        private final long end;
 
         public Interval(long start, long end) {
-            super(start, end);
+            this.start = start;
+            this.end = end;
+        }
+
+        /**
+         * Get the starting transaction number in this interval.
+         * @return this interval's first transaction number
+         */
+        public long getStart() {
+            return start;
+        }
+
+        /**
+         * Get the ending transaction number in this interval.
+         * @return this interval's last transaction number
+         */
+        public long getEnd() {
+            return end;
         }
 
         /**
@@ -244,6 +275,15 @@ public boolean isSubsetOf(Interval other) {
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
         }
+        
+        @Override
+        public int compareTo(Interval that) {
+            if ( that == this ) return 0;
+            long diff = this.start - that.start;
+            if ( diff > Integer.MAX_VALUE ) return Integer.MAX_VALUE;
+            if ( diff < Integer.MIN_VALUE ) return Integer.MIN_VALUE;
+            return (int) diff;
+        }
 
         @Override
         public int hashCode() {",2016-06-07T17:01:51Z,101
"@@ -9,6 +9,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -110,9 +111,16 @@ public void start(Map<String, String> props) {
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;
+                // The snapshot will determine if GTIDs are set
             }
         }
 
+        if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
+            // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
+            // previously used but the MySQL server has them enabled ...
+            source.setGtidSet("""");
+        }
+
         // Set up the readers ...
         this.binlogReader = new BinlogReader(taskContext);
         if (startWithSnapshot) {
@@ -174,13 +182,33 @@ protected void transitionToReadBinlog() {
      * @return {@code true} if the server has the binlog coordinates, or {@code false} otherwise
      */
     protected boolean isBinlogAvailable() {
+        String gtidStr = taskContext.source().gtidSet();
+        if ( gtidStr != null) {
+            if ( gtidStr.trim().isEmpty() ) return true; // start at beginning ...
+            String availableGtidStr = knownGtidSet();
+            if ( availableGtidStr == null || availableGtidStr.trim().isEmpty() ) {
+                // Last offsets had GTIDs but the server does not use them ...
+                logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
+                return false;
+            }
+            // GTIDs are enabled, and we used them previously ...
+            GtidSet gtidSet = new GtidSet(gtidStr);
+            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+                return true;
+            }
+            logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);
+            return false;
+        }
+        
         String binlogFilename = taskContext.source().binlogFilename();
         if (binlogFilename == null) return true; // start at current position
         if (binlogFilename.equals("""")) return true; // start at beginning
 
         // Accumulate the available binlog filenames ...
         List<String> logNames = new ArrayList<>();
         try {
+            logger.info(""Stop 0: Get all known binlogs from MySQL"");
             taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
                 while (rs.next()) {
                     logNames.add(rs.getString(1));
@@ -191,6 +219,50 @@ protected boolean isBinlogAvailable() {
         }
 
         // And compare with the one we're supposed to use ...
-        return logNames.stream().anyMatch(binlogFilename::equals);
+        boolean found = logNames.stream().anyMatch(binlogFilename::equals);
+        if ( !found ) {
+            logger.info(""Connector requires binlog file '{}', but MySQL only has {}"",binlogFilename,String.join("", "",logNames));
+        }
+        return found;
+    }
+
+    /**
+     * Determine whether the MySQL server has GTIDs enabled.
+     * 
+     * @return {@code false} if the server's {@code gtid_mode} is set and is {@code OFF}, or {@code true} otherwise
+     */
+    protected boolean isGtidModeEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>(""off"");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'GTID_MODE'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return !""OFF"".equalsIgnoreCase(mode.get());
+    }
+
+    /**
+     * Determine the available GTID set for MySQL.
+     * 
+     * @return the string representation of MySQL's GTID sets.
+     */
+    protected String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return gtidSetStr.get();
     }
 }",2016-06-07T17:01:51Z,10
"@@ -188,7 +188,7 @@ protected void execute() {
                 if (rs.next()) {
                     source.setBinlogFilename(rs.getString(1));
                     source.setBinlogPosition(rs.getLong(2));
-                    source.setGtids(rs.getString(5));// GTIDs
+                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
                     source.startSnapshot();
                 }
             });",2016-06-07T17:01:51Z,15
"@@ -14,6 +14,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.data.Envelope;
 import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
@@ -26,7 +27,7 @@
  * 
  * <pre>
  * {
- *     ""db"" : ""myDatabase""
+ *     ""server"" : ""production-server""
  * }
  * </pre>
  * 
@@ -36,14 +37,40 @@
  * 
  * <pre>
  * {
- *         ""gtids"" = ""3E11FA47-71CA-11E1-9E33-C80AA9429562:1-5"",
- *         ""file"" = ""mysql-bin.000003"",
- *         ""pos"" = 105586,
- *         ""row"" = 0
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
+ *     ""file"" = ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"" = 0,
+ *     ""snapshot"": true
  * }
  * </pre>
  * 
- * Note that the ""{@code gtid}"" field is present only when GTIDs are enabled.
+ * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
+ * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
+ * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
+ * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
+ * 
+ * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
+ * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
+ * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
+ * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
+ * an event that corresponds to the above partition and offset:
+ * 
+ * <pre>
+ * {
+ *     ""name"": ""production-server"",
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"": 0,
+ *     ""snapshot"": true
+ * }
+ * </pre>
  * 
  * @author Randall Hauch
  */
@@ -56,11 +83,12 @@ final class SourceInfo {
 
     public static final String SERVER_NAME_KEY = ""name"";
     public static final String SERVER_PARTITION_KEY = ""server"";
-    public static final String GTID_KEY = ""gtids"";
+    public static final String GTID_SET_KEY = ""gtids"";
+    public static final String GTID_KEY = ""gtid"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
     public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
-    public static final String TIMESTAMP_KEY = ""ts"";
+    public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
     /**
@@ -78,13 +106,14 @@ final class SourceInfo {
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private GtidSet binlogGtids;
+    private String gtidSet;
+    private String binlogGtid;
     private String binlogFilename;
     private long binlogPosition = 4;
     private int eventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
-    private long binlogTs = 0;
+    private long binlogTimestampSeconds = 0;
     private Map<String, String> sourcePartition;
     private boolean snapshot = false;
 
@@ -122,11 +151,11 @@ public Map<String, String> partition() {
      */
     public Map<String, ?> offset() {
         Map<String, Object> map = new HashMap<>();
-        if (binlogGtids != null) {
-            map.put(GTID_KEY, binlogGtids.toString());
+        if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
+        if (gtidSet != null) {
+            map.put(GTID_SET_KEY, gtidSet);
         }
-        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTs != 0 ) map.put(TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -158,13 +187,14 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        if (binlogGtids != null) {
-            result.put(GTID_KEY, binlogGtids.toString());
+        // Don't put the GTID Set into the struct; only the current GTID is fine ...
+        if (binlogGtid != null) {
+            result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
-        result.put(TIMESTAMP_KEY, binlogTs);
+        result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
@@ -193,12 +223,23 @@ public boolean isSnapshotInEffect() {
     }
 
     /**
-     * Set the GTID range for the MySQL binary log file.
+     * Set the latest GTID from the MySQL binary log file.
+     * 
+     * @param gtid the string representation of a specific GTID; may not be null
+     */
+    public void setGtid(String gtid) {
+        this.binlogGtid = gtid;
+    }
+
+    /**
+     * Set the set of GTIDs known to the MySQL server.
      * 
-     * @param gtids the string representation of the binlog GTIDs; may not be null
+     * @param gtidSet the string representation of GTID set; may not be null
      */
-    public void setGtids(String gtids) {
-        this.binlogGtids = gtids != null && !gtids.trim().isEmpty() ? new GtidSet(gtids) : null;
+    public void setGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            this.gtidSet = gtidSet;
+        }
     }
 
     /**
@@ -239,12 +280,12 @@ public void setBinlogServerId(long serverId) {
     }
 
     /**
-     * Set the timestamp as found within the MySQL binary log file.
+     * Set the number of <em>seconds</em> since Unix epoch (January 1, 1970) as found within the MySQL binary log file.
      * 
-     * @param timestamp the timestamp found within the binary log file
+     * @param timestampInSeconds the timestamp in <em>seconds</em> found within the binary log file
      */
-    public void setBinlogTimestamp(long timestamp) {
-        this.binlogTs = timestamp;
+    public void setBinlogTimestampSeconds(long timestampInSeconds) {
+        this.binlogTimestampSeconds = timestampInSeconds / 1000;
     }
 
     /**
@@ -270,7 +311,7 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtids((String) sourceOffset.get(GTID_KEY)); // may be null
+            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
             binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
@@ -297,7 +338,7 @@ private long longOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.binlogGtids != null ? this.binlogGtids.toString() : null;
+        return this.gtidSet != null ? this.gtidSet.toString() : null;
     }
 
     /**
@@ -340,9 +381,9 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (binlogGtids != null) {
+        if (gtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(binlogGtids);
+            sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
             sb.append("", pos="").append(binlogPosition());
             sb.append("", row="").append(eventRowNumber());
@@ -379,17 +420,17 @@ public String toString() {
      * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
      */
     public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
-        String recordedGtidSetStr = recorded.getString(GTID_KEY);
-        String desiredGtidSetStr = desired.getString(GTID_KEY);
+        String recordedGtidSetStr = recorded.getString(GTID_SET_KEY);
+        String desiredGtidSetStr = desired.getString(GTID_SET_KEY);
         if (desiredGtidSetStr != null) {
             // The desired position uses GTIDs, so we ideally compare using GTIDs ...
             if (recordedGtidSetStr != null) {
                 // Both have GTIDs, so base the comparison entirely on the GTID sets.
                 GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
                 GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
-                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                if (recordedGtidSet.equals(desiredGtidSet)) {
                     // They are exactly the same, which means the recorded position exactly matches the desired ...
-                    if ( !recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
+                    if (!recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
@@ -414,35 +455,35 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         }
 
         // Both positions are missing GTIDs. Look at the servers ...
-        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        if ( recordedServerId != desiredServerId ) {
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        if (recordedServerId != desiredServerId) {
             // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
             // is compare timestamps, and we have to assume that the server timestamps can be compared ...
-            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
-            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
+            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
+            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
             return recordedTimestamp <= desiredTimestamp;
         }
-        
+
         // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
         // comparable ...
         String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
         String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
-        if ( diff > 0 ) return false;
-        
+        if (diff > 0) return false;
+
         // The positions are the same, so compare the row number ...
         int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The binlog coordinates are the same ...
         return true;",2016-06-07T17:01:51Z,11
"@@ -0,0 +1,52 @@
+# For advice on how to change settings please see
+# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
+
+[mysqld]
+#
+# Remove leading # and set to the amount of RAM for the most important data
+# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
+# innodb_buffer_pool_size = 128M
+#
+# Remove leading # to turn on a very important data integrity option: logging
+# changes to the binary log between backups.
+# log_bin
+#
+# Remove leading # to set options mainly useful for reporting servers.
+# The server defaults are faster for transactions and fast SELECTs.
+# Adjust sizes as needed, experiment to find the optimal values.
+# join_buffer_size = 128M
+# sort_buffer_size = 2M
+# read_rnd_buffer_size = 2M
+skip-host-cache
+skip-name-resolve
+datadir=/var/lib/mysql
+socket=/var/lib/mysql/mysql.sock
+secure-file-priv=/var/lib/mysql-files
+user=mysql
+
+# Disabling symbolic-links is recommended to prevent assorted security risks
+symbolic-links=0
+
+log-error=/var/log/mysqld.log
+pid-file=/var/run/mysqld/mysqld.pid
+
+# ----------------------------------------------
+# Enable GTIDs on this master
+# ----------------------------------------------
+gtid_mode                 = on
+enforce_gtid_consistency  = on
+
+# ----------------------------------------------
+# Debezium ingest
+# ----------------------------------------------
+
+# Enable binary replication log and set the prefix, expiration, and log format.
+# The prefix is arbitrary, expiration can be short for integration tests but would
+# be longer on a production system. Row-level info is required for ingest to work.
+# Server ID is required, but this will vary on production systems
+server-id         = 112233
+log_bin           = mysql-bin
+expire_logs_days  = 1
+binlog_format     = row
+
+",2016-06-07T17:01:51Z,117
"@@ -0,0 +1,107 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.connector.mysql.GtidSet.Interval;
+import io.debezium.connector.mysql.GtidSet.UUIDSet;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class GtidSetTest {
+    
+    private static final String UUID1 = ""24bc7850-2c16-11e6-a073-0242ac110002"";
+
+    private GtidSet gtids;
+    
+    @Test
+    public void shouldCreateSetWithSingleInterval() {
+        gtids = new GtidSet(UUID1 + "":1-191"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,191);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,1,191);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191"");
+    }
+    
+    @Test
+    public void shouldCollapseAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,199);
+        asertFirstInterval(UUID1,1,199);
+        asertLastInterval(UUID1,1,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199"");
+    }
+
+    
+    @Test
+    public void shouldNotCollapseNonAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199"");
+        asertIntervalCount(UUID1,2);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,193,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199:1000-1033"");
+        asertIntervalCount(UUID1,3);
+        asertFirstInterval(UUID1,1,191);
+        asertIntervalExists(UUID1,193,199);
+        asertLastInterval(UUID1,1000,1033);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199:1000-1033"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervalsThatMayBeAdjacent() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199:1000-1033:1035-1036:1038-1039"");
+        asertIntervalCount(UUID1, 4);
+        asertFirstInterval(UUID1, 1, 199);
+        asertIntervalExists(UUID1, 1000, 1033);
+        asertIntervalExists(UUID1, 1035, 1036);
+        asertLastInterval(UUID1, 1038, 1039);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199:1000-1033:1035-1036:1038-1039""); // ??
+    }
+    
+    protected void asertIntervalCount( String uuid, int count) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        assertThat(set.getIntervals().size()).isEqualTo(count);
+    }
+    
+    protected void asertIntervalExists( String uuid, int start, int end) {
+        assertThat(hasInterval(uuid,start,end)).isTrue();
+    }
+    
+    protected void asertFirstInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getFirstInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected void asertLastInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getLastInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected boolean hasInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        for ( Interval interval : set.getIntervals() ) {
+            if ( interval.getStart() == start && interval.getEnd() == end ) return true;
+        }
+        return false;
+    }
+
+}",2016-06-07T17:01:51Z,118
"@@ -86,9 +86,9 @@ protected Document positionWithGtids(String gtids) {
 
     protected Document positionWithGtids(String gtids, boolean snapshot) {
         if (snapshot) {
-            return Document.create(SourceInfo.GTID_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
+            return Document.create(SourceInfo.GTID_SET_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
         }
-        return Document.create(SourceInfo.GTID_KEY, gtids);
+        return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
     protected Document positionWithoutGtids(String filename, int position, int row) {",2016-06-07T17:01:51Z,11
"@@ -45,15 +45,18 @@ public static enum Operation {
          */
         DELETE(""d"");
         private final String code;
+
         private Operation(String code) {
             this.code = code;
         }
-        public static Operation forCode( String code ) {
-            for ( Operation op : Operation.values()) {
-                if ( op.code().equalsIgnoreCase(code)) return op;
+
+        public static Operation forCode(String code) {
+            for (Operation op : Operation.values()) {
+                if (op.code().equalsIgnoreCase(code)) return op;
             }
             return null;
         }
+
         public String code() {
             return code;
         }
@@ -81,11 +84,13 @@ public static final class FieldName {
          */
         public static final String SOURCE = ""source"";
         /**
-         * The {@code ts} field is used to store the information about the local time at which the connector processed/generated
-         * the event. Note that the accuracy of the timestamp is not defined, and the values may not always be monotonically
-         * increasing.
+         * The {@code ts_ms} field is used to store the information about the local time at which the connector
+         * processed/generated the event. The timestamp values are the number of milliseconds past epoch (January 1, 1970), and
+         * determined by the {@link System#currentTimeMillis() JVM current time in milliseconds}. Note that the <em>accuracy</em>
+         * of the timestamp value depends on the JVM's system clock and all of its assumptions, limitations, conditions, and
+         * variations.
          */
-        public static final String TIMESTAMP = ""ts"";
+        public static final String TIMESTAMP = ""ts_ms"";
     }
 
     /**
@@ -304,6 +309,7 @@ public Struct delete(Struct before, Struct source, Long timestamp) {
 
     /**
      * Obtain the operation for the given source record.
+     * 
      * @param record the source record; may not be null
      * @return the operation, or null if no valid operation was found in the record
      */",2016-06-07T17:01:51Z,119
"@@ -5,10 +5,12 @@
  */
 package io.debezium.connector.mysql;
 
+import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.Iterator;
 import java.util.LinkedList;
+import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
 
@@ -23,7 +25,6 @@
 @Immutable
 public final class GtidSet {
 
-    private final String orderedString;
     private final Map<String, UUIDSet> uuidSetsByServerId = new TreeMap<>(); // sorts on keys
 
     /**
@@ -38,7 +39,6 @@ public GtidSet(String gtids) {
             if (sb.length() != 0) sb.append(',');
             sb.append(uuidSet.toString());
         });
-        orderedString = sb.toString();
     }
 
     /**
@@ -67,34 +67,38 @@ public UUIDSet forServerWithId(String uuid) {
      * @return {@code true} if all of the GTIDs in this set are completely contained within the supplied set of GTIDs, or
      *         {@code false} otherwise
      */
-    public boolean isSubsetOf(GtidSet other) {
+    public boolean isContainedWithin(GtidSet other) {
         if (other == null) return false;
         if (this.equals(other)) return true;
         for (UUIDSet uuidSet : uuidSetsByServerId.values()) {
             UUIDSet thatSet = other.forServerWithId(uuidSet.getUUID());
-            if (!uuidSet.isSubsetOf(thatSet)) return false;
+            if (!uuidSet.isContainedWithin(thatSet)) return false;
         }
         return true;
     }
 
     @Override
     public int hashCode() {
-        return orderedString.hashCode();
+        return uuidSetsByServerId.keySet().hashCode();
     }
 
     @Override
     public boolean equals(Object obj) {
         if (obj == this) return true;
         if (obj instanceof GtidSet) {
             GtidSet that = (GtidSet) obj;
-            return this.orderedString.equalsIgnoreCase(that.orderedString);
+            return this.uuidSetsByServerId.equals(that.uuidSetsByServerId);
         }
         return false;
     }
 
     @Override
     public String toString() {
-        return orderedString;
+        List<String> gtids = new ArrayList<String>();
+        for (UUIDSet uuidSet : uuidSetsByServerId.values()) {
+            gtids.add(uuidSet.toString());
+        }
+        return String.join("","",gtids);
     }
 
     /**
@@ -139,37 +143,8 @@ public String getUUID() {
          * 
          * @return the immutable transaction intervals; never null
          */
-        public Collection<Interval> getIntervals() {
-            return Collections.unmodifiableCollection(intervals);
-        }
-
-        /**
-         * Get the first interval of transaction numbers for this server.
-         * 
-         * @return the first interval, or {@code null} if there is none
-         */
-        public Interval getFirstInterval() {
-            return intervals.isEmpty() ? null : intervals.getFirst();
-        }
-
-        /**
-         * Get the last interval of transaction numbers for this server.
-         * 
-         * @return the last interval, or {@code null} if there is none
-         */
-        public Interval getLastInterval() {
-            return intervals.isEmpty() ? null : intervals.getLast();
-        }
-
-        /**
-         * Get the interval that contains the full range (and possibly more) of all of the individual intervals for this server.
-         * 
-         * @return the complete interval comprised of the {@link Interval#getStart() start} of the {@link #getFirstInterval()
-         *         first interval} and the {@link Interval#getEnd() end} of the {@link #getLastInterval()}, or {@code null} if
-         *         this server has no intervals at all
-         */
-        public Interval getCompleteInterval() {
-            return intervals.isEmpty() ? null : new Interval(getFirstInterval().getStart(), getLastInterval().getEnd());
+        public List<Interval> getIntervals() {
+            return Collections.unmodifiableList(intervals);
         }
 
         /**
@@ -180,7 +155,7 @@ public Interval getCompleteInterval() {
          * @return {@code true} if this server's transaction numbers are a subset of the transaction numbers of the supplied set,
          *         or false otherwise
          */
-        public boolean isSubsetOf(UUIDSet other) {
+        public boolean isContainedWithin(UUIDSet other) {
             if (other == null) return false;
             if (!this.getUUID().equalsIgnoreCase(other.getUUID())) {
                 // Not even the same server ...
@@ -195,7 +170,7 @@ public boolean isSubsetOf(UUIDSet other) {
             for (Interval thisInterval : this.intervals) {
                 boolean found = false;
                 for (Interval otherInterval : other.intervals) {
-                    if (thisInterval.isSubsetOf(otherInterval)) {
+                    if (thisInterval.isContainedWithin(otherInterval)) {
                         found = true;
                         break;
                     }
@@ -270,7 +245,7 @@ public long getEnd() {
          *         {@link #getStart() start} and the {@link #getEnd() end} is less than or equal to the supplied interval's
          *         {@link #getEnd() end}, or {@code false} otherwise
          */
-        public boolean isSubsetOf(Interval other) {
+        public boolean isContainedWithin(Interval other) {
             if (other == this) return true;
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
@@ -293,16 +268,16 @@ public int hashCode() {
         @Override
         public boolean equals(Object obj) {
             if (this == obj) return true;
-            if (obj instanceof com.github.shyiko.mysql.binlog.GtidSet.Interval) {
-                com.github.shyiko.mysql.binlog.GtidSet.Interval that = (com.github.shyiko.mysql.binlog.GtidSet.Interval) obj;
+            if (obj instanceof Interval) {
+                Interval that = (Interval) obj;
                 return this.getStart() == that.getStart() && this.getEnd() == that.getEnd();
             }
             return false;
         }
 
         @Override
         public String toString() {
-            return getStart() == getEnd() ? Long.toString(getStart()) : """" + getStart() + ""-"" + getEnd();
+            return """" + getStart() + ""-"" + getEnd();
         }
     }
 }",2016-06-16T15:57:58Z,101
"@@ -194,7 +194,7 @@ protected boolean isBinlogAvailable() {
             // GTIDs are enabled, and we used them previously ...
             GtidSet gtidSet = new GtidSet(gtidStr);
             GtidSet availableGtidSet = new GtidSet(knownGtidSet());
-            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+            if ( gtidSet.isContainedWithin(availableGtidSet)) {
                 return true;
             }
             logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);",2016-06-16T15:57:58Z,10
"@@ -503,7 +503,7 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
                     return true;
                 }
                 // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
-                return recordedGtidSet.isSubsetOf(desiredGtidSet);
+                return recordedGtidSet.isContainedWithin(desiredGtidSet);
             }
             // The desired position did use GTIDs while the recorded did not use GTIDs. So, we assume that the
             // recorded position is older since GTIDs are often enabled but rarely disabled. And if they are disabled,",2016-06-16T15:57:58Z,11
"@@ -5,6 +5,8 @@
  */
 package io.debezium.connector.mysql;
 
+import java.util.LinkedList;
+
 import org.junit.Test;
 
 import static org.fest.assertions.Assertions.assertThat;
@@ -84,14 +86,14 @@ protected void asertIntervalExists( String uuid, int start, int end) {
     
     protected void asertFirstInterval( String uuid, int start, int end) {
         UUIDSet set = gtids.forServerWithId(uuid);
-        Interval interval = set.getFirstInterval();
+        Interval interval = set.getIntervals().iterator().next();
         assertThat(interval.getStart()).isEqualTo(start);
         assertThat(interval.getEnd()).isEqualTo(end);
     }
     
     protected void asertLastInterval( String uuid, int start, int end) {
         UUIDSet set = gtids.forServerWithId(uuid);
-        Interval interval = set.getLastInterval();
+        Interval interval = new LinkedList<>(set.getIntervals()).getLast();
         assertThat(interval.getStart()).isEqualTo(start);
         assertThat(interval.getEnd()).isEqualTo(end);
     }",2016-06-16T15:57:58Z,118
"@@ -268,6 +268,15 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                                       .withDescription(""Frequency in milliseconds to wait for new change events to appear after receiving no events. Defaults to 1 second (1000 ms)."")
                                                       .withDefault(TimeUnit.SECONDS.toMillis(1))
                                                       .withValidation(Field::isPositiveInteger);
+    
+    public static final Field ROW_COUNT_FOR_STREAMING_RESULT_SETS = Field.create(""min.row.count.to.stream.results"")
+                                                                         .withDisplayName(""Stream result set larger than"")
+                                                                         .withType(Type.LONG)
+                                                                         .withWidth(Width.MEDIUM)
+                                                                         .withImportance(Importance.LOW)
+                                                                         .withDescription(""The number of rows a table must contain to stream results rather than pull all into memory during snapshots. Defaults to 1,000."")
+                                                                         .withDefault(1_000)
+                                                                         .withValidation(Field::isPositiveInteger);
 
     /**
      * The database history class is hidden in the {@link #configDef()} since that is designed to work with a user interface,",2016-08-04T21:06:50Z,65
"@@ -102,6 +102,10 @@ public long timeoutInMilliseconds() {
     public long pollIntervalInMillseconds() {
         return config.getLong(MySqlConnectorConfig.POLL_INTERVAL_MS);
     }
+    
+    public long rowCountForLargeTable() {
+        return config.getLong(MySqlConnectorConfig.ROW_COUNT_FOR_STREAMING_RESULT_SETS);
+    }
 
     public boolean includeSchemaChangeRecords() {
         return config.getBoolean(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES);",2016-08-04T21:06:50Z,87
"@@ -80,6 +80,20 @@ public RecordsForTable forTable(TableId tableId, BitSet includedColumns, Blockin
         return tableNumber != null ? forTable(tableNumber, includedColumns, consumer) : null;
     }
 
+    /**
+     * Determine if there is a record maker for the given table.
+     * 
+     * @param tableId the identifier of the table
+     * @return {@code true} if there is a {@link #forTable(TableId, BitSet, BlockingConsumer) record maker}, or {@code false}
+     * if there is none
+     */
+    public boolean hasTable(TableId tableId) {
+        Long tableNumber = tableNumbersByTableId.get(tableId);
+        if ( tableNumber == null ) return false;
+        Converter converter = convertersByTableNumber.get(tableNumber);
+        return converter != null;
+    }
+
     /**
      * Obtain the record maker for the given table, using the specified columns and sending records to the given consumer.
      * ",2016-08-04T21:06:50Z,128
"@@ -5,7 +5,10 @@
  */
 package io.debezium.connector.mysql;
 
+import java.sql.Connection;
+import java.sql.ResultSet;
 import java.sql.SQLException;
+import java.sql.Statement;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -14,11 +17,16 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
 
+import org.apache.kafka.connect.source.SourceRecord;
+
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
+import io.debezium.function.BufferedBlockingConsumer;
 import io.debezium.function.Predicates;
 import io.debezium.jdbc.JdbcConnection;
+import io.debezium.jdbc.JdbcConnection.StatementFactory;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.util.Clock;
@@ -140,8 +148,8 @@ protected void execute() {
         final Clock clock = context.clock();
         final long ts = clock.currentTimeInMillis();
         logger.info(""Starting snapshot for {} with user '{}'"", context.connectionString(), mysql.username());
-        logRolesForCurrentUser(sql, mysql);
-        logServerInformation(sql, mysql);
+        logRolesForCurrentUser(mysql);
+        logServerInformation(mysql);
         try {
             // ------
             // STEP 0
@@ -222,15 +230,15 @@ protected void execute() {
                 }
             });
             logger.debug(""\t list of available databases is: {}"", databaseNames);
-           
+
             // ------
             // STEP 5
             // ------
             // Get the list of table IDs for each database. We can't use a prepared statement with MySQL, so we have to
             // build the SQL statement each time. Although in other cases this might lead to SQL injection, in our case
             // we are reading the database names from the database and not taking them from the user ...
             logger.info(""Step 5: read list of available tables in each database"");
-            final List<TableId> tableIds = new ArrayList<>();
+            List<TableId> tableIds = new ArrayList<>();
             final Map<String, List<TableId>> tableIdsByDbName = new HashMap<>();
             for (String dbName : databaseNames) {
                 sql.set(""SHOW TABLES IN "" + dbName);
@@ -301,28 +309,50 @@ protected void execute() {
                 mysql.execute(sql.get());
                 unlocked = true;
                 long lockReleased = clock.currentTimeInMillis();
-                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased - lockAcquired));
+                logger.info(""Step 7: blocked writes to MySQL for a total of {}"", Strings.duration(lockReleased - lockAcquired));
             }
 
             // ------
             // STEP 8
             // ------
+            // Use a buffered blocking consumer to buffer all of the records, so that after we copy all of the tables
+            // and produce events we can update the very last event with the non-snapshot offset ...
+            BufferedBlockingConsumer<SourceRecord> bufferedRecordQueue = BufferedBlockingConsumer.bufferLast(super::enqueueRecord);
+
             // Dump all of the tables and generate source records ...
             logger.info(""Step 8: scanning contents of {} tables"", tableIds.size());
             long startScan = clock.currentTimeInMillis();
             AtomicBoolean interrupted = new AtomicBoolean(false);
+            AtomicLong totalRowCount = new AtomicLong();
             int counter = 0;
+            int completedCounter = 0;
+            long largeTableCount = context.rowCountForLargeTable();
             Iterator<TableId> tableIdIter = tableIds.iterator();
             while (tableIdIter.hasNext()) {
                 TableId tableId = tableIdIter.next();
-                boolean isLastTable = tableIdIter.hasNext() == false;
-                long start = clock.currentTimeInMillis();
-                logger.debug(""Step 8.{}: scanning table '{}'; {} tables remain"", ++counter, tableId, tableIds.size() - counter);
-                sql.set(""SELECT * FROM "" + tableId);
-                mysql.query(sql.get(), rs -> {
-                    RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, super::enqueueRecord);
-                    if (recordMaker != null) {
-                        boolean completed = false;
+
+                // Obtain a record maker for this table, which knows about the schema ...
+                RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, bufferedRecordQueue);
+                if (recordMaker != null) {
+
+                    // Choose how we create statements based on the # of rows ...
+                    sql.set(""SELECT COUNT(*) FROM "" + tableId);
+                    AtomicLong numRows = new AtomicLong();
+                    mysql.query(sql.get(), rs -> {
+                        if (rs.next()) numRows.set(rs.getLong(1));
+                    });
+                    StatementFactory statementFactory = this::createStatement;
+                    if (numRows.get() > largeTableCount) {
+                        statementFactory = this::createStatementWithLargeResultSet;
+                    }
+
+                    // Scan the rows in the table ...
+                    long start = clock.currentTimeInMillis();
+                    logger.debug(""Step 8: - scanning table '{}' ({} of {} tables)"", tableId, ++counter, tableIds.size());
+                    sql.set(""SELECT * FROM "" + tableId);
+                    mysql.query(sql.get(), statementFactory, rs -> {
+                        long rowNum = 0;
+                        long rowCount = numRows.get();
                         try {
                             // The table is included in the connector's filters, so process all of the table records ...
                             final Table table = schema.tableFor(tableId);
@@ -332,37 +362,52 @@ protected void execute() {
                                 for (int i = 0, j = 1; i != numColumns; ++i, ++j) {
                                     row[i] = rs.getObject(j);
                                 }
-                                if (isLastTable && rs.isLast()) {
-                                    // This is the last record, so mark the offset as having completed the snapshot
-                                    // but the SourceInfo.struct() will still be marked as being part of the snapshot ...
-                                    source.markLastSnapshot();
-                                }
                                 recorder.recordRow(recordMaker, row, ts); // has no row number!
+                                ++rowNum;
+                                if (rowNum % 10_000 == 0 || rowNum == rowCount) {
+                                    long stop = clock.currentTimeInMillis();
+                                    logger.info(""Step 8: - {} of {} rows scanned from table '{}' after {}"", rowNum, rowCount, tableId,
+                                                Strings.duration(stop - start));
+                                }
                             }
-                            if (isLastTable) completed = true;
                         } catch (InterruptedException e) {
                             Thread.interrupted();
-                            if (!completed) {
-                                // We were not able to finish all rows in all tables ...
-                                logger.info(""Stopping the snapshot after thread interruption"");
-                                interrupted.set(true);
-                            }
+                            // We were not able to finish all rows in all tables ...
+                            logger.info(""Step 8: Stopping the snapshot due to thread interruption"");
+                            interrupted.set(true);
+                        } finally {
+                            totalRowCount.addAndGet(rowCount);
                         }
-                    }
-                });
-                if (interrupted.get()) break;
-                long stop = clock.currentTimeInMillis();
-                logger.info(""Step 8.{}: scanned table '{}' in {}"", counter, tableId, Strings.duration(stop - start));
+                    });
+
+                    if (interrupted.get()) break;
+                }
+                ++completedCounter;
             }
+
+            // We've copied all of the tables, but our buffer holds onto the very last record.
+            // First mark the snapshot as complete and then apply the updated offset to the buffered record ...
+            source.markLastSnapshot();
             long stop = clock.currentTimeInMillis();
-            logger.info(""Step 8: scanned contents of {} tables in {}"", tableIds.size(), Strings.duration(stop - startScan));
+            try {
+                bufferedRecordQueue.flush(this::replaceOffset);
+                logger.info(""Step 8: scanned {} rows in {} tables in {}"",
+                            totalRowCount, tableIds.size(), Strings.duration(stop - startScan));
+            } catch (InterruptedException e) {
+                Thread.interrupted();
+                // We were not able to finish all rows in all tables ...
+                logger.info(""Step 8: aborting the snapshot after {} rows in {} of {} tables {}"",
+                            totalRowCount, completedCounter, tableIds.size(), Strings.duration(stop - startScan));
+                interrupted.set(true);
+            }
 
             // ------
             // STEP 9
             // ------
             // Release the read lock if we have not yet done so ...
+            int step = 9;
             if (!unlocked) {
-                logger.info(""Step 9: releasing global read lock to enable MySQL writes"");
+                logger.info(""Step {}: releasing global read lock to enable MySQL writes"", step++);
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
@@ -375,13 +420,13 @@ protected void execute() {
             // -------
             if (interrupted.get()) {
                 // We were interrupted while reading the tables, so roll back the transaction and return immediately ...
-                logger.info(""Step 10: rolling back transaction after request to stop"");
+                logger.info(""Step {}: rolling back transaction after abort"", step++);
                 sql.set(""ROLLBACK"");
                 mysql.execute(sql.get());
                 return;
             }
             // Otherwise, commit our transaction
-            logger.info(""Step 10: committing transaction"");
+            logger.info(""Step {}: committing transaction"", step++);
             sql.set(""COMMIT"");
             mysql.execute(sql.get());
 
@@ -399,35 +444,93 @@ protected void execute() {
             failed(e, ""Aborting snapshot after running '"" + sql.get() + ""': "" + e.getMessage());
         }
     }
-    
-    private void logServerInformation(AtomicReference<String> sql, JdbcConnection mysql) {
+
+    /**
+     * Create a JDBC statement that can be used for large result sets.
+     * <p>
+     * By default, the MySQL Connector/J driver retrieves all rows for ResultSets and stores them in memory. In most cases this
+     * is the most efficient way to operate and, due to the design of the MySQL network protocol, is easier to implement.
+     * However, when ResultSets that have a large number of rows or large values, the driver may not be able to allocate
+     * heap space in the JVM and may result in an {@link OutOfMemoryError}. See
+     * <a href=""https://issues.jboss.org/browse/DBZ-94"">DBZ-94</a> for details.
+     * <p>
+     * This method handles such cases using the
+     * <a href=""https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-reference-implementation-notes.html"">recommended
+     * technique</a> for MySQL by creating the JDBC {@link Statement} with {@link ResultSet#TYPE_FORWARD_ONLY forward-only} cursor
+     * and {@link ResultSet#CONCUR_READ_ONLY read-only concurrency} flags, and with a {@link Integer#MIN_VALUE minimum value}
+     * {@link Statement#setFetchSize(int) fetch size hint}.
+     * 
+     * @param connection the JDBC connection; may not be null
+     * @return the statement; never null
+     * @throws SQLException if there is a problem creating the statement
+     */
+    private Statement createStatementWithLargeResultSet(Connection connection) throws SQLException {
+        Statement stmt = connection.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
+        stmt.setFetchSize(Integer.MIN_VALUE);
+        return stmt;
+    }
+
+    private Statement createStatement(Connection connection) throws SQLException {
+        return connection.createStatement();
+    }
+
+    private void logServerInformation(JdbcConnection mysql) {
         try {
-            sql.set(""SHOW VARIABLES LIKE 'version'"");
-            mysql.query(sql.get(), rs -> {
-                if (rs.next()) {
-                    logger.info(""MySql server version is '{}'"", rs.getString(2));        
+            logger.info(""MySQL server variables related to change data capture:"");
+            mysql.query(""SHOW VARIABLES WHERE Variable_name REGEXP 'version|binlog|tx_|gtid'"", rs -> {
+                while (rs.next()) {
+                    logger.info(""\t{} = {}"",
+                                Strings.pad(rs.getString(1), 45, ' '),
+                                Strings.pad(rs.getString(2), 45, ' '));
                 }
             });
         } catch (SQLException e) {
             logger.info(""Cannot determine MySql server version"", e);
-        }       
+        }
     }
 
-    private void logRolesForCurrentUser(AtomicReference<String> sql, JdbcConnection mysql) {
+    private void logRolesForCurrentUser(JdbcConnection mysql) {
         try {
-            List<String> privileges = new ArrayList<>();
-            sql.set(""SHOW GRANTS"");
-            mysql.query(sql.get(), rs -> {
+            List<String> grants = new ArrayList<>();
+            mysql.query(""SHOW GRANTS FOR CURRENT_USER"", rs -> {
                 while (rs.next()) {
-                    privileges.add(rs.getString(1));
+                    grants.add(rs.getString(1));
                 }
             });
-            logger.info(""User '{}' has '{}'"", mysql.username(), privileges);
+            if (grants.isEmpty()) {
+                logger.warn(""Snapshot is using user '{}' but it likely doesn't have proper privileges. "" +
+                        ""If tables are missing or are empty, ensure connector is configured with the correct MySQL user "" +
+                        ""and/or ensure that the MySQL user has the required privileges."",
+                            mysql.username());
+            } else {
+                logger.info(""Snapshot is using user '{}' with these MySQL grants:"", mysql.username());
+                grants.forEach(grant -> logger.info(""\t{}"", grant));
+            }
         } catch (SQLException e) {
             logger.info(""Cannot determine the privileges for '{}' "", mysql.username(), e);
         }
     }
 
+    /**
+     * Utility method to replace the offset in the given record with the latest. This is used on the last record produced
+     * during the snapshot.
+     * 
+     * @param record the record
+     * @return the updated record
+     */
+    protected SourceRecord replaceOffset(SourceRecord record) {
+        if (record == null) return null;
+        Map<String, ?> newOffset = context.source().offset();
+        return new SourceRecord(record.sourcePartition(),
+                newOffset,
+                record.topic(),
+                record.kafkaPartition(),
+                record.keySchema(),
+                record.key(),
+                record.valueSchema(),
+                record.value());
+    }
+
     protected void enqueueSchemaChanges(String dbName, String ddlStatements) {
         if (context.includeSchemaChangeRecords() &&
                 context.makeRecord().schemaChanges(dbName, ddlStatements, super::enqueueRecord) > 0) {",2016-08-04T21:06:50Z,15
"@@ -0,0 +1,71 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.function;
+
+import java.util.function.Function;
+
+/**
+ * A {@link BlockingConsumer} that retains a maximum number of values in a buffer before sending them to
+ * a delegate consumer. Note that any buffered values may need to be {@link #flush() flushed} periodically.
+ * <p>
+ * This maintains the same order of the values.
+ * 
+ * @param <T> the type of the input to the operation
+ * @author Randall Hauch
+ */
+public interface BufferedBlockingConsumer<T> extends BlockingConsumer<T> {
+
+    /**
+     * Flush all of the buffered values to the delegate.
+     * 
+     * @throws InterruptedException if the thread is interrupted while this consumer is blocked
+     */
+    public default void flush() throws InterruptedException {
+        flush(t -> t);
+    }
+
+    /**
+     * Flush all of the buffered values to the delegate by first running each buffered value through the given function
+     * to generate a new value to be flushed to the delegate consumer.
+     * 
+     * @param function the function to apply to the values that are flushed
+     * @throws InterruptedException if the thread is interrupted while this consumer is blocked
+     */
+    public void flush(Function<T, T> function) throws InterruptedException;
+
+    /**
+     * Get a {@link BufferedBlockingConsumer} that buffers just the last value seen by the consumer.
+     * When another value is then added to the consumer, this buffered consumer will push the prior value into the delegate
+     * and buffer the latest.
+     * <p>
+     * The resulting consumer is not threadsafe.
+     * 
+     * @param delegate the delegate to which values should be flushed; may not be null
+     * @return the blocking consumer that buffers a single value at a time; never null
+     */
+    public static <T> BufferedBlockingConsumer<T> bufferLast(BlockingConsumer<T> delegate) {
+        return new BufferedBlockingConsumer<T>() {
+            private T last;
+
+            @Override
+            public void accept(T t) throws InterruptedException {
+                if (last != null) delegate.accept(last);
+                last = t;
+            }
+
+            @Override
+            public void flush(Function<T, T> function) throws InterruptedException {
+                if (last != null) {
+                    try {
+                        delegate.accept(function.apply(last));
+                    } finally {
+                        last = null;
+                    }
+                }
+            }
+        };
+    }
+}",2016-08-04T21:06:50Z,129
"@@ -27,8 +27,10 @@
 import java.util.concurrent.ConcurrentMap;
 import java.util.function.Consumer;
 import java.util.stream.Stream;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import io.debezium.annotation.ThreadSafe;
 import io.debezium.config.Configuration;
 import io.debezium.config.Field;
@@ -275,8 +277,22 @@ public static interface StatementPreparer {
      * @see #execute(Operations)
      */
     public JdbcConnection query(String query, ResultSetConsumer resultConsumer) throws SQLException {
+        return query(query,conn->conn.createStatement(),resultConsumer);
+    }
+
+    /**
+     * Execute a SQL query.
+     * 
+     * @param query the SQL query
+     * @param statementFactory the function that should be used to create the statement from the connection; may not be null
+     * @param resultConsumer the consumer of the query results
+     * @return this object for chaining methods together
+     * @throws SQLException if there is an error connecting to the database or executing the statements
+     * @see #execute(Operations)
+     */
+    public JdbcConnection query(String query, StatementFactory statementFactory, ResultSetConsumer resultConsumer) throws SQLException {
         Connection conn = connection();
-        try (Statement statement = conn.createStatement();) {
+        try (Statement statement = statementFactory.createStatement(conn);) {
             if (LOGGER.isTraceEnabled()) {
                 LOGGER.trace(""running '{}'"", query);
             }
@@ -288,6 +304,21 @@ public JdbcConnection query(String query, ResultSetConsumer resultConsumer) thro
         }
         return this;
     }
+    
+    /**
+     * A function to create a statement from a connection.
+     * @author Randall Hauch
+     */
+    @FunctionalInterface
+    public interface StatementFactory {
+        /**
+         * Use the given connection to create a statement.
+         * @param connection the JDBC connection; never null
+         * @return the statement
+         * @throws SQLException if there are problems creating a statement
+         */
+        Statement createStatement(Connection connection) throws SQLException;
+    }
 
     /**
      * Execute a SQL prepared query.
@@ -521,24 +552,24 @@ public Set<TableId> readTableNames(String databaseCatalog, String schemaNamePatt
     }
 
     /**
-     * Returns a JDBC connection string using the current configuration and url. 
+     * Returns a JDBC connection string using the current configuration and url.
      * 
      * @param urlPattern a {@code String} representing a JDBC connection with variables that will be replaced
      * @return a {@code String} where the variables in {@code urlPattern} are replaced with values from the configuration
      */
     public String connectionString(String urlPattern) {
         Properties props = config.asProperties();
         return findAndReplace(urlPattern, props, JdbcConfiguration.DATABASE, JdbcConfiguration.HOSTNAME, JdbcConfiguration.PORT,
-                              JdbcConfiguration.USER, JdbcConfiguration.PASSWORD);   
+                              JdbcConfiguration.USER, JdbcConfiguration.PASSWORD);
     }
 
     /**
-     * Returns the username for this connection 
+     * Returns the username for this connection
      * 
      * @return a {@code String}, never {@code null}
      */
     public String username()  {
-        return config.getString(JdbcConfiguration.USER);    
+        return config.getString(JdbcConfiguration.USER);
     }
 
     /**",2016-08-04T21:06:50Z,46
"@@ -264,6 +264,26 @@ public static String createString(final char charToRepeat,
         return sb.toString();
     }
 
+    /**
+     * Pad the string with the specific character to ensure the string is at least the specified length.
+     * 
+     * @param original the string to be padded; may not be null
+     * @param length the minimum desired length; must be positive
+     * @param padChar the character to use for padding, if the supplied string is not long enough
+     * @return the padded string of the desired length
+     * @see #justifyLeft(String, int, char)
+     */
+    public static String pad(String original,
+                                   int length,
+                                   char padChar) {
+        if ( original.length() >= length ) return original;
+        StringBuilder sb = new StringBuilder(original);
+        while ( sb.length() < length ) {
+            sb.append(padChar);
+        }
+        return sb.toString();
+    }
+
     /**
      * Set the length of the string, padding with the supplied character if the supplied string is shorter than desired, or
      * truncating the string if it is longer than desired. Unlike {@link #justifyLeft(String, int, char)}, this method does not",2016-08-04T21:06:50Z,115
"@@ -0,0 +1,52 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.function;
+
+import java.util.LinkedList;
+import java.util.List;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author Randall Hauch
+ */
+public class BufferedBlockingConsumerTest {
+
+    private List<Integer> history;
+    private BlockingConsumer<Integer> consumer;
+    
+
+    @Before
+    public void beforeEach() {
+        history = new LinkedList<>();
+        consumer = history::add;
+    }
+    
+    @Test
+    public void shouldMaintainSameOrder() throws InterruptedException {
+        BufferedBlockingConsumer<Integer> buffered = BufferedBlockingConsumer.bufferLast(consumer);
+        
+        // Add several values ...
+        buffered.accept(1);
+        buffered.accept(2);
+        buffered.accept(3);
+        buffered.accept(4);
+        buffered.accept(5);
+
+        // And verify the history contains all but the last value ...
+        assertThat(history).containsExactly(1,2,3,4);
+        
+        // Flush the last value...
+        buffered.flush();
+        
+        // And verify the history contains the same values ...
+        assertThat(history).containsExactly(1,2,3,4,5);
+    }
+
+}",2016-08-04T21:06:50Z,130
"@@ -32,6 +32,61 @@
  */
 public class MySqlConnectorConfig {
 
+    /**
+     * The set of predefined TemporalPrecisionMode options or aliases.
+     */
+    public static enum TemporalPrecisionMode {
+        /**
+         * Represent time and date values based upon the resolution in the database, using {@link io.debezium.time} semantic
+         * types.
+         */
+        ADAPTIVE(""adaptive""),
+
+        /**
+         * Represent time and date values using Kafka Connect {@link org.apache.kafka.connect.data} logical types, which always
+         * have millisecond precision.
+         */
+        CONNECT(""connect"");
+
+        private final String value;
+
+        private TemporalPrecisionMode(String value) {
+            this.value = value;
+        }
+
+        public String getValue() {
+            return value;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         * 
+         * @param value the configuration property value; may not be null
+         * @return the matching option, or null if no match is found
+         */
+        public static TemporalPrecisionMode parse(String value) {
+            if (value == null) return null;
+            value = value.trim();
+            for (TemporalPrecisionMode option : TemporalPrecisionMode.values()) {
+                if (option.getValue().equalsIgnoreCase(value)) return option;
+            }
+            return null;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         * 
+         * @param value the configuration property value; may not be null
+         * @param defaultValue the default value; may be null
+         * @return the matching option, or null if no match is found and the non-null default is invalid
+         */
+        public static TemporalPrecisionMode parse(String value, String defaultValue) {
+            TemporalPrecisionMode mode = parse(value);
+            if (mode == null && defaultValue != null) mode = parse(defaultValue);
+            return mode;
+        }
+    }
+
     /**
      * The set of predefined SnapshotMode options or aliases.
      */
@@ -268,7 +323,7 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                                       .withDescription(""Frequency in milliseconds to wait for new change events to appear after receiving no events. Defaults to 1 second (1000 ms)."")
                                                       .withDefault(TimeUnit.SECONDS.toMillis(1))
                                                       .withValidation(Field::isPositiveInteger);
-    
+
     public static final Field ROW_COUNT_FOR_STREAMING_RESULT_SETS = Field.create(""min.row.count.to.stream.results"")
                                                                          .withDisplayName(""Stream result set larger than"")
                                                                          .withType(Type.LONG)
@@ -330,6 +385,17 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                                                       + ""of the snapshot; in such cases set this property to 'false'."")
                                                               .withDefault(true);
 
+    public static final Field TIME_PRECISION_MODE = Field.create(""time.precision.mode"")
+                                                         .withDisplayName(""Time Precision"")
+                                                         .withEnum(TemporalPrecisionMode.class)
+                                                         .withWidth(Width.SHORT)
+                                                         .withImportance(Importance.MEDIUM)
+                                                         .withDescription(""Time, date, and timestamps can be represented with different kinds of precisions, including:""
+                                                                 + ""'adaptive' (the default) bases the precision of time, date, and timestamp values on the database column's precision; ""
+                                                                 + ""'connect' always represents time, date, and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, ""
+                                                                 + ""which uses millisecond precision regardless of the database columns' precision ."")
+                                                         .withDefault(TemporalPrecisionMode.ADAPTIVE.getValue());
+
     /**
      * Method that generates a Field for specifying that string columns whose names match a set of regular expressions should
      * have their values truncated to be no longer than the specified number of characters.
@@ -371,7 +437,8 @@ public static final Field MASK_COLUMN(int length) {
                                                      DATABASE_HISTORY, INCLUDE_SCHEMA_CHANGES,
                                                      TABLE_WHITELIST, TABLE_BLACKLIST, TABLES_IGNORE_BUILTIN,
                                                      DATABASE_WHITELIST, DATABASE_BLACKLIST,
-                                                     COLUMN_BLACKLIST, SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING);
+                                                     COLUMN_BLACKLIST, SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING,
+                                                     TIME_PRECISION_MODE);
 
     /**
      * The set of {@link Field}s that are included in the {@link #configDef() configuration definition}. This includes
@@ -393,7 +460,7 @@ protected static ConfigDef configDef() {
         Field.group(config, ""Events"", INCLUDE_SCHEMA_CHANGES, TABLES_IGNORE_BUILTIN, DATABASE_WHITELIST, TABLE_WHITELIST,
                     COLUMN_BLACKLIST, TABLE_BLACKLIST, DATABASE_BLACKLIST);
         Field.group(config, ""Connector"", CONNECTION_TIMEOUT_MS, KEEP_ALIVE, MAX_QUEUE_SIZE, MAX_BATCH_SIZE, POLL_INTERVAL_MS,
-                    SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING);
+                    SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING, TIME_PRECISION_MODE);
         return config;
     }
 ",2016-08-11T15:48:07Z,65
"@@ -23,6 +23,7 @@
 
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.TemporalPrecisionMode;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.TimeZoneAdapter;
 import io.debezium.relational.Table;
@@ -100,7 +101,10 @@ public MySqlSchema(Configuration config, String serverName) {
         this.ddlParser.addListener(ddlChanges);
 
         // Use MySQL-specific converters and schemas for values ...
-        MySqlValueConverters valueConverters = new MySqlValueConverters();
+        String timePrecisionModeStr = config.getString(MySqlConnectorConfig.TIME_PRECISION_MODE);
+        TemporalPrecisionMode timePrecisionMode = TemporalPrecisionMode.parse(timePrecisionModeStr);
+        boolean adaptiveTimePrecision = TemporalPrecisionMode.ADAPTIVE.equals(timePrecisionMode);
+        MySqlValueConverters valueConverters = new MySqlValueConverters(adaptiveTimePrecision);
         this.schemaBuilder = new TableSchemaBuilder(valueConverters, schemaNameValidator::validate);
         
         // Set up the server name and schema prefix ...",2016-08-11T15:48:07Z,16
"@@ -42,21 +42,28 @@ public class MySqlValueConverters extends JdbcValueConverters {
      * Create a new instance that always uses UTC for the default time zone when converting values without timezone information
      * to values that require timezones.
      * <p>
+     * 
+     * @param adaptiveTimePrecision {@code true} if the time, date, and timestamp values should be based upon the precision of the
+     *            database columns using {@link io.debezium.time} semantic types, or {@code false} if they should be fixed to
+     *            millisecond precision using Kafka Connect {@link org.apache.kafka.connect.data} logical types.
      */
-    public MySqlValueConverters() {
-        super();
+    public MySqlValueConverters(boolean adaptiveTimePrecision) {
+        this(adaptiveTimePrecision, ZoneOffset.UTC);
     }
 
     /**
      * Create a new instance, and specify the time zone offset that should be used only when converting values without timezone
      * information to values that require timezones. This default offset should not be needed when values are highly-correlated
      * with the expected SQL/JDBC types.
      * 
+     * @param adaptiveTimePrecision {@code true} if the time, date, and timestamp values should be based upon the precision of the
+     *            database columns using {@link io.debezium.time} semantic types, or {@code false} if they should be fixed to
+     *            millisecond precision using Kafka Connect {@link org.apache.kafka.connect.data} logical types.
      * @param defaultOffset the zone offset that is to be used when converting non-timezone related values to values that do
      *            have timezones; may be null if UTC is to be used
      */
-    public MySqlValueConverters(ZoneOffset defaultOffset) {
-        super(defaultOffset);
+    public MySqlValueConverters(boolean adaptiveTimePrecision, ZoneOffset defaultOffset) {
+        super(adaptiveTimePrecision, defaultOffset);
     }
 
     @Override
@@ -105,17 +112,17 @@ public ValueConverter converter(Column column, Field fieldDefn) {
     @SuppressWarnings(""deprecation"")
     protected Object convertYear(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
-        if ( data instanceof java.time.Year ) {
+        if (data instanceof java.time.Year) {
             // The MySQL binlog always returns a Year object ...
-            return ((java.time.Year)data).getValue();
+            return ((java.time.Year) data).getValue();
         }
-        if ( data instanceof java.sql.Date ) {
+        if (data instanceof java.sql.Date) {
             // MySQL JDBC driver sometimes returns a Java SQL Date object ...
-            return ((java.sql.Date)data).getYear();
+            return ((java.sql.Date) data).getYear();
         }
         if (data instanceof Number) {
             // MySQL JDBC driver sometimes returns a short ...
-            return ((Number)data).intValue();
+            return ((Number) data).intValue();
         }
         return handleUnknownData(column, fieldDefn, data);
     }",2016-08-11T15:48:07Z,70
"@@ -24,6 +24,7 @@
 
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.connector.mysql.MySqlConnectorConfig.TemporalPrecisionMode;
 import io.debezium.data.Envelope;
 import io.debezium.doc.FixFor;
 import io.debezium.embedded.AbstractConnectorTest;
@@ -111,7 +112,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 // {""c1"" : ""16321"", ""c2"" : ""64264780"", ""c3"" : ""1410198664780"", ""c4"" : ""2014-09-08T17:51:04.78-05:00""}
 
                 // '2014-09-08'
-                Long c1 = after.getInt64(""c1""); // epoch days
+                Integer c1 = after.getInt32(""c1""); // epoch days
                 LocalDate c1Date = LocalDate.ofEpochDay(c1);
                 assertThat(c1Date.getYear()).isEqualTo(2014);
                 assertThat(c1Date.getMonth()).isEqualTo(Month.SEPTEMBER);
@@ -162,6 +163,112 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
         });
     }
 
+    @Test
+    @FixFor( ""DBZ-61"" )
+    public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnectTimesTypes() throws SQLException, InterruptedException {
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(MySqlConnectorConfig.TIME_PRECISION_MODE, TemporalPrecisionMode.CONNECT.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+
+        // ---------------------------------------------------------------------------------------------------------------
+        // Consume all of the events due to startup and initialization of the database
+        // ---------------------------------------------------------------------------------------------------------------
+        // Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(4 + 3); // 4 schema change record, 3 inserts
+        stopConnector();
+        assertThat(records).isNotNull();
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(4);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(4);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+        records.forEach(record -> {
+            Struct value = (Struct) record.value();
+            if (record.topic().endsWith(""dbz_85_fractest"")) {
+                // The microseconds of all three should be exactly 780
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                // c1 DATE,
+                // c2 TIME(2),
+                // c3 DATETIME(2),
+                // c4 TIMESTAMP(2)
+                //
+                // {""c1"" : ""16321"", ""c2"" : ""64264780"", ""c3"" : ""1410198664780"", ""c4"" : ""2014-09-08T17:51:04.78-05:00""}
+
+                // '2014-09-08'
+                java.util.Date c1 = (java.util.Date)after.get(""c1""); // epoch days
+                LocalDate c1Date = LocalDate.ofEpochDay(c1.getTime() / TimeUnit.DAYS.toMillis(1));
+                assertThat(c1Date.getYear()).isEqualTo(2014);
+                assertThat(c1Date.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(c1Date.getDayOfMonth()).isEqualTo(8);
+
+                // '17:51:04.777'
+                java.util.Date c2 = (java.util.Date)after.get(""c2""); // milliseconds past midnight
+                LocalTime c2Time = LocalTime.ofNanoOfDay(TimeUnit.MILLISECONDS.toNanos(c2.getTime()));
+                assertThat(c2Time.getHour()).isEqualTo(17);
+                assertThat(c2Time.getMinute()).isEqualTo(51);
+                assertThat(c2Time.getSecond()).isEqualTo(4);
+                assertThat(c2Time.getNano()).isEqualTo((int) TimeUnit.MILLISECONDS.toNanos(780));
+                assertThat(io.debezium.time.Time.toMilliOfDay(c2Time)).isEqualTo((int)c2.getTime());
+
+                // '2014-09-08 17:51:04.777'
+                java.util.Date c3 = (java.util.Date)after.get(""c3""); // epoch millis
+                long c3Seconds = c3.getTime() / 1000;
+                long c3Millis = c3.getTime() % 1000;
+                LocalDateTime c3DateTime = LocalDateTime.ofEpochSecond(c3Seconds,
+                                                                       (int) TimeUnit.MILLISECONDS.toNanos(c3Millis),
+                                                                       ZoneOffset.UTC);
+                assertThat(c3DateTime.getYear()).isEqualTo(2014);
+                assertThat(c3DateTime.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(c3DateTime.getDayOfMonth()).isEqualTo(8);
+                assertThat(c3DateTime.getHour()).isEqualTo(17);
+                assertThat(c3DateTime.getMinute()).isEqualTo(51);
+                assertThat(c3DateTime.getSecond()).isEqualTo(4);
+                assertThat(c3DateTime.getNano()).isEqualTo((int) TimeUnit.MILLISECONDS.toNanos(780));
+                assertThat(io.debezium.time.Timestamp.toEpochMillis(c3DateTime)).isEqualTo(c3.getTime());
+
+                // '2014-09-08 17:51:04.777'
+                String c4 = after.getString(""c4""); // MySQL timestamp, so always ZonedTimestamp
+                OffsetDateTime c4DateTime = OffsetDateTime.parse(c4, ZonedTimestamp.FORMATTER);
+                // In case the timestamp string not in our timezone, convert to ours so we can compare ...
+                c4DateTime = c4DateTime.withOffsetSameInstant(OffsetDateTime.now().getOffset());
+                assertThat(c4DateTime.getYear()).isEqualTo(2014);
+                assertThat(c4DateTime.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(c4DateTime.getDayOfMonth()).isEqualTo(8);
+                assertThat(c4DateTime.getHour()).isEqualTo(17);
+                assertThat(c4DateTime.getMinute()).isEqualTo(51);
+                assertThat(c4DateTime.getSecond()).isEqualTo(4);
+                assertThat(c4DateTime.getNano()).isEqualTo((int) TimeUnit.MILLISECONDS.toNanos(780));
+                // We're running the connector in the same timezone as the server, so the timezone in the timestamp
+                // should match our current offset ...
+                assertThat(c4DateTime.getOffset()).isEqualTo(OffsetDateTime.now().getOffset());
+            }
+        });
+    }
+
     @Test
     public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
@@ -217,7 +324,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                 // {""c1"" : ""16321"", ""c2"" : ""64264780"", ""c3"" : ""1410198664780"", ""c4"" : ""2014-09-08T17:51:04.78-05:00""}
 
                 // '2014-09-08'
-                Long c1 = after.getInt64(""c1""); // epoch days
+                Integer c1 = after.getInt32(""c1""); // epoch days
                 LocalDate c1Date = LocalDate.ofEpochDay(c1);
                 assertThat(c1Date.getYear()).isEqualTo(2014);
                 assertThat(c1Date.getMonth()).isEqualTo(Month.SEPTEMBER);",2016-08-11T15:48:07Z,71
"@@ -11,6 +11,7 @@
 import java.time.OffsetDateTime;
 import java.time.OffsetTime;
 import java.time.ZoneOffset;
+import java.util.concurrent.TimeUnit;
 
 import org.apache.kafka.connect.data.Decimal;
 import org.apache.kafka.connect.data.Field;
@@ -62,23 +63,31 @@ public class JdbcValueConverters implements ValueConverterProvider {
 
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final ZoneOffset defaultOffset;
+    private final boolean adaptiveTimePrecision;
 
     /**
-     * Create a new instance.
+     * Create a new instance that always uses UTC for the default time zone when converting values without timezone information
+     * to values that require timezones, and uses adapts time and timestamp values based upon the precision of the database
+     * columns.
      */
     public JdbcValueConverters() {
-        this(ZoneOffset.UTC);
+        this(true, ZoneOffset.UTC);
     }
 
     /**
      * Create a new instance, and specify the time zone offset that should be used only when converting values without timezone
      * information to values that require timezones. This default offset should not be needed when values are highly-correlated
      * with the expected SQL/JDBC types.
+     * 
+     * @param adaptiveTimePrecision {@code true} if the time, date, and timestamp values should be based upon the precision of the
+     *            database columns using {@link io.debezium.time} semantic types, or {@code false} if they should be fixed to
+     *            millisecond precision using Kafka Connect {@link org.apache.kafka.connect.data} logical types.
      * @param defaultOffset the zone offset that is to be used when converting non-timezone related values to values that do
-     * have timezones; may be null if UTC is to be used
+     *            have timezones; may be null if UTC is to be used
      */
-    public JdbcValueConverters(ZoneOffset defaultOffset) {
+    public JdbcValueConverters(boolean adaptiveTimePrecision, ZoneOffset defaultOffset) {
         this.defaultOffset = defaultOffset != null ? defaultOffset : ZoneOffset.UTC;
+        this.adaptiveTimePrecision = adaptiveTimePrecision;
     }
 
     @Override
@@ -153,15 +162,24 @@ public SchemaBuilder schemaBuilder(Column column) {
 
             // Date and time values
             case Types.DATE:
+                if (adaptiveTimePrecision) {
                 return Date.builder();
+                }
+                return org.apache.kafka.connect.data.Date.builder();
             case Types.TIME:
-                if (column.length() <= 3) return Time.builder();
-                if (column.length() <= 6) return MicroTime.builder();
-                return NanoTime.builder();
+                if (adaptiveTimePrecision) {
+                    if (column.length() <= 3) return Time.builder();
+                    if (column.length() <= 6) return MicroTime.builder();
+                    return NanoTime.builder();
+                }
+                return org.apache.kafka.connect.data.Time.builder();
             case Types.TIMESTAMP:
-                if (column.length() <= 3) return Timestamp.builder();
-                if (column.length() <= 6) return MicroTimestamp.builder();
-                return NanoTimestamp.builder();
+                if (adaptiveTimePrecision) {
+                    if (column.length() <= 3 || !adaptiveTimePrecision) return Timestamp.builder();
+                    if (column.length() <= 6) return MicroTimestamp.builder();
+                    return NanoTimestamp.builder();
+                }
+                return org.apache.kafka.connect.data.Timestamp.builder();
             case Types.TIME_WITH_TIMEZONE:
                 return ZonedTime.builder();
             case Types.TIMESTAMP_WITH_TIMEZONE:
@@ -184,6 +202,7 @@ public SchemaBuilder schemaBuilder(Column column) {
                 break;
         }
         return null;
+
     }
 
     @Override
@@ -240,15 +259,24 @@ public ValueConverter converter(Column column, Field fieldDefn) {
 
             // Date and time values
             case Types.DATE:
+                if (adaptiveTimePrecision) {
                 return (data) -> convertDateToEpochDays(column, fieldDefn, data);
+                }
+                return (data) -> convertDateToEpochDaysAsDate(column, fieldDefn, data);
             case Types.TIME:
-                if (column.length() <= 3) return (data) -> convertTimeToMillisPastMidnight(column, fieldDefn, data);
-                if (column.length() <= 6) return (data) -> convertTimeToMicrosPastMidnight(column, fieldDefn, data);
-                return (data) -> convertTimeToNanosPastMidnight(column, fieldDefn, data);
+                if (adaptiveTimePrecision) {
+                    if (column.length() <= 3) return (data) -> convertTimeToMillisPastMidnight(column, fieldDefn, data);
+                    if (column.length() <= 6) return (data) -> convertTimeToMicrosPastMidnight(column, fieldDefn, data);
+                    return (data) -> convertTimeToNanosPastMidnight(column, fieldDefn, data);
+                }
+                return (data) -> convertTimeToMillisPastMidnightAsDate(column, fieldDefn, data);
             case Types.TIMESTAMP:
-                if (column.length() <= 3) return (data) -> convertTimestampToEpochMillis(column, fieldDefn, data);
-                if (column.length() <= 6) return (data) -> convertTimestampToEpochMicros(column, fieldDefn, data);
-                return (data) -> convertTimestampToEpochNanos(column, fieldDefn, data);
+                if (adaptiveTimePrecision) {
+                    if (column.length() <= 3) return (data) -> convertTimestampToEpochMillis(column, fieldDefn, data);
+                    if (column.length() <= 6) return (data) -> convertTimestampToEpochMicros(column, fieldDefn, data);
+                    return (data) -> convertTimestampToEpochNanos(column, fieldDefn, data);
+                }
+                return (data) -> convertTimestampToEpochMillisAsDate(column, fieldDefn, data);
             case Types.TIME_WITH_TIMEZONE:
                 return (data) -> convertTimeWithZone(column, fieldDefn, data);
             case Types.TIMESTAMP_WITH_TIMEZONE:
@@ -379,6 +407,29 @@ protected Object convertTimestampToEpochNanos(Column column, Field fieldDefn, Ob
         }
     }
 
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#TIMESTAMP} to {@link java.util.Date} values representing
+     * milliseconds past epoch.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Timestamp} instances, which have date and time info
+     * but no time zone info. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such
+     * as {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTimestampToEpochMillisAsDate(Column column, Field fieldDefn, Object data) {
+        try {
+            Long epochMillis = Timestamp.toEpochMillis(data);
+            if ( epochMillis == null ) return null;
+            return new java.util.Date(epochMillis.longValue());
+        } catch (IllegalArgumentException e) {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+    }
+
     /**
      * Converts a value object for an expected JDBC type of {@link Types#TIME} to {@link Time} values, or milliseconds past
      * midnight.
@@ -446,7 +497,31 @@ protected Object convertTimeToNanosPastMidnight(Column column, Field fieldDefn,
     }
 
     /**
-     * Converts a value object for an expected JDBC type of {@link Types#DATE}.
+     * Converts a value object for an expected JDBC type of {@link Types#TIME} to {@link java.util.Date} values representing
+     * the milliseconds past midnight on the epoch day.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Time} instances that have no notion of date or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have date components, those date components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTimeToMillisPastMidnightAsDate(Column column, Field fieldDefn, Object data) {
+        try {
+            Integer millisOfDay = Time.toMilliOfDay(data);
+            if ( millisOfDay == null ) return null;
+            return new java.util.Date(millisOfDay.longValue());
+        } catch (IllegalArgumentException e) {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DATE} to the number of days past epoch.
      * <p>
      * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
      * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
@@ -469,6 +544,34 @@ protected Object convertDateToEpochDays(Column column, Field fieldDefn, Object d
         }
     }
 
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DATE} to the number of days past epoch, but represented
+     * as a {@link java.util.Date} value at midnight on the date.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have time components, those time components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDateToEpochDaysAsDate(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        try {
+            Integer epochDay = Date.toEpochDay(data);
+            if ( epochDay == null ) return null;
+            long epochMillis = TimeUnit.DAYS.toMillis(epochDay.longValue());
+            return new java.util.Date(epochMillis);
+        } catch (IllegalArgumentException e) {
+            logger.warn(""Unexpected JDBC DATE value for field {} with schema {}: class={}, value={}"", fieldDefn.name(),
+                        fieldDefn.schema(), data.getClass(), data);
+            return null;
+        }
+    }
+
     /**
      * Converts a value object for an expected JDBC type of {@link Types#BLOB}, {@link Types#BINARY},
      * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.",2016-08-11T15:48:07Z,91
"@@ -23,7 +23,7 @@ public class Date {
 
     /**
      * Returns a {@link SchemaBuilder} for a {@link Date}. The builder will create a schema that describes a field
-     * with the {@value #SCHEMA_NAME} as the {@link Schema#name() name} and {@link SchemaBuilder#int64() INT64} for the literal
+     * with the {@value #SCHEMA_NAME} as the {@link Schema#name() name} and {@link SchemaBuilder#int32() INT32} for the literal
      * type storing the number of <em>days</em> since January 1, 1970, at 00:00:00Z.
      * <p>
      * You can use the resulting SchemaBuilder to set or override additional schema settings such as required/optional, default
@@ -32,14 +32,14 @@ public class Date {
      * @return the schema builder
      */
     public static SchemaBuilder builder() {
-        return SchemaBuilder.int64()
+        return SchemaBuilder.int32()
                             .name(SCHEMA_NAME)
                             .version(1);
     }
 
     /**
      * Returns a Schema for a {@link Date} but with all other default Schema settings. The schema describes a field
-     * with the {@value #SCHEMA_NAME} as the {@link Schema#name() name} and {@link SchemaBuilder#int64() INT64} for the literal
+     * with the {@value #SCHEMA_NAME} as the {@link Schema#name() name} and {@link SchemaBuilder#int32() INT32} for the literal
      * type storing the number of <em>days</em> since January 1, 1970, at 00:00:00Z.
      * 
      * @return the schema
@@ -55,12 +55,12 @@ public static Schema schema() {
      * {@link java.sql.Timestamp}, ignoring any time portions of the supplied value.
      * 
      * @param value the local or SQL date, time, or timestamp value
-     * @return the microseconds past midnight
+     * @return the number of days past epoch
      * @throws IllegalArgumentException if the value is not an instance of the acceptable types
      */
-    public static Long toEpochDay(Object value) {
+    public static Integer toEpochDay(Object value) {
         if ( value == null ) return null;
-        return Conversions.toLocalDate(value).toEpochDay();
+        return (int)Conversions.toLocalDate(value).toEpochDay();
     }
 
     private Date() {",2016-08-11T15:48:07Z,131
"@@ -166,20 +166,17 @@ public void shouldValidateAcceptableConfiguration() {
 
         Recommender tableNameRecommender = MySqlConnectorConfig.TABLE_WHITELIST.recommender();
         List<Object> tableNames = tableNameRecommender.validValues(MySqlConnectorConfig.TABLE_WHITELIST, config);
-        assertThat(tableNames).containsOnly(""connector_test.customers"",
-                                            ""connector_test.orders"",
-                                            ""connector_test.products"",
-                                            ""connector_test.products_on_hand"",
-                                            ""connector_test_ro.customers"",
-                                            ""connector_test_ro.orders"",
-                                            ""connector_test_ro.products"",
-                                            ""connector_test_ro.products_on_hand"",
-                                            ""regression_test.t1464075356413_testtable6"",
-                                            ""regression_test.dbz_85_fractest"",
-                                            ""regression_test.dbz84_integer_types_table"",
-                                            ""readbinlog_test.product"",
-                                            ""readbinlog_test.purchased"",
-                                            ""readbinlog_test.person"");
+        assertThat(tableNames).contains(""connector_test.customers"",
+                                        ""connector_test.orders"",
+                                        ""connector_test.products"",
+                                        ""connector_test.products_on_hand"",
+                                        ""connector_test_ro.customers"",
+                                        ""connector_test_ro.orders"",
+                                        ""connector_test_ro.products"",
+                                        ""connector_test_ro.products_on_hand"",
+                                        ""regression_test.t1464075356413_testtable6"",
+                                        ""regression_test.dbz_85_fractest"",
+                                        ""regression_test.dbz84_integer_types_table"");
         Testing.debug(""List of tableNames: "" + tableNames);
 
         // Now set the whitelist to two databases ...",2016-08-08T11:25:38Z,88
"@@ -24,6 +24,7 @@
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
+import io.debezium.doc.FixFor;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.util.Testing;
@@ -54,6 +55,7 @@ public void afterEach() {
     }
 
     @Test
+    @FixFor( ""DBZ-61"" )
     public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()",2016-08-08T11:25:38Z,71
"@@ -10,4 +10,5 @@ log4j.rootLogger=INFO, stdout
 # Set up the default logging to be INFO level, then override specific units
 log4j.logger.io.debezium=INFO
 log4j.logger.io.debezium.embedded.EmbeddedEngine$EmbeddedConfig=WARN
-#log4j.logger.io.debezium.connector.mysql.BinlogReader=DEBUG
\ No newline at end of file
+#log4j.logger.io.debezium.connector.mysql.BinlogReader=DEBUG
+#log4j.logger.io.debezium.connector.mysql.SnapshotReader=DEBUG
\ No newline at end of file",2016-08-08T11:25:38Z,89
"@@ -0,0 +1,49 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.doc;
+
+import static java.lang.annotation.ElementType.METHOD;
+import static java.lang.annotation.RetentionPolicy.CLASS;
+
+import java.lang.annotation.Documented;
+import java.lang.annotation.Retention;
+import java.lang.annotation.Target;
+
+/**
+ * Annotation that can be used to help track that a test is verifying the fix for one or more specific issues. To use, simply
+ * place this annotation on the test method and reference the JIRA issue number:
+ * 
+ * <pre>
+ *    &#064;FixFor(""DBZ-123"")
+ *    &#064;Test
+ *    public void shouldVerifyBehavior() {
+ *     ...
+ *    }
+ * </pre>
+ * <p>
+ * It is also possible to reference multiple JIRA issues if the test is verifying multiple ones:
+ * 
+ * <pre>
+ *    &#064;FixFor({""DBZ-123"",""DBZ-456""})
+ *    &#064;Test
+ *    public void shouldVerifyBehavior() {
+ *     ...
+ *    }
+ * </pre>
+ * 
+ * </p>
+ */
+@Documented
+@Retention( CLASS )
+@Target( METHOD )
+public @interface FixFor {
+    /**
+     * The JIRA issue for which this is a fix. For example, ""DBZ-123"".
+     * 
+     * @return the issue
+     */
+    String[] value();
+}",2016-08-08T11:25:38Z,132
"@@ -285,6 +285,8 @@ protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordC
                             + (numberOfRecords - recordsConsumed) + "" more)"");
                     print(record);
                 }
+            } else {
+                return recordsConsumed;
             }
         }
         return recordsConsumed;",2016-08-08T11:25:38Z,61
"@@ -6,10 +6,8 @@
 package io.debezium.connector.mysql;
 
 import java.sql.SQLException;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
 import io.debezium.config.Configuration;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
@@ -21,7 +19,8 @@
  */
 public class MySqlJdbcContext implements AutoCloseable {
 
-    protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false"");
+    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false"";
+    protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(MYSQL_CONNECTION_URL);
 
     protected final Logger logger = LoggerFactory.getLogger(getClass());
     protected final Configuration config;
@@ -83,4 +82,8 @@ public void shutdown() {
     public void close() {
         shutdown();
     }
+    
+    protected String connectionString() {
+        return jdbc.connectionString(MYSQL_CONNECTION_URL);
+    }
 }",2016-08-03T13:54:17Z,29
"@@ -5,6 +5,7 @@
  */
 package io.debezium.connector.mysql;
 
+import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -131,14 +132,16 @@ protected void doCleanup() {
      */
     protected void execute() {
         context.configureLoggingContext(""snapshot"");
-        logger.info(""Starting snapshot"");
         final AtomicReference<String> sql = new AtomicReference<>();
         final JdbcConnection mysql = context.jdbc();
         final MySqlSchema schema = context.dbSchema();
         final Filters filters = schema.filters();
         final SourceInfo source = context.source();
         final Clock clock = context.clock();
         final long ts = clock.currentTimeInMillis();
+        logger.info(""Starting snapshot for {} with user '{}'"", context.connectionString(), mysql.username());
+        logRolesForCurrentUser(sql, mysql);
+        logServerInformation(sql, mysql);
         try {
             // ------
             // STEP 0
@@ -194,6 +197,10 @@ protected void execute() {
                         // This column exists only in MySQL 5.6.5 or later ...
                         String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
                         source.setGtidSet(gtidSet);
+                        logger.debug(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
+                                     gtidSet);
+                    } else {
+                        logger.debug(""\t using binlog '{}' at position '{}'"", binlogFilename, binlogPosition);
                     }
                     source.startSnapshot();
                 }
@@ -214,7 +221,8 @@ protected void execute() {
                     databaseNames.add(rs.getString(1));
                 }
             });
-
+            logger.debug(""\t list of available databases is: {}"", databaseNames);
+           
             // ------
             // STEP 5
             // ------
@@ -232,6 +240,9 @@ protected void execute() {
                         if (filters.tableFilter().test(id)) {
                             tableIds.add(id);
                             tableIdsByDbName.computeIfAbsent(dbName, k -> new ArrayList<>()).add(id);
+                            logger.debug(""\t including '{}'"", id);
+                        } else {
+                            logger.debug(""\t '{}' is filtered out, discarding"", id);
                         }
                     }
                 });
@@ -388,6 +399,34 @@ protected void execute() {
             failed(e, ""Aborting snapshot after running '"" + sql.get() + ""': "" + e.getMessage());
         }
     }
+    
+    private void logServerInformation(AtomicReference<String> sql, JdbcConnection mysql) {
+        try {
+            sql.set(""SHOW VARIABLES LIKE 'version'"");
+            mysql.query(sql.get(), rs -> {
+                if (rs.next()) {
+                    logger.info(""MySql server version is '{}'"", rs.getString(2));        
+                }
+            });
+        } catch (SQLException e) {
+            logger.info(""Cannot determine MySql server version"", e);
+        }       
+    }
+
+    private void logRolesForCurrentUser(AtomicReference<String> sql, JdbcConnection mysql) {
+        try {
+            List<String> privileges = new ArrayList<>();
+            sql.set(""SHOW GRANTS"");
+            mysql.query(sql.get(), rs -> {
+                while (rs.next()) {
+                    privileges.add(rs.getString(1));
+                }
+            });
+            logger.info(""User '{}' has '{}'"", mysql.username(), privileges);
+        } catch (SQLException e) {
+            logger.info(""Cannot determine the privileges for '{}' "", mysql.username(), e);
+        }
+    }
 
     protected void enqueueSchemaChanges(String dbName, String ddlStatements) {
         if (context.includeSchemaChangeRecords() &&",2016-08-03T13:54:17Z,15
"@@ -48,7 +48,7 @@ public static MySQLConnection forTestDatabase(String databaseName, String userna
     protected static void addDefaults(Configuration.Builder builder) {
         builder.withDefault(JdbcConfiguration.HOSTNAME, ""localhost"")
                .withDefault(JdbcConfiguration.PORT, 3306)
-               .withDefault(JdbcConfiguration.USER, ""mysql"")
+               .withDefault(JdbcConfiguration.USER, ""mysqluser"")
                .withDefault(JdbcConfiguration.PASSWORD, ""mysqlpw"");
     }
 ",2016-08-03T13:54:17Z,21
"@@ -27,10 +27,8 @@
 import java.util.concurrent.ConcurrentMap;
 import java.util.function.Consumer;
 import java.util.stream.Stream;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
 import io.debezium.annotation.ThreadSafe;
 import io.debezium.config.Configuration;
 import io.debezium.config.Field;
@@ -228,7 +226,12 @@ public JdbcConnection connect() throws SQLException {
     public JdbcConnection execute(String... sqlStatements) throws SQLException {
         return execute(statement -> {
             for (String sqlStatement : sqlStatements) {
-                if (sqlStatement != null) statement.execute(sqlStatement);
+                if (sqlStatement != null) {
+                    if (LOGGER.isTraceEnabled()) {
+                        LOGGER.trace(""executing '{}'"", sqlStatement);
+                    }
+                    statement.execute(sqlStatement);
+                }
             }
         });
     }
@@ -274,6 +277,9 @@ public static interface StatementPreparer {
     public JdbcConnection query(String query, ResultSetConsumer resultConsumer) throws SQLException {
         Connection conn = connection();
         try (Statement statement = conn.createStatement();) {
+            if (LOGGER.isTraceEnabled()) {
+                LOGGER.trace(""running '{}'"", query);
+            }
             try (ResultSet resultSet = statement.executeQuery(query);) {
                 if (resultConsumer != null) {
                     resultConsumer.accept(resultSet);
@@ -514,6 +520,27 @@ public Set<TableId> readTableNames(String databaseCatalog, String schemaNamePatt
         return tableIds;
     }
 
+    /**
+     * Returns a JDBC connection string using the current configuration and url. 
+     * 
+     * @param urlPattern a {@code String} representing a JDBC connection with variables that will be replaced
+     * @return a {@code String} where the variables in {@code urlPattern} are replaced with values from the configuration
+     */
+    public String connectionString(String urlPattern) {
+        Properties props = config.asProperties();
+        return findAndReplace(urlPattern, props, JdbcConfiguration.DATABASE, JdbcConfiguration.HOSTNAME, JdbcConfiguration.PORT,
+                              JdbcConfiguration.USER, JdbcConfiguration.PASSWORD);   
+    }
+
+    /**
+     * Returns the username for this connection 
+     * 
+     * @return a {@code String}, never {@code null}
+     */
+    public String username()  {
+        return config.getString(JdbcConfiguration.USER);    
+    }
+
     /**
      * Create definitions for each tables in the database, given the catalog name, schema pattern, table filter, and
      * column filter.",2016-08-03T13:54:17Z,46
"@@ -232,7 +232,7 @@ protected boolean isBinlogAvailable() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking for binary logs: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking for binary logs: "", e);
         }
 
         // And compare with the one we're supposed to use ...
@@ -257,7 +257,7 @@ protected boolean isGtidModeEnabled() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
         }
 
         return !""OFF"".equalsIgnoreCase(mode.get());
@@ -277,7 +277,7 @@ protected String knownGtidSet() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
         }
 
         return gtidSetStr.get();",2016-07-29T05:57:47Z,10
"@@ -5,21 +5,22 @@
  */
 package io.debezium.connector.mysql;
 
+import static org.fest.assertions.Assertions.assertThat;
+
 import java.nio.file.Path;
 import java.sql.SQLException;
 import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.time.LocalTime;
 import java.time.Month;
 import java.time.ZoneId;
-
+import java.time.ZonedDateTime;
+import java.time.temporal.ChronoField;
+import java.time.temporal.ChronoUnit;
 import org.apache.kafka.connect.data.Struct;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
-
-import static org.fest.assertions.Assertions.assertThat;
-
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
@@ -110,12 +111,23 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 assertThat(c2.getTime() % 1000).isEqualTo(780);
                 assertThat(c3.getTime() % 1000).isEqualTo(780);
                 assertThat(c4.getTime() % 1000).isEqualTo(780);
-                assertThat(c1.getTime()).isEqualTo(1410134400000L);
-                assertThat(c2.getTime()).isEqualTo(64264780L);
-                assertThat(c3.getTime()).isEqualTo(1410198664780L);
-                assertThat(c4.getTime()).isEqualTo(1410198664780L);
-                // None of these Dates have timezone information, so to convert to locals we have to use our local timezone ...
                 ZoneId utc = ZoneId.of(""UTC"");
+                ZoneId defaultTZ = ZoneId.systemDefault();
+                LocalDate expectedDate = LocalDate.of(2014, 9, 8);
+                // the time is stored as 17:51:04.777 but rounded up to 780 due to the column configs
+                LocalTime expectedTime = LocalTime.of(17, 51, 4).plus(780, ChronoUnit.MILLIS);
+                // c1 '2014-09-08' is stored as a MySQL DATE (without any time) in the local TZ and then converted to 
+                // a truncated UTC by the connector, so we must assert against the same thing....
+                ZonedDateTime expectedC1UTC = ZonedDateTime.of(expectedDate, LocalTime.of(0, 0), defaultTZ)
+                                                           .withZoneSameInstant(utc)
+                                                           .truncatedTo(ChronoUnit.DAYS);
+                assertThat(c1.getTime()).isEqualTo(expectedC1UTC.toInstant().toEpochMilli());
+                ZonedDateTime expectedC2UTC = ZonedDateTime.of(LocalDate.ofEpochDay(0), expectedTime, utc);                         
+                assertThat(c2.getTime()).isEqualTo(expectedC2UTC.toInstant().toEpochMilli());
+                ZonedDateTime expectedC3UTC = ZonedDateTime.of(expectedDate, expectedTime, utc);
+                assertThat(c3.getTime()).isEqualTo(expectedC3UTC.toInstant().toEpochMilli());
+                assertThat(c4.getTime()).isEqualTo(expectedC3UTC.toInstant().toEpochMilli());
+                // None of these Dates have timezone information, so to convert to locals we have to use our local timezone ...
                 LocalDate localC1 = c1.toInstant().atZone(utc).toLocalDate();
                 LocalTime localC2 = c2.toInstant().atZone(utc).toLocalTime();
                 LocalDateTime localC3 = c3.toInstant().atZone(utc).toLocalDateTime();
@@ -124,7 +136,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 final int expectedNanos = 780 * 1000 * 1000;
                 assertThat(localC1.getYear()).isEqualTo(2014);
                 assertThat(localC1.getMonth()).isEqualTo(Month.SEPTEMBER);
-                assertThat(localC1.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC1.getDayOfMonth()).isEqualTo(expectedC1UTC.get(ChronoField.DAY_OF_MONTH));
                 assertThat(localC2.getHour()).isEqualTo(17);
                 assertThat(localC2.getMinute()).isEqualTo(51);
                 assertThat(localC2.getSecond()).isEqualTo(4);",2016-07-29T05:57:47Z,71
"@@ -298,21 +298,11 @@ protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordC
      * @throws InterruptedException if the thread was interrupted while waiting for a record to be returned
      */
     protected SourceRecords consumeRecordsByTopic(int numRecords) throws InterruptedException {
-        return consumeRecordsByTopic(numRecords, new SourceRecords());
-    }
-
-    /**
-     * Try to consume and capture exactly the specified number of records from the connector.
-     * 
-     * @param numRecords the number of records that should be consumed
-     * @param records the collector into which all consumed messages should be placed
-     * @return the actual number of records that were consumed
-     * @throws InterruptedException if the thread was interrupted while waiting for a record to be returned
-     */
-    protected SourceRecords consumeRecordsByTopic(int numRecords, SourceRecords records) throws InterruptedException {
+        SourceRecords records = new SourceRecords();
         consumeRecords(numRecords, records::add);
         return records;
     }
+    
 
     protected class SourceRecords {
         private final List<SourceRecord> records = new ArrayList<>();",2016-07-29T05:57:47Z,61
"@@ -11,14 +11,20 @@
 import java.util.Set;
 import java.util.concurrent.Callable;
 
+import org.apache.kafka.connect.data.Date;
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.Time;
+import org.apache.kafka.connect.data.Timestamp;
 import org.apache.kafka.connect.errors.ConnectException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer;
+
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
 import io.debezium.jdbc.JdbcConnection;
+import io.debezium.jdbc.TimeZoneAdapter;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.relational.TableSchema;
@@ -70,6 +76,18 @@ public class MySqlSchema {
 
     /**
      * Create a schema component given the supplied {@link MySqlConnectorConfig MySQL connector configuration}.
+     * <p>
+     * This component sets up a {@link TimeZoneAdapter} that is specific to how the MySQL Binary Log client library
+     * works. The {@link AbstractRowsEventDataDeserializer} class has various methods to instantiate the
+     * {@link java.util.Date}, {@link java.sql.Date}, {@link java.sql.Time}, and {@link java.sql.Timestamp} temporal values,
+     * where the values for {@link java.util.Date}, {@link java.sql.Date}, and {@link java.sql.Time} are all in terms of
+     * the <em>local time zone</em> (since it uses {@link java.util.Calendar#getInstance()}), but where the
+     * {@link java.sql.Timestamp} values are created differently using the milliseconds past epoch and therefore in terms of
+     * the <em>UTC time zone</em>.
+     * <p>
+     * And, because Kafka Connect {@link Time}, {@link Date}, and {@link Timestamp} logical
+     * schema types all expect the {@link java.util.Date} to be in terms of the <em>UTC time zone</em>, the
+     * {@link TimeZoneAdapter} also needs to produce {@link java.util.Date} values that will be correct in UTC.
      * 
      * @param config the connector configuration, which is presumed to be valid
      * @param serverName the name of the server
@@ -80,10 +98,18 @@ public MySqlSchema(Configuration config, String serverName) {
         this.tables = new Tables();
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
-        this.schemaBuilder = new TableSchemaBuilder(schemaNameValidator::validate);
-        if ( serverName != null ) serverName = serverName.trim();
+
+        // Specific to how the MySQL Binary Log client library creates temporal values ...
+        TimeZoneAdapter tzAdapter = TimeZoneAdapter.create()
+                                                   .withLocalZoneForUtilDate()
+                                                   .withLocalZoneForSqlDate()
+                                                   .withLocalZoneForSqlTime()
+                                                   .withUtcZoneForSqlTimestamp()
+                                                   .withUtcTargetZone();
+        this.schemaBuilder = new TableSchemaBuilder(tzAdapter, schemaNameValidator::validate);
+        if (serverName != null) serverName = serverName.trim();
         this.serverName = serverName;
-        if ( this.serverName == null || serverName.isEmpty() ) {
+        if (this.serverName == null || serverName.isEmpty()) {
             this.schemaPrefix = """";
         } else {
             this.schemaPrefix = serverName.endsWith(""."") ? serverName : serverName + ""."";
@@ -97,7 +123,7 @@ public MySqlSchema(Configuration config, String serverName) {
         }
         // Do not remove the prefix from the subset of config properties ...
         Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false);
-        this.dbHistory.configure(dbHistoryConfig,HISTORY_COMPARATOR); // validates
+        this.dbHistory.configure(dbHistoryConfig, HISTORY_COMPARATOR); // validates
     }
 
     /**
@@ -212,8 +238,8 @@ protected void changeTablesAndRecordInHistory(SourceInfo source, Callable<Void>
             changeFunction.call();
         } catch (Exception e) {
             this.tables = copy;
-            if ( e instanceof SQLException) throw (SQLException)e;
-            this.logger.error(""Unexpected error whle changing model of MySQL schemas: {}"",e.getMessage(),e);
+            if (e instanceof SQLException) throw (SQLException) e;
+            this.logger.error(""Unexpected error whle changing model of MySQL schemas: {}"", e.getMessage(), e);
         }
 
         // At least one table has changed or was removed, so first refresh the Kafka Connect schemas ...
@@ -309,12 +335,12 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
                     // to the same _affected_ database...
                     ddlChanges.groupStatementStringsByDatabase((dbName, ddl) -> {
                         if (filters.databaseFilter().test(dbName)) {
-                            if ( dbName == null ) dbName = """";
+                            if (dbName == null) dbName = """";
                             statementConsumer.consume(dbName, ddlStatements);
                         }
                     });
                 } else if (filters.databaseFilter().test(databaseName)) {
-                    if ( databaseName == null ) databaseName = """";
+                    if (databaseName == null) databaseName = """";
                     statementConsumer.consume(databaseName, ddlStatements);
                 }
             }",2016-07-20T22:07:56Z,16
"@@ -220,3 +220,12 @@ CREATE TABLE dbz84_integer_types_table (
 );
 INSERT INTO dbz84_integer_types_table
 VALUES(127,-128,128,255, default,201,202,203, default,301,302,303, default,401,402,403, default,501,502,503);
+
+-- DBZ-85 handle fractional part of seconds
+CREATE TABLE dbz_85_fractest (
+  c1 DATE,
+  c2 TIME(2),
+  c3 DATETIME(2),
+  c4 TIMESTAMP(2)
+);
+INSERT INTO dbz_85_fractest VALUES ('2014-09-08', '17:51:04.777', '2014-09-08 17:51:04.777', '2014-09-08 17:51:04.777');
\ No newline at end of file",2016-07-20T22:07:56Z,93
"@@ -7,7 +7,13 @@
 
 import java.nio.file.Path;
 import java.sql.SQLException;
+import java.time.LocalDate;
+import java.time.LocalDateTime;
+import java.time.LocalTime;
+import java.time.Month;
+import java.time.ZoneId;
 
+import org.apache.kafka.connect.data.Struct;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
@@ -16,6 +22,7 @@
 
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.data.Envelope;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.util.Testing;
@@ -69,22 +76,75 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
         // ---------------------------------------------------------------------------------------------------------------
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
-        // Testing.Debug.enable();
-        SourceRecords records = consumeRecordsByTopic(3 + 2); // 3 schema change record, 2 inserts
+        Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(4 + 3); // 4 schema change record, 3 inserts
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(3);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(4);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
-        assertThat(records.topics().size()).isEqualTo(3);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(4);
         assertThat(records.databaseNames().size()).isEqualTo(1);
-        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(3);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(4);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
 
         // Check that all records are valid, can be serialized and deserialized ...
         records.forEach(this::validate);
+        records.forEach(record->{
+            Struct value = (Struct)record.value();
+            if ( record.topic().endsWith(""dbz_85_fractest"")) {
+                // The microseconds of all three should be exactly 780
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                java.util.Date c1 = (java.util.Date)after.get(""c1"");
+                java.util.Date c2 = (java.util.Date)after.get(""c2"");
+                java.util.Date c3 = (java.util.Date)after.get(""c3"");
+                java.util.Date c4 = (java.util.Date)after.get(""c4"");
+                Testing.debug(""c1 = "" + c1.getTime());
+                Testing.debug(""c2 = "" + c2.getTime());
+                Testing.debug(""c3 = "" + c3.getTime());
+                Testing.debug(""c4 = "" + c4.getTime());
+                assertThat(c1.getTime() % 1000).isEqualTo(0);   // date only, no time
+                assertThat(c2.getTime() % 1000).isEqualTo(780);
+                assertThat(c3.getTime() % 1000).isEqualTo(780);
+                assertThat(c4.getTime() % 1000).isEqualTo(780);
+                assertThat(c1.getTime()).isEqualTo(1410134400000L);
+                assertThat(c2.getTime()).isEqualTo(64264780L);
+                assertThat(c3.getTime()).isEqualTo(1410198664780L);
+                assertThat(c4.getTime()).isEqualTo(1410198664780L);
+                // None of these Dates have timezone information, so to convert to locals we have to use our local timezone ...
+                ZoneId utc = ZoneId.of(""UTC"");
+                LocalDate localC1 = c1.toInstant().atZone(utc).toLocalDate();
+                LocalTime localC2 = c2.toInstant().atZone(utc).toLocalTime();
+                LocalDateTime localC3 = c3.toInstant().atZone(utc).toLocalDateTime();
+                LocalDateTime localC4 = c4.toInstant().atZone(utc).toLocalDateTime();
+                // row is ('2014-09-08', '17:51:04.78', '2014-09-08 17:51:04.78', '2014-09-08 17:51:04.78')
+                final int expectedNanos = 780 * 1000 * 1000;
+                assertThat(localC1.getYear()).isEqualTo(2014);
+                assertThat(localC1.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(localC1.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC2.getHour()).isEqualTo(17);
+                assertThat(localC2.getMinute()).isEqualTo(51);
+                assertThat(localC2.getSecond()).isEqualTo(4);
+                assertThat(localC2.getNano()).isEqualTo(expectedNanos);
+                assertThat(localC3.getYear()).isEqualTo(2014);
+                assertThat(localC3.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(localC3.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC3.getHour()).isEqualTo(17);
+                assertThat(localC3.getMinute()).isEqualTo(51);
+                assertThat(localC3.getSecond()).isEqualTo(4);
+                assertThat(localC3.getNano()).isEqualTo(expectedNanos);
+                assertThat(localC4.getYear()).isEqualTo(2014);
+                assertThat(localC4.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(localC4.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC4.getHour()).isEqualTo(17);
+                assertThat(localC4.getMinute()).isEqualTo(51);
+                assertThat(localC4.getSecond()).isEqualTo(4);
+                assertThat(localC4.getNano()).isEqualTo(expectedNanos);
+            }
+        });
     }
 
 }",2016-07-20T22:07:56Z,71
"@@ -6,6 +6,9 @@
 package io.debezium.data;
 
 import java.nio.ByteBuffer;
+import java.time.Instant;
+import java.time.format.DateTimeFormatter;
+import java.time.temporal.TemporalAccessor;
 import java.util.Base64;
 import java.util.List;
 import java.util.Map;
@@ -22,7 +25,7 @@
  * @author Randall Hauch
  */
 public class SchemaUtil {
-
+    
     private SchemaUtil() {
     }
 
@@ -228,8 +231,24 @@ public RecordWriter append(Object obj) {
                 }
                 appendAdditional(""value"", record.value());
                 sb.append('}');
+            } else if ( obj instanceof java.sql.Time ){
+                java.sql.Time time = (java.sql.Time)obj;
+                append(DateTimeFormatter.ISO_LOCAL_TIME.format(time.toLocalTime()));
+            } else if ( obj instanceof java.sql.Date ){
+                java.sql.Date date = (java.sql.Date)obj;
+                append(DateTimeFormatter.ISO_DATE.format(date.toLocalDate()));
+            } else if ( obj instanceof java.sql.Timestamp ){
+                java.sql.Timestamp ts = (java.sql.Timestamp)obj;
+                Instant instant = ts.toInstant();
+                append(DateTimeFormatter.ISO_INSTANT.format(instant));
+            } else if ( obj instanceof java.util.Date ){
+                java.util.Date date = (java.util.Date)obj;
+                append(DateTimeFormatter.ISO_INSTANT.format(date.toInstant()));
+            } else if ( obj instanceof TemporalAccessor ){
+                TemporalAccessor temporal = (TemporalAccessor)obj;
+                append(DateTimeFormatter.ISO_INSTANT.format(temporal));
             } else {
-                sb.append(obj.toString());
+                append(obj.toString());
             }
             return this;
         }",2016-07-20T22:07:56Z,133
"@@ -0,0 +1,330 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.jdbc;
+
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.ZoneId;
+import java.time.ZonedDateTime;
+import java.time.temporal.ChronoUnit;
+
+import io.debezium.annotation.Immutable;
+
+/**
+ * An adapter that can convert {@link java.util.Date}, {@link java.sql.Date}, {@link java.sql.Time}, and
+ * {@link java.sql.Timestamp} objects to {@link ZonedDateTime} instances, where the time zone in which the temporal objects
+ * were created by the database/driver can be adjusted.
+ * 
+ * @author Randall Hauch
+ */
+@Immutable
+public class TimeZoneAdapter {
+
+    private static final LocalDate EPOCH = LocalDate.ofEpochDay(0);
+    public static final ZoneId UTC = ZoneId.of(""UTC"");
+
+    /**
+     * Create a new adapter with UTC as the target zone and for a database that uses UTC for all temporal values.
+     * 
+     * @return the new adapter
+     */
+    public static TimeZoneAdapter create() {
+        return new TimeZoneAdapter(UTC, UTC, UTC, UTC, UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the specified zone for all temporal values.
+     * 
+     * @param zoneId the zone in which all temporal values are created by the database; may not be null
+     * @return the new adapter
+     */
+    public static TimeZoneAdapter originatingIn(ZoneId zoneId) {
+        return new TimeZoneAdapter(ZoneId.systemDefault(), zoneId, zoneId, zoneId, zoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that creates all temporal values in UTC.
+     * 
+     * @return the new adapter
+     */
+    public static TimeZoneAdapter originatingInUtc() {
+        return originatingIn(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that creates all temporal values in the local system time zone,
+     * which is the same time zone used by {@link java.util.Calendar#getInstance()}.
+     * 
+     * @return the new adapter
+     */
+    public static TimeZoneAdapter originatingInLocal() {
+        return originatingIn(ZoneId.systemDefault()); // same as Calendar.getInstance().getTimeZone().toZoneId()
+    }
+
+    private final ZoneId targetZoneId;
+    private final ZoneId utilDateZoneId;
+    private final ZoneId sqlDateZoneId;
+    private final ZoneId sqlTimeZoneId;
+    private final ZoneId sqlTimestampZoneId;
+
+    /**
+     * Create an adapter for temporal values defined in terms of the given zone.
+     * 
+     * @param targetZoneId the zone in which the output temporal values are defined; may not be null
+     * @param utilDateZoneId the zone in which {@link java.util.Date} values are defined; may not be null
+     * @param sqlDateZoneId the zone in which {@link java.sql.Date} values are defined; may not be null
+     * @param sqlTimeZoneId the zone in which {@link java.sql.Time} values are defined; may not be null
+     * @param sqlTimestampZoneId the zone in which {@link java.sql.Timestamp} values are defined; may not be null
+     */
+    protected TimeZoneAdapter(ZoneId targetZoneId, ZoneId utilDateZoneId, ZoneId sqlDateZoneId, ZoneId sqlTimeZoneId,
+            ZoneId sqlTimestampZoneId) {
+        this.targetZoneId = targetZoneId;
+        this.utilDateZoneId = utilDateZoneId;
+        this.sqlDateZoneId = sqlDateZoneId;
+        this.sqlTimeZoneId = sqlTimeZoneId;
+        this.sqlTimestampZoneId = sqlTimestampZoneId;
+    }
+
+    protected ZoneId targetZoneId() {
+        return targetZoneId;
+    }
+
+    /**
+     * Convert the specified database {@link java.util.Date}, {@link java.sql.Date}, {@link java.sql.Time}, or
+     * {@link java.sql.Timestamp} objects to a date and time in the same time zone in which the database created the
+     * value. If only {@link java.sql.Time time} information is provided in the input value, the date information will
+     * be set to the first day of the epoch. If only {@link java.sql.Date date} information is provided in the input
+     * value, the time information will be at midnight on the specified day.
+     * 
+     * @param dbDate the database-generated value; may not be null
+     * @return the date time in the same zone used by the database; never null
+     */
+    public ZonedDateTime toZonedDateTime(java.util.Date dbDate) {
+        if (dbDate instanceof java.sql.Date) {
+            return toZonedDateTime((java.sql.Date) dbDate);
+        }
+        if (dbDate instanceof java.sql.Time) {
+            return toZonedDateTime((java.sql.Time) dbDate);
+        }
+        if (dbDate instanceof java.sql.Timestamp) {
+            return toZonedDateTime((java.sql.Timestamp) dbDate);
+        }
+        return dbDate.toInstant().atZone(UTC) // milliseconds is in terms of UTC
+                     .withZoneSameInstant(sqlTimeZoneId) // correct value in the zone where it was created
+                     .withZoneSameLocal(targetZoneId); // use same value, but in our desired timezone
+    }
+
+    /**
+     * Convert the specified database {@link java.sql.Date} to a date (at midnight) in the same time zone in which the
+     * database created the value.
+     * 
+     * @param dbDate the database-generated value; may not be null
+     * @return the date (at midnight) in the same zone used by the database; never null
+     */
+    public ZonedDateTime toZonedDateTime(java.sql.Date dbDate) {
+        long millis = dbDate.getTime();
+        Instant instant = Instant.ofEpochMilli(millis).truncatedTo(ChronoUnit.DAYS);
+        return instant.atZone(sqlDateZoneId).withZoneSameInstant(targetZoneId);
+    }
+
+    /**
+     * Convert the specified database {@link java.sql.Time} to a time (on the first epoch day) in the same time zone in which
+     * the database created the value.
+     * 
+     * @param dbTime the database-generated value; may not be null
+     * @return the time (on the first epoch day) in the same zone used by the database; never null
+     */
+    public ZonedDateTime toZonedDateTime(java.sql.Time dbTime) {
+        long millis = dbTime.getTime();
+        LocalTime local = LocalTime.ofNanoOfDay(millis * 1000 * 1000);
+        return ZonedDateTime.of(EPOCH, local, UTC) // milliseconds is in terms of UTC
+                            .withZoneSameInstant(sqlTimeZoneId) // correct value in the zone where it was created
+                            .withZoneSameLocal(targetZoneId); // use same value, but in our desired timezone
+    }
+
+    /**
+     * Convert the specified database {@link java.sql.Timestamp} to a timestamp in the same time zone in which
+     * the database created the value.
+     * 
+     * @param dbTimestamp the database-generated value; may not be null
+     * @return the timestamp in the same zone used by the database; never null
+     */
+    public ZonedDateTime toZonedDateTime(java.sql.Timestamp dbTimestamp) {
+        return dbTimestamp.toInstant().atZone(UTC) // milliseconds is in terms of UTC
+                          .withZoneSameInstant(sqlTimestampZoneId) // correct value in the zone where it was created
+                          .withZoneSameLocal(targetZoneId); // use same value, but in our desired timezone
+    }
+
+    /**
+     * Create a new adapter that produces temporal values in the specified time zone.
+     * 
+     * @param zoneId the zone in which all temporal values are to be defined; may not be null
+     * @return the new adapter
+     */
+    public TimeZoneAdapter withTargetZone(ZoneId zoneId) {
+        if (targetZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(zoneId, utilDateZoneId, sqlDateZoneId, sqlTimeZoneId, sqlTimestampZoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the specified zone for all temporal values and this adapter's target zone.
+     * 
+     * @param zoneId the zone in which all temporal values are created by the database; may not be null
+     * @return the new adapter
+     */
+    public TimeZoneAdapter withZoneForAll(ZoneId zoneId) {
+        return new TimeZoneAdapter(targetZoneId, zoneId, zoneId, zoneId, zoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the specified
+     * zone for {@link java.util.Date} temporal values.
+     * 
+     * @param zoneId the zone in which all {@link java.util.Date} values are created by the database; may not be null
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withZoneForUtilDate(ZoneId zoneId) {
+        if (utilDateZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(targetZoneId, zoneId, sqlDateZoneId, sqlTimeZoneId, sqlTimestampZoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the specified
+     * zone for {@link java.sql.Date} temporal values.
+     * 
+     * @param zoneId the zone in which all {@link java.sql.Date} values are created by the database; may not be null
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withZoneForSqlDate(ZoneId zoneId) {
+        if (sqlDateZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(targetZoneId, utilDateZoneId, zoneId, sqlTimeZoneId, sqlTimestampZoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the specified
+     * zone for {@link java.sql.Time} temporal values.
+     * 
+     * @param zoneId the zone in which all {@link java.sql.Time} values are created by the database; may not be null
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withZoneForSqlTime(ZoneId zoneId) {
+        if (sqlTimeZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(targetZoneId, utilDateZoneId, sqlDateZoneId, zoneId, sqlTimestampZoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the specified
+     * zone for {@link java.sql.Timestamp} temporal values.
+     * 
+     * @param zoneId the zone in which all {@link java.sql.Timestamp} values are created by the database; may not be null
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withZoneForSqlTimestamp(ZoneId zoneId) {
+        if (sqlTimestampZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(targetZoneId, utilDateZoneId, sqlDateZoneId, sqlTimeZoneId, zoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for the target.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcTargetZone() {
+        return withTargetZone(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.util.Date} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcZoneForUtilDate() {
+        return withZoneForUtilDate(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Date} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcZoneForSqlDate() {
+        return withZoneForSqlDate(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Time} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcZoneForSqlTime() {
+        return withZoneForSqlTime(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Timestamp} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcZoneForSqlTimestamp() {
+        return withZoneForSqlTimestamp(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for the target.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalTargetZone() {
+        return withTargetZone(ZoneId.systemDefault());
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.util.Date} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalZoneForUtilDate() {
+        return withZoneForUtilDate(ZoneId.systemDefault());
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Date} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalZoneForSqlDate() {
+        return withZoneForSqlDate(ZoneId.systemDefault());
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Time} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalZoneForSqlTime() {
+        return withZoneForSqlTime(ZoneId.systemDefault());
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Timestamp} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalZoneForSqlTimestamp() {
+        return withZoneForSqlTimestamp(ZoneId.systemDefault());
+    }
+}
\ No newline at end of file",2016-07-20T22:07:56Z,134
"@@ -10,11 +10,11 @@
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
-import java.time.Instant;
 import java.time.LocalDate;
 import java.time.OffsetDateTime;
 import java.time.OffsetTime;
 import java.time.ZoneOffset;
+import java.time.ZonedDateTime;
 import java.time.temporal.ChronoField;
 import java.time.temporal.ChronoUnit;
 import java.util.ArrayList;
@@ -43,6 +43,7 @@
 import io.debezium.data.IsoTimestamp;
 import io.debezium.data.SchemaUtil;
 import io.debezium.jdbc.JdbcConnection;
+import io.debezium.jdbc.TimeZoneAdapter;
 import io.debezium.relational.mapping.ColumnMapper;
 import io.debezium.relational.mapping.ColumnMappers;
 
@@ -77,14 +78,26 @@ public class TableSchemaBuilder {
     private static final LocalDate EPOCH_DAY = LocalDate.ofEpochDay(0);
 
     private final Function<String, String> schemaNameValidator;
+    private final TimeZoneAdapter timeZoneAdapter;
 
     /**
-     * Create a new instance of the builder.
+     * Create a new instance of the builder that uses the {@link TimeZoneAdapter#create() default time zone adapter}.
      * 
      * @param schemaNameValidator the validation function for schema names; may not be null
      */
     public TableSchemaBuilder(Function<String, String> schemaNameValidator) {
+        this(TimeZoneAdapter.create(),schemaNameValidator);
+    }
+
+    /**
+     * Create a new instance of the builder.
+     * 
+     * @param timeZoneAdapter the adapter for temporal objects created by the source database; may not be null
+     * @param schemaNameValidator the validation function for schema names; may not be null
+     */
+    public TableSchemaBuilder(TimeZoneAdapter timeZoneAdapter, Function<String, String> schemaNameValidator) {
         this.schemaNameValidator = schemaNameValidator;
+        this.timeZoneAdapter = timeZoneAdapter;
     }
 
     /**
@@ -612,6 +625,7 @@ protected Object handleUnknownData(Column column, Field fieldDefn, Object data)
     protected Object convertTimestampWithZone(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         OffsetDateTime dateTime = null;
+        LoggerFactory.getLogger(getClass()).info(""TimestampWithZone: "" + data + "" , class="" + data.getClass());
         if (data instanceof OffsetDateTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
             dateTime = (OffsetDateTime) data;
@@ -673,6 +687,7 @@ protected OffsetDateTime unexpectedTimestampWithZone(Object value, Field fieldDe
     protected Object convertTimeWithZone(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         OffsetTime time = null;
+        LoggerFactory.getLogger(getClass()).info(""TimeWithZone: "" + data + "" , class="" + data.getClass());
         if (data instanceof OffsetTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
             time = (OffsetTime) data;
@@ -727,15 +742,10 @@ protected OffsetTime unexpectedTimeWithZone(Object value, Field fieldDefn) {
     protected Object convertTimestamp(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         java.util.Date date = null;
-        if (data instanceof java.sql.Timestamp) {
-            // JDBC specification indicates that this will be the canonical object for this JDBC type.
-            date = (java.util.Date) data;
-        } else if (data instanceof java.sql.Date) {
-            // This should still work, even though it should have just date info
-            date = (java.util.Date) data;
-        } else if (data instanceof java.util.Date) {
-            // Possible that some implementations might use this.
-            date = (java.util.Date) data;
+        LoggerFactory.getLogger(getClass()).info(""Timestamp: "" + data + "" , class="" + data.getClass());
+        if (data instanceof java.util.Date) {
+            ZonedDateTime zdt = timeZoneAdapter.toZonedDateTime((java.util.Date)data);
+            date = java.util.Date.from(zdt.toInstant());
         } else if (data instanceof java.time.LocalDate) {
             // If we get a local date (no TZ info), we need to just convert to a util.Date (no TZ info) ...
             java.time.LocalDate local = (java.time.LocalDate) data;
@@ -780,16 +790,10 @@ protected java.util.Date unexpectedTimestamp(Object value, Field fieldDefn) {
     protected Object convertTime(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         java.util.Date date = null;
-        if (data instanceof java.sql.Time) {
-            // JDBC specification indicates that this will be the canonical object for this JDBC type.
-            // Contains only time info, with the date set to the epoch day ...
-            date = (java.sql.Date) data;
-        } else if (data instanceof java.util.Date) {
-            // Possible that some implementations might use this. We ignore any date info by converting to an
-            // instant and changing the date to the epoch date, and finally creating a new java.util.Date ...
-            date = (java.util.Date) data;
-            Instant instant = Instant.ofEpochMilli(date.getTime()).with(ChronoField.EPOCH_DAY, 0);
-            date = new java.util.Date(instant.toEpochMilli());
+        LoggerFactory.getLogger(getClass()).info(""Time: "" + data + "" , class="" + data.getClass());
+        if (data instanceof java.util.Date) {
+            ZonedDateTime zdt = timeZoneAdapter.toZonedDateTime((java.util.Date)data);
+            date = java.util.Date.from(zdt.toInstant());
         } else if (data instanceof java.time.LocalTime) {
             // If we get a local time (no TZ info), we need to just convert to a util.Date (no TZ info) ...
             java.time.LocalTime local = (java.time.LocalTime) data;
@@ -834,21 +838,10 @@ protected java.util.Date unexpectedTime(Object value, Field fieldDefn) {
     protected Object convertDate(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         java.util.Date date = null;
-        if (data instanceof java.sql.Date) {
-            // JDBC specification indicates that this will be the nominal object for this JDBC type.
-            // Contains only date info, with all time values set to all zeros (e.g. midnight).
-            // However, the java.sql.Date object *may* contain timezone information for some DBMS+Driver combinations.
-            // Therefore, first convert it to a local LocalDate, then to a LocalDateTime at midnight, and then to an
-            // instant in UTC ...
-            java.sql.Date sqlDate = (java.sql.Date) data;
-            LocalDate localDate = sqlDate.toLocalDate();
-            date = java.util.Date.from(localDate.atStartOfDay().toInstant(ZoneOffset.UTC));
-        } else if (data instanceof java.util.Date) {
-            // Possible that some implementations might use this. We should be prepared to ignore any time,
-            // information by truncating to days and creating a new java.util.Date ...
-            date = (java.util.Date) data;
-            Instant instant = Instant.ofEpochMilli(date.getTime()).truncatedTo(ChronoUnit.DAYS);
-            date = new java.util.Date(instant.toEpochMilli());
+        LoggerFactory.getLogger(getClass()).info(""Date: "" + data + "" , class="" + data.getClass());
+        if (data instanceof java.util.Date) {
+            ZonedDateTime zdt = timeZoneAdapter.toZonedDateTime((java.util.Date)data);
+            date = java.util.Date.from(zdt.toInstant());
         } else if (data instanceof java.time.LocalDate) {
             // If we get a local date (no TZ info), we need to just convert to a util.Date (no TZ info) ...
             java.time.LocalDate local = (java.time.LocalDate) data;",2016-07-20T22:07:56Z,135
"@@ -0,0 +1,208 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.jdbc;
+
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.Month;
+import java.time.ZonedDateTime;
+import java.util.Calendar;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class TimeZoneAdapterTest {
+
+    private TimeZoneAdapter adapter;
+
+    @Before
+    public void beforeEach() {
+        adapter = TimeZoneAdapter.create()
+                                 .withLocalZoneForUtilDate()
+                                 .withLocalZoneForSqlDate()
+                                 .withLocalZoneForSqlTime()
+                                 .withLocalZoneForSqlTimestamp()
+                                 .withUtcTargetZone();
+    }
+
+    @Test
+    public void shouldAdaptSqlDate() {
+        // '2014-09-08', '17:51:04.777', '2014-09-08 17:51:04.777', '2014-09-08 17:51:04.777'
+        java.sql.Date sqlDate = createSqlDate(2014, Month.SEPTEMBER, 8);
+        ZonedDateTime zdt = adapter.toZonedDateTime(sqlDate);
+        // The date should match ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(2014);
+        assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
+        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        // There should be no time component ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(0);
+        assertThat(time.getMinute()).isEqualTo(0);
+        assertThat(time.getSecond()).isEqualTo(0);
+        assertThat(time.getNano()).isEqualTo(0);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    @Test
+    public void shouldAdaptSqlTime() {
+        // '17:51:04.777'
+        java.sql.Time sqlTime = createSqlTime(17, 51, 04, 777);
+        ZonedDateTime zdt = adapter.toZonedDateTime(sqlTime);
+        // The date should be at epoch ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(1970);
+        assertThat(date.getMonth()).isEqualTo(Month.JANUARY);
+        assertThat(date.getDayOfMonth()).isEqualTo(1);
+        // The time should match exactly ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(17);
+        assertThat(time.getMinute()).isEqualTo(51);
+        assertThat(time.getSecond()).isEqualTo(4);
+        assertThat(time.getNano()).isEqualTo(777 * 1000 * 1000);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    @Test
+    public void shouldAdaptSqlTimestamp() {
+        adapter = TimeZoneAdapter.create()
+                                 .withLocalZoneForSqlTimestamp()
+                                 .withUtcTargetZone();
+
+        // '2014-09-08 17:51:04.777'
+        // This technique creates the timestamp using the milliseconds from epoch in terms of the local zone ...
+        java.sql.Timestamp sqlTimestamp = createSqlTimestamp(2014, Month.SEPTEMBER, 8, 17, 51, 04, 777);
+        ZonedDateTime zdt = adapter.toZonedDateTime(sqlTimestamp);
+        // The date should match ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(2014);
+        assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
+        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        // The time should match exactly ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(17);
+        assertThat(time.getMinute()).isEqualTo(51);
+        assertThat(time.getSecond()).isEqualTo(4);
+        assertThat(time.getNano()).isEqualTo(777 * 1000 * 1000);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    @Test
+    public void shouldAdaptSqlTimestampViaSecondsAndMillis() {
+        adapter = TimeZoneAdapter.create()
+                                 .withUtcZoneForSqlTimestamp()
+                                 .withUtcTargetZone();
+
+        // '2014-09-08 17:51:04.777'
+        // This technique creates the timestamp using the milliseconds from epoch in terms of UTC ...
+        java.sql.Timestamp sqlTimestamp = createSqlTimestamp(1410198664L, 777);
+        ZonedDateTime zdt = adapter.toZonedDateTime(sqlTimestamp);
+        // The date should match ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(2014);
+        assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
+        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        // The time should match exactly ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(17);
+        assertThat(time.getMinute()).isEqualTo(51);
+        assertThat(time.getSecond()).isEqualTo(4);
+        assertThat(time.getNano()).isEqualTo(777 * 1000 * 1000);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    @Test
+    public void shouldAdaptUtilDate() {
+        // '2014-09-08 17:51:04.777'
+        java.util.Date utilDate = createUtilDate(2014, Month.SEPTEMBER, 8, 17, 51, 04, 777);
+        ZonedDateTime zdt = adapter.toZonedDateTime(utilDate);
+        // The date should match ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(2014);
+        assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
+        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        // The time should match exactly ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(17);
+        assertThat(time.getMinute()).isEqualTo(51);
+        assertThat(time.getSecond()).isEqualTo(4);
+        assertThat(time.getNano()).isEqualTo(777 * 1000 * 1000);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    protected java.sql.Date createSqlDate(int year, Month month, int dayOfMonth) {
+        Calendar cal = Calendar.getInstance();
+        cal.clear();
+        cal.set(Calendar.YEAR, year);
+        cal.set(Calendar.MONTH, month.getValue() - 1);
+        cal.set(Calendar.DATE, dayOfMonth);
+        return new java.sql.Date(cal.getTimeInMillis());
+    }
+
+    protected java.sql.Time createSqlTime(int hourOfDay, int minute, int second, int milliseconds) {
+        Calendar c = Calendar.getInstance();
+        c.clear();
+        c.set(Calendar.HOUR_OF_DAY, hourOfDay);
+        c.set(Calendar.MINUTE, minute);
+        c.set(Calendar.SECOND, second);
+        c.set(Calendar.MILLISECOND, milliseconds);
+        return new java.sql.Time(c.getTimeInMillis());
+    }
+
+    /**
+     * This sets the calendar via the milliseconds past epoch, and this behaves differently than actually setting the various
+     * components of the calendar (see {@link #createSqlTimestamp(int, Month, int, int, int, int, int)}). This is how the
+     * MySQL Binary Log client library creates timestamps (v2).
+     * 
+     * @param secondsFromEpoch the number of seconds since epoch
+     * @param millis the number of milliseconds
+     * @return the SQL timestamp
+     */
+    protected java.sql.Timestamp createSqlTimestamp(long secondsFromEpoch, int millis) {
+        Calendar c = Calendar.getInstance();
+        c.setTimeInMillis(secondsFromEpoch * 1000);
+        c.set(Calendar.MILLISECOND, millis);
+        return new java.sql.Timestamp(c.getTimeInMillis());
+    }
+
+    protected java.sql.Timestamp createSqlTimestamp(int year, Month month, int dayOfMonth, int hourOfDay, int minute, int second,
+                                                    int milliseconds) {
+        Calendar c = Calendar.getInstance();
+        c.set(Calendar.YEAR, year);
+        c.set(Calendar.MONTH, month.getValue() - 1);
+        c.set(Calendar.DAY_OF_MONTH, dayOfMonth);
+        c.set(Calendar.HOUR_OF_DAY, hourOfDay);
+        c.set(Calendar.MINUTE, minute);
+        c.set(Calendar.SECOND, second);
+        c.set(Calendar.MILLISECOND, milliseconds);
+        return new java.sql.Timestamp(c.getTimeInMillis());
+    }
+
+    protected java.util.Date createUtilDate(int year, Month month, int dayOfMonth, int hourOfDay, int minute, int second,
+                                            int milliseconds) {
+        Calendar c = Calendar.getInstance();
+        c.set(Calendar.YEAR, year);
+        c.set(Calendar.MONTH, month.getValue() - 1);
+        c.set(Calendar.DAY_OF_MONTH, dayOfMonth);
+        c.set(Calendar.HOUR_OF_DAY, hourOfDay);
+        c.set(Calendar.MINUTE, minute);
+        c.set(Calendar.SECOND, second);
+        c.set(Calendar.MILLISECOND, milliseconds);
+        return c.getTime();
+    }
+
+}",2016-07-20T22:07:56Z,126
"@@ -64,7 +64,7 @@
         <version.postgresql.server>9.4</version.postgresql.server>
         <version.mysql.server>5.7</version.mysql.server>
         <version.mysql.driver>5.1.39</version.mysql.driver>
-        <version.mysql.binlog>0.3.1</version.mysql.binlog>
+        <version.mysql.binlog>0.3.2</version.mysql.binlog>
         <version.mongo.server>3.2.6</version.mongo.server>
         <version.mongo.driver>3.2.2</version.mongo.driver>
 ",2016-07-20T22:07:56Z,82
"@@ -172,3 +172,24 @@ VALUES (default, '2016-01-16', 1001, 1, 102),
        (default, '2016-02-18', 1004, 3, 109),
        (default, '2016-02-19', 1002, 2, 106),
        (default, '2016-02-21', 1003, 1, 107);
+
+
+
+# ----------------------------------------------------------------------------------------------------------------
+# DATABASE:  regression_test
+# ----------------------------------------------------------------------------------------------------------------
+# The integration test for this database expects to scans all of the binlog events associated with this database
+# without error or problems. The integration test does not modify any records in this database, so this script
+# must contain all operations to these tables.
+#
+CREATE DATABASE regression_test;
+USE regression_test;
+
+# DBZ-61 handle binary value recorded as hex string value
+CREATE TABLE t1464075356413_testtable6 (
+  pk_column int auto_increment NOT NULL,
+  varbinary_col varbinary(20) NOT NULL,
+  PRIMARY KEY(pk_column)
+);
+INSERT INTO t1464075356413_testtable6 (pk_column, varbinary_col)
+VALUES(default, 0x4D7953514C)",2016-06-07T22:53:07Z,93
"@@ -0,0 +1,89 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.nio.file.Path;
+import java.sql.SQLException;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.relational.history.FileDatabaseHistory;
+import io.debezium.util.Testing;
+
+/**
+ * @author Randall Hauch
+ */
+public class MySqlConnectorRegressionIT extends AbstractConnectorTest {
+
+    private static final Path DB_HISTORY_PATH = Testing.Files.createTestingPath(""file-db-history-regression.txt"").toAbsolutePath();
+
+    private Configuration config;
+
+    @Before
+    public void beforeEach() {
+        stopConnector();
+        initializeConnectorTestFramework();
+        Testing.Files.delete(DB_HISTORY_PATH);
+    }
+
+    @After
+    public void afterEach() {
+        try {
+            stopConnector();
+        } finally {
+            Testing.Files.delete(DB_HISTORY_PATH);
+        }
+    }
+
+    @Test
+    public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+        
+        // ---------------------------------------------------------------------------------------------------------------
+        // Consume all of the events due to startup and initialization of the database
+        // ---------------------------------------------------------------------------------------------------------------
+        //Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(2+1);   // 2 schema change record, 1 insert
+        stopConnector();
+        assertThat(records).isNotNull();
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(2);
+        assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(2);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(2);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+    }
+
+}",2016-06-07T22:53:07Z,71
"@@ -5,6 +5,8 @@
  */
 package io.debezium.data;
 
+import java.nio.ByteBuffer;
+import java.util.Base64;
 import java.util.List;
 import java.util.Map;
 
@@ -169,6 +171,11 @@ public RecordWriter append(Object obj) {
                     appendFirst(field.name(), s.get(field));
                 }
                 sb.append('}');
+            } else if (obj instanceof ByteBuffer) {
+                ByteBuffer b = (ByteBuffer) obj;
+                sb.append('""').append(Base64.getEncoder().encode(b.array())).append('""');
+            } else if (obj instanceof byte[]) {
+                sb.append('""').append(Base64.getEncoder().encode((byte[])obj)).append('""');
             } else if (obj instanceof Map<?, ?>) {
                 Map<?, ?> map = (Map<?, ?>) obj;
                 sb.append('{');",2016-06-07T22:53:07Z,133
"@@ -6,6 +6,7 @@
 package io.debezium.relational;
 
 import java.math.BigDecimal;
+import java.nio.ByteBuffer;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
@@ -105,7 +106,8 @@ public TableSchema create(ResultSet resultSet, String name) throws SQLException
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
@@ -121,15 +123,16 @@ public TableSchema create(String schemaPrefix, Table table) {
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @param filter the filter that specifies whether columns in the table should be included; may be null if all columns
      *            are to be included
      * @param mappers the mapping functions for columns; may be null if none of the columns are to be mapped to different values
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
     public TableSchema create(String schemaPrefix, Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
-        if ( schemaPrefix == null ) schemaPrefix = """";
+        if (schemaPrefix == null) schemaPrefix = """";
         // Build the schemas ...
         final TableId tableId = table.id();
         final String tableIdStr = tableId.toString();
@@ -285,7 +288,10 @@ protected ValueConverter[] convertersForColumns(Schema schema, TableId tableId,
                 if (mappers != null) {
                     ValueConverter mappingConverter = mappers.mappingConverterFor(tableId, column);
                     if (mappingConverter != null) {
-                        converter = (value) -> mappingConverter.convert(valueConverter.convert(value));
+                        converter = (value) -> {
+                            if (value != null) value = valueConverter.convert(value);
+                            return mappingConverter.convert(value);
+                        };
                     }
                 }
                 if (converter == null) converter = valueConverter;
@@ -376,7 +382,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
             case Types.NCLOB:
                 fieldBuilder = SchemaBuilder.string();
                 break;
-                
+
             // Variable-length string values
             case Types.VARCHAR:
             case Types.LONGVARCHAR:
@@ -424,7 +430,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
         if (fieldBuilder != null) {
             if (mapper != null) {
                 // Let the mapper add properties to the schema ...
-                mapper.alterFieldSchema(column,fieldBuilder);
+                mapper.alterFieldSchema(column, fieldBuilder);
             }
             if (column.isOptional()) fieldBuilder.optional();
             builder.field(column.name(), fieldBuilder.build());
@@ -450,6 +456,14 @@ protected SchemaBuilder addOtherField(Column column, ColumnMapper mapper) {
     /**
      * Create a {@link ValueConverter} that can be used to convert row values for the given column into the Kafka Connect value
      * object described by the {@link Field field definition}.
+     * <p>
+     * Subclasses can override this method to specialize the behavior. The subclass method should do custom checks and
+     * conversions,
+     * and then delegate to this method implementation to handle all other cases.
+     * <p>
+     * Alternatively, subclasses can leave this method as-is and instead override one of the lower-level type-specific methods
+     * that this method calls (e.g., {@link #convertBinary(Column, Field, Object)},
+     * {@link #convertTinyInt(Column, Field, Object)}, etc.).
      * 
      * @param column the column describing the input values; never null
      * @param fieldDefn the definition for the field in a Kafka Connect {@link Schema} describing the output of the function;
@@ -461,94 +475,38 @@ protected ValueConverter createValueConverterFor(Column column, Field fieldDefn)
             case Types.NULL:
                 return (data) -> null;
             case Types.BIT:
+                return (data) -> convertBit(column, fieldDefn, data);
             case Types.BOOLEAN:
-                return (data) -> {
-                    if (data instanceof Boolean) return (Boolean) data;
-                    if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBoolean(column, fieldDefn, data);
 
             // Binary values ...
             case Types.BLOB:
             case Types.BINARY:
             case Types.VARBINARY:
             case Types.LONGVARBINARY:
-                return (data) -> (byte[]) data;
+                return (data) -> convertBinary(column, fieldDefn, data);
 
             // Numeric integers
             case Types.TINYINT:
-                return (data) -> {
-                    if (data instanceof Byte) return (Byte) data;
-                    if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertTinyInt(column, fieldDefn, data);
             case Types.SMALLINT:
-                return (data) -> {
-                    if (data instanceof Short) return (Short) data;
-                    if (data instanceof Integer) return new Short(((Integer) data).shortValue());
-                    if (data instanceof Long) return new Short(((Long) data).shortValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertSmallInt(column, fieldDefn, data);
             case Types.INTEGER:
-                return (data) -> {
-                    if (data instanceof Integer) return (Integer) data;
-                    if (data instanceof Short) return new Integer(((Short) data).intValue());
-                    if (data instanceof Long) return new Integer(((Long) data).intValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertInteger(column, fieldDefn, data);
             case Types.BIGINT:
-                return (data) -> {
-                    if (data instanceof Long) return (Long) data;
-                    if (data instanceof Integer) return new Long(((Integer) data).longValue());
-                    if (data instanceof Short) return new Long(((Short) data).longValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBigInt(column, fieldDefn, data);
 
             // Numeric decimal numbers
             case Types.FLOAT:
+                return (data) -> convertFloat(column, fieldDefn, data);
             case Types.DOUBLE:
-                return (data) -> {
-                    if (data instanceof Double) return (Double) data;
-                    if (data instanceof Float) return new Double(((Float) data).doubleValue());
-                    if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
-                    if (data instanceof Long) return new Double(((Long) data).doubleValue());
-                    if (data instanceof Short) return new Double(((Short) data).doubleValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertDouble(column, fieldDefn, data);
             case Types.REAL:
-                return (data) -> {
-                    if (data instanceof Float) return (Float) data;
-                    if (data instanceof Double) return new Float(((Double) data).floatValue());
-                    if (data instanceof Integer) return new Float(((Integer) data).floatValue());
-                    if (data instanceof Long) return new Float(((Long) data).floatValue());
-                    if (data instanceof Short) return new Float(((Short) data).floatValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertReal(column, fieldDefn, data);
             case Types.NUMERIC:
+                return (data) -> convertNumeric(column, fieldDefn, data);
             case Types.DECIMAL:
-                return (data) -> {
-                    BigDecimal decimal = null;
-                    if (data instanceof BigDecimal)
-                        decimal = (BigDecimal) data;
-                    else if (data instanceof Boolean)
-                        decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
-                    else if (data instanceof Short)
-                        decimal = new BigDecimal(((Short) data).intValue());
-                    else if (data instanceof Integer)
-                        decimal = new BigDecimal(((Integer) data).intValue());
-                    else if (data instanceof Long)
-                        decimal = BigDecimal.valueOf(((Long) data).longValue());
-                    else if (data instanceof Float)
-                        decimal = BigDecimal.valueOf(((Float) data).doubleValue());
-                    else if (data instanceof Double)
-                        decimal = BigDecimal.valueOf(((Double) data).doubleValue());
-                    else {
-                        handleUnknownData(column, fieldDefn, data);
-                    }
-                    return decimal;
-                };
+                return (data) -> convertDecimal(column, fieldDefn, data);
 
             // String values
             case Types.CHAR: // variable-length
@@ -561,26 +519,23 @@ else if (data instanceof Double)
             case Types.NCLOB: // fixed-length
             case Types.DATALINK:
             case Types.SQLXML:
-                return (data) -> data.toString();
+                return (data) -> convertString(column, fieldDefn, data);
 
             // Date and time values
             case Types.DATE:
-                return (data) -> convertDate(fieldDefn, data);
+                return (data) -> convertDate(column, fieldDefn, data);
             case Types.TIME:
-                return (data) -> convertTime(fieldDefn, data);
+                return (data) -> convertTime(column, fieldDefn, data);
             case Types.TIMESTAMP:
-                return (data) -> convertTimestamp(fieldDefn, data);
+                return (data) -> convertTimestamp(column, fieldDefn, data);
             case Types.TIME_WITH_TIMEZONE:
-                return (data) -> convertTimeWithZone(fieldDefn, data);
+                return (data) -> convertTimeWithZone(column, fieldDefn, data);
             case Types.TIMESTAMP_WITH_TIMEZONE:
-                return (data) -> convertTimestampWithZone(fieldDefn, data);
+                return (data) -> convertTimestampWithZone(column, fieldDefn, data);
 
             // Other types ...
             case Types.ROWID:
-                return (data) -> {
-                    java.sql.RowId rowId = (java.sql.RowId) data;
-                    return rowId.getBytes();
-                };
+                return (data) -> convertRowId(column, fieldDefn, data);
 
             // Unhandled types
             case Types.ARRAY:
@@ -620,11 +575,13 @@ protected Object handleUnknownData(Column column, Field fieldDefn, Object data)
      * This method handles several types of objects, including {@link OffsetDateTime}, {@link java.sql.Timestamp},
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestampWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimestampWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetDateTime dateTime = null;
         if (data instanceof OffsetDateTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -679,11 +636,13 @@ protected OffsetDateTime unexpectedTimestampWithZone(Object value, Field fieldDe
      * {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types have date components, those date
      * components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimeWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimeWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetTime time = null;
         if (data instanceof OffsetTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -731,11 +690,13 @@ protected OffsetTime unexpectedTimeWithZone(Object value, Field fieldDefn) {
      * but no time zone info. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such
      * as {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestamp(Field fieldDefn, Object data) {
+    protected Object convertTimestamp(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Timestamp) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -782,11 +743,13 @@ protected java.util.Date unexpectedTimestamp(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types might
      * have date components, those date components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTime(Field fieldDefn, Object data) {
+    protected Object convertTime(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Time) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -834,11 +797,13 @@ protected java.util.Date unexpectedTime(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
      * have time components, those time components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertDate(Field fieldDefn, Object data) {
+    protected Object convertDate(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Date) {
             // JDBC specification indicates that this will be the nominal object for this JDBC type.
@@ -882,4 +847,286 @@ protected java.util.Date unexpectedDate(Object value, Field fieldDefn) {
                     fieldDefn.schema(), value.getClass(), value);
         return null;
     }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have time components, those time components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBinary(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof char[]) {
+            data = new String((char[]) data); // convert to string
+        }
+        if (data instanceof String) {
+            // This was encoded as a hexadecimal string, but we receive it as a normal string ...
+            data = ((String) data).getBytes();
+        }
+        if (data instanceof byte[]) {
+            return ByteBuffer.wrap((byte[])data);
+        }
+        // An unexpected value
+        return unexpectedBinary(data, fieldDefn);
+    }
+
+    /**
+     * Handle the unexpected value from a row with a column type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * 
+     * @param value the binary value for which no conversion was found; never null
+     * @param fieldDefn the field definition in the Kafka Connect schema; never null
+     * @return the converted value, or null
+     * @see #convertBinary(Column, Field, Object)
+     */
+    protected byte[] unexpectedBinary(Object value, Field fieldDefn) {
+        LOGGER.warn(""Unexpected JDBC BINARY value for field {} with schema {}: class={}, value={}"", fieldDefn.name(),
+                    fieldDefn.schema(), value.getClass(), value);
+        return null;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#TINYINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTinyInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Byte) return data;
+        if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#SMALLINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertSmallInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Short) return data;
+        if (data instanceof Integer) return new Short(((Integer) data).shortValue());
+        if (data instanceof Long) return new Short(((Long) data).shortValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertInteger(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Integer) return data;
+        if (data instanceof Short) return new Integer(((Short) data).intValue());
+        if (data instanceof Long) return new Integer(((Long) data).intValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBigInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Long) return data;
+        if (data instanceof Integer) return new Long(((Integer) data).longValue());
+        if (data instanceof Short) return new Long(((Short) data).longValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#FLOAT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertFloat(Column column, Field fieldDefn, Object data) {
+        return convertDouble(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DOUBLE}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDouble(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Double) return data;
+        if (data instanceof Float) return new Double(((Float) data).doubleValue());
+        if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
+        if (data instanceof Long) return new Double(((Long) data).doubleValue());
+        if (data instanceof Short) return new Double(((Short) data).doubleValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#REAL}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertReal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Float) return data;
+        if (data instanceof Double) return new Float(((Double) data).floatValue());
+        if (data instanceof Integer) return new Float(((Integer) data).floatValue());
+        if (data instanceof Long) return new Float(((Long) data).floatValue());
+        if (data instanceof Short) return new Float(((Short) data).floatValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertNumeric(Column column, Field fieldDefn, Object data) {
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDecimal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#CHAR}, {@link Types#VARCHAR},
+     * {@link Types#LONGVARCHAR}, {@link Types#CLOB}, {@link Types#NCHAR}, {@link Types#NVARCHAR}, {@link Types#LONGNVARCHAR},
+     * {@link Types#NCLOB}, {@link Types#DATALINK}, and {@link Types#SQLXML}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertString(Column column, Field fieldDefn, Object data) {
+        return data == null ? null : data.toString();
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#ROWID}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertRowId(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof java.sql.RowId) {
+            java.sql.RowId row = (java.sql.RowId)data;
+            return ByteBuffer.wrap(row.getBytes());
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BIT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBit(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BOOLEAN}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
 }",2016-06-07T22:53:07Z,135
"@@ -7,8 +7,13 @@
 
 import static org.junit.Assert.fail;
 
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.HashMap;
+import java.util.Iterator;
 import java.util.Map;
+import java.util.Objects;
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
@@ -47,18 +52,18 @@ public class VerifyRecord {
     private static final MockSchemaRegistryClient schemaRegistry = new MockSchemaRegistryClient();
     private static final AvroConverter avroKeyConverter = new AvroConverter(schemaRegistry);
     private static final AvroConverter avroValueConverter = new AvroConverter(schemaRegistry);
-    
+
     static {
-        Map<String,Object> config = new HashMap<>();
-        config.put(""schemas.enable"",Boolean.TRUE.toString());
-        config.put(""schemas.cache.size"",100);
+        Map<String, Object> config = new HashMap<>();
+        config.put(""schemas.enable"", Boolean.TRUE.toString());
+        config.put(""schemas.cache.size"", 100);
         keyJsonConverter.configure(config, true);
         keyJsonDeserializer.configure(config, true);
         valueJsonConverter.configure(config, false);
         valueJsonDeserializer.configure(config, false);
 
         config = new HashMap<>();
-        config.put(""schema.registry.url"",""http://fake-url"");
+        config.put(""schema.registry.url"", ""http://fake-url"");
         avroKeyConverter.configure(config, false);
         avroValueConverter.configure(config, false);
     }
@@ -276,73 +281,97 @@ public static void isValid(SourceRecord record) {
         SchemaAndValue valueWithSchema = null;
         SchemaAndValue avroKeyWithSchema = null;
         SchemaAndValue avroValueWithSchema = null;
+        String msg = null;
         try {
             // The key should never be null ...
+            msg = ""checking key is not null"";
             assertThat(record.key()).isNotNull();
             assertThat(record.keySchema()).isNotNull();
 
             // If the value is not null there must be a schema; otherwise, the schema should also be null ...
             if (record.value() == null) {
+                msg = ""checking value schema is null"";
                 assertThat(record.valueSchema()).isNull();
             } else {
+                msg = ""checking value schema is not null"";
                 assertThat(record.valueSchema()).isNotNull();
             }
 
             // First serialize and deserialize the key ...
+            msg = ""serializing key using JSON converter"";
             byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using JSON deserializer"";
             keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            msg = ""deserializing key using JSON converter"";
             keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            msg = ""comparing key schema to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // then the value ...
+            msg = ""serializing value using JSON converter"";
             byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using JSON deserializer"";
             valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            msg = ""deserializing value using JSON converter"";
             valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
             // Serialize and deserialize the key using the Avro converter, and check that we got the same result ...
+            msg = ""serializing key using Avro converter"";
             byte[] avroKeyBytes = avroValueConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using Avro converter"";
             avroKeyWithSchema = avroValueConverter.toConnectData(record.topic(), avroKeyBytes);
-            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
-            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key schema to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.schema(),record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.value(),record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // Serialize and deserialize the value using the Avro converter, and check that we got the same result ...
+            msg = ""serializing value using Avro converter"";
             byte[] avroValueBytes = avroValueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using Avro converter"";
             avroValueWithSchema = avroValueConverter.toConnectData(record.topic(), avroValueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
         } catch (Throwable t) {
             Testing.Print.enable();
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
             Testing.printError(t);
-            if (keyJson == null ){
-                Testing.print(""error deserializing key from JSON: "" + SchemaUtil.asString(record.key()));
-            } else if (keyWithSchema == null ){
-                Testing.print(""error using JSON converter on key: "" + prettyJson(keyJson));
-            } else if (avroKeyWithSchema == null ){
-                Testing.print(""error using Avro converter on key: "" + prettyJson(keyJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            Testing.print(""error "" + msg);
+            Testing.print(""  key: "" + SchemaUtil.asString(record.key()));
+            Testing.print(""  key deserialized from JSON: "" + prettyJson(keyJson));
+            if (keyWithSchema != null) {
+                Testing.print(""  key to/from JSON: "" + SchemaUtil.asString(keyWithSchema.value()));
             }
-
-            if (valueJson == null ){
-                Testing.print(""error deserializing value from JSON: "" + SchemaUtil.asString(record.value()));
-            } else if (valueWithSchema == null ){
-                Testing.print(""error using JSON converter on value: "" + prettyJson(valueJson));
-            } else if (avroValueWithSchema == null ){
-                Testing.print(""error using Avro converter on value: "" + prettyJson(valueJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            if (avroKeyWithSchema != null) {
+                Testing.print(""  key to/from Avro: "" + SchemaUtil.asString(avroKeyWithSchema.value()));
+            }
+            Testing.print(""  value: "" + SchemaUtil.asString(record.value()));
+            Testing.print(""  value deserialized from JSON: "" + prettyJson(valueJson));
+            if (valueWithSchema != null) {
+                Testing.print(""  value to/from JSON: "" + SchemaUtil.asString(valueWithSchema.value()));
+            }
+            if (avroValueWithSchema != null) {
+                Testing.print(""  value to/from Avro: "" + SchemaUtil.asString(avroValueWithSchema.value()));
             }
             if (t instanceof AssertionError) throw t;
-            fail(t.getMessage());
+            fail(""error "" + msg + "": "" + t.getMessage());
         }
     }
 
@@ -390,5 +419,135 @@ protected static String prettyJson(JsonNode json) {
             return null;
         }
     }
+    
+    // The remaining methods are needed simply because of the KAFKA-3803, so our comparisons cannot rely upon Struct.equals
+    
+    protected static void assertEquals( Object o1, Object o2 ) {
+        // assertThat(o1).isEqualTo(o2);
+        if ( !equals(o1,o2) ) {
+            fail(SchemaUtil.asString(o1) + "" was not equal to "" + SchemaUtil.asString(o2));
+        }
+    }
+    
+    @SuppressWarnings(""unchecked"")
+    protected static boolean equals( Object o1, Object o2 ) {
+        if ( o1 == o2 ) return true;
+        if (o1 == null) return o2 == null ? true : false;
+        if (o2 == null ) return false;
+        if ( o1 instanceof ByteBuffer ) {
+            o1 = ((ByteBuffer)o1).array();
+        }
+        if ( o2 instanceof ByteBuffer ) {
+            o2 = ((ByteBuffer)o2).array();
+        }
+        if ( o1 instanceof byte[] && o2 instanceof byte[] ) {
+            boolean result = Arrays.equals((byte[])o1,(byte[])o2);
+            return result;
+        }
+        if ( o1 instanceof Object[] && o2 instanceof Object[] ) {
+            boolean result = deepEquals((Object[])o1,(Object[])o2);
+            return result;
+        }
+        if ( o1 instanceof Map && o2 instanceof Map ) {
+            Map<String,Object> m1 = (Map<String,Object>)o1;
+            Map<String,Object> m2 = (Map<String,Object>)o2;
+            if ( !m1.keySet().equals(m2.keySet())) return false;
+            for ( Map.Entry<String, Object> entry : m1.entrySet()) {
+                Object v1 = entry.getValue();
+                Object v2 = m2.get(entry.getKey());
+                if ( !equals(v1,v2) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Collection && o2 instanceof Collection ) {
+            Collection<Object> m1 = (Collection<Object>)o1;
+            Collection<Object> m2 = (Collection<Object>)o2;
+            if ( m1.size() != m2.size() ) return false;
+            Iterator<?> iter1 = m1.iterator();
+            Iterator<?> iter2 = m2.iterator();
+            while ( iter1.hasNext() && iter2.hasNext() ) {
+                if ( !equals(iter1.next(),iter2.next()) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Struct && o2 instanceof Struct ) {
+            // Unfortunately, the Struct.equals() method has a bug in that it is not using Arrays.deepEquals(...) to
+            // compare values in two Struct objects. The result is that the equals only works if the values of the
+            // first level Struct are non arrays; otherwise, the array values are compared using == and that obviously
+            // does not work for non-primitive values.
+            Struct struct1 = (Struct) o1;
+            Struct struct2 = (Struct) o2;
+            if (! Objects.equals(struct1.schema(),struct2.schema()) ) {
+                return false;
+            }
+            Object[] array1 = valuesFor(struct1);
+            Object[] array2 = valuesFor(struct2);
+            boolean result = deepEquals(array1, array2);
+            return result;
+        }
+        return Objects.equals(o1, o2);
+    }
+    
+    private static Object[] valuesFor( Struct struct ) {
+        Object[] array = new Object[struct.schema().fields().size()];
+        int index = 0;
+        for ( Field field : struct.schema().fields() ) {
+            array[index] = struct.get(field);
+            ++index;
+        }
+        return array;
+    }
 
+    private static boolean deepEquals(Object[] a1, Object[] a2) {
+        if (a1 == a2)
+            return true;
+        if (a1 == null || a2==null)
+            return false;
+        int length = a1.length;
+        if (a2.length != length)
+            return false;
+
+        for (int i = 0; i < length; i++) {
+            Object e1 = a1[i];
+            Object e2 = a2[i];
+
+            if (e1 == e2)
+                continue;
+            if (e1 == null)
+                return false;
+
+            // Figure out whether the two elements are equal
+            boolean eq = deepEquals0(e1, e2);
+
+            if (!eq)
+                return false;
+        }
+        return true;
+    }
+
+    private static boolean deepEquals0(Object e1, Object e2) {
+        assert e1 != null;
+        boolean eq;
+        if (e1 instanceof Object[] && e2 instanceof Object[])
+            eq = deepEquals ((Object[]) e1, (Object[]) e2);
+        else if (e1 instanceof byte[] && e2 instanceof byte[])
+            eq = Arrays.equals((byte[]) e1, (byte[]) e2);
+        else if (e1 instanceof short[] && e2 instanceof short[])
+            eq = Arrays.equals((short[]) e1, (short[]) e2);
+        else if (e1 instanceof int[] && e2 instanceof int[])
+            eq = Arrays.equals((int[]) e1, (int[]) e2);
+        else if (e1 instanceof long[] && e2 instanceof long[])
+            eq = Arrays.equals((long[]) e1, (long[]) e2);
+        else if (e1 instanceof char[] && e2 instanceof char[])
+            eq = Arrays.equals((char[]) e1, (char[]) e2);
+        else if (e1 instanceof float[] && e2 instanceof float[])
+            eq = Arrays.equals((float[]) e1, (float[]) e2);
+        else if (e1 instanceof double[] && e2 instanceof double[])
+            eq = Arrays.equals((double[]) e1, (double[]) e2);
+        else if (e1 instanceof boolean[] && e2 instanceof boolean[])
+            eq = Arrays.equals((boolean[]) e1, (boolean[]) e2);
+        else
+            eq = equals(e1,e2);
+        return eq;
+    }
 }",2016-06-07T22:53:07Z,92
"@@ -63,6 +63,7 @@ public class MySqlSchema {
     private final TableSchemaBuilder schemaBuilder;
     private final DdlChanges ddlChanges;
     private final String serverName;
+    private final String schemaPrefix;
     private Tables tables;
 
     /**
@@ -78,7 +79,13 @@ public MySqlSchema(Configuration config, String serverName) {
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
         this.schemaBuilder = new TableSchemaBuilder();
+        if ( serverName != null ) serverName = serverName.trim();
         this.serverName = serverName;
+        if ( this.serverName == null || serverName.isEmpty() ) {
+            this.schemaPrefix = """";
+        } else {
+            this.schemaPrefix = serverName.endsWith(""."") ? serverName : serverName + ""."";
+        }
 
         // Create and configure the database history ...
         this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
@@ -253,7 +260,7 @@ protected void refreshSchemas() {
         // Create TableSchema instances for any existing table ...
         this.tables.tableIds().forEach(id -> {
             Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(serverName,table, filters.columnFilter(), filters.columnMappers());
+            TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
             tableSchemaByTableId.put(id, schema);
         });
     }
@@ -323,7 +330,7 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
             if (table == null) { // removed
                 tableSchemaByTableId.remove(tableId);
             } else {
-                TableSchema schema = schemaBuilder.create(serverName, table, filters.columnFilter(), filters.columnMappers());
+                TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
                 tableSchemaByTableId.put(tableId, schema);
             }
         });",2016-06-10T02:08:09Z,16
"@@ -30,6 +30,10 @@ public Configurator with(Field field, boolean value) {
         return this;
     }
 
+    public Configurator serverName(String serverName) {
+        return with(MySqlConnectorConfig.SERVER_NAME, serverName);
+    }
+
     public Configurator includeDatabases(String regexList) {
         return with(MySqlConnectorConfig.DATABASE_WHITELIST, regexList);
     }",2016-06-10T02:08:09Z,35
"@@ -29,6 +29,7 @@
 public class MySqlSchemaTest {
 
     private static final Path TEST_FILE_PATH = Testing.Files.createTestingPath(""dbHistory.log"");
+    private static final String SERVER_NAME = ""test-server"";
 
     private Configurator build;
     private MySqlSchema mysql;
@@ -55,7 +56,7 @@ public void afterEach() {
 
     @Test
     public void shouldApplyDdlStatementsAndRecover() {
-        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).serverName(SERVER_NAME).createSchemas();
         mysql.start();
 
         // Testing.Print.enable();
@@ -74,6 +75,7 @@ public void shouldApplyDdlStatementsAndRecover() {
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .excludeBuiltInTables()
                      .createSchemas();
@@ -99,6 +101,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .includeBuiltInTables()
                      .createSchemas();
@@ -124,7 +127,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
     protected void assertTableIncluded(String fullyQualifiedTableName) {
         TableId tableId = TableId.parse(fullyQualifiedTableName);
         assertThat(mysql.tables().forTable(tableId)).isNotNull();
-        assertThat(mysql.schemaFor(tableId)).isNotNull();
+        TableSchema tableSchema = mysql.schemaFor(tableId);
+        assertThat(tableSchema).isNotNull();
+        assertThat(tableSchema.keySchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Key"");
+        assertThat(tableSchema.valueSchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Value"");
     }
 
     protected void assertTableExcluded(String fullyQualifiedTableName) {",2016-06-10T02:08:09Z,16
"@@ -52,6 +52,7 @@ public class BinlogReader extends AbstractReader {
     private final SourceInfo source;
     private final EnumMap<EventType, BlockingConsumer<Event>> eventHandlers = new EnumMap<>(EventType.class);
     private BinaryLogClient client;
+    private int startingRowNumber = 0;
 
     /**
      * Create a binlog reader.
@@ -93,12 +94,15 @@ protected void doStart() {
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
 
-        // And set the client to start from that point ...
+        // The 'source' object holds the starting point in the binlog where we should start reading,
+        // set set the client to start from that point ...
         client.setGtidSet(source.gtidSet()); // may be null
         client.setBinlogFilename(source.binlogFilename());
-        client.setBinlogPosition(source.binlogPosition());
-        // The event row number will be used when processing the first event ...
+        client.setBinlogPosition(source.nextBinlogPosition());
 
+        // Set the starting row number, which is the next row number to be read ...
+        startingRowNumber = source.nextEventRowNumber();
+        
         // Start the log reader, which starts background threads ...
         long timeoutInMilliseconds = context.timeoutInMilliseconds();
         try {
@@ -157,21 +161,19 @@ protected void handleEvent(Event event) {
             } else {
                 rotateEventData = (RotateEventData) eventData;
             }
-            source.setBinlogFilename(rotateEventData.getBinlogFilename());
-            source.setBinlogPosition(rotateEventData.getBinlogPosition());
-            source.setRowInEvent(0);
+            source.setBinlogStartPoint(rotateEventData.getBinlogFilename(), rotateEventData.getBinlogPosition());
         } else if (eventHeader instanceof EventHeaderV4) {
             EventHeaderV4 trackableEventHeader = (EventHeaderV4) eventHeader;
-            long nextBinlogPosition = trackableEventHeader.getNextPosition();
-            if (nextBinlogPosition > 0) {
-                source.setBinlogPosition(nextBinlogPosition);
-                source.setRowInEvent(0);
-            }
+            source.setEventPosition(trackableEventHeader.getPosition(), trackableEventHeader.getEventLength());
         }
 
         // If there is a handler for this event, forward the event to it ...
         try {
+            // Forward the event to the handler ...
             eventHandlers.getOrDefault(eventType, this::ignoreEvent).accept(event);
+            
+            // And after that event has been processed, always set the starting row number to 0 ...
+            startingRowNumber = 0;
         } catch (InterruptedException e) {
             // Most likely because this reader was stopped and our thread was interrupted ...
             Thread.interrupted();
@@ -301,7 +303,11 @@ protected void handleInsert(Event event) throws InterruptedException {
         if (recordMaker != null) {
             List<Serializable[]> rows = write.getRows();
             Long ts = context.clock().currentTimeInMillis();
-            int count = recordMaker.createEach(rows, ts);
+            int count = 0;
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
+                count += recordMaker.create(rows.get(row), ts, row, numRows);
+            }
             logger.debug(""Recorded {} insert records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping insert row event: {}"", event);
@@ -324,11 +330,12 @@ protected void handleUpdate(Event event) throws InterruptedException {
             List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
-            for (int row = 0; row != rows.size(); ++row) {
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
                 Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
                 Serializable[] before = changes.getKey();
                 Serializable[] after = changes.getValue();
-                count += recordMaker.update(before, after, ts, row);
+                count += recordMaker.update(before, after, ts, row, numRows);
             }
             logger.debug(""Recorded {} update records for event: {}"", count, event);
         } else {
@@ -350,7 +357,11 @@ protected void handleDelete(Event event) throws InterruptedException {
         if (recordMaker != null) {
             List<Serializable[]> rows = deleted.getRows();
             Long ts = context.clock().currentTimeInMillis();
-            int count = recordMaker.deleteEach(rows, ts);
+            int count = 0;
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
+                count += recordMaker.delete(rows.get(row), ts, row, numRows);
+            }
             logger.debug(""Recorded {} delete records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping delete row event: {}"", event);",2016-06-14T22:43:58Z,24
"@@ -107,7 +107,7 @@ public void start(Map<String, String> props) {
             if (taskContext.isSnapshotNeverAllowed()) {
                 // We're not allowed to take a snapshot, so instead we have to assume that the binlog contains the
                 // full history of the database.
-                source.setBinlogFilename("""");// start from the beginning of the binlog
+                source.setBinlogStartPoint("""", 0L);// start from the beginning of the binlog
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;",2016-06-14T22:43:58Z,10
"@@ -7,7 +7,6 @@
 
 import java.util.BitSet;
 import java.util.HashMap;
-import java.util.Iterator;
 import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
@@ -152,7 +151,8 @@ public void regenerate() {
      */
     public boolean assign(long tableNumber, TableId id) {
         Long existingTableNumber = tableNumbersByTableId.get(id);
-        if ( existingTableNumber != null && existingTableNumber.longValue() == tableNumber && convertersByTableNumber.containsKey(tableNumber)) {
+        if (existingTableNumber != null && existingTableNumber.longValue() == tableNumber
+                && convertersByTableNumber.containsKey(tableNumber)) {
             // This is the exact same table number for the same table, so do nothing ...
             return true;
         }
@@ -171,15 +171,15 @@ public boolean assign(long tableNumber, TableId id) {
         Converter converter = new Converter() {
 
             @Override
-            public int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int read(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                             BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 Object key = tableSchema.keyFromColumnData(row);
                 Struct value = tableSchema.valueFromColumnData(row);
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
                             keySchema, key, envelope.schema(), envelope.read(value, origin, ts));
@@ -190,15 +190,15 @@ public int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedC
             }
 
             @Override
-            public int insert(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int insert(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 Object key = tableSchema.keyFromColumnData(row);
                 Struct value = tableSchema.valueFromColumnData(row);
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
                             keySchema, key, envelope.schema(), envelope.create(value, origin, ts));
@@ -209,7 +209,7 @@ public int insert(SourceInfo source, Object[] row, int rowNumber, BitSet include
             }
 
             @Override
-            public int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, BitSet includedColumns, long ts,
+            public int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 int count = 0;
@@ -220,7 +220,7 @@ public int update(SourceInfo source, Object[] before, Object[] after, int rowNum
                     Struct valueBefore = tableSchema.valueFromColumnData(before);
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     if (key != null && !Objects.equals(key, oldKey)) {
                         // The key has indeed changed, so first send a create event ...
@@ -251,7 +251,7 @@ record = new SourceRecord(partition, offset, topicName, partitionNum, keySchema,
             }
 
             @Override
-            public int delete(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int delete(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 int count = 0;
@@ -260,7 +260,7 @@ public int delete(SourceInfo source, Object[] row, int rowNumber, BitSet include
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     // Send a delete message ...
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
@@ -275,7 +275,7 @@ record = new SourceRecord(partition, offset, topicName, partitionNum,
                 }
                 return count;
             }
-            
+
             @Override
             public String toString() {
                 return ""RecordMaker.Converter("" + id + "")"";
@@ -307,17 +307,20 @@ protected Struct schemaChangeRecordValue(String databaseName, String ddlStatemen
     }
 
     protected static interface Converter {
-        int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int read(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                 BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int insert(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int insert(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                   BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, BitSet includedColumns, long ts,
+        int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                    BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int delete(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int delete(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                   BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
     }
@@ -346,7 +349,7 @@ protected RecordsForTable(Converter converter, BitSet includedColumns, BlockingC
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int read(Object[] row, long ts) throws InterruptedException {
-            return read(row, ts, 0);
+            return read(row, ts, 0, 1);
         }
 
         /**
@@ -356,29 +359,12 @@ public int read(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int read(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.read(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#READ read} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int readEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += read(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int read(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.read(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -391,7 +377,7 @@ public int readEach(Iterable<? extends Object[]> rows, long ts) throws Interrupt
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int create(Object[] row, long ts) throws InterruptedException {
-            return create(row, ts, 0);
+            return create(row, ts, 0, 1);
         }
 
         /**
@@ -401,29 +387,12 @@ public int create(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int create(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.insert(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#CREATE create} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int createEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += create(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int create(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.insert(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -438,7 +407,7 @@ public int createEach(Iterable<? extends Object[]> rows, long ts) throws Interru
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int update(Object[] before, Object[] after, long ts) throws InterruptedException {
-            return update(before, after, ts, 0);
+            return update(before, after, ts, 0, 1);
         }
 
         /**
@@ -450,11 +419,12 @@ public int update(Object[] before, Object[] after, long ts) throws InterruptedEx
          *            definition in the {@link MySqlSchema}
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int update(Object[] before, Object[] after, long ts, int rowNumber) throws InterruptedException {
-            return converter.update(source, before, after, rowNumber, includedColumns, ts, consumer);
+        public int update(Object[] before, Object[] after, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.update(source, before, after, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -467,7 +437,7 @@ public int update(Object[] before, Object[] after, long ts, int rowNumber) throw
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int delete(Object[] row, long ts) throws InterruptedException {
-            return delete(row, ts, 0);
+            return delete(row, ts, 0, 1);
         }
 
         /**
@@ -477,29 +447,12 @@ public int delete(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int delete(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.delete(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#DELETE delete} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int deleteEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += delete(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int delete(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.delete(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
     }
 }",2016-06-14T22:43:58Z,128
"@@ -186,9 +186,11 @@ protected void execute() {
             sql.set(""SHOW MASTER STATUS"");
             mysql.query(sql.get(), rs -> {
                 if (rs.next()) {
-                    source.setBinlogFilename(rs.getString(1));
-                    source.setBinlogPosition(rs.getLong(2));
-                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                    String binlogFilename = rs.getString(1);
+                    long binlogPosition = rs.getLong(2);
+                    String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                    source.setBinlogStartPoint(binlogFilename, binlogPosition);
+                    source.setGtidSet(gtidSet);
                     source.startSnapshot();
                 }
             });",2016-06-14T22:43:58Z,15
"@@ -33,16 +33,18 @@
  * 
  * <p>
  * The {@link #offset() source offset} information describes how much of the database's binary log the source the change detector
- * has processed. Here's a JSON-like representation of an example:
+ * has already processed, and it includes the {@link #binlogFilename() binlog filename}, the {@link #nextBinlogPosition() next
+ * position} in the binlog to start reading, and the {@link #nextEventRowNumber() next event row number}. Here's a JSON-like
+ * representation of an example:
  * 
  * <pre>
  * {
  *     ""server_id"": 112233,
- *     ""ts_sec"": 1465236179,
- *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
- *     ""file"" = ""mysql-bin.000003"",
- *     ""pos"" = 105586,
- *     ""row"" = 0,
+ *     ""ts_sec"": 1465937,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 990,
+ *     ""row"": 0,
  *     ""snapshot"": true
  * }
  * </pre>
@@ -53,20 +55,21 @@
  * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
  * 
  * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
- * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
- * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
- * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
- * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
- * an event that corresponds to the above partition and offset:
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), the
+ * {@link #lastBinlogPosition() position} of the event (and {@link #lastEventRowNumber() row number} within the event) inside
+ * the {@link #binlogFilename() binlog file}. When GTIDs are enabled, it also includes the GTID of the transaction in which the
+ * event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced when the connector is in the
+ * middle of a snapshot. Here's a JSON-like representation of the source for an event that corresponds to the above partition and
+ * offset:
  * 
  * <pre>
  * {
  *     ""name"": ""production-server"",
  *     ""server_id"": 112233,
- *     ""ts_sec"": 1465236179,
+ *     ""ts_sec"": 1465937,
  *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
  *     ""file"": ""mysql-bin.000003"",
- *     ""pos"" = 105586,
+ *     ""pos"" = 1081,
  *     ""row"": 0,
  *     ""snapshot"": true
  * }
@@ -109,8 +112,10 @@ final class SourceInfo {
     private String gtidSet;
     private String binlogGtid;
     private String binlogFilename;
-    private long binlogPosition = 4;
-    private int eventRowNumber = 0;
+    private long lastBinlogPosition = 0;
+    private int lastEventRowNumber = 0;
+    private long nextBinlogPosition = 4;
+    private int nextEventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
     private long binlogTimestampSeconds = 0;
@@ -150,6 +155,32 @@ public Map<String, String> partition() {
      * @return a copy of the current offset; never null
      */
     public Map<String, ?> offset() {
+        return offsetUsingPosition(nextBinlogPosition);
+    }
+
+    /**
+     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
+     * describes the position within the source where we have last read.
+     * 
+     * @param eventRowNumber the 0-based row number within the event being processed
+     * @param totalNumberOfRows the total number of rows within the event being processed
+     * @return a copy of the current offset; never null
+     */
+    public Map<String, ?> offset(int eventRowNumber, int totalNumberOfRows) {
+        if (eventRowNumber < (totalNumberOfRows - 1)) {
+            // This is not the last row, so our offset should record the next row to be used ...
+            this.lastEventRowNumber = eventRowNumber;
+            this.nextEventRowNumber = eventRowNumber + 1;
+            // so write out the offset with the position of this event
+            return offsetUsingPosition(lastBinlogPosition);
+        }
+        // This is the last row, so write out the offset that has the position of the next event ...
+        this.lastEventRowNumber = this.nextEventRowNumber;
+        this.nextEventRowNumber = 0;
+        return offsetUsingPosition(nextBinlogPosition);
+    }
+
+    private Map<String, ?> offsetUsingPosition( long binlogPosition ) {
         Map<String, Object> map = new HashMap<>();
         if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
         if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
@@ -158,7 +189,7 @@ public Map<String, String> partition() {
         }
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
+        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, nextEventRowNumber);
         if (isSnapshotInEffect()) {
             map.put(SNAPSHOT_KEY, true);
         }
@@ -192,27 +223,15 @@ public Struct struct() {
             result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
+        result.put(BINLOG_POSITION_OFFSET_KEY, lastBinlogPosition);
+        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, lastEventRowNumber);
         result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
         return result;
     }
 
-    /**
-     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
-     * describes the position within the source where we have last read.
-     * 
-     * @param eventRowNumber the 0-based row number within the last event that was successfully processed
-     * @return a copy of the current offset; never null
-     */
-    public Map<String, ?> offset(int eventRowNumber) {
-        setRowInEvent(eventRowNumber);
-        return offset();
-    }
-
     /**
      * Determine whether a snapshot is currently in effect.
      * 
@@ -246,28 +265,27 @@ public void setGtidSet(String gtidSet) {
      * Set the name of the MySQL binary log file.
      * 
      * @param binlogFilename the name of the binary log file; may not be null
+     * @param positionOfFirstEvent the position in the binary log file to begin processing
      */
-    public void setBinlogFilename(String binlogFilename) {
+    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
         this.binlogFilename = binlogFilename;
+        this.nextBinlogPosition = positionOfFirstEvent;
+        this.lastBinlogPosition = this.nextBinlogPosition;
+        this.nextEventRowNumber = 0;
+        this.lastEventRowNumber = 0;
     }
 
     /**
-     * Set the position within the MySQL binary log file.
-     * 
-     * @param binlogPosition the position within the binary log file
-     */
-    public void setBinlogPosition(long binlogPosition) {
-        this.binlogPosition = binlogPosition;
-    }
-
-    /**
-     * Set the index of the row within the event appearing at the {@link #binlogPosition() position} within the
-     * {@link #binlogFilename() binary log file}.
+     * Set the position within the MySQL binary log file of the <em>current event</em>.
      * 
-     * @param rowNumber the 0-based row number
+     * @param positionOfCurrentEvent the position within the binary log file of the current event
+     * @param eventSizeInBytes the size in bytes of this event
      */
-    public void setRowInEvent(int rowNumber) {
-        this.eventRowNumber = rowNumber;
+    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
+        this.lastBinlogPosition = positionOfCurrentEvent;
+        this.nextBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
+        this.nextEventRowNumber = 0;
+        this.lastEventRowNumber = 0;
     }
 
     /**
@@ -316,8 +334,10 @@ public void setOffset(Map<String, ?> sourceOffset) {
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
             }
-            binlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
-            eventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
+            nextBinlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
+            nextEventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
+            lastBinlogPosition = nextBinlogPosition;
+            lastEventRowNumber = nextEventRowNumber;
         }
     }
 
@@ -344,29 +364,50 @@ public String gtidSet() {
     /**
      * Get the name of the MySQL binary log file that has been processed.
      * 
-     * @return the name of the binary log file; null if it has not been {@link #setBinlogFilename(String) set}
+     * @return the name of the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
     public String binlogFilename() {
         return binlogFilename;
     }
 
     /**
-     * Get the position within the MySQL binary log file that has been processed.
+     * Get the position within the MySQL binary log file of the next event to be processed.
      * 
-     * @return the position within the binary log file; null if it has not been {@link #setBinlogPosition(long) set}
+     * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
+     */
+    public long nextBinlogPosition() {
+        return nextBinlogPosition;
+    }
+
+    /**
+     * Get the position within the MySQL binary log file of the most recently processed event.
+     * 
+     * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
+     */
+    public long lastBinlogPosition() {
+        return lastBinlogPosition;
+    }
+
+    /**
+     * Get the next row within the event at the {@link #nextBinlogPosition() position} within the {@link #binlogFilename() binary
+     * log file}
+     * .
+     * 
+     * @return the 0-based row number
      */
-    public long binlogPosition() {
-        return binlogPosition;
+    public int nextEventRowNumber() {
+        return nextEventRowNumber;
     }
 
     /**
-     * Get the row within the event at the {@link #binlogPosition() position} within the {@link #binlogFilename() binary log file}
+     * Get the previous row within the event at the {@link #lastBinlogPosition() position} within the {@link #binlogFilename()
+     * binary log file}
      * .
      * 
      * @return the 0-based row number
      */
-    public int eventRowNumber() {
-        return eventRowNumber;
+    public int lastEventRowNumber() {
+        return lastEventRowNumber;
     }
 
     /**
@@ -385,8 +426,8 @@ public String toString() {
             sb.append(""GTIDs "");
             sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
-            sb.append("", pos="").append(binlogPosition());
-            sb.append("", row="").append(eventRowNumber());
+            sb.append("", pos="").append(nextBinlogPosition());
+            sb.append("", row="").append(nextEventRowNumber());
         } else {
             if (binlogFilename == null) {
                 sb.append(""<latest>"");
@@ -395,8 +436,8 @@ public String toString() {
                     sb.append(""earliest binlog file and position"");
                 } else {
                     sb.append(""binlog file '"").append(binlogFilename).append(""'"");
-                    sb.append("", pos="").append(binlogPosition());
-                    sb.append("", row="").append(eventRowNumber());
+                    sb.append("", pos="").append(nextBinlogPosition());
+                    sb.append("", row="").append(nextEventRowNumber());
                 }
             }
         }",2016-06-14T22:43:58Z,11
"@@ -114,7 +114,7 @@ public void shouldCreateSnapshotOfSingleDatabase() throws Exception {
         config = simpleConfig().build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setBinlogFilename(""""); // start from beginning
+        context.source().setBinlogStartPoint("""",0L); // start from beginning
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...
@@ -173,7 +173,7 @@ public void shouldCreateSnapshotOfSingleDatabaseWithSchemaChanges() throws Excep
         config = simpleConfig().with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true).build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setBinlogFilename(""""); // start from beginning
+        context.source().setBinlogStartPoint("""",0L); // start from beginning
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...",2016-06-14T22:43:58Z,24
"@@ -22,6 +22,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.history.FileDatabaseHistory;
@@ -284,6 +285,56 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         stopConnector();
     }
 
+    @Test
+    public void shouldConsumeEventsWithNoSnapshot() throws SQLException, InterruptedException {
+        Testing.Files.delete(DB_HISTORY_PATH);
+        
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18780)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""kafka-connect-2"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test_ro"")
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.name().toLowerCase())
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+
+        // Consume the first records due to startup and initialization of the database ...
+        // Testing.Print.enable();
+        SourceRecords records = consumeRecordsByTopic(9+9+4+5+6);   // 6 DDL changes
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.products"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.products_on_hand"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.customers"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.orders"").size()).isEqualTo(5);
+        assertThat(records.topics().size()).isEqualTo(4+1);
+        assertThat(records.ddlRecordsForDatabase(""connector_test_ro"").size()).isEqualTo(6);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+        
+        // More records may have been written (if this method were run after the others), but we don't care ...
+        stopConnector();
+
+        records.recordsForTopic(""kafka-connect-2.connector_test_ro.orders"").forEach(record->{
+            print(record);
+        });
+        
+        records.recordsForTopic(""kafka-connect-2.connector_test_ro.customers"").forEach(record->{
+            print(record);
+        });
+    }
+
+
     @Test
     public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLException, InterruptedException {
         Testing.Files.delete(DB_HISTORY_PATH);",2016-06-14T22:43:58Z,88
"@@ -60,8 +60,7 @@ public void shouldApplyDdlStatementsAndRecover() {
         mysql.start();
 
         // Testing.Print.enable();
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -81,11 +80,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
                      .createSchemas();
         mysql.start();
 
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
-        source.setBinlogPosition(1000);
+        source.setBinlogStartPoint(""binlog-001"",1000);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -107,11 +105,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
                      .createSchemas();
         mysql.start();
 
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
-        source.setBinlogPosition(1000);
+        source.setBinlogStartPoint(""binlog-001"",1000);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...",2016-06-14T22:43:58Z,16
"@@ -189,9 +189,12 @@ protected void execute() {
                 if (rs.next()) {
                     String binlogFilename = rs.getString(1);
                     long binlogPosition = rs.getLong(2);
-                    String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
                     source.setBinlogStartPoint(binlogFilename, binlogPosition);
-                    source.setGtidSet(gtidSet);
+                    if ( rs.getMetaData().getColumnCount() > 4 ) {
+                        // This column exists only in MySQL 5.6.5 or later ...
+                        String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                        source.setGtidSet(gtidSet);
+                    }
                     source.startSnapshot();
                 }
             });",2016-06-27T14:23:12Z,15
"@@ -172,3 +172,24 @@ VALUES (default, '2016-01-16', 1001, 1, 102),
        (default, '2016-02-18', 1004, 3, 109),
        (default, '2016-02-19', 1002, 2, 106),
        (default, '2016-02-21', 1003, 1, 107);
+
+
+
+# ----------------------------------------------------------------------------------------------------------------
+# DATABASE:  regression_test
+# ----------------------------------------------------------------------------------------------------------------
+# The integration test for this database expects to scans all of the binlog events associated with this database
+# without error or problems. The integration test does not modify any records in this database, so this script
+# must contain all operations to these tables.
+#
+CREATE DATABASE regression_test;
+USE regression_test;
+
+# DBZ-61 handle binary value recorded as hex string value
+CREATE TABLE t1464075356413_testtable6 (
+  pk_column int auto_increment NOT NULL,
+  varbinary_col varbinary(20) NOT NULL,
+  PRIMARY KEY(pk_column)
+);
+INSERT INTO t1464075356413_testtable6 (pk_column, varbinary_col)
+VALUES(default, 0x4D7953514C)",2016-06-07T22:53:07Z,93
"@@ -0,0 +1,89 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.nio.file.Path;
+import java.sql.SQLException;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.relational.history.FileDatabaseHistory;
+import io.debezium.util.Testing;
+
+/**
+ * @author Randall Hauch
+ */
+public class MySqlConnectorRegressionIT extends AbstractConnectorTest {
+
+    private static final Path DB_HISTORY_PATH = Testing.Files.createTestingPath(""file-db-history-regression.txt"").toAbsolutePath();
+
+    private Configuration config;
+
+    @Before
+    public void beforeEach() {
+        stopConnector();
+        initializeConnectorTestFramework();
+        Testing.Files.delete(DB_HISTORY_PATH);
+    }
+
+    @After
+    public void afterEach() {
+        try {
+            stopConnector();
+        } finally {
+            Testing.Files.delete(DB_HISTORY_PATH);
+        }
+    }
+
+    @Test
+    public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+        
+        // ---------------------------------------------------------------------------------------------------------------
+        // Consume all of the events due to startup and initialization of the database
+        // ---------------------------------------------------------------------------------------------------------------
+        //Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(2+1);   // 2 schema change record, 1 insert
+        stopConnector();
+        assertThat(records).isNotNull();
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(2);
+        assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(2);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(2);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+    }
+
+}",2016-06-07T22:53:07Z,71
"@@ -5,6 +5,8 @@
  */
 package io.debezium.data;
 
+import java.nio.ByteBuffer;
+import java.util.Base64;
 import java.util.List;
 import java.util.Map;
 
@@ -169,6 +171,11 @@ public RecordWriter append(Object obj) {
                     appendFirst(field.name(), s.get(field));
                 }
                 sb.append('}');
+            } else if (obj instanceof ByteBuffer) {
+                ByteBuffer b = (ByteBuffer) obj;
+                sb.append('""').append(Base64.getEncoder().encode(b.array())).append('""');
+            } else if (obj instanceof byte[]) {
+                sb.append('""').append(Base64.getEncoder().encode((byte[])obj)).append('""');
             } else if (obj instanceof Map<?, ?>) {
                 Map<?, ?> map = (Map<?, ?>) obj;
                 sb.append('{');",2016-06-07T22:53:07Z,133
"@@ -6,6 +6,7 @@
 package io.debezium.relational;
 
 import java.math.BigDecimal;
+import java.nio.ByteBuffer;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
@@ -105,7 +106,8 @@ public TableSchema create(ResultSet resultSet, String name) throws SQLException
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
@@ -121,15 +123,16 @@ public TableSchema create(String schemaPrefix, Table table) {
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @param filter the filter that specifies whether columns in the table should be included; may be null if all columns
      *            are to be included
      * @param mappers the mapping functions for columns; may be null if none of the columns are to be mapped to different values
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
     public TableSchema create(String schemaPrefix, Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
-        if ( schemaPrefix == null ) schemaPrefix = """";
+        if (schemaPrefix == null) schemaPrefix = """";
         // Build the schemas ...
         final TableId tableId = table.id();
         final String tableIdStr = tableId.toString();
@@ -285,7 +288,10 @@ protected ValueConverter[] convertersForColumns(Schema schema, TableId tableId,
                 if (mappers != null) {
                     ValueConverter mappingConverter = mappers.mappingConverterFor(tableId, column);
                     if (mappingConverter != null) {
-                        converter = (value) -> mappingConverter.convert(valueConverter.convert(value));
+                        converter = (value) -> {
+                            if (value != null) value = valueConverter.convert(value);
+                            return mappingConverter.convert(value);
+                        };
                     }
                 }
                 if (converter == null) converter = valueConverter;
@@ -376,7 +382,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
             case Types.NCLOB:
                 fieldBuilder = SchemaBuilder.string();
                 break;
-                
+
             // Variable-length string values
             case Types.VARCHAR:
             case Types.LONGVARCHAR:
@@ -424,7 +430,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
         if (fieldBuilder != null) {
             if (mapper != null) {
                 // Let the mapper add properties to the schema ...
-                mapper.alterFieldSchema(column,fieldBuilder);
+                mapper.alterFieldSchema(column, fieldBuilder);
             }
             if (column.isOptional()) fieldBuilder.optional();
             builder.field(column.name(), fieldBuilder.build());
@@ -450,6 +456,14 @@ protected SchemaBuilder addOtherField(Column column, ColumnMapper mapper) {
     /**
      * Create a {@link ValueConverter} that can be used to convert row values for the given column into the Kafka Connect value
      * object described by the {@link Field field definition}.
+     * <p>
+     * Subclasses can override this method to specialize the behavior. The subclass method should do custom checks and
+     * conversions,
+     * and then delegate to this method implementation to handle all other cases.
+     * <p>
+     * Alternatively, subclasses can leave this method as-is and instead override one of the lower-level type-specific methods
+     * that this method calls (e.g., {@link #convertBinary(Column, Field, Object)},
+     * {@link #convertTinyInt(Column, Field, Object)}, etc.).
      * 
      * @param column the column describing the input values; never null
      * @param fieldDefn the definition for the field in a Kafka Connect {@link Schema} describing the output of the function;
@@ -461,94 +475,38 @@ protected ValueConverter createValueConverterFor(Column column, Field fieldDefn)
             case Types.NULL:
                 return (data) -> null;
             case Types.BIT:
+                return (data) -> convertBit(column, fieldDefn, data);
             case Types.BOOLEAN:
-                return (data) -> {
-                    if (data instanceof Boolean) return (Boolean) data;
-                    if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBoolean(column, fieldDefn, data);
 
             // Binary values ...
             case Types.BLOB:
             case Types.BINARY:
             case Types.VARBINARY:
             case Types.LONGVARBINARY:
-                return (data) -> (byte[]) data;
+                return (data) -> convertBinary(column, fieldDefn, data);
 
             // Numeric integers
             case Types.TINYINT:
-                return (data) -> {
-                    if (data instanceof Byte) return (Byte) data;
-                    if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertTinyInt(column, fieldDefn, data);
             case Types.SMALLINT:
-                return (data) -> {
-                    if (data instanceof Short) return (Short) data;
-                    if (data instanceof Integer) return new Short(((Integer) data).shortValue());
-                    if (data instanceof Long) return new Short(((Long) data).shortValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertSmallInt(column, fieldDefn, data);
             case Types.INTEGER:
-                return (data) -> {
-                    if (data instanceof Integer) return (Integer) data;
-                    if (data instanceof Short) return new Integer(((Short) data).intValue());
-                    if (data instanceof Long) return new Integer(((Long) data).intValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertInteger(column, fieldDefn, data);
             case Types.BIGINT:
-                return (data) -> {
-                    if (data instanceof Long) return (Long) data;
-                    if (data instanceof Integer) return new Long(((Integer) data).longValue());
-                    if (data instanceof Short) return new Long(((Short) data).longValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBigInt(column, fieldDefn, data);
 
             // Numeric decimal numbers
             case Types.FLOAT:
+                return (data) -> convertFloat(column, fieldDefn, data);
             case Types.DOUBLE:
-                return (data) -> {
-                    if (data instanceof Double) return (Double) data;
-                    if (data instanceof Float) return new Double(((Float) data).doubleValue());
-                    if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
-                    if (data instanceof Long) return new Double(((Long) data).doubleValue());
-                    if (data instanceof Short) return new Double(((Short) data).doubleValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertDouble(column, fieldDefn, data);
             case Types.REAL:
-                return (data) -> {
-                    if (data instanceof Float) return (Float) data;
-                    if (data instanceof Double) return new Float(((Double) data).floatValue());
-                    if (data instanceof Integer) return new Float(((Integer) data).floatValue());
-                    if (data instanceof Long) return new Float(((Long) data).floatValue());
-                    if (data instanceof Short) return new Float(((Short) data).floatValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertReal(column, fieldDefn, data);
             case Types.NUMERIC:
+                return (data) -> convertNumeric(column, fieldDefn, data);
             case Types.DECIMAL:
-                return (data) -> {
-                    BigDecimal decimal = null;
-                    if (data instanceof BigDecimal)
-                        decimal = (BigDecimal) data;
-                    else if (data instanceof Boolean)
-                        decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
-                    else if (data instanceof Short)
-                        decimal = new BigDecimal(((Short) data).intValue());
-                    else if (data instanceof Integer)
-                        decimal = new BigDecimal(((Integer) data).intValue());
-                    else if (data instanceof Long)
-                        decimal = BigDecimal.valueOf(((Long) data).longValue());
-                    else if (data instanceof Float)
-                        decimal = BigDecimal.valueOf(((Float) data).doubleValue());
-                    else if (data instanceof Double)
-                        decimal = BigDecimal.valueOf(((Double) data).doubleValue());
-                    else {
-                        handleUnknownData(column, fieldDefn, data);
-                    }
-                    return decimal;
-                };
+                return (data) -> convertDecimal(column, fieldDefn, data);
 
             // String values
             case Types.CHAR: // variable-length
@@ -561,26 +519,23 @@ else if (data instanceof Double)
             case Types.NCLOB: // fixed-length
             case Types.DATALINK:
             case Types.SQLXML:
-                return (data) -> data.toString();
+                return (data) -> convertString(column, fieldDefn, data);
 
             // Date and time values
             case Types.DATE:
-                return (data) -> convertDate(fieldDefn, data);
+                return (data) -> convertDate(column, fieldDefn, data);
             case Types.TIME:
-                return (data) -> convertTime(fieldDefn, data);
+                return (data) -> convertTime(column, fieldDefn, data);
             case Types.TIMESTAMP:
-                return (data) -> convertTimestamp(fieldDefn, data);
+                return (data) -> convertTimestamp(column, fieldDefn, data);
             case Types.TIME_WITH_TIMEZONE:
-                return (data) -> convertTimeWithZone(fieldDefn, data);
+                return (data) -> convertTimeWithZone(column, fieldDefn, data);
             case Types.TIMESTAMP_WITH_TIMEZONE:
-                return (data) -> convertTimestampWithZone(fieldDefn, data);
+                return (data) -> convertTimestampWithZone(column, fieldDefn, data);
 
             // Other types ...
             case Types.ROWID:
-                return (data) -> {
-                    java.sql.RowId rowId = (java.sql.RowId) data;
-                    return rowId.getBytes();
-                };
+                return (data) -> convertRowId(column, fieldDefn, data);
 
             // Unhandled types
             case Types.ARRAY:
@@ -620,11 +575,13 @@ protected Object handleUnknownData(Column column, Field fieldDefn, Object data)
      * This method handles several types of objects, including {@link OffsetDateTime}, {@link java.sql.Timestamp},
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestampWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimestampWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetDateTime dateTime = null;
         if (data instanceof OffsetDateTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -679,11 +636,13 @@ protected OffsetDateTime unexpectedTimestampWithZone(Object value, Field fieldDe
      * {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types have date components, those date
      * components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimeWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimeWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetTime time = null;
         if (data instanceof OffsetTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -731,11 +690,13 @@ protected OffsetTime unexpectedTimeWithZone(Object value, Field fieldDefn) {
      * but no time zone info. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such
      * as {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestamp(Field fieldDefn, Object data) {
+    protected Object convertTimestamp(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Timestamp) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -782,11 +743,13 @@ protected java.util.Date unexpectedTimestamp(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types might
      * have date components, those date components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTime(Field fieldDefn, Object data) {
+    protected Object convertTime(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Time) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -834,11 +797,13 @@ protected java.util.Date unexpectedTime(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
      * have time components, those time components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertDate(Field fieldDefn, Object data) {
+    protected Object convertDate(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Date) {
             // JDBC specification indicates that this will be the nominal object for this JDBC type.
@@ -882,4 +847,286 @@ protected java.util.Date unexpectedDate(Object value, Field fieldDefn) {
                     fieldDefn.schema(), value.getClass(), value);
         return null;
     }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have time components, those time components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBinary(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof char[]) {
+            data = new String((char[]) data); // convert to string
+        }
+        if (data instanceof String) {
+            // This was encoded as a hexadecimal string, but we receive it as a normal string ...
+            data = ((String) data).getBytes();
+        }
+        if (data instanceof byte[]) {
+            return ByteBuffer.wrap((byte[])data);
+        }
+        // An unexpected value
+        return unexpectedBinary(data, fieldDefn);
+    }
+
+    /**
+     * Handle the unexpected value from a row with a column type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * 
+     * @param value the binary value for which no conversion was found; never null
+     * @param fieldDefn the field definition in the Kafka Connect schema; never null
+     * @return the converted value, or null
+     * @see #convertBinary(Column, Field, Object)
+     */
+    protected byte[] unexpectedBinary(Object value, Field fieldDefn) {
+        LOGGER.warn(""Unexpected JDBC BINARY value for field {} with schema {}: class={}, value={}"", fieldDefn.name(),
+                    fieldDefn.schema(), value.getClass(), value);
+        return null;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#TINYINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTinyInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Byte) return data;
+        if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#SMALLINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertSmallInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Short) return data;
+        if (data instanceof Integer) return new Short(((Integer) data).shortValue());
+        if (data instanceof Long) return new Short(((Long) data).shortValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertInteger(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Integer) return data;
+        if (data instanceof Short) return new Integer(((Short) data).intValue());
+        if (data instanceof Long) return new Integer(((Long) data).intValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBigInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Long) return data;
+        if (data instanceof Integer) return new Long(((Integer) data).longValue());
+        if (data instanceof Short) return new Long(((Short) data).longValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#FLOAT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertFloat(Column column, Field fieldDefn, Object data) {
+        return convertDouble(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DOUBLE}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDouble(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Double) return data;
+        if (data instanceof Float) return new Double(((Float) data).doubleValue());
+        if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
+        if (data instanceof Long) return new Double(((Long) data).doubleValue());
+        if (data instanceof Short) return new Double(((Short) data).doubleValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#REAL}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertReal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Float) return data;
+        if (data instanceof Double) return new Float(((Double) data).floatValue());
+        if (data instanceof Integer) return new Float(((Integer) data).floatValue());
+        if (data instanceof Long) return new Float(((Long) data).floatValue());
+        if (data instanceof Short) return new Float(((Short) data).floatValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertNumeric(Column column, Field fieldDefn, Object data) {
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDecimal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#CHAR}, {@link Types#VARCHAR},
+     * {@link Types#LONGVARCHAR}, {@link Types#CLOB}, {@link Types#NCHAR}, {@link Types#NVARCHAR}, {@link Types#LONGNVARCHAR},
+     * {@link Types#NCLOB}, {@link Types#DATALINK}, and {@link Types#SQLXML}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertString(Column column, Field fieldDefn, Object data) {
+        return data == null ? null : data.toString();
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#ROWID}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertRowId(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof java.sql.RowId) {
+            java.sql.RowId row = (java.sql.RowId)data;
+            return ByteBuffer.wrap(row.getBytes());
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BIT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBit(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BOOLEAN}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
 }",2016-06-07T22:53:07Z,135
"@@ -7,8 +7,13 @@
 
 import static org.junit.Assert.fail;
 
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.HashMap;
+import java.util.Iterator;
 import java.util.Map;
+import java.util.Objects;
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
@@ -47,18 +52,18 @@ public class VerifyRecord {
     private static final MockSchemaRegistryClient schemaRegistry = new MockSchemaRegistryClient();
     private static final AvroConverter avroKeyConverter = new AvroConverter(schemaRegistry);
     private static final AvroConverter avroValueConverter = new AvroConverter(schemaRegistry);
-    
+
     static {
-        Map<String,Object> config = new HashMap<>();
-        config.put(""schemas.enable"",Boolean.TRUE.toString());
-        config.put(""schemas.cache.size"",100);
+        Map<String, Object> config = new HashMap<>();
+        config.put(""schemas.enable"", Boolean.TRUE.toString());
+        config.put(""schemas.cache.size"", 100);
         keyJsonConverter.configure(config, true);
         keyJsonDeserializer.configure(config, true);
         valueJsonConverter.configure(config, false);
         valueJsonDeserializer.configure(config, false);
 
         config = new HashMap<>();
-        config.put(""schema.registry.url"",""http://fake-url"");
+        config.put(""schema.registry.url"", ""http://fake-url"");
         avroKeyConverter.configure(config, false);
         avroValueConverter.configure(config, false);
     }
@@ -276,73 +281,97 @@ public static void isValid(SourceRecord record) {
         SchemaAndValue valueWithSchema = null;
         SchemaAndValue avroKeyWithSchema = null;
         SchemaAndValue avroValueWithSchema = null;
+        String msg = null;
         try {
             // The key should never be null ...
+            msg = ""checking key is not null"";
             assertThat(record.key()).isNotNull();
             assertThat(record.keySchema()).isNotNull();
 
             // If the value is not null there must be a schema; otherwise, the schema should also be null ...
             if (record.value() == null) {
+                msg = ""checking value schema is null"";
                 assertThat(record.valueSchema()).isNull();
             } else {
+                msg = ""checking value schema is not null"";
                 assertThat(record.valueSchema()).isNotNull();
             }
 
             // First serialize and deserialize the key ...
+            msg = ""serializing key using JSON converter"";
             byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using JSON deserializer"";
             keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            msg = ""deserializing key using JSON converter"";
             keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            msg = ""comparing key schema to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // then the value ...
+            msg = ""serializing value using JSON converter"";
             byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using JSON deserializer"";
             valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            msg = ""deserializing value using JSON converter"";
             valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
             // Serialize and deserialize the key using the Avro converter, and check that we got the same result ...
+            msg = ""serializing key using Avro converter"";
             byte[] avroKeyBytes = avroValueConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using Avro converter"";
             avroKeyWithSchema = avroValueConverter.toConnectData(record.topic(), avroKeyBytes);
-            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
-            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key schema to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.schema(),record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.value(),record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // Serialize and deserialize the value using the Avro converter, and check that we got the same result ...
+            msg = ""serializing value using Avro converter"";
             byte[] avroValueBytes = avroValueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using Avro converter"";
             avroValueWithSchema = avroValueConverter.toConnectData(record.topic(), avroValueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
         } catch (Throwable t) {
             Testing.Print.enable();
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
             Testing.printError(t);
-            if (keyJson == null ){
-                Testing.print(""error deserializing key from JSON: "" + SchemaUtil.asString(record.key()));
-            } else if (keyWithSchema == null ){
-                Testing.print(""error using JSON converter on key: "" + prettyJson(keyJson));
-            } else if (avroKeyWithSchema == null ){
-                Testing.print(""error using Avro converter on key: "" + prettyJson(keyJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            Testing.print(""error "" + msg);
+            Testing.print(""  key: "" + SchemaUtil.asString(record.key()));
+            Testing.print(""  key deserialized from JSON: "" + prettyJson(keyJson));
+            if (keyWithSchema != null) {
+                Testing.print(""  key to/from JSON: "" + SchemaUtil.asString(keyWithSchema.value()));
             }
-
-            if (valueJson == null ){
-                Testing.print(""error deserializing value from JSON: "" + SchemaUtil.asString(record.value()));
-            } else if (valueWithSchema == null ){
-                Testing.print(""error using JSON converter on value: "" + prettyJson(valueJson));
-            } else if (avroValueWithSchema == null ){
-                Testing.print(""error using Avro converter on value: "" + prettyJson(valueJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            if (avroKeyWithSchema != null) {
+                Testing.print(""  key to/from Avro: "" + SchemaUtil.asString(avroKeyWithSchema.value()));
+            }
+            Testing.print(""  value: "" + SchemaUtil.asString(record.value()));
+            Testing.print(""  value deserialized from JSON: "" + prettyJson(valueJson));
+            if (valueWithSchema != null) {
+                Testing.print(""  value to/from JSON: "" + SchemaUtil.asString(valueWithSchema.value()));
+            }
+            if (avroValueWithSchema != null) {
+                Testing.print(""  value to/from Avro: "" + SchemaUtil.asString(avroValueWithSchema.value()));
             }
             if (t instanceof AssertionError) throw t;
-            fail(t.getMessage());
+            fail(""error "" + msg + "": "" + t.getMessage());
         }
     }
 
@@ -390,5 +419,135 @@ protected static String prettyJson(JsonNode json) {
             return null;
         }
     }
+    
+    // The remaining methods are needed simply because of the KAFKA-3803, so our comparisons cannot rely upon Struct.equals
+    
+    protected static void assertEquals( Object o1, Object o2 ) {
+        // assertThat(o1).isEqualTo(o2);
+        if ( !equals(o1,o2) ) {
+            fail(SchemaUtil.asString(o1) + "" was not equal to "" + SchemaUtil.asString(o2));
+        }
+    }
+    
+    @SuppressWarnings(""unchecked"")
+    protected static boolean equals( Object o1, Object o2 ) {
+        if ( o1 == o2 ) return true;
+        if (o1 == null) return o2 == null ? true : false;
+        if (o2 == null ) return false;
+        if ( o1 instanceof ByteBuffer ) {
+            o1 = ((ByteBuffer)o1).array();
+        }
+        if ( o2 instanceof ByteBuffer ) {
+            o2 = ((ByteBuffer)o2).array();
+        }
+        if ( o1 instanceof byte[] && o2 instanceof byte[] ) {
+            boolean result = Arrays.equals((byte[])o1,(byte[])o2);
+            return result;
+        }
+        if ( o1 instanceof Object[] && o2 instanceof Object[] ) {
+            boolean result = deepEquals((Object[])o1,(Object[])o2);
+            return result;
+        }
+        if ( o1 instanceof Map && o2 instanceof Map ) {
+            Map<String,Object> m1 = (Map<String,Object>)o1;
+            Map<String,Object> m2 = (Map<String,Object>)o2;
+            if ( !m1.keySet().equals(m2.keySet())) return false;
+            for ( Map.Entry<String, Object> entry : m1.entrySet()) {
+                Object v1 = entry.getValue();
+                Object v2 = m2.get(entry.getKey());
+                if ( !equals(v1,v2) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Collection && o2 instanceof Collection ) {
+            Collection<Object> m1 = (Collection<Object>)o1;
+            Collection<Object> m2 = (Collection<Object>)o2;
+            if ( m1.size() != m2.size() ) return false;
+            Iterator<?> iter1 = m1.iterator();
+            Iterator<?> iter2 = m2.iterator();
+            while ( iter1.hasNext() && iter2.hasNext() ) {
+                if ( !equals(iter1.next(),iter2.next()) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Struct && o2 instanceof Struct ) {
+            // Unfortunately, the Struct.equals() method has a bug in that it is not using Arrays.deepEquals(...) to
+            // compare values in two Struct objects. The result is that the equals only works if the values of the
+            // first level Struct are non arrays; otherwise, the array values are compared using == and that obviously
+            // does not work for non-primitive values.
+            Struct struct1 = (Struct) o1;
+            Struct struct2 = (Struct) o2;
+            if (! Objects.equals(struct1.schema(),struct2.schema()) ) {
+                return false;
+            }
+            Object[] array1 = valuesFor(struct1);
+            Object[] array2 = valuesFor(struct2);
+            boolean result = deepEquals(array1, array2);
+            return result;
+        }
+        return Objects.equals(o1, o2);
+    }
+    
+    private static Object[] valuesFor( Struct struct ) {
+        Object[] array = new Object[struct.schema().fields().size()];
+        int index = 0;
+        for ( Field field : struct.schema().fields() ) {
+            array[index] = struct.get(field);
+            ++index;
+        }
+        return array;
+    }
 
+    private static boolean deepEquals(Object[] a1, Object[] a2) {
+        if (a1 == a2)
+            return true;
+        if (a1 == null || a2==null)
+            return false;
+        int length = a1.length;
+        if (a2.length != length)
+            return false;
+
+        for (int i = 0; i < length; i++) {
+            Object e1 = a1[i];
+            Object e2 = a2[i];
+
+            if (e1 == e2)
+                continue;
+            if (e1 == null)
+                return false;
+
+            // Figure out whether the two elements are equal
+            boolean eq = deepEquals0(e1, e2);
+
+            if (!eq)
+                return false;
+        }
+        return true;
+    }
+
+    private static boolean deepEquals0(Object e1, Object e2) {
+        assert e1 != null;
+        boolean eq;
+        if (e1 instanceof Object[] && e2 instanceof Object[])
+            eq = deepEquals ((Object[]) e1, (Object[]) e2);
+        else if (e1 instanceof byte[] && e2 instanceof byte[])
+            eq = Arrays.equals((byte[]) e1, (byte[]) e2);
+        else if (e1 instanceof short[] && e2 instanceof short[])
+            eq = Arrays.equals((short[]) e1, (short[]) e2);
+        else if (e1 instanceof int[] && e2 instanceof int[])
+            eq = Arrays.equals((int[]) e1, (int[]) e2);
+        else if (e1 instanceof long[] && e2 instanceof long[])
+            eq = Arrays.equals((long[]) e1, (long[]) e2);
+        else if (e1 instanceof char[] && e2 instanceof char[])
+            eq = Arrays.equals((char[]) e1, (char[]) e2);
+        else if (e1 instanceof float[] && e2 instanceof float[])
+            eq = Arrays.equals((float[]) e1, (float[]) e2);
+        else if (e1 instanceof double[] && e2 instanceof double[])
+            eq = Arrays.equals((double[]) e1, (double[]) e2);
+        else if (e1 instanceof boolean[] && e2 instanceof boolean[])
+            eq = Arrays.equals((boolean[]) e1, (boolean[]) e2);
+        else
+            eq = equals(e1,e2);
+        return eq;
+    }
 }",2016-06-07T22:53:07Z,92
"@@ -63,6 +63,7 @@ public class MySqlSchema {
     private final TableSchemaBuilder schemaBuilder;
     private final DdlChanges ddlChanges;
     private final String serverName;
+    private final String schemaPrefix;
     private Tables tables;
 
     /**
@@ -78,7 +79,13 @@ public MySqlSchema(Configuration config, String serverName) {
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
         this.schemaBuilder = new TableSchemaBuilder();
+        if ( serverName != null ) serverName = serverName.trim();
         this.serverName = serverName;
+        if ( this.serverName == null || serverName.isEmpty() ) {
+            this.schemaPrefix = """";
+        } else {
+            this.schemaPrefix = serverName.endsWith(""."") ? serverName : serverName + ""."";
+        }
 
         // Create and configure the database history ...
         this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
@@ -253,7 +260,7 @@ protected void refreshSchemas() {
         // Create TableSchema instances for any existing table ...
         this.tables.tableIds().forEach(id -> {
             Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(serverName,table, filters.columnFilter(), filters.columnMappers());
+            TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
             tableSchemaByTableId.put(id, schema);
         });
     }
@@ -323,7 +330,7 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
             if (table == null) { // removed
                 tableSchemaByTableId.remove(tableId);
             } else {
-                TableSchema schema = schemaBuilder.create(serverName, table, filters.columnFilter(), filters.columnMappers());
+                TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
                 tableSchemaByTableId.put(tableId, schema);
             }
         });",2016-06-10T02:08:09Z,16
"@@ -30,6 +30,10 @@ public Configurator with(Field field, boolean value) {
         return this;
     }
 
+    public Configurator serverName(String serverName) {
+        return with(MySqlConnectorConfig.SERVER_NAME, serverName);
+    }
+
     public Configurator includeDatabases(String regexList) {
         return with(MySqlConnectorConfig.DATABASE_WHITELIST, regexList);
     }",2016-06-10T02:08:09Z,35
"@@ -29,6 +29,7 @@
 public class MySqlSchemaTest {
 
     private static final Path TEST_FILE_PATH = Testing.Files.createTestingPath(""dbHistory.log"");
+    private static final String SERVER_NAME = ""test-server"";
 
     private Configurator build;
     private MySqlSchema mysql;
@@ -55,7 +56,7 @@ public void afterEach() {
 
     @Test
     public void shouldApplyDdlStatementsAndRecover() {
-        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).serverName(SERVER_NAME).createSchemas();
         mysql.start();
 
         // Testing.Print.enable();
@@ -74,6 +75,7 @@ public void shouldApplyDdlStatementsAndRecover() {
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .excludeBuiltInTables()
                      .createSchemas();
@@ -99,6 +101,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .includeBuiltInTables()
                      .createSchemas();
@@ -124,7 +127,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
     protected void assertTableIncluded(String fullyQualifiedTableName) {
         TableId tableId = TableId.parse(fullyQualifiedTableName);
         assertThat(mysql.tables().forTable(tableId)).isNotNull();
-        assertThat(mysql.schemaFor(tableId)).isNotNull();
+        TableSchema tableSchema = mysql.schemaFor(tableId);
+        assertThat(tableSchema).isNotNull();
+        assertThat(tableSchema.keySchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Key"");
+        assertThat(tableSchema.valueSchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Value"");
     }
 
     protected void assertTableExcluded(String fullyQualifiedTableName) {",2016-06-10T02:08:09Z,16
"@@ -172,3 +172,24 @@ VALUES (default, '2016-01-16', 1001, 1, 102),
        (default, '2016-02-18', 1004, 3, 109),
        (default, '2016-02-19', 1002, 2, 106),
        (default, '2016-02-21', 1003, 1, 107);
+
+
+
+# ----------------------------------------------------------------------------------------------------------------
+# DATABASE:  regression_test
+# ----------------------------------------------------------------------------------------------------------------
+# The integration test for this database expects to scans all of the binlog events associated with this database
+# without error or problems. The integration test does not modify any records in this database, so this script
+# must contain all operations to these tables.
+#
+CREATE DATABASE regression_test;
+USE regression_test;
+
+# DBZ-61 handle binary value recorded as hex string value
+CREATE TABLE t1464075356413_testtable6 (
+  pk_column int auto_increment NOT NULL,
+  varbinary_col varbinary(20) NOT NULL,
+  PRIMARY KEY(pk_column)
+);
+INSERT INTO t1464075356413_testtable6 (pk_column, varbinary_col)
+VALUES(default, 0x4D7953514C)",2016-06-07T22:53:07Z,93
"@@ -0,0 +1,89 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.nio.file.Path;
+import java.sql.SQLException;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.relational.history.FileDatabaseHistory;
+import io.debezium.util.Testing;
+
+/**
+ * @author Randall Hauch
+ */
+public class MySqlConnectorRegressionIT extends AbstractConnectorTest {
+
+    private static final Path DB_HISTORY_PATH = Testing.Files.createTestingPath(""file-db-history-regression.txt"").toAbsolutePath();
+
+    private Configuration config;
+
+    @Before
+    public void beforeEach() {
+        stopConnector();
+        initializeConnectorTestFramework();
+        Testing.Files.delete(DB_HISTORY_PATH);
+    }
+
+    @After
+    public void afterEach() {
+        try {
+            stopConnector();
+        } finally {
+            Testing.Files.delete(DB_HISTORY_PATH);
+        }
+    }
+
+    @Test
+    public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+        
+        // ---------------------------------------------------------------------------------------------------------------
+        // Consume all of the events due to startup and initialization of the database
+        // ---------------------------------------------------------------------------------------------------------------
+        //Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(2+1);   // 2 schema change record, 1 insert
+        stopConnector();
+        assertThat(records).isNotNull();
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(2);
+        assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(2);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(2);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+    }
+
+}",2016-06-07T22:53:07Z,71
"@@ -5,6 +5,8 @@
  */
 package io.debezium.data;
 
+import java.nio.ByteBuffer;
+import java.util.Base64;
 import java.util.List;
 import java.util.Map;
 
@@ -169,6 +171,11 @@ public RecordWriter append(Object obj) {
                     appendFirst(field.name(), s.get(field));
                 }
                 sb.append('}');
+            } else if (obj instanceof ByteBuffer) {
+                ByteBuffer b = (ByteBuffer) obj;
+                sb.append('""').append(Base64.getEncoder().encode(b.array())).append('""');
+            } else if (obj instanceof byte[]) {
+                sb.append('""').append(Base64.getEncoder().encode((byte[])obj)).append('""');
             } else if (obj instanceof Map<?, ?>) {
                 Map<?, ?> map = (Map<?, ?>) obj;
                 sb.append('{');",2016-06-07T22:53:07Z,133
"@@ -6,6 +6,7 @@
 package io.debezium.relational;
 
 import java.math.BigDecimal;
+import java.nio.ByteBuffer;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
@@ -105,7 +106,8 @@ public TableSchema create(ResultSet resultSet, String name) throws SQLException
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
@@ -121,15 +123,16 @@ public TableSchema create(String schemaPrefix, Table table) {
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @param filter the filter that specifies whether columns in the table should be included; may be null if all columns
      *            are to be included
      * @param mappers the mapping functions for columns; may be null if none of the columns are to be mapped to different values
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
     public TableSchema create(String schemaPrefix, Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
-        if ( schemaPrefix == null ) schemaPrefix = """";
+        if (schemaPrefix == null) schemaPrefix = """";
         // Build the schemas ...
         final TableId tableId = table.id();
         final String tableIdStr = tableId.toString();
@@ -285,7 +288,10 @@ protected ValueConverter[] convertersForColumns(Schema schema, TableId tableId,
                 if (mappers != null) {
                     ValueConverter mappingConverter = mappers.mappingConverterFor(tableId, column);
                     if (mappingConverter != null) {
-                        converter = (value) -> mappingConverter.convert(valueConverter.convert(value));
+                        converter = (value) -> {
+                            if (value != null) value = valueConverter.convert(value);
+                            return mappingConverter.convert(value);
+                        };
                     }
                 }
                 if (converter == null) converter = valueConverter;
@@ -376,7 +382,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
             case Types.NCLOB:
                 fieldBuilder = SchemaBuilder.string();
                 break;
-                
+
             // Variable-length string values
             case Types.VARCHAR:
             case Types.LONGVARCHAR:
@@ -424,7 +430,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
         if (fieldBuilder != null) {
             if (mapper != null) {
                 // Let the mapper add properties to the schema ...
-                mapper.alterFieldSchema(column,fieldBuilder);
+                mapper.alterFieldSchema(column, fieldBuilder);
             }
             if (column.isOptional()) fieldBuilder.optional();
             builder.field(column.name(), fieldBuilder.build());
@@ -450,6 +456,14 @@ protected SchemaBuilder addOtherField(Column column, ColumnMapper mapper) {
     /**
      * Create a {@link ValueConverter} that can be used to convert row values for the given column into the Kafka Connect value
      * object described by the {@link Field field definition}.
+     * <p>
+     * Subclasses can override this method to specialize the behavior. The subclass method should do custom checks and
+     * conversions,
+     * and then delegate to this method implementation to handle all other cases.
+     * <p>
+     * Alternatively, subclasses can leave this method as-is and instead override one of the lower-level type-specific methods
+     * that this method calls (e.g., {@link #convertBinary(Column, Field, Object)},
+     * {@link #convertTinyInt(Column, Field, Object)}, etc.).
      * 
      * @param column the column describing the input values; never null
      * @param fieldDefn the definition for the field in a Kafka Connect {@link Schema} describing the output of the function;
@@ -461,94 +475,38 @@ protected ValueConverter createValueConverterFor(Column column, Field fieldDefn)
             case Types.NULL:
                 return (data) -> null;
             case Types.BIT:
+                return (data) -> convertBit(column, fieldDefn, data);
             case Types.BOOLEAN:
-                return (data) -> {
-                    if (data instanceof Boolean) return (Boolean) data;
-                    if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBoolean(column, fieldDefn, data);
 
             // Binary values ...
             case Types.BLOB:
             case Types.BINARY:
             case Types.VARBINARY:
             case Types.LONGVARBINARY:
-                return (data) -> (byte[]) data;
+                return (data) -> convertBinary(column, fieldDefn, data);
 
             // Numeric integers
             case Types.TINYINT:
-                return (data) -> {
-                    if (data instanceof Byte) return (Byte) data;
-                    if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertTinyInt(column, fieldDefn, data);
             case Types.SMALLINT:
-                return (data) -> {
-                    if (data instanceof Short) return (Short) data;
-                    if (data instanceof Integer) return new Short(((Integer) data).shortValue());
-                    if (data instanceof Long) return new Short(((Long) data).shortValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertSmallInt(column, fieldDefn, data);
             case Types.INTEGER:
-                return (data) -> {
-                    if (data instanceof Integer) return (Integer) data;
-                    if (data instanceof Short) return new Integer(((Short) data).intValue());
-                    if (data instanceof Long) return new Integer(((Long) data).intValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertInteger(column, fieldDefn, data);
             case Types.BIGINT:
-                return (data) -> {
-                    if (data instanceof Long) return (Long) data;
-                    if (data instanceof Integer) return new Long(((Integer) data).longValue());
-                    if (data instanceof Short) return new Long(((Short) data).longValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBigInt(column, fieldDefn, data);
 
             // Numeric decimal numbers
             case Types.FLOAT:
+                return (data) -> convertFloat(column, fieldDefn, data);
             case Types.DOUBLE:
-                return (data) -> {
-                    if (data instanceof Double) return (Double) data;
-                    if (data instanceof Float) return new Double(((Float) data).doubleValue());
-                    if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
-                    if (data instanceof Long) return new Double(((Long) data).doubleValue());
-                    if (data instanceof Short) return new Double(((Short) data).doubleValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertDouble(column, fieldDefn, data);
             case Types.REAL:
-                return (data) -> {
-                    if (data instanceof Float) return (Float) data;
-                    if (data instanceof Double) return new Float(((Double) data).floatValue());
-                    if (data instanceof Integer) return new Float(((Integer) data).floatValue());
-                    if (data instanceof Long) return new Float(((Long) data).floatValue());
-                    if (data instanceof Short) return new Float(((Short) data).floatValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertReal(column, fieldDefn, data);
             case Types.NUMERIC:
+                return (data) -> convertNumeric(column, fieldDefn, data);
             case Types.DECIMAL:
-                return (data) -> {
-                    BigDecimal decimal = null;
-                    if (data instanceof BigDecimal)
-                        decimal = (BigDecimal) data;
-                    else if (data instanceof Boolean)
-                        decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
-                    else if (data instanceof Short)
-                        decimal = new BigDecimal(((Short) data).intValue());
-                    else if (data instanceof Integer)
-                        decimal = new BigDecimal(((Integer) data).intValue());
-                    else if (data instanceof Long)
-                        decimal = BigDecimal.valueOf(((Long) data).longValue());
-                    else if (data instanceof Float)
-                        decimal = BigDecimal.valueOf(((Float) data).doubleValue());
-                    else if (data instanceof Double)
-                        decimal = BigDecimal.valueOf(((Double) data).doubleValue());
-                    else {
-                        handleUnknownData(column, fieldDefn, data);
-                    }
-                    return decimal;
-                };
+                return (data) -> convertDecimal(column, fieldDefn, data);
 
             // String values
             case Types.CHAR: // variable-length
@@ -561,26 +519,23 @@ else if (data instanceof Double)
             case Types.NCLOB: // fixed-length
             case Types.DATALINK:
             case Types.SQLXML:
-                return (data) -> data.toString();
+                return (data) -> convertString(column, fieldDefn, data);
 
             // Date and time values
             case Types.DATE:
-                return (data) -> convertDate(fieldDefn, data);
+                return (data) -> convertDate(column, fieldDefn, data);
             case Types.TIME:
-                return (data) -> convertTime(fieldDefn, data);
+                return (data) -> convertTime(column, fieldDefn, data);
             case Types.TIMESTAMP:
-                return (data) -> convertTimestamp(fieldDefn, data);
+                return (data) -> convertTimestamp(column, fieldDefn, data);
             case Types.TIME_WITH_TIMEZONE:
-                return (data) -> convertTimeWithZone(fieldDefn, data);
+                return (data) -> convertTimeWithZone(column, fieldDefn, data);
             case Types.TIMESTAMP_WITH_TIMEZONE:
-                return (data) -> convertTimestampWithZone(fieldDefn, data);
+                return (data) -> convertTimestampWithZone(column, fieldDefn, data);
 
             // Other types ...
             case Types.ROWID:
-                return (data) -> {
-                    java.sql.RowId rowId = (java.sql.RowId) data;
-                    return rowId.getBytes();
-                };
+                return (data) -> convertRowId(column, fieldDefn, data);
 
             // Unhandled types
             case Types.ARRAY:
@@ -620,11 +575,13 @@ protected Object handleUnknownData(Column column, Field fieldDefn, Object data)
      * This method handles several types of objects, including {@link OffsetDateTime}, {@link java.sql.Timestamp},
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestampWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimestampWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetDateTime dateTime = null;
         if (data instanceof OffsetDateTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -679,11 +636,13 @@ protected OffsetDateTime unexpectedTimestampWithZone(Object value, Field fieldDe
      * {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types have date components, those date
      * components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimeWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimeWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetTime time = null;
         if (data instanceof OffsetTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -731,11 +690,13 @@ protected OffsetTime unexpectedTimeWithZone(Object value, Field fieldDefn) {
      * but no time zone info. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such
      * as {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestamp(Field fieldDefn, Object data) {
+    protected Object convertTimestamp(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Timestamp) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -782,11 +743,13 @@ protected java.util.Date unexpectedTimestamp(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types might
      * have date components, those date components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTime(Field fieldDefn, Object data) {
+    protected Object convertTime(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Time) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -834,11 +797,13 @@ protected java.util.Date unexpectedTime(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
      * have time components, those time components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertDate(Field fieldDefn, Object data) {
+    protected Object convertDate(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Date) {
             // JDBC specification indicates that this will be the nominal object for this JDBC type.
@@ -882,4 +847,286 @@ protected java.util.Date unexpectedDate(Object value, Field fieldDefn) {
                     fieldDefn.schema(), value.getClass(), value);
         return null;
     }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have time components, those time components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBinary(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof char[]) {
+            data = new String((char[]) data); // convert to string
+        }
+        if (data instanceof String) {
+            // This was encoded as a hexadecimal string, but we receive it as a normal string ...
+            data = ((String) data).getBytes();
+        }
+        if (data instanceof byte[]) {
+            return ByteBuffer.wrap((byte[])data);
+        }
+        // An unexpected value
+        return unexpectedBinary(data, fieldDefn);
+    }
+
+    /**
+     * Handle the unexpected value from a row with a column type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * 
+     * @param value the binary value for which no conversion was found; never null
+     * @param fieldDefn the field definition in the Kafka Connect schema; never null
+     * @return the converted value, or null
+     * @see #convertBinary(Column, Field, Object)
+     */
+    protected byte[] unexpectedBinary(Object value, Field fieldDefn) {
+        LOGGER.warn(""Unexpected JDBC BINARY value for field {} with schema {}: class={}, value={}"", fieldDefn.name(),
+                    fieldDefn.schema(), value.getClass(), value);
+        return null;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#TINYINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTinyInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Byte) return data;
+        if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#SMALLINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertSmallInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Short) return data;
+        if (data instanceof Integer) return new Short(((Integer) data).shortValue());
+        if (data instanceof Long) return new Short(((Long) data).shortValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertInteger(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Integer) return data;
+        if (data instanceof Short) return new Integer(((Short) data).intValue());
+        if (data instanceof Long) return new Integer(((Long) data).intValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBigInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Long) return data;
+        if (data instanceof Integer) return new Long(((Integer) data).longValue());
+        if (data instanceof Short) return new Long(((Short) data).longValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#FLOAT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertFloat(Column column, Field fieldDefn, Object data) {
+        return convertDouble(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DOUBLE}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDouble(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Double) return data;
+        if (data instanceof Float) return new Double(((Float) data).doubleValue());
+        if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
+        if (data instanceof Long) return new Double(((Long) data).doubleValue());
+        if (data instanceof Short) return new Double(((Short) data).doubleValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#REAL}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertReal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Float) return data;
+        if (data instanceof Double) return new Float(((Double) data).floatValue());
+        if (data instanceof Integer) return new Float(((Integer) data).floatValue());
+        if (data instanceof Long) return new Float(((Long) data).floatValue());
+        if (data instanceof Short) return new Float(((Short) data).floatValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertNumeric(Column column, Field fieldDefn, Object data) {
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDecimal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#CHAR}, {@link Types#VARCHAR},
+     * {@link Types#LONGVARCHAR}, {@link Types#CLOB}, {@link Types#NCHAR}, {@link Types#NVARCHAR}, {@link Types#LONGNVARCHAR},
+     * {@link Types#NCLOB}, {@link Types#DATALINK}, and {@link Types#SQLXML}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertString(Column column, Field fieldDefn, Object data) {
+        return data == null ? null : data.toString();
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#ROWID}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertRowId(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof java.sql.RowId) {
+            java.sql.RowId row = (java.sql.RowId)data;
+            return ByteBuffer.wrap(row.getBytes());
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BIT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBit(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BOOLEAN}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
 }",2016-06-07T22:53:07Z,135
"@@ -7,8 +7,13 @@
 
 import static org.junit.Assert.fail;
 
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.HashMap;
+import java.util.Iterator;
 import java.util.Map;
+import java.util.Objects;
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
@@ -47,18 +52,18 @@ public class VerifyRecord {
     private static final MockSchemaRegistryClient schemaRegistry = new MockSchemaRegistryClient();
     private static final AvroConverter avroKeyConverter = new AvroConverter(schemaRegistry);
     private static final AvroConverter avroValueConverter = new AvroConverter(schemaRegistry);
-    
+
     static {
-        Map<String,Object> config = new HashMap<>();
-        config.put(""schemas.enable"",Boolean.TRUE.toString());
-        config.put(""schemas.cache.size"",100);
+        Map<String, Object> config = new HashMap<>();
+        config.put(""schemas.enable"", Boolean.TRUE.toString());
+        config.put(""schemas.cache.size"", 100);
         keyJsonConverter.configure(config, true);
         keyJsonDeserializer.configure(config, true);
         valueJsonConverter.configure(config, false);
         valueJsonDeserializer.configure(config, false);
 
         config = new HashMap<>();
-        config.put(""schema.registry.url"",""http://fake-url"");
+        config.put(""schema.registry.url"", ""http://fake-url"");
         avroKeyConverter.configure(config, false);
         avroValueConverter.configure(config, false);
     }
@@ -276,73 +281,97 @@ public static void isValid(SourceRecord record) {
         SchemaAndValue valueWithSchema = null;
         SchemaAndValue avroKeyWithSchema = null;
         SchemaAndValue avroValueWithSchema = null;
+        String msg = null;
         try {
             // The key should never be null ...
+            msg = ""checking key is not null"";
             assertThat(record.key()).isNotNull();
             assertThat(record.keySchema()).isNotNull();
 
             // If the value is not null there must be a schema; otherwise, the schema should also be null ...
             if (record.value() == null) {
+                msg = ""checking value schema is null"";
                 assertThat(record.valueSchema()).isNull();
             } else {
+                msg = ""checking value schema is not null"";
                 assertThat(record.valueSchema()).isNotNull();
             }
 
             // First serialize and deserialize the key ...
+            msg = ""serializing key using JSON converter"";
             byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using JSON deserializer"";
             keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            msg = ""deserializing key using JSON converter"";
             keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            msg = ""comparing key schema to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // then the value ...
+            msg = ""serializing value using JSON converter"";
             byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using JSON deserializer"";
             valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            msg = ""deserializing value using JSON converter"";
             valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
             // Serialize and deserialize the key using the Avro converter, and check that we got the same result ...
+            msg = ""serializing key using Avro converter"";
             byte[] avroKeyBytes = avroValueConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using Avro converter"";
             avroKeyWithSchema = avroValueConverter.toConnectData(record.topic(), avroKeyBytes);
-            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
-            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key schema to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.schema(),record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.value(),record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // Serialize and deserialize the value using the Avro converter, and check that we got the same result ...
+            msg = ""serializing value using Avro converter"";
             byte[] avroValueBytes = avroValueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using Avro converter"";
             avroValueWithSchema = avroValueConverter.toConnectData(record.topic(), avroValueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
         } catch (Throwable t) {
             Testing.Print.enable();
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
             Testing.printError(t);
-            if (keyJson == null ){
-                Testing.print(""error deserializing key from JSON: "" + SchemaUtil.asString(record.key()));
-            } else if (keyWithSchema == null ){
-                Testing.print(""error using JSON converter on key: "" + prettyJson(keyJson));
-            } else if (avroKeyWithSchema == null ){
-                Testing.print(""error using Avro converter on key: "" + prettyJson(keyJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            Testing.print(""error "" + msg);
+            Testing.print(""  key: "" + SchemaUtil.asString(record.key()));
+            Testing.print(""  key deserialized from JSON: "" + prettyJson(keyJson));
+            if (keyWithSchema != null) {
+                Testing.print(""  key to/from JSON: "" + SchemaUtil.asString(keyWithSchema.value()));
             }
-
-            if (valueJson == null ){
-                Testing.print(""error deserializing value from JSON: "" + SchemaUtil.asString(record.value()));
-            } else if (valueWithSchema == null ){
-                Testing.print(""error using JSON converter on value: "" + prettyJson(valueJson));
-            } else if (avroValueWithSchema == null ){
-                Testing.print(""error using Avro converter on value: "" + prettyJson(valueJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            if (avroKeyWithSchema != null) {
+                Testing.print(""  key to/from Avro: "" + SchemaUtil.asString(avroKeyWithSchema.value()));
+            }
+            Testing.print(""  value: "" + SchemaUtil.asString(record.value()));
+            Testing.print(""  value deserialized from JSON: "" + prettyJson(valueJson));
+            if (valueWithSchema != null) {
+                Testing.print(""  value to/from JSON: "" + SchemaUtil.asString(valueWithSchema.value()));
+            }
+            if (avroValueWithSchema != null) {
+                Testing.print(""  value to/from Avro: "" + SchemaUtil.asString(avroValueWithSchema.value()));
             }
             if (t instanceof AssertionError) throw t;
-            fail(t.getMessage());
+            fail(""error "" + msg + "": "" + t.getMessage());
         }
     }
 
@@ -390,5 +419,135 @@ protected static String prettyJson(JsonNode json) {
             return null;
         }
     }
+    
+    // The remaining methods are needed simply because of the KAFKA-3803, so our comparisons cannot rely upon Struct.equals
+    
+    protected static void assertEquals( Object o1, Object o2 ) {
+        // assertThat(o1).isEqualTo(o2);
+        if ( !equals(o1,o2) ) {
+            fail(SchemaUtil.asString(o1) + "" was not equal to "" + SchemaUtil.asString(o2));
+        }
+    }
+    
+    @SuppressWarnings(""unchecked"")
+    protected static boolean equals( Object o1, Object o2 ) {
+        if ( o1 == o2 ) return true;
+        if (o1 == null) return o2 == null ? true : false;
+        if (o2 == null ) return false;
+        if ( o1 instanceof ByteBuffer ) {
+            o1 = ((ByteBuffer)o1).array();
+        }
+        if ( o2 instanceof ByteBuffer ) {
+            o2 = ((ByteBuffer)o2).array();
+        }
+        if ( o1 instanceof byte[] && o2 instanceof byte[] ) {
+            boolean result = Arrays.equals((byte[])o1,(byte[])o2);
+            return result;
+        }
+        if ( o1 instanceof Object[] && o2 instanceof Object[] ) {
+            boolean result = deepEquals((Object[])o1,(Object[])o2);
+            return result;
+        }
+        if ( o1 instanceof Map && o2 instanceof Map ) {
+            Map<String,Object> m1 = (Map<String,Object>)o1;
+            Map<String,Object> m2 = (Map<String,Object>)o2;
+            if ( !m1.keySet().equals(m2.keySet())) return false;
+            for ( Map.Entry<String, Object> entry : m1.entrySet()) {
+                Object v1 = entry.getValue();
+                Object v2 = m2.get(entry.getKey());
+                if ( !equals(v1,v2) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Collection && o2 instanceof Collection ) {
+            Collection<Object> m1 = (Collection<Object>)o1;
+            Collection<Object> m2 = (Collection<Object>)o2;
+            if ( m1.size() != m2.size() ) return false;
+            Iterator<?> iter1 = m1.iterator();
+            Iterator<?> iter2 = m2.iterator();
+            while ( iter1.hasNext() && iter2.hasNext() ) {
+                if ( !equals(iter1.next(),iter2.next()) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Struct && o2 instanceof Struct ) {
+            // Unfortunately, the Struct.equals() method has a bug in that it is not using Arrays.deepEquals(...) to
+            // compare values in two Struct objects. The result is that the equals only works if the values of the
+            // first level Struct are non arrays; otherwise, the array values are compared using == and that obviously
+            // does not work for non-primitive values.
+            Struct struct1 = (Struct) o1;
+            Struct struct2 = (Struct) o2;
+            if (! Objects.equals(struct1.schema(),struct2.schema()) ) {
+                return false;
+            }
+            Object[] array1 = valuesFor(struct1);
+            Object[] array2 = valuesFor(struct2);
+            boolean result = deepEquals(array1, array2);
+            return result;
+        }
+        return Objects.equals(o1, o2);
+    }
+    
+    private static Object[] valuesFor( Struct struct ) {
+        Object[] array = new Object[struct.schema().fields().size()];
+        int index = 0;
+        for ( Field field : struct.schema().fields() ) {
+            array[index] = struct.get(field);
+            ++index;
+        }
+        return array;
+    }
 
+    private static boolean deepEquals(Object[] a1, Object[] a2) {
+        if (a1 == a2)
+            return true;
+        if (a1 == null || a2==null)
+            return false;
+        int length = a1.length;
+        if (a2.length != length)
+            return false;
+
+        for (int i = 0; i < length; i++) {
+            Object e1 = a1[i];
+            Object e2 = a2[i];
+
+            if (e1 == e2)
+                continue;
+            if (e1 == null)
+                return false;
+
+            // Figure out whether the two elements are equal
+            boolean eq = deepEquals0(e1, e2);
+
+            if (!eq)
+                return false;
+        }
+        return true;
+    }
+
+    private static boolean deepEquals0(Object e1, Object e2) {
+        assert e1 != null;
+        boolean eq;
+        if (e1 instanceof Object[] && e2 instanceof Object[])
+            eq = deepEquals ((Object[]) e1, (Object[]) e2);
+        else if (e1 instanceof byte[] && e2 instanceof byte[])
+            eq = Arrays.equals((byte[]) e1, (byte[]) e2);
+        else if (e1 instanceof short[] && e2 instanceof short[])
+            eq = Arrays.equals((short[]) e1, (short[]) e2);
+        else if (e1 instanceof int[] && e2 instanceof int[])
+            eq = Arrays.equals((int[]) e1, (int[]) e2);
+        else if (e1 instanceof long[] && e2 instanceof long[])
+            eq = Arrays.equals((long[]) e1, (long[]) e2);
+        else if (e1 instanceof char[] && e2 instanceof char[])
+            eq = Arrays.equals((char[]) e1, (char[]) e2);
+        else if (e1 instanceof float[] && e2 instanceof float[])
+            eq = Arrays.equals((float[]) e1, (float[]) e2);
+        else if (e1 instanceof double[] && e2 instanceof double[])
+            eq = Arrays.equals((double[]) e1, (double[]) e2);
+        else if (e1 instanceof boolean[] && e2 instanceof boolean[])
+            eq = Arrays.equals((boolean[]) e1, (boolean[]) e2);
+        else
+            eq = equals(e1,e2);
+        return eq;
+    }
 }",2016-06-07T22:53:07Z,92
"@@ -63,6 +63,7 @@ public class MySqlSchema {
     private final TableSchemaBuilder schemaBuilder;
     private final DdlChanges ddlChanges;
     private final String serverName;
+    private final String schemaPrefix;
     private Tables tables;
 
     /**
@@ -78,7 +79,13 @@ public MySqlSchema(Configuration config, String serverName) {
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
         this.schemaBuilder = new TableSchemaBuilder();
+        if ( serverName != null ) serverName = serverName.trim();
         this.serverName = serverName;
+        if ( this.serverName == null || serverName.isEmpty() ) {
+            this.schemaPrefix = """";
+        } else {
+            this.schemaPrefix = serverName.endsWith(""."") ? serverName : serverName + ""."";
+        }
 
         // Create and configure the database history ...
         this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
@@ -253,7 +260,7 @@ protected void refreshSchemas() {
         // Create TableSchema instances for any existing table ...
         this.tables.tableIds().forEach(id -> {
             Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(serverName,table, filters.columnFilter(), filters.columnMappers());
+            TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
             tableSchemaByTableId.put(id, schema);
         });
     }
@@ -323,7 +330,7 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
             if (table == null) { // removed
                 tableSchemaByTableId.remove(tableId);
             } else {
-                TableSchema schema = schemaBuilder.create(serverName, table, filters.columnFilter(), filters.columnMappers());
+                TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
                 tableSchemaByTableId.put(tableId, schema);
             }
         });",2016-06-10T02:08:09Z,16
"@@ -30,6 +30,10 @@ public Configurator with(Field field, boolean value) {
         return this;
     }
 
+    public Configurator serverName(String serverName) {
+        return with(MySqlConnectorConfig.SERVER_NAME, serverName);
+    }
+
     public Configurator includeDatabases(String regexList) {
         return with(MySqlConnectorConfig.DATABASE_WHITELIST, regexList);
     }",2016-06-10T02:08:09Z,35
"@@ -29,6 +29,7 @@
 public class MySqlSchemaTest {
 
     private static final Path TEST_FILE_PATH = Testing.Files.createTestingPath(""dbHistory.log"");
+    private static final String SERVER_NAME = ""test-server"";
 
     private Configurator build;
     private MySqlSchema mysql;
@@ -55,7 +56,7 @@ public void afterEach() {
 
     @Test
     public void shouldApplyDdlStatementsAndRecover() {
-        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).serverName(SERVER_NAME).createSchemas();
         mysql.start();
 
         // Testing.Print.enable();
@@ -74,6 +75,7 @@ public void shouldApplyDdlStatementsAndRecover() {
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .excludeBuiltInTables()
                      .createSchemas();
@@ -99,6 +101,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .includeBuiltInTables()
                      .createSchemas();
@@ -124,7 +127,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
     protected void assertTableIncluded(String fullyQualifiedTableName) {
         TableId tableId = TableId.parse(fullyQualifiedTableName);
         assertThat(mysql.tables().forTable(tableId)).isNotNull();
-        assertThat(mysql.schemaFor(tableId)).isNotNull();
+        TableSchema tableSchema = mysql.schemaFor(tableId);
+        assertThat(tableSchema).isNotNull();
+        assertThat(tableSchema.keySchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Key"");
+        assertThat(tableSchema.valueSchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Value"");
     }
 
     protected void assertTableExcluded(String fullyQualifiedTableName) {",2016-06-10T02:08:09Z,16
"@@ -52,6 +52,7 @@ public class BinlogReader extends AbstractReader {
     private final SourceInfo source;
     private final EnumMap<EventType, BlockingConsumer<Event>> eventHandlers = new EnumMap<>(EventType.class);
     private BinaryLogClient client;
+    private int startingRowNumber = 0;
 
     /**
      * Create a binlog reader.
@@ -93,12 +94,15 @@ protected void doStart() {
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
 
-        // And set the client to start from that point ...
+        // The 'source' object holds the starting point in the binlog where we should start reading,
+        // set set the client to start from that point ...
         client.setGtidSet(source.gtidSet()); // may be null
         client.setBinlogFilename(source.binlogFilename());
-        client.setBinlogPosition(source.binlogPosition());
-        // The event row number will be used when processing the first event ...
+        client.setBinlogPosition(source.nextBinlogPosition());
 
+        // Set the starting row number, which is the next row number to be read ...
+        startingRowNumber = source.nextEventRowNumber();
+        
         // Start the log reader, which starts background threads ...
         long timeoutInMilliseconds = context.timeoutInMilliseconds();
         try {
@@ -157,21 +161,19 @@ protected void handleEvent(Event event) {
             } else {
                 rotateEventData = (RotateEventData) eventData;
             }
-            source.setBinlogFilename(rotateEventData.getBinlogFilename());
-            source.setBinlogPosition(rotateEventData.getBinlogPosition());
-            source.setRowInEvent(0);
+            source.setBinlogStartPoint(rotateEventData.getBinlogFilename(), rotateEventData.getBinlogPosition());
         } else if (eventHeader instanceof EventHeaderV4) {
             EventHeaderV4 trackableEventHeader = (EventHeaderV4) eventHeader;
-            long nextBinlogPosition = trackableEventHeader.getNextPosition();
-            if (nextBinlogPosition > 0) {
-                source.setBinlogPosition(nextBinlogPosition);
-                source.setRowInEvent(0);
-            }
+            source.setEventPosition(trackableEventHeader.getPosition(), trackableEventHeader.getEventLength());
         }
 
         // If there is a handler for this event, forward the event to it ...
         try {
+            // Forward the event to the handler ...
             eventHandlers.getOrDefault(eventType, this::ignoreEvent).accept(event);
+            
+            // And after that event has been processed, always set the starting row number to 0 ...
+            startingRowNumber = 0;
         } catch (InterruptedException e) {
             // Most likely because this reader was stopped and our thread was interrupted ...
             Thread.interrupted();
@@ -301,7 +303,11 @@ protected void handleInsert(Event event) throws InterruptedException {
         if (recordMaker != null) {
             List<Serializable[]> rows = write.getRows();
             Long ts = context.clock().currentTimeInMillis();
-            int count = recordMaker.createEach(rows, ts);
+            int count = 0;
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
+                count += recordMaker.create(rows.get(row), ts, row, numRows);
+            }
             logger.debug(""Recorded {} insert records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping insert row event: {}"", event);
@@ -324,11 +330,12 @@ protected void handleUpdate(Event event) throws InterruptedException {
             List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
-            for (int row = 0; row != rows.size(); ++row) {
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
                 Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
                 Serializable[] before = changes.getKey();
                 Serializable[] after = changes.getValue();
-                count += recordMaker.update(before, after, ts, row);
+                count += recordMaker.update(before, after, ts, row, numRows);
             }
             logger.debug(""Recorded {} update records for event: {}"", count, event);
         } else {
@@ -350,7 +357,11 @@ protected void handleDelete(Event event) throws InterruptedException {
         if (recordMaker != null) {
             List<Serializable[]> rows = deleted.getRows();
             Long ts = context.clock().currentTimeInMillis();
-            int count = recordMaker.deleteEach(rows, ts);
+            int count = 0;
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
+                count += recordMaker.delete(rows.get(row), ts, row, numRows);
+            }
             logger.debug(""Recorded {} delete records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping delete row event: {}"", event);",2016-06-14T22:43:58Z,24
"@@ -107,7 +107,7 @@ public void start(Map<String, String> props) {
             if (taskContext.isSnapshotNeverAllowed()) {
                 // We're not allowed to take a snapshot, so instead we have to assume that the binlog contains the
                 // full history of the database.
-                source.setBinlogFilename("""");// start from the beginning of the binlog
+                source.setBinlogStartPoint("""", 0L);// start from the beginning of the binlog
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;",2016-06-14T22:43:58Z,10
"@@ -7,7 +7,6 @@
 
 import java.util.BitSet;
 import java.util.HashMap;
-import java.util.Iterator;
 import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
@@ -152,7 +151,8 @@ public void regenerate() {
      */
     public boolean assign(long tableNumber, TableId id) {
         Long existingTableNumber = tableNumbersByTableId.get(id);
-        if ( existingTableNumber != null && existingTableNumber.longValue() == tableNumber && convertersByTableNumber.containsKey(tableNumber)) {
+        if (existingTableNumber != null && existingTableNumber.longValue() == tableNumber
+                && convertersByTableNumber.containsKey(tableNumber)) {
             // This is the exact same table number for the same table, so do nothing ...
             return true;
         }
@@ -171,15 +171,15 @@ public boolean assign(long tableNumber, TableId id) {
         Converter converter = new Converter() {
 
             @Override
-            public int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int read(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                             BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 Object key = tableSchema.keyFromColumnData(row);
                 Struct value = tableSchema.valueFromColumnData(row);
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
                             keySchema, key, envelope.schema(), envelope.read(value, origin, ts));
@@ -190,15 +190,15 @@ public int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedC
             }
 
             @Override
-            public int insert(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int insert(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 Object key = tableSchema.keyFromColumnData(row);
                 Struct value = tableSchema.valueFromColumnData(row);
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
                             keySchema, key, envelope.schema(), envelope.create(value, origin, ts));
@@ -209,7 +209,7 @@ public int insert(SourceInfo source, Object[] row, int rowNumber, BitSet include
             }
 
             @Override
-            public int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, BitSet includedColumns, long ts,
+            public int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 int count = 0;
@@ -220,7 +220,7 @@ public int update(SourceInfo source, Object[] before, Object[] after, int rowNum
                     Struct valueBefore = tableSchema.valueFromColumnData(before);
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     if (key != null && !Objects.equals(key, oldKey)) {
                         // The key has indeed changed, so first send a create event ...
@@ -251,7 +251,7 @@ record = new SourceRecord(partition, offset, topicName, partitionNum, keySchema,
             }
 
             @Override
-            public int delete(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int delete(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 int count = 0;
@@ -260,7 +260,7 @@ public int delete(SourceInfo source, Object[] row, int rowNumber, BitSet include
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     // Send a delete message ...
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
@@ -275,7 +275,7 @@ record = new SourceRecord(partition, offset, topicName, partitionNum,
                 }
                 return count;
             }
-            
+
             @Override
             public String toString() {
                 return ""RecordMaker.Converter("" + id + "")"";
@@ -307,17 +307,20 @@ protected Struct schemaChangeRecordValue(String databaseName, String ddlStatemen
     }
 
     protected static interface Converter {
-        int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int read(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                 BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int insert(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int insert(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                   BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, BitSet includedColumns, long ts,
+        int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                    BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int delete(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int delete(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                   BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
     }
@@ -346,7 +349,7 @@ protected RecordsForTable(Converter converter, BitSet includedColumns, BlockingC
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int read(Object[] row, long ts) throws InterruptedException {
-            return read(row, ts, 0);
+            return read(row, ts, 0, 1);
         }
 
         /**
@@ -356,29 +359,12 @@ public int read(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int read(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.read(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#READ read} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int readEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += read(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int read(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.read(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -391,7 +377,7 @@ public int readEach(Iterable<? extends Object[]> rows, long ts) throws Interrupt
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int create(Object[] row, long ts) throws InterruptedException {
-            return create(row, ts, 0);
+            return create(row, ts, 0, 1);
         }
 
         /**
@@ -401,29 +387,12 @@ public int create(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int create(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.insert(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#CREATE create} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int createEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += create(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int create(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.insert(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -438,7 +407,7 @@ public int createEach(Iterable<? extends Object[]> rows, long ts) throws Interru
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int update(Object[] before, Object[] after, long ts) throws InterruptedException {
-            return update(before, after, ts, 0);
+            return update(before, after, ts, 0, 1);
         }
 
         /**
@@ -450,11 +419,12 @@ public int update(Object[] before, Object[] after, long ts) throws InterruptedEx
          *            definition in the {@link MySqlSchema}
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int update(Object[] before, Object[] after, long ts, int rowNumber) throws InterruptedException {
-            return converter.update(source, before, after, rowNumber, includedColumns, ts, consumer);
+        public int update(Object[] before, Object[] after, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.update(source, before, after, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -467,7 +437,7 @@ public int update(Object[] before, Object[] after, long ts, int rowNumber) throw
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int delete(Object[] row, long ts) throws InterruptedException {
-            return delete(row, ts, 0);
+            return delete(row, ts, 0, 1);
         }
 
         /**
@@ -477,29 +447,12 @@ public int delete(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int delete(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.delete(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#DELETE delete} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int deleteEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += delete(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int delete(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.delete(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
     }
 }",2016-06-14T22:43:58Z,128
"@@ -186,9 +186,11 @@ protected void execute() {
             sql.set(""SHOW MASTER STATUS"");
             mysql.query(sql.get(), rs -> {
                 if (rs.next()) {
-                    source.setBinlogFilename(rs.getString(1));
-                    source.setBinlogPosition(rs.getLong(2));
-                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                    String binlogFilename = rs.getString(1);
+                    long binlogPosition = rs.getLong(2);
+                    String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                    source.setBinlogStartPoint(binlogFilename, binlogPosition);
+                    source.setGtidSet(gtidSet);
                     source.startSnapshot();
                 }
             });",2016-06-14T22:43:58Z,15
"@@ -33,16 +33,18 @@
  * 
  * <p>
  * The {@link #offset() source offset} information describes how much of the database's binary log the source the change detector
- * has processed. Here's a JSON-like representation of an example:
+ * has already processed, and it includes the {@link #binlogFilename() binlog filename}, the {@link #nextBinlogPosition() next
+ * position} in the binlog to start reading, and the {@link #nextEventRowNumber() next event row number}. Here's a JSON-like
+ * representation of an example:
  * 
  * <pre>
  * {
  *     ""server_id"": 112233,
- *     ""ts_sec"": 1465236179,
- *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
- *     ""file"" = ""mysql-bin.000003"",
- *     ""pos"" = 105586,
- *     ""row"" = 0,
+ *     ""ts_sec"": 1465937,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 990,
+ *     ""row"": 0,
  *     ""snapshot"": true
  * }
  * </pre>
@@ -53,20 +55,21 @@
  * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
  * 
  * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
- * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
- * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
- * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
- * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
- * an event that corresponds to the above partition and offset:
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), the
+ * {@link #lastBinlogPosition() position} of the event (and {@link #lastEventRowNumber() row number} within the event) inside
+ * the {@link #binlogFilename() binlog file}. When GTIDs are enabled, it also includes the GTID of the transaction in which the
+ * event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced when the connector is in the
+ * middle of a snapshot. Here's a JSON-like representation of the source for an event that corresponds to the above partition and
+ * offset:
  * 
  * <pre>
  * {
  *     ""name"": ""production-server"",
  *     ""server_id"": 112233,
- *     ""ts_sec"": 1465236179,
+ *     ""ts_sec"": 1465937,
  *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
  *     ""file"": ""mysql-bin.000003"",
- *     ""pos"" = 105586,
+ *     ""pos"" = 1081,
  *     ""row"": 0,
  *     ""snapshot"": true
  * }
@@ -109,8 +112,10 @@ final class SourceInfo {
     private String gtidSet;
     private String binlogGtid;
     private String binlogFilename;
-    private long binlogPosition = 4;
-    private int eventRowNumber = 0;
+    private long lastBinlogPosition = 0;
+    private int lastEventRowNumber = 0;
+    private long nextBinlogPosition = 4;
+    private int nextEventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
     private long binlogTimestampSeconds = 0;
@@ -150,6 +155,32 @@ public Map<String, String> partition() {
      * @return a copy of the current offset; never null
      */
     public Map<String, ?> offset() {
+        return offsetUsingPosition(nextBinlogPosition);
+    }
+
+    /**
+     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
+     * describes the position within the source where we have last read.
+     * 
+     * @param eventRowNumber the 0-based row number within the event being processed
+     * @param totalNumberOfRows the total number of rows within the event being processed
+     * @return a copy of the current offset; never null
+     */
+    public Map<String, ?> offset(int eventRowNumber, int totalNumberOfRows) {
+        if (eventRowNumber < (totalNumberOfRows - 1)) {
+            // This is not the last row, so our offset should record the next row to be used ...
+            this.lastEventRowNumber = eventRowNumber;
+            this.nextEventRowNumber = eventRowNumber + 1;
+            // so write out the offset with the position of this event
+            return offsetUsingPosition(lastBinlogPosition);
+        }
+        // This is the last row, so write out the offset that has the position of the next event ...
+        this.lastEventRowNumber = this.nextEventRowNumber;
+        this.nextEventRowNumber = 0;
+        return offsetUsingPosition(nextBinlogPosition);
+    }
+
+    private Map<String, ?> offsetUsingPosition( long binlogPosition ) {
         Map<String, Object> map = new HashMap<>();
         if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
         if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
@@ -158,7 +189,7 @@ public Map<String, String> partition() {
         }
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
+        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, nextEventRowNumber);
         if (isSnapshotInEffect()) {
             map.put(SNAPSHOT_KEY, true);
         }
@@ -192,27 +223,15 @@ public Struct struct() {
             result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
+        result.put(BINLOG_POSITION_OFFSET_KEY, lastBinlogPosition);
+        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, lastEventRowNumber);
         result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
         return result;
     }
 
-    /**
-     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
-     * describes the position within the source where we have last read.
-     * 
-     * @param eventRowNumber the 0-based row number within the last event that was successfully processed
-     * @return a copy of the current offset; never null
-     */
-    public Map<String, ?> offset(int eventRowNumber) {
-        setRowInEvent(eventRowNumber);
-        return offset();
-    }
-
     /**
      * Determine whether a snapshot is currently in effect.
      * 
@@ -246,28 +265,27 @@ public void setGtidSet(String gtidSet) {
      * Set the name of the MySQL binary log file.
      * 
      * @param binlogFilename the name of the binary log file; may not be null
+     * @param positionOfFirstEvent the position in the binary log file to begin processing
      */
-    public void setBinlogFilename(String binlogFilename) {
+    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
         this.binlogFilename = binlogFilename;
+        this.nextBinlogPosition = positionOfFirstEvent;
+        this.lastBinlogPosition = this.nextBinlogPosition;
+        this.nextEventRowNumber = 0;
+        this.lastEventRowNumber = 0;
     }
 
     /**
-     * Set the position within the MySQL binary log file.
-     * 
-     * @param binlogPosition the position within the binary log file
-     */
-    public void setBinlogPosition(long binlogPosition) {
-        this.binlogPosition = binlogPosition;
-    }
-
-    /**
-     * Set the index of the row within the event appearing at the {@link #binlogPosition() position} within the
-     * {@link #binlogFilename() binary log file}.
+     * Set the position within the MySQL binary log file of the <em>current event</em>.
      * 
-     * @param rowNumber the 0-based row number
+     * @param positionOfCurrentEvent the position within the binary log file of the current event
+     * @param eventSizeInBytes the size in bytes of this event
      */
-    public void setRowInEvent(int rowNumber) {
-        this.eventRowNumber = rowNumber;
+    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
+        this.lastBinlogPosition = positionOfCurrentEvent;
+        this.nextBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
+        this.nextEventRowNumber = 0;
+        this.lastEventRowNumber = 0;
     }
 
     /**
@@ -316,8 +334,10 @@ public void setOffset(Map<String, ?> sourceOffset) {
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
             }
-            binlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
-            eventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
+            nextBinlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
+            nextEventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
+            lastBinlogPosition = nextBinlogPosition;
+            lastEventRowNumber = nextEventRowNumber;
         }
     }
 
@@ -344,29 +364,50 @@ public String gtidSet() {
     /**
      * Get the name of the MySQL binary log file that has been processed.
      * 
-     * @return the name of the binary log file; null if it has not been {@link #setBinlogFilename(String) set}
+     * @return the name of the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
     public String binlogFilename() {
         return binlogFilename;
     }
 
     /**
-     * Get the position within the MySQL binary log file that has been processed.
+     * Get the position within the MySQL binary log file of the next event to be processed.
      * 
-     * @return the position within the binary log file; null if it has not been {@link #setBinlogPosition(long) set}
+     * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
+     */
+    public long nextBinlogPosition() {
+        return nextBinlogPosition;
+    }
+
+    /**
+     * Get the position within the MySQL binary log file of the most recently processed event.
+     * 
+     * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
+     */
+    public long lastBinlogPosition() {
+        return lastBinlogPosition;
+    }
+
+    /**
+     * Get the next row within the event at the {@link #nextBinlogPosition() position} within the {@link #binlogFilename() binary
+     * log file}
+     * .
+     * 
+     * @return the 0-based row number
      */
-    public long binlogPosition() {
-        return binlogPosition;
+    public int nextEventRowNumber() {
+        return nextEventRowNumber;
     }
 
     /**
-     * Get the row within the event at the {@link #binlogPosition() position} within the {@link #binlogFilename() binary log file}
+     * Get the previous row within the event at the {@link #lastBinlogPosition() position} within the {@link #binlogFilename()
+     * binary log file}
      * .
      * 
      * @return the 0-based row number
      */
-    public int eventRowNumber() {
-        return eventRowNumber;
+    public int lastEventRowNumber() {
+        return lastEventRowNumber;
     }
 
     /**
@@ -385,8 +426,8 @@ public String toString() {
             sb.append(""GTIDs "");
             sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
-            sb.append("", pos="").append(binlogPosition());
-            sb.append("", row="").append(eventRowNumber());
+            sb.append("", pos="").append(nextBinlogPosition());
+            sb.append("", row="").append(nextEventRowNumber());
         } else {
             if (binlogFilename == null) {
                 sb.append(""<latest>"");
@@ -395,8 +436,8 @@ public String toString() {
                     sb.append(""earliest binlog file and position"");
                 } else {
                     sb.append(""binlog file '"").append(binlogFilename).append(""'"");
-                    sb.append("", pos="").append(binlogPosition());
-                    sb.append("", row="").append(eventRowNumber());
+                    sb.append("", pos="").append(nextBinlogPosition());
+                    sb.append("", row="").append(nextEventRowNumber());
                 }
             }
         }",2016-06-14T22:43:58Z,11
"@@ -114,7 +114,7 @@ public void shouldCreateSnapshotOfSingleDatabase() throws Exception {
         config = simpleConfig().build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setBinlogFilename(""""); // start from beginning
+        context.source().setBinlogStartPoint("""",0L); // start from beginning
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...
@@ -173,7 +173,7 @@ public void shouldCreateSnapshotOfSingleDatabaseWithSchemaChanges() throws Excep
         config = simpleConfig().with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true).build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setBinlogFilename(""""); // start from beginning
+        context.source().setBinlogStartPoint("""",0L); // start from beginning
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...",2016-06-14T22:43:58Z,24
"@@ -22,6 +22,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.history.FileDatabaseHistory;
@@ -284,6 +285,56 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         stopConnector();
     }
 
+    @Test
+    public void shouldConsumeEventsWithNoSnapshot() throws SQLException, InterruptedException {
+        Testing.Files.delete(DB_HISTORY_PATH);
+        
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18780)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""kafka-connect-2"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test_ro"")
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.name().toLowerCase())
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+
+        // Consume the first records due to startup and initialization of the database ...
+        // Testing.Print.enable();
+        SourceRecords records = consumeRecordsByTopic(9+9+4+5+6);   // 6 DDL changes
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.products"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.products_on_hand"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.customers"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.orders"").size()).isEqualTo(5);
+        assertThat(records.topics().size()).isEqualTo(4+1);
+        assertThat(records.ddlRecordsForDatabase(""connector_test_ro"").size()).isEqualTo(6);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+        
+        // More records may have been written (if this method were run after the others), but we don't care ...
+        stopConnector();
+
+        records.recordsForTopic(""kafka-connect-2.connector_test_ro.orders"").forEach(record->{
+            print(record);
+        });
+        
+        records.recordsForTopic(""kafka-connect-2.connector_test_ro.customers"").forEach(record->{
+            print(record);
+        });
+    }
+
+
     @Test
     public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLException, InterruptedException {
         Testing.Files.delete(DB_HISTORY_PATH);",2016-06-14T22:43:58Z,88
"@@ -60,8 +60,7 @@ public void shouldApplyDdlStatementsAndRecover() {
         mysql.start();
 
         // Testing.Print.enable();
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -81,11 +80,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
                      .createSchemas();
         mysql.start();
 
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
-        source.setBinlogPosition(1000);
+        source.setBinlogStartPoint(""binlog-001"",1000);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -107,11 +105,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
                      .createSchemas();
         mysql.start();
 
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
-        source.setBinlogPosition(1000);
+        source.setBinlogStartPoint(""binlog-001"",1000);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...",2016-06-14T22:43:58Z,16
"@@ -0,0 +1,268 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.TreeMap;
+
+import io.debezium.annotation.Immutable;
+
+/**
+ * A set of MySQL GTIDs. This is an improvement of {@link com.github.shyiko.mysql.binlog.GtidSet} that is immutable,
+ * and more properly supports comparisons.
+ * 
+ * @author Randall Hauch
+ */
+@Immutable
+public final class GtidSet {
+
+    private final String orderedString;
+    private final Map<String, UUIDSet> uuidSetsByServerId = new TreeMap<>(); // sorts on keys
+
+    /**
+     * @param gtids the string representation of the GTIDs.
+     */
+    public GtidSet(String gtids) {
+        new com.github.shyiko.mysql.binlog.GtidSet(gtids).getUUIDSets().forEach(uuidSet -> {
+            uuidSetsByServerId.put(uuidSet.getUUID(), new UUIDSet(uuidSet));
+        });
+        StringBuilder sb = new StringBuilder();
+        uuidSetsByServerId.values().forEach(uuidSet -> {
+            if (sb.length() != 0) sb.append(',');
+            sb.append(uuidSet.toString());
+        });
+        orderedString = sb.toString();
+    }
+
+    /**
+     * Get an immutable collection of the {@link UUIDSet range of GTIDs for a single server}.
+     * 
+     * @return the {@link UUIDSet GTID ranges for each server}; never null
+     */
+    public Collection<UUIDSet> getUUIDSets() {
+        return Collections.unmodifiableCollection(uuidSetsByServerId.values());
+    }
+
+    /**
+     * Find the {@link UUIDSet} for the server with the specified UUID.
+     * 
+     * @param uuid the UUID of the server
+     * @return the {@link UUIDSet} for the identified server, or {@code null} if there are no GTIDs from that server.
+     */
+    public UUIDSet forServerWithId(String uuid) {
+        return uuidSetsByServerId.get(uuid);
+    }
+
+    /**
+     * Determine if the GTIDs represented by this object are contained completely within the supplied set of GTIDs.
+     * 
+     * @param other the other set of GTIDs; may be null
+     * @return {@code true} if all of the GTIDs in this set are completely contained within the supplied set of GTIDs, or
+     *         {@code false} otherwise
+     */
+    public boolean isSubsetOf(GtidSet other) {
+        if (other == null) return false;
+        if (this.equals(other)) return true;
+        for (UUIDSet uuidSet : uuidSetsByServerId.values()) {
+            UUIDSet thatSet = other.forServerWithId(uuidSet.getUUID());
+            if (!uuidSet.isSubsetOf(thatSet)) return false;
+        }
+        return true;
+    }
+
+    @Override
+    public int hashCode() {
+        return orderedString.hashCode();
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (obj == this) return true;
+        if (obj instanceof GtidSet) {
+            GtidSet that = (GtidSet) obj;
+            return this.orderedString.equalsIgnoreCase(that.orderedString);
+        }
+        return false;
+    }
+
+    @Override
+    public String toString() {
+        return orderedString;
+    }
+
+    /**
+     * A range of GTIDs for a single server with a specific UUID.
+     */
+    @Immutable
+    public static class UUIDSet {
+
+        private String uuid;
+        private LinkedList<Interval> intervals = new LinkedList<>();
+
+        protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
+            this.uuid = uuidSet.getUUID();
+            uuidSet.getIntervals().forEach(interval -> {
+                intervals.add(new Interval(interval.getStart(), interval.getEnd()));
+            });
+            Collections.sort(this.intervals);
+        }
+
+        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
+            this.uuid = uuid;
+            this.intervals = intervals;
+        }
+
+        /**
+         * Get the UUID for the server that generated the GTIDs.
+         * 
+         * @return the server's UUID; never null
+         */
+        public String getUUID() {
+            return uuid;
+        }
+
+        /**
+         * Get the intervals of transaction numbers.
+         * 
+         * @return the immutable transaction intervals; never null
+         */
+        public Collection<Interval> getIntervals() {
+            return Collections.unmodifiableCollection(intervals);
+        }
+
+        /**
+         * Get the first interval of transaction numbers for this server.
+         * 
+         * @return the first interval, or {@code null} if there is none
+         */
+        public Interval getFirstInterval() {
+            return intervals.isEmpty() ? null : intervals.getFirst();
+        }
+
+        /**
+         * Get the last interval of transaction numbers for this server.
+         * 
+         * @return the last interval, or {@code null} if there is none
+         */
+        public Interval getLastInterval() {
+            return intervals.isEmpty() ? null : intervals.getLast();
+        }
+
+        /**
+         * Get the interval that contains the full range (and possibly more) of all of the individual intervals for this server.
+         * 
+         * @return the complete interval comprised of the {@link Interval#getStart() start} of the {@link #getFirstInterval()
+         *         first interval} and the {@link Interval#getEnd() end} of the {@link #getLastInterval()}, or {@code null} if
+         *         this server has no intervals at all
+         */
+        public Interval getCompleteInterval() {
+            return intervals.isEmpty() ? null : new Interval(getFirstInterval().getStart(), getLastInterval().getEnd());
+        }
+
+        /**
+         * Determine if the set of transaction numbers from this server is completely within the set of transaction numbers from
+         * the set of transaction numbers in the supplied set.
+         * 
+         * @param other the set to compare with this set
+         * @return {@code true} if this server's transaction numbers are a subset of the transaction numbers of the supplied set,
+         *         or false otherwise
+         */
+        public boolean isSubsetOf(UUIDSet other) {
+            if (other == null) return false;
+            if (!this.getUUID().equalsIgnoreCase(other.getUUID())) {
+                // Not even the same server ...
+                return false;
+            }
+            if (this.intervals.isEmpty()) return true;
+            if (other.intervals.isEmpty()) return false;
+            assert this.intervals.size() > 0;
+            assert other.intervals.size() > 0;
+
+            // Every interval in this must be within an interval of the other ...
+            for (Interval thisInterval : this.intervals) {
+                boolean found = false;
+                for (Interval otherInterval : other.intervals) {
+                    if (thisInterval.isSubsetOf(otherInterval)) {
+                        found = true;
+                        break;
+                    }
+                }
+                if (!found) return false; // didn't find a match
+            }
+            return true;
+        }
+
+        @Override
+        public int hashCode() {
+            return uuid.hashCode();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (obj == this) return true;
+            if (obj instanceof UUIDSet) {
+                UUIDSet that = (UUIDSet) obj;
+                return this.getUUID().equalsIgnoreCase(that.getUUID()) && this.getIntervals().equals(that.getIntervals());
+            }
+            return super.equals(obj);
+        }
+
+        @Override
+        public String toString() {
+            StringBuilder sb = new StringBuilder();
+            if (sb.length() != 0) sb.append(',');
+            sb.append(uuid).append(':');
+            sb.append(intervals.getFirst().getStart());
+            sb.append(intervals.getLast().getEnd());
+            return sb.toString();
+        }
+    }
+
+    @Immutable
+    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+
+        public Interval(long start, long end) {
+            super(start, end);
+        }
+
+        /**
+         * Determine if this interval is completely within the supplied interval.
+         * 
+         * @param other the interval to compare with
+         * @return {@code true} if the {@link #getStart() start} is greater than or equal to the supplied interval's
+         *         {@link #getStart() start} and the {@link #getEnd() end} is less than or equal to the supplied interval's
+         *         {@link #getEnd() end}, or {@code false} otherwise
+         */
+        public boolean isSubsetOf(Interval other) {
+            if (other == this) return true;
+            if (other == null) return false;
+            return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
+        }
+
+        @Override
+        public int hashCode() {
+            return (int) getStart();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (this == obj) return true;
+            if (obj instanceof com.github.shyiko.mysql.binlog.GtidSet.Interval) {
+                com.github.shyiko.mysql.binlog.GtidSet.Interval that = (com.github.shyiko.mysql.binlog.GtidSet.Interval) obj;
+                return this.getStart() == that.getStart() && this.getEnd() == that.getEnd();
+            }
+            return false;
+        }
+
+        @Override
+        public String toString() {
+            return getStart() == getEnd() ? Long.toString(getStart()) : """" + getStart() + ""-"" + getEnd();
+        }
+    }
+}",2016-06-04T21:20:26Z,101
"@@ -76,7 +76,8 @@ public class MySqlConnectorConfig {
                                                       .withDescription(""The name of the DatabaseHistory class that should be used to store and recover database schema changes. ""
                                                               + ""The configuration properties for the history are prefixed with the '""
                                                               + DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING + ""' string."")
-                                                      .withDefault(KafkaDatabaseHistory.class.getName());
+                                                      .withDefault(KafkaDatabaseHistory.class.getName())
+                                                      .withValidation(Field::isClassName);
 
     public static final Field INCLUDE_SCHEMA_CHANGES = Field.create(""include.schema.changes"")
                                                             .withDescription(""Whether the connector should publish changes in the database schema to a Kafka topic with """,2016-06-04T21:20:26Z,65
"@@ -27,6 +27,7 @@
 import io.debezium.relational.ddl.DdlChanges;
 import io.debezium.relational.ddl.DdlChanges.DatabaseStatementStringConsumer;
 import io.debezium.relational.history.DatabaseHistory;
+import io.debezium.relational.history.HistoryRecordComparator;
 import io.debezium.text.ParsingException;
 import io.debezium.util.Collect;
 
@@ -51,6 +52,8 @@
 @NotThreadSafe
 public class MySqlSchema {
 
+    private static final HistoryRecordComparator HISTORY_COMPARATOR = HistoryRecordComparator.usingPositions(SourceInfo::isPositionAtOrBefore);
+
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final Set<String> ignoredQueryStatements = Collect.unmodifiableSet(""BEGIN"", ""END"", ""FLUSH PRIVILEGES"");
     private final MySqlDdlParser ddlParser;
@@ -85,7 +88,7 @@ public MySqlSchema(Configuration config, String serverName) {
         }
         // Do not remove the prefix from the subset of config properties ...
         Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false);
-        this.dbHistory.configure(dbHistoryConfig); // validates
+        this.dbHistory.configure(dbHistoryConfig,HISTORY_COMPARATOR); // validates
     }
 
     /**",2016-06-04T21:20:26Z,16
"@@ -13,9 +13,8 @@
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.errors.ConnectException;
 
-import com.github.shyiko.mysql.binlog.GtidSet;
-
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
 /**
@@ -126,6 +125,8 @@ public Map<String, String> partition() {
         if (binlogGtids != null) {
             map.put(BINLOG_GTID_KEY, binlogGtids.toString());
         }
+        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTs != 0 ) map.put(BINLOG_EVENT_TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -360,4 +361,90 @@ public String toString() {
         }
         return sb.toString();
     }
+
+    /**
+     * Determine whether the first {@link #offset() offset} is at or before the point in time of the second
+     * offset, where the offsets are given in JSON representation of the maps returned by {@link #offset()}.
+     * <p>
+     * This logic makes a significant assumption: once a MySQL server/cluster has GTIDs enabled, they will
+     * never be disabled. This is the only way to compare a position with a GTID to a position without a GTID,
+     * and we conclude that any position with a GTID is *after* the position without.
+     * <p>
+     * When both positions have GTIDs, then we compare the positions by using only the GTIDs. Of course, if the
+     * GTIDs are the same, then we also look at whether they have snapshots enabled.
+     * 
+     * @param recorded the position obtained from recorded history; never null
+     * @param desired the desired position that we want to obtain, which should be after some recorded positions,
+     *            at some recorded positions, and before other recorded positions; never null
+     * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
+     */
+    public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
+        String recordedGtidSetStr = recorded.getString(BINLOG_GTID_KEY);
+        String desiredGtidSetStr = desired.getString(BINLOG_GTID_KEY);
+        if (desiredGtidSetStr != null) {
+            // The desired position uses GTIDs, so we ideally compare using GTIDs ...
+            if (recordedGtidSetStr != null) {
+                // Both have GTIDs, so base the comparison entirely on the GTID sets.
+                GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
+                GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
+                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                    // They are exactly the same, which means the recorded position exactly matches the desired ...
+                    if ( !recorded.has(BINLOG_SNAPSHOT_KEY) && desired.has(BINLOG_SNAPSHOT_KEY)) {
+                        // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
+                        return false;
+                    }
+                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired ...
+                    return true;
+                }
+                // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
+                return recordedGtidSet.isSubsetOf(desiredGtidSet);
+            }
+            // The desired position did use GTIDs while the recorded did not use GTIDs. So, we assume that the
+            // recorded position is older since GTIDs are often enabled but rarely disabled. And if they are disabled,
+            // it is likely that the desired position would not include GTIDs as we would be trying to read the binlog of a
+            // server that no longer has GTIDs. And if they are enabled, disabled, and re-enabled, per
+            // https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-failover.html all properly configured slaves that
+            // use GTIDs should always have the complete set of GTIDs copied from the master, in which case
+            // again we know that recorded not having GTIDs is before the desired position ...
+            return true;
+        } else if (recordedGtidSetStr != null) {
+            // The recorded has a GTID but the desired does not, so per the previous paragraph we assume that previous
+            // is not at or before ...
+            return false;
+        }
+
+        // Both positions are missing GTIDs. Look at the servers ...
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
+        if ( recordedServerId != desiredServerId ) {
+            // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
+            // is compare timestamps, and we have to assume that the server timestamps can be compared ...
+            long recordedTimestamp = recorded.getLong(BINLOG_EVENT_TIMESTAMP_KEY,0);
+            long desiredTimestamp = recorded.getLong(BINLOG_EVENT_TIMESTAMP_KEY,0);
+            return recordedTimestamp <= desiredTimestamp;
+        }
+        
+        // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
+        // comparable ...
+        String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
+        String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
+        assert recordedFilename != null;
+        int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
+        if ( diff > 0 ) return false;
+
+        // The filenames are the same, so compare the positions ...
+        int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
+        int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
+        diff = recordedPosition - desiredPosition;
+        if ( diff > 0 ) return false;
+        
+        // The positions are the same, so compare the row number ...
+        int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        diff = recordedRow - desiredRow;
+        if ( diff > 0 ) return false;
+
+        // The binlog coordinates are the same ...
+        return true;
+    }
 }",2016-06-04T21:20:26Z,11
"@@ -5,33 +5,153 @@
  */
 package io.debezium.connector.mysql;
 
-import io.confluent.connect.avro.AvroData;
+import static org.junit.Assert.assertTrue;
 
 import org.apache.avro.Schema;
-
-
+import org.fest.assertions.GenericAssert;
 import org.junit.Test;
 
-import static org.junit.Assert.assertTrue;
+import io.confluent.connect.avro.AvroData;
+import io.debezium.document.Document;
 
 public class SourceInfoTest {
-    private static final AvroData avroData;
-    private static int avroSchemaCacheSize = 1000;
 
-    static {
-        avroData = new AvroData(avroSchemaCacheSize);
-    }
+    private static int avroSchemaCacheSize = 1000;
+    private static final AvroData avroData = new AvroData(avroSchemaCacheSize);
 
     /**
      * When we want to consume SinkRecord which generated by debezium-connector-mysql, it should not
      * throw error ""org.apache.avro.SchemaParseException: Illegal character in: server-id""
      */
     @Test
-    public void testValidateSourceInfoSchema() {
+    public void shouldValidateSourceInfoSchema() {
         org.apache.kafka.connect.data.Schema kafkaSchema = SourceInfo.SCHEMA;
         Schema avroSchema = avroData.fromConnectSchema(kafkaSchema);
         assertTrue(avroSchema != null);
     }
 
-}
+    @Test
+    public void shouldConsiderPositionsWithSameGtidSetsAsSame() {
+        assertPositionWithGtids(""IdA:1-5"").isAtOrBefore(positionWithGtids(""IdA:1-5""));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"").isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"")); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"").isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"")); // equivalent
+    }
+
+    @Test
+    public void shouldConsiderPositionsWithSameGtidSetsAndSnapshotAsSame() {
+        assertPositionWithGtids(""IdA:1-5"",true).isAtOrBefore(positionWithGtids(""IdA:1-5"",true));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"",true)); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"",true)); // equivalent
+    }
 
+    @Test
+    public void shouldOrderPositionWithGtidAndSnapshotBeforePositionWithSameGtidButNoSnapshot() {
+        assertPositionWithGtids(""IdA:1-5"",true).isAtOrBefore(positionWithGtids(""IdA:1-5""));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"")); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"")); // equivalent
+    }
+
+    @Test
+    public void shouldOrderPositionWithoutGtidAndSnapshotAfterPositionWithSameGtidAndSnapshot() {
+        assertPositionWithGtids(""IdA:1-5"",false).isAfter(positionWithGtids(""IdA:1-5"",true));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",false).isAfter(positionWithGtids(""IdA:1-5,IdB:1-20"",true)); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",false).isAfter(positionWithGtids(""IdB:1-20,IdA:1-5"",true)); // equivalent
+    }
+
+    @Test
+    public void shouldOrderPositionWithGtidsAsBeforePositionWithExtraServerUuidInGtids() {
+        assertPositionWithGtids(""IdA:1-5"").isBefore(positionWithGtids(""IdA:1-5,IdB:1-20""));
+    }
+
+    @Test
+    public void shouldOrderPositionsWithSameServerButLowerUpperLimitAsBeforePositionWithSameServerUuidInGtids() {
+        assertPositionWithGtids(""IdA:1-5"").isBefore(positionWithGtids(""IdA:1-6""));
+        assertPositionWithGtids(""IdA:1-5:7-9"").isBefore(positionWithGtids(""IdA:1-10""));
+        assertPositionWithGtids(""IdA:2-5:8-9"").isBefore(positionWithGtids(""IdA:1-10""));
+    }
+
+    @Test
+    public void shouldOrderPositionWithoutGtidAsBeforePositionWithGtid() {
+        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0).isBefore(positionWithGtids(""IdA:1-5""));
+    }
+
+    @Test
+    public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
+        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
+    }
+
+    protected Document positionWithGtids(String gtids) {
+        return positionWithGtids(gtids, false);
+    }
+
+    protected Document positionWithGtids(String gtids, boolean snapshot) {
+        if (snapshot) {
+            return Document.create(SourceInfo.BINLOG_GTID_KEY, gtids, SourceInfo.BINLOG_SNAPSHOT_KEY, true);
+        }
+        return Document.create(SourceInfo.BINLOG_GTID_KEY, gtids);
+    }
+
+    protected Document positionWithoutGtids(String filename, int position, int row) {
+        return positionWithoutGtids(filename, position, row, false);
+    }
+
+    protected Document positionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+        if (snapshot) {
+            return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
+                                   SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
+                                   SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row,
+                                   SourceInfo.BINLOG_SNAPSHOT_KEY, true);
+        }
+        return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
+                               SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
+                               SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row);
+    }
+
+    protected PositionAssert assertThat(Document position) {
+        return new PositionAssert(position);
+    }
+
+    protected PositionAssert assertPositionWithGtids(String gtids) {
+        return assertThat(positionWithGtids(gtids));
+    }
+
+    protected PositionAssert assertPositionWithGtids(String gtids, boolean snapshot) {
+        return assertThat(positionWithGtids(gtids, snapshot));
+    }
+
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row) {
+        return assertPositionWithoutGtids(filename, position, row, false);
+    }
+
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+        return assertThat(positionWithoutGtids(filename, position, row, snapshot));
+    }
+
+    protected static class PositionAssert extends GenericAssert<PositionAssert, Document> {
+        public PositionAssert(Document position) {
+            super(PositionAssert.class, position);
+        }
+
+        public PositionAssert isAt(Document otherPosition) {
+            if (SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider same position as "" + otherPosition);
+        }
+
+        public PositionAssert isBefore(Document otherPosition) {
+            return isAtOrBefore(otherPosition);
+        }
+
+        public PositionAssert isAtOrBefore(Document otherPosition) {
+            if (SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider same position as or before "" + otherPosition);
+        }
+
+        public PositionAssert isAfter(Document otherPosition) {
+            if (!SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider after "" + otherPosition);
+        }
+    }
+}",2016-06-04T21:20:26Z,11
"@@ -14,6 +14,8 @@
 import java.util.function.Predicate;
 import java.util.function.Supplier;
 
+import javax.lang.model.SourceVersion;
+
 import io.debezium.annotation.Immutable;
 
 /**
@@ -395,6 +397,13 @@ public String toString() {
         return name();
     }
 
+    public static int isClassName(Configuration config, Field field, Consumer<String> problems) {
+        String value = config.getString(field);
+        if (value == null || SourceVersion.isName(value)) return 0;
+        problems.accept(""The '"" + field.name() + ""' field must contain a valid name of a Java class."");
+        return 1;
+    }
+
     public static int isRequired(Configuration config, Field field, Consumer<String> problems) {
         String value = config.getString(field);
         if (value != null && value.trim().length() > 0) return 0;",2016-06-04T21:20:26Z,5
"@@ -21,15 +21,17 @@
  */
 public abstract class AbstractDatabaseHistory implements DatabaseHistory {
 
-    protected Configuration config;
     protected final Logger logger = LoggerFactory.getLogger(getClass());
+    protected Configuration config;
+    private HistoryRecordComparator comparator = HistoryRecordComparator.INSTANCE;
 
     protected AbstractDatabaseHistory() {
     }
-
+    
     @Override
-    public void configure(Configuration config) {
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
         this.config = config;
+        this.comparator = comparator != null ? comparator : HistoryRecordComparator.INSTANCE;
     }
     
     @Override
@@ -46,7 +48,7 @@ public final void record(Map<String, ?> source, Map<String, ?> position, String
     public final void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser) {
         HistoryRecord stopPoint = new HistoryRecord(source, position, null, null);
         recoverRecords(schema,ddlParser,recovered->{
-            if (recovered.isAtOrBefore(stopPoint)) {
+            if (comparator.isAtOrBefore(recovered,stopPoint)) {
                 String ddl = recovered.ddl();
                 if (ddl != null) {
                     ddlParser.setCurrentSchema(recovered.databaseName()); // may be null",2016-06-04T21:20:26Z,3
"@@ -26,8 +26,11 @@ public interface DatabaseHistory {
      * Configure this instance.
      * 
      * @param config the configuration for this history store
+     * @param comparator the function that should be used to compare history records during
+     *            {@link #recover(Map, Map, Tables, DdlParser) recovery}; may be null if the
+     *            {@link HistoryRecordComparator#INSTANCE default comparator} is to be used
      */
-    void configure(Configuration config);
+    void configure(Configuration config, HistoryRecordComparator comparator);
 
     /**
      * Start the history.
@@ -62,7 +65,7 @@ public interface DatabaseHistory {
     void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser);
 
     /**
-     * Stop recording history and release any resources acquired since {@link #configure(Configuration)}.
+     * Stop recording history and release any resources acquired since {@link #configure(Configuration, HistoryRecordComparator)}.
      */
     void stop();
 }",2016-06-04T21:20:26Z,14
"@@ -49,15 +49,14 @@ public final class FileDatabaseHistory extends AbstractDatabaseHistory {
     private Path path;
 
     @Override
-    public void configure(Configuration config) {
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
         lock.write(() -> {
-            super.configure(config);
             if (!config.validate(ALL_FIELDS, logger::error)) {
                 throw new ConnectException(
                         ""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
             }
             config.validate(ALL_FIELDS, logger::error);
-            super.configure(config);
+            super.configure(config,comparator);
             path = Paths.get(config.getString(FILE_PATH));
         });
     }",2016-06-04T21:20:26Z,4
"@@ -0,0 +1,62 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.relational.history;
+
+import java.util.function.BiFunction;
+
+import io.debezium.document.Document;
+
+/**
+ * Compares HistoryRecord instances to determine which came first.
+ * 
+ * @author Randall Hauch
+ * @since 0.2
+ */
+public class HistoryRecordComparator {
+
+    /**
+     * A comparator instance that requires the {@link HistoryRecord#source() records' sources} to be the same and considers only
+     * those fields that are in both records' {@link HistoryRecord#position() positions}.
+     */
+    public static final HistoryRecordComparator INSTANCE = new HistoryRecordComparator();
+
+    /**
+     * Create a {@link HistoryRecordComparator} that requires identical sources but will use the supplied function to compare
+     * positions.
+     * 
+     * @param positionComparator the non-null function that returns {@code true} if the first position is at or before
+     *            the second position or {@code false} otherwise
+     * @return the comparator instance; never null
+     */
+    public static HistoryRecordComparator usingPositions(BiFunction<Document, Document, Boolean> positionComparator) {
+        return new HistoryRecordComparator() {
+            @Override
+            protected boolean isPositionAtOrBefore(Document position1, Document position2) {
+                return positionComparator.apply(position1, position2);
+            }
+        };
+    }
+
+    /**
+     * Determine if the first {@link HistoryRecord} is at the same or earlier point in time than the second {@link HistoryRecord}.
+     * 
+     * @param record1 the first record; never null
+     * @param record2 the second record; never null
+     * @return {@code true} if the first record is at the same or earlier point in time than the second record, or {@code false}
+     *         otherwise
+     */
+    public boolean isAtOrBefore(HistoryRecord record1, HistoryRecord record2) {
+        return isSameSource(record1.source(), record2.source()) && isPositionAtOrBefore(record1.position(), record2.position());
+    }
+
+    protected boolean isPositionAtOrBefore(Document position1, Document position2) {
+        return position1.compareToUsingSimilarFields(position2) <= 0;
+    }
+
+    protected boolean isSameSource(Document source1, Document source2) {
+        return source1.equals(source2);
+    }
+}
\ No newline at end of file",2016-06-04T21:20:26Z,136
"@@ -83,8 +83,8 @@ public class KafkaDatabaseHistory extends AbstractDatabaseHistory {
     private int pollIntervalMs = -1;
 
     @Override
-    public void configure(Configuration config) {
-        super.configure(config);
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
+        super.configure(config,comparator);
         if (!config.validate(ALL_FIELDS, logger::error)) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }",2016-06-04T21:20:26Z,30
"@@ -31,7 +31,7 @@ protected DatabaseHistory createHistory() {
         DatabaseHistory history = new FileDatabaseHistory();
         history.configure(Configuration.create()
                                        .with(FileDatabaseHistory.FILE_PATH, TEST_FILE_PATH.toAbsolutePath().toString())
-                                       .build());
+                                       .build(),null);
         return history;
     }
 }",2016-06-04T21:20:26Z,19
"@@ -76,7 +76,7 @@ public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exc
                               .with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS, kafka.brokerList())
                               .with(KafkaDatabaseHistory.TOPIC, topicName)
                               .build();
-        history.configure(config);
+        history.configure(config,null);
         history.start();
 
         DdlParser recoveryParser = new DdlParserSql2003();
@@ -123,7 +123,7 @@ public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exc
         // Stop the history (which should stop the producer) ...
         history.stop();
         history = new KafkaDatabaseHistory();
-        history.configure(config);
+        history.configure(config, null);
         // no need to start
 
         // Recover from the very beginning to just past the first change ...",2016-06-04T21:20:26Z,22
"@@ -18,7 +18,6 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import com.github.shyiko.mysql.binlog.BinaryLogClient;
-import com.github.shyiko.mysql.binlog.BinaryLogClient.AbstractLifecycleListener;
 import com.github.shyiko.mysql.binlog.BinaryLogClient.LifecycleListener;
 import com.github.shyiko.mysql.binlog.event.DeleteRowsEventData;
 import com.github.shyiko.mysql.binlog.event.Event;
@@ -68,17 +67,7 @@ public BinlogReader(MySqlTaskContext context) {
         client.setServerId(context.serverId());
         client.setKeepAlive(context.config().getBoolean(MySqlConnectorConfig.KEEP_ALIVE));
         client.registerEventListener(this::handleEvent);
-        client.registerLifecycleListener(new AbstractLifecycleListener(){
-            @Override
-            public void onCommunicationFailure(BinaryLogClient client, Exception ex) {
-                failed(ex,""Stopped reading binlog due to error: "" + ex.getMessage());
-            }
-            @Override
-            public void onEventDeserializationFailure(BinaryLogClient client, Exception ex) {
-                failed(ex,""Stopped reading binlog due to error: "" + ex.getMessage());
-            }
-        });
-        client.registerLifecycleListener(new TraceLifecycleListener());
+        client.registerLifecycleListener(new ReaderThreadLifecycleListener());
         if (logger.isDebugEnabled()) client.registerEventListener(this::logEvent);
 
         // Set up the event deserializer with additional type(s) ...
@@ -104,34 +93,31 @@ protected void doStart() {
         client.setBinlogFilename(source.binlogFilename());
         client.setBinlogPosition(source.binlogPosition());
         // The event row number will be used when processing the first event ...
-        logger.info(""Reading from MySQL {} starting at {}"",context.serverName(), source);
 
         // Start the log reader, which starts background threads ...
         long timeoutInMilliseconds = context.timeoutInMilliseconds();
         try {
-            logger.debug(""Binlog reader connecting to MySQL server '{}'"", context.serverName());
+            logger.debug(""Attempting to establish binlog reader connection with timeout of {} ms"", timeoutInMilliseconds);
             client.connect(context.timeoutInMilliseconds());
-            logger.info(""Successfully started reading MySQL binlog"");
         } catch (TimeoutException e) {
             double seconds = TimeUnit.MILLISECONDS.toSeconds(timeoutInMilliseconds);
-            throw new ConnectException(""Timed out after "" + seconds + "" seconds while waiting to connect to the MySQL database at "" +
-                    context.username() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
+            throw new ConnectException(""Timed out after "" + seconds + "" seconds while waiting to connect to MySQL at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
         } catch (AuthenticationException e) {
-            throw new ConnectException(""Failed to authenticate to the MySQL database at "" + context.hostname() + "":"" +
-                    context.port() + "" with user '"" + context.username() + ""'"", e);
+            throw new ConnectException(""Failed to authenticate to the MySQL database at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
         } catch (Throwable e) {
-            throw new ConnectException(""Unable to connect to the MySQL database at "" + context.hostname() + "":"" + context.port() +
-                    "" with user '"" + context.username() + ""': "" + e.getMessage(), e);
+            throw new ConnectException(""Unable to connect to the MySQL database at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""': "" + e.getMessage(), e);
         }
 
     }
 
     @Override
     protected void doStop() {
         try {
-            logger.debug(""Binlog reader disconnecting from MySQL server '{}'"", context.serverName());
+            logger.debug(""Stopping binlog reader"");
             client.disconnect();
-            logger.info(""Stopped connector to MySQL server '{}'"", context.serverName());
         } catch (IOException e) {
             logger.error(""Unexpected error when disconnecting from the MySQL binary log reader"", e);
         }
@@ -142,7 +128,7 @@ protected void doCleanup() {
     }
 
     protected void logEvent(Event event) {
-        //logger.debug(""Received event: {}"", event);
+        logger.trace(""Received event: {}"", event);
     }
 
     protected void ignoreEvent(Event event) {
@@ -344,25 +330,31 @@ protected void handleDelete(Event event) throws InterruptedException {
         }
     }
 
-    protected final class TraceLifecycleListener implements LifecycleListener {
+    protected final class ReaderThreadLifecycleListener implements LifecycleListener {
         @Override
         public void onDisconnect(BinaryLogClient client) {
-            logger.debug(""MySQL Connector disconnected"");
+            context.temporaryLoggingContext(""binlog"", () -> {
+                logger.info(""Stopped reading binlog and closed connection"");
+            });
         }
 
         @Override
         public void onConnect(BinaryLogClient client) {
-            logger.info(""MySQL Connector connected"");
+            // Set up the MDC logging context for this thread ...
+            context.configureLoggingContext(""binlog"");
+
+            // The event row number will be used when processing the first event ...
+            logger.info(""Connected to MySQL binlog at {}:{}, starting at {}"", context.hostname(), context.port(), source);
         }
 
         @Override
         public void onCommunicationFailure(BinaryLogClient client, Exception ex) {
-            logger.error(""MySQL Connector communication failure"", ex);
+            BinlogReader.this.failed(ex);
         }
 
         @Override
         public void onEventDeserializationFailure(BinaryLogClient client, Exception ex) {
-            logger.error(""MySQL Connector received event deserialization failure"", ex);
+            BinlogReader.this.failed(ex);
         }
     }
 }",2016-06-02T19:05:06Z,24
"@@ -60,7 +60,7 @@ public void start(Map<String, String> props) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }
 
-        // Create the task and set our running flag ...
+        // Create and start the task context ...
         this.taskContext = new MySqlTaskContext(config);
         this.taskContext.start();
 
@@ -133,13 +133,13 @@ public void start(Map<String, String> props) {
 
     @Override
     public List<SourceRecord> poll() throws InterruptedException {
-        logger.trace(""Polling for events from MySQL connector"");
+        logger.trace(""Polling for events"");
         return currentReader.poll();
     }
 
     @Override
     public void stop() {
-        logger.info(""Stopping MySQL Connector"");
+        logger.info(""Stopping MySQL connector task"");
         // We need to explicitly stop both readers, in this order. If we were to instead call 'currentReader.stop()', there
         // is a chance without synchronization that we'd miss the transition and stop only the snapshot reader. And stopping both
         // is far simpler and more efficient than synchronizing ...
@@ -155,7 +155,7 @@ public void stop() {
                 } catch (Throwable e) {
                     logger.error(""Unexpected error shutting down the database history and/or closing JDBC connections"", e);
                 } finally {
-                    logger.info(""Stopped connector to MySQL server '{}'"", taskContext.serverName());
+                    logger.info(""Connector task successfully stopped"");
                 }
             }
         }",2016-06-02T19:05:06Z,10
"@@ -15,6 +15,8 @@
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
 import io.debezium.util.Clock;
+import io.debezium.util.LoggingContext;
+import io.debezium.util.LoggingContext.PreviousContext;
 
 /**
  * A Kafka Connect source task reads the MySQL binary log and generate the corresponding data change events.
@@ -96,7 +98,7 @@ public void loadHistory(SourceInfo startingPoint) {
         dbSchema.loadHistory(startingPoint);
         recordProcessor.regenerate();
     }
-    
+
     public Clock clock() {
         return clock;
     }
@@ -157,21 +159,23 @@ protected SnapshotMode snapshotMode() {
         String value = config.getString(MySqlConnectorConfig.SNAPSHOT_MODE);
         return SnapshotMode.parse(value, MySqlConnectorConfig.SNAPSHOT_MODE.defaultValue());
     }
-    
+
     public boolean useMinimalSnapshotLocking() {
         return config.getBoolean(MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
     }
 
     public void start() {
-        // Start the MySQL database history, which simply starts up resources but does not recover the history to a specific
-        // point.
+        // First, configure the logging context for the thread that created this context object ...
+        this.configureLoggingContext(""task"");
+
+        // Start the MySQL database history, which simply starts up resources but does not recover the history to a specific point
         dbSchema().start();
     }
 
     public void shutdown() {
         try {
             // Flush and stop the database history ...
-            logger.debug(""Stopping database history for MySQL server '{}'"", serverName());
+            logger.debug(""Stopping database history"");
             dbSchema.shutdown();
         } catch (Throwable e) {
             logger.error(""Unexpected error shutting down the database history"", e);
@@ -184,4 +188,26 @@ public void shutdown() {
         }
     }
 
+    /**
+     * Configure the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * @param contextName the name of the context; may not be null
+     * @return the previous MDC context; never null
+     * @throws IllegalArgumentException if {@code contextName} is null
+     */
+    public PreviousContext configureLoggingContext(String contextName) {
+        return LoggingContext.forConnector(""MySQL"", serverName(), contextName);
+    }
+    
+    /**
+     * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
+     * state before this method was called.
+     * 
+     * @param contextName the name of the context; may not be null
+     * @param operation the function to run in the new MDC context; may not be null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public void temporaryLoggingContext(String contextName, Runnable operation) {
+        LoggingContext.temporarilyForConnector(""MySQL"", serverName(), contextName, operation);
+    }
+
 }",2016-06-02T19:05:06Z,87
"@@ -19,6 +19,8 @@
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
+import io.debezium.util.Clock;
+import io.debezium.util.Strings;
 
 /**
  * A component that performs a snapshot of a MySQL server, and records the schema changes in {@link MySqlSchema}.
@@ -127,13 +129,15 @@ protected void doCleanup() {
      * Perform the snapshot using the same logic as the ""mysqldump"" utility.
      */
     protected void execute() {
-        logger.info(""Starting snapshot for MySQL server {}"", context.serverName());
+        context.configureLoggingContext(""snapshot"");
+        logger.info(""Starting snapshot"");
         final AtomicReference<String> sql = new AtomicReference<>();
         final JdbcConnection mysql = context.jdbc();
         final MySqlSchema schema = context.dbSchema();
         final Filters filters = schema.filters();
         final SourceInfo source = context.source();
-        final long ts = context.clock().currentTimeInMillis();
+        final Clock clock = context.clock();
+        final long ts = clock.currentTimeInMillis();
         try {
             // ------
             // STEP 0
@@ -148,6 +152,7 @@ protected void execute() {
             // See: https://dev.mysql.com/doc/refman/5.7/en/set-transaction.html
             // See: https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html
             // See: https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html
+            logger.info(""Step 0: disabling autocommit and enabling repeatable read transactions"");
             mysql.setAutoCommit(false);
             sql.set(""SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"");
             mysql.execute(sql.get());
@@ -157,6 +162,7 @@ protected void execute() {
             // ------
             // First, start a transaction and request that a consistent MVCC snapshot is obtained immediately.
             // See http://dev.mysql.com/doc/refman/5.7/en/commit.html
+            logger.info(""Step 1: start transaction with consistent snapshot"");
             sql.set(""START TRANSACTION WITH CONSISTENT SNAPSHOT"");
             mysql.execute(sql.get());
 
@@ -166,6 +172,8 @@ protected void execute() {
             // Obtain read lock on all tables. This statement closes all open tables and locks all tables
             // for all databases with a global read lock, and it prevents ALL updates while we have this lock.
             // It also ensures that everything we do while we have this lock will be consistent.
+            long lockAcquired = clock.currentTimeInMillis();
+            logger.info(""Step 2: flush and obtain global read lock (preventing writes to database)"");
             sql.set(""FLUSH TABLES WITH READ LOCK"");
             mysql.execute(sql.get());
 
@@ -174,6 +182,7 @@ protected void execute() {
             // ------
             // Obtain the binlog position and update the SourceInfo in the context. This means that all source records generated
             // as part of the snapshot will contain the binlog position of the snapshot.
+            logger.info(""Step 3: read binlog position of MySQL master"");
             sql.set(""SHOW MASTER STATUS"");
             mysql.query(sql.get(), rs -> {
                 if (rs.next()) {
@@ -191,6 +200,7 @@ protected void execute() {
             // STEP 4
             // ------
             // Get the list of databases ...
+            logger.info(""Step 4: read list of available databases"");
             final List<String> databaseNames = new ArrayList<>();
             sql.set(""SHOW DATABASES"");
             mysql.query(sql.get(), rs -> {
@@ -205,6 +215,7 @@ protected void execute() {
             // Get the list of table IDs for each database. We can't use a prepared statement with MySQL, so we have to
             // build the SQL statement each time. Although in other cases this might lead to SQL injection, in our case
             // we are reading the database names from the database and not taking them from the user ...
+            logger.info(""Step 5: read list of available tables in each database"");
             final List<TableId> tableIds = new ArrayList<>();
             final Map<String,List<TableId>> tableIdsByDbName = new HashMap<>();
             for (String dbName : databaseNames) {
@@ -225,6 +236,7 @@ protected void execute() {
             // ------
             // Transform the current schema so that it reflects the *current* state of the MySQL server's contents.
             // First, get the DROP TABLE and CREATE TABLE statement (with keys and constraint definitions) for our tables ...
+            logger.info(""Step 6: generating DROP and CREATE statements to reflect current database schemas"");
             final List<String> ddlStatements = new ArrayList<>();
             // Add DROP TABLE statements for all tables that we knew about AND those tables found in the databases ...
             Set<TableId> allTableIds = new HashSet<>(schema.tables().tableIds());
@@ -253,6 +265,7 @@ protected void execute() {
                 }
             }
             // Finally, apply the DDL statements to the schema and then update the record maker...
+            logger.debug(""Step 6b: applying DROP and CREATE statements to connector's table model"");
             String ddlStatementsStr = String.join("";"" + System.lineSeparator(), ddlStatements);
             schema.applyDdl(source, null, ddlStatementsStr, this::enqueueSchemaChanges);
             context.makeRecord().regenerate();
@@ -266,17 +279,25 @@ protected void execute() {
                 // should still use the MVCC snapshot obtained when we started our transaction (since we started it
                 // ""...with consistent snapshot""). So, since we're only doing very simple SELECT without WHERE predicates,
                 // we can release the lock now ...
+                logger.info(""Step 7: releasing global read lock to enable MySQL writes"");
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
+                long lockReleased = clock.currentTimeInMillis();
+                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased-lockAcquired));
             }
 
             // ------
             // STEP 8
             // ------
             // Dump all of the tables and generate source records ...
+            logger.info(""Step 8: scanning contents of {} tables"",tableIds.size());
+            long startScan = clock.currentTimeInMillis();
             AtomicBoolean interrupted = new AtomicBoolean(false);
+            int counter = 0;
             for (TableId tableId : tableIds) {
+                long start = clock.currentTimeInMillis();
+                logger.debug(""Step 8.{}: scanning table '{}'; {} tables remain"",++counter,tableId,tableIds.size()-counter);
                 sql.set(""SELECT * FROM "" + tableId);
                 mysql.query(sql.get(), rs -> {
                     RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, super::enqueueRecord);
@@ -300,32 +321,42 @@ protected void execute() {
                     }
                 });
                 if ( interrupted.get() ) break;
+                long stop = clock.currentTimeInMillis();
+                logger.info(""Step 8.{}: scanned table '{}' in {}"",counter,tableId,Strings.duration(stop-start));
             }
+            long stop = clock.currentTimeInMillis();
+            logger.info(""Step 8: scanned contents of {} tables in {}"",tableIds.size(),Strings.duration(stop-startScan));
 
             // ------
             // STEP 9
             // ------
             // Release the read lock if we have not yet done so ...
             if (!unlocked) {
+                logger.info(""Step 9: releasing global read lock to enable MySQL writes"");
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
+                long lockReleased = clock.currentTimeInMillis();
+                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased-lockAcquired));
             }
 
             // -------
             // STEP 10
             // -------
             if (interrupted.get()) {
                 // We were interrupted while reading the tables, so roll back the transaction and return immediately ...
+                logger.info(""Step 10: rolling back transaction after request to stop"");
                 sql.set(""ROLLBACK"");
                 mysql.execute(sql.get());
                 return;
             }
             // Otherwise, commit our transaction
+            logger.info(""Step 10: committing transaction"");
             sql.set(""COMMIT"");
             mysql.execute(sql.get());
 
             try {
+                logger.info(""Step 11: recording completion of snapshot"");
                 // Mark the source as having completed the snapshot. Because of this, **subsequent** source records
                 // produced by the connector (to any topic) will have a normal (not snapshot) offset ...
                 source.completeSnapshot();
@@ -338,7 +369,8 @@ protected void execute() {
             } finally {
                 // Set the completion flag ...
                 super.completeSuccessfully();
-                logger.info(""Completed snapshot for MySQL server {}"", context.serverName());
+                stop = clock.currentTimeInMillis();
+                logger.info(""Completed snapshot in {}"", Strings.duration(stop-ts));
             }
         } catch (Throwable e) {
             failed(e, ""Aborting snapshot after running '"" + sql.get() + ""': "" + e.getMessage());",2016-06-02T19:05:06Z,15
"@@ -2,7 +2,7 @@
 log4j.appender.stdout=org.apache.log4j.ConsoleAppender
 log4j.appender.stdout.Target=System.out
 log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
-log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p     %m (%c)%n
+log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n
 
 # Root logger option
 log4j.rootLogger=INFO, stdout",2016-06-02T19:05:06Z,89
"@@ -0,0 +1,100 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.util;
+
+import java.util.Map;
+
+import org.slf4j.MDC;
+
+/**
+ * A utility that provides a consistent set of properties for the Mapped Diagnostic Context (MDC) properties used by Debezium
+ * components.
+ * 
+ * @author Randall Hauch
+ * @since 0.2
+ */
+public class LoggingContext {
+
+    /**
+     * The key for the connector type MDC property.
+     */
+    public static final String CONNECTOR_TYPE = ""dbz.connectorType"";
+    /**
+     * The key for the connector logical name MDC property.
+     */
+    public static final String CONNECTOR_NAME = ""dbz.connectorName"";
+    /**
+     * The key for the connector context name MDC property.
+     */
+    public static final String CONNECTOR_CONTEXT = ""dbz.connectorContext"";
+
+    private LoggingContext() {
+    }
+    
+    /**
+     * A snapshot of an MDC context that can be {@link #restore()}.
+     */
+    public static final class PreviousContext {
+        private final Map<String,String> context;
+        @SuppressWarnings(""unchecked"")
+        protected PreviousContext() {
+            context = MDC.getCopyOfContextMap();
+        }
+        /**
+         * Restore this logging context.
+         */
+        public void restore() {
+            for ( Map.Entry<String, String> entry : context.entrySet() ) {
+                MDC.put(entry.getKey(), entry.getValue());
+            }
+        }
+    }
+
+    /**
+     * Configure for a connector the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * 
+     * @param connectorType the type of connector; may not be null
+     * @param connectorName the name of the connector; may not be null
+     * @param contextName the name of the context; may not be null
+     * @return the previous MDC context; never null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public static PreviousContext forConnector(String connectorType, String connectorName, String contextName) {
+        if (connectorType == null) throw new IllegalArgumentException(""The MDC value for the connector type may not be null"");
+        if (connectorName == null) throw new IllegalArgumentException(""The MDC value for the connector name may not be null"");
+        if (contextName == null) throw new IllegalArgumentException(""The MDC value for the connector context may not be null"");
+        PreviousContext previous = new PreviousContext();
+        MDC.put(CONNECTOR_TYPE, connectorType);
+        MDC.put(CONNECTOR_NAME, connectorName);
+        MDC.put(CONNECTOR_CONTEXT, contextName);
+        return previous;
+    }
+    
+    /**
+     * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
+     * state before this method was called.
+     * 
+     * @param connectorType the type of connector; may not be null
+     * @param connectorName the logical name of the connector; may not be null
+     * @param contextName the name of the context; may not be null
+     * @param operation the function to run in the new MDC context; may not be null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public static void temporarilyForConnector(String connectorType, String connectorName, String contextName, Runnable operation) {
+        if (connectorType == null) throw new IllegalArgumentException(""The MDC value for the connector type may not be null"");
+        if (connectorName == null) throw new IllegalArgumentException(""The MDC value for the connector name may not be null"");
+        if (contextName == null) throw new IllegalArgumentException(""The MDC value for the connector context may not be null"");
+        if (operation == null) throw new IllegalArgumentException(""The operation may not be null"");
+        PreviousContext previous = new PreviousContext();
+        try {
+            forConnector(connectorType,connectorName,contextName);
+            operation.run();
+        } finally {
+            previous.restore();
+        }
+    }
+    
+}",2016-06-02T19:05:06Z,137
"@@ -7,6 +7,8 @@
 
 import java.io.ByteArrayOutputStream;
 import java.io.PrintWriter;
+import java.math.BigDecimal;
+import java.text.DecimalFormat;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashSet;
@@ -36,11 +38,11 @@ public final class Strings {
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, Function<String, String[]> splitter, Function<String, T> factory) {
-        if ( input == null ) return Collections.emptySet();
+        if (input == null) return Collections.emptySet();
         Set<T> matches = new HashSet<>();
         for (String item : splitter.apply(input)) {
             T obj = factory.apply(item);
-            if ( obj != null ) matches.add(obj);
+            if (obj != null) matches.add(obj);
         }
         return matches;
     }
@@ -54,7 +56,7 @@ public static <T> Set<T> listOf(String input, Function<String, String[]> splitte
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, char delimiter, Function<String, T> factory) {
-        return listOf(input,(str) -> str.split(""["" + delimiter + ""]""),factory);
+        return listOf(input, (str) -> str.split(""["" + delimiter + ""]""), factory);
     }
 
     /**
@@ -65,7 +67,7 @@ public static <T> Set<T> listOf(String input, char delimiter, Function<String, T
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, Function<String, T> factory) {
-        return listOf(input,',',factory);
+        return listOf(input, ',', factory);
     }
 
     /**
@@ -76,7 +78,7 @@ public static <T> Set<T> listOf(String input, Function<String, T> factory) {
      * @return the list of regular expression {@link Pattern}s included in the list; never null
      */
     public static Set<Pattern> listOfRegex(String input) {
-        return listOf(input,',',Pattern::compile);
+        return listOf(input, ',', Pattern::compile);
     }
 
     /**
@@ -88,7 +90,7 @@ public static Set<Pattern> listOfRegex(String input) {
      * @return the list of regular expression {@link Pattern}s included in the list; never null
      */
     public static Set<Pattern> listOfRegex(String input, int regexFlags) {
-        return listOf(input,',',(str)->Pattern.compile(str,regexFlags));
+        return listOf(input, ',', (str) -> Pattern.compile(str, regexFlags));
     }
 
     /**
@@ -111,7 +113,7 @@ public static interface CharacterPredicate {
      * @param content the string content that is to be split
      * @return the list of lines; never null but may be an empty (unmodifiable) list if the supplied content is null or empty
      */
-    public static List<String> splitLines( final String content ) {
+    public static List<String> splitLines(final String content) {
         if (content == null || content.length() == 0) return Collections.emptyList();
         String[] lines = content.split(""[\\r]?\\n"");
         return Arrays.asList(lines);
@@ -410,8 +412,7 @@ public static int asInt(String value, int defaultValue) {
         if (value != null) {
             try {
                 return Integer.parseInt(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -427,8 +428,7 @@ public static long asLong(String value, long defaultValue) {
         if (value != null) {
             try {
                 return Long.parseLong(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -444,8 +444,7 @@ public static double asDouble(String value, double defaultValue) {
         if (value != null) {
             try {
                 return Double.parseDouble(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -461,12 +460,46 @@ public static boolean asBoolean(String value, boolean defaultValue) {
         if (value != null) {
             try {
                 return Boolean.parseBoolean(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
 
+    /**
+     * For the given duration in milliseconds, obtain a readable representation of the form {@code HHH:MM:SS.mmm}, where
+     * <dl>
+     * <dt>HHH</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""03"")</dd>
+     * <dt>MM</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""05"")</dd>
+     * <dt>SS</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""09"")</dd>
+     * <dt>mmm</dt>
+     * <dd>is the fractional part of seconds, written with 1-3 digits (any trailing zeros are dropped)</dd>
+     * </dl>
+     * 
+     * @param durationInMillis the duration in milliseconds
+     * @return the readable duration.
+     */
+    public static String duration(long durationInMillis) {
+        // Calculate how many seconds, and don't lose any information ...
+        BigDecimal bigSeconds = new BigDecimal(Math.abs(durationInMillis)).divide(new BigDecimal(1000));
+        // Calculate the minutes, and round to lose the seconds
+        int minutes = bigSeconds.intValue() / 60;
+        // Remove the minutes from the seconds, to just have the remainder of seconds
+        double dMinutes = minutes;
+        double seconds = bigSeconds.doubleValue() - dMinutes * 60;
+        // Now compute the number of full hours, and change 'minutes' to hold the remaining minutes
+        int hours = minutes / 60;
+        minutes = minutes - (hours * 60);
+
+        // Format the string, and have at least 2 digits for the hours, minutes and whole seconds,
+        // and between 3 and 6 digits for the fractional part of the seconds...
+        String result = new DecimalFormat(""######00"").format(hours) + ':' + new DecimalFormat(""00"").format(minutes) + ':'
+                + new DecimalFormat(""00.0##"").format(seconds);
+        return result;
+    }
+
     private Strings() {
     }
 }",2016-06-02T19:05:06Z,115
"@@ -146,6 +146,53 @@
                       </assembly>
                     </build>
                   </image>
+                  <image>
+                    <!-- A Docker image using a partial MySQL installation maintained by MySQL team. -->
+                    <name>debezium/mysql-server-gtids-test-databases</name>
+                    <alias>database</alias>
+                    <run>
+                      <namingStrategy>alias</namingStrategy>
+                      <env>
+                        <MYSQL_ROOT_PASSWORD>debezium-rocks</MYSQL_ROOT_PASSWORD>
+                        <MYSQL_DATABASE>mysql</MYSQL_DATABASE> <!-- database created upon init -->
+                        <MYSQL_USER>${database.user}</MYSQL_USER>
+                        <MYSQL_PASSWORD>${database.password}</MYSQL_PASSWORD>
+                      </env>
+                      <ports>
+                        <port>${database.port}:3306</port>
+                      </ports>
+                      <log>
+                        <prefix>mysql</prefix>
+                        <enabled>true</enabled>
+                        <color>yellow</color>
+                      </log>
+                      <wait>
+                        <log>MySQL init process done. Ready for start up.</log>
+                        <time>30000</time> <!-- 30 seconds max -->
+                      </wait>
+                    </run>
+                    <build>
+                      <from>mysql/mysql-server:${version.mysql.server}</from>
+                      <assembly>
+                        <inline>
+                          <fileSets>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/server-gtids</directory>
+                              <includes>
+                                <include>my.cnf</include>
+                              </includes>
+                              <outputDirectory>etc/mysql</outputDirectory>
+                            </fileSet>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/init</directory>
+                              <outputDirectory>docker-entrypoint-initdb.d</outputDirectory>
+                            </fileSet>
+                          </fileSets>
+                        </inline>
+                        <basedir>/</basedir>
+                      </assembly>
+                    </build>
+                  </image>
                   <image>
                     <!-- A Docker image using a complete MySQL installation maintained by Docker team. -->
                     <name>debezium/mysql-test--databases</name>
@@ -350,5 +397,22 @@
           <docker.skip>false</docker.skip>
         </properties>
       </profile>
+      <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+            Use the Docker image for MySQL configured to use GTIDs.
+            To use, specify ""-Dgtid-mysql"" or -Pgtid-mysql on the Maven command line.
+            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+      <profile>
+        <id>gtid-mysql</id>
+        <activation>
+          <activeByDefault>false</activeByDefault>
+          <property>
+            <name>gtid-mysql</name>
+          </property>
+        </activation>
+        <properties>
+          <docker.image>debezium/mysql-server-gtids-test-databases</docker.image>
+          <docker.skip>false</docker.skip>
+        </properties>
+      </profile>
     </profiles>
 </project>",2016-06-07T17:01:51Z,97
"@@ -88,6 +88,7 @@ protected void doStart() {
         eventHandlers.put(EventType.ROTATE, this::handleRotateLogsEvent);
         eventHandlers.put(EventType.TABLE_MAP, this::handleUpdateTableMetadata);
         eventHandlers.put(EventType.QUERY, this::handleQueryEvent);
+        eventHandlers.put(EventType.GTID, this::handleGtidEvent);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, this::handleInsert);
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
@@ -142,9 +143,10 @@ protected void ignoreEvent(Event event) {
     protected void handleEvent(Event event) {
         if (event == null) return;
 
-        // Update the source offset info ...
+        // Update the source offset info. Note that the client returns the value in *milliseconds*, even though the binlog
+        // contains only *seconds* precision ...
         EventHeader eventHeader = event.getHeader();
-        source.setBinlogTimestamp(eventHeader.getTimestamp());
+        source.setBinlogTimestampSeconds(eventHeader.getTimestamp()/1000L); // client returns milliseconds, we record seconds
         source.setBinlogServerId(eventHeader.getServerId());
         EventType eventType = eventHeader.getEventType();
         if (eventType == EventType.ROTATE) {
@@ -166,16 +168,6 @@ protected void handleEvent(Event event) {
                 source.setRowInEvent(0);
             }
         }
-        if (eventType == EventType.GTID) {
-            EventData eventData = event.getData();
-            GtidEventData gtidEventData;
-            if (eventData instanceof EventDeserializer.EventDataWrapper) {
-                gtidEventData = (GtidEventData) ((EventDeserializer.EventDataWrapper) eventData).getInternal();
-            } else {
-                gtidEventData = (GtidEventData) eventData;
-            }
-            source.setGtids(gtidEventData.getGtid());
-        }
 
         // If there is a handler for this event, forward the event to it ...
         try {
@@ -188,6 +180,15 @@ protected void handleEvent(Event event) {
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    protected <T extends EventData> T unwrapData(Event event) {
+        EventData eventData = event.getData();
+        if (eventData instanceof EventDeserializer.EventDataWrapper) {
+            eventData = ((EventDeserializer.EventDataWrapper) eventData).getInternal();
+        }
+        return (T)eventData;
+    }
+
     /**
      * Handle the supplied event that signals that mysqld has stopped.
      * 
@@ -226,19 +227,31 @@ protected void handleServerIncident(Event event) {
      */
     protected void handleRotateLogsEvent(Event event) {
         logger.debug(""Rotating logs: {}"", event);
-        RotateEventData command = event.getData();
+        RotateEventData command = unwrapData(event);
         assert command != null;
         recordMakers.clear();
     }
 
+    /**
+     * Handle the supplied event with a {@link GtidEventData} that signals the beginning of a GTID transaction.
+     * 
+     * @param event the GTID event to be processed; may not be null
+     */
+    protected void handleGtidEvent(Event event) {
+        logger.debug(""GTID transaction: {}"", event);
+        GtidEventData gtidEvent = unwrapData(event);
+        source.setGtid(gtidEvent.getGtid());
+        source.setGtidSet(client.getGtidSet());
+    }
+
     /**
      * Handle the supplied event with an {@link QueryEventData} by possibly recording the DDL statements as changes in the
      * MySQL schemas.
      * 
      * @param event the database change data event to be processed; may not be null
      */
     protected void handleQueryEvent(Event event) {
-        QueryEventData command = event.getData();
+        QueryEventData command = unwrapData(event);
         logger.debug(""Received update table command: {}"", event);
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords && recordMakers.schemaChanges(dbName, statements, super::enqueueRecord) > 0) {
@@ -262,7 +275,7 @@ protected void handleQueryEvent(Event event) {
      * @param event the update event; never null
      */
     protected void handleUpdateTableMetadata(Event event) {
-        TableMapEventData metadata = event.getData();
+        TableMapEventData metadata = unwrapData(event);
         long tableNumber = metadata.getTableId();
         String databaseName = metadata.getDatabase();
         String tableName = metadata.getTable();
@@ -281,7 +294,7 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
-        WriteRowsEventData write = event.getData();
+        WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);
@@ -302,7 +315,7 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
-        UpdateRowsEventData update = event.getData();
+        UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
         // BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
@@ -330,7 +343,7 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
-        DeleteRowsEventData deleted = event.getData();
+        DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);",2016-06-07T17:01:51Z,24
"@@ -7,6 +7,7 @@
 
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.TreeMap;
@@ -111,11 +112,17 @@ protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
                 intervals.add(new Interval(interval.getStart(), interval.getEnd()));
             });
             Collections.sort(this.intervals);
-        }
-
-        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
-            this.uuid = uuid;
-            this.intervals = intervals;
+            if ( this.intervals.size() > 1 ) {
+                // Collapse adjacent intervals ...
+                for ( int i=intervals.size()-1; i!=0; --i) {
+                    Interval before = this.intervals.get(i-1);
+                    Interval after = this.intervals.get(i);
+                    if ( (before.getEnd() + 1) == after.getStart() ) {
+                        this.intervals.set(i-1,new Interval(before.getStart(),after.getEnd()));
+                        this.intervals.remove(i);
+                    }
+                }
+            }
         }
 
         /**
@@ -218,17 +225,41 @@ public String toString() {
             StringBuilder sb = new StringBuilder();
             if (sb.length() != 0) sb.append(',');
             sb.append(uuid).append(':');
-            sb.append(intervals.getFirst().getStart());
-            sb.append(intervals.getLast().getEnd());
+            Iterator<Interval> iter = intervals.iterator();
+            if ( iter.hasNext() ) sb.append(iter.next());
+            while ( iter.hasNext() ) {
+                sb.append(':');
+                sb.append(iter.next());
+            }
             return sb.toString();
         }
     }
 
     @Immutable
-    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+    public static class Interval implements Comparable<Interval> {
+
+        private final long start;
+        private final long end;
 
         public Interval(long start, long end) {
-            super(start, end);
+            this.start = start;
+            this.end = end;
+        }
+
+        /**
+         * Get the starting transaction number in this interval.
+         * @return this interval's first transaction number
+         */
+        public long getStart() {
+            return start;
+        }
+
+        /**
+         * Get the ending transaction number in this interval.
+         * @return this interval's last transaction number
+         */
+        public long getEnd() {
+            return end;
         }
 
         /**
@@ -244,6 +275,15 @@ public boolean isSubsetOf(Interval other) {
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
         }
+        
+        @Override
+        public int compareTo(Interval that) {
+            if ( that == this ) return 0;
+            long diff = this.start - that.start;
+            if ( diff > Integer.MAX_VALUE ) return Integer.MAX_VALUE;
+            if ( diff < Integer.MIN_VALUE ) return Integer.MIN_VALUE;
+            return (int) diff;
+        }
 
         @Override
         public int hashCode() {",2016-06-07T17:01:51Z,101
"@@ -9,6 +9,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -110,9 +111,16 @@ public void start(Map<String, String> props) {
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;
+                // The snapshot will determine if GTIDs are set
             }
         }
 
+        if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
+            // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
+            // previously used but the MySQL server has them enabled ...
+            source.setGtidSet("""");
+        }
+
         // Set up the readers ...
         this.binlogReader = new BinlogReader(taskContext);
         if (startWithSnapshot) {
@@ -174,13 +182,33 @@ protected void transitionToReadBinlog() {
      * @return {@code true} if the server has the binlog coordinates, or {@code false} otherwise
      */
     protected boolean isBinlogAvailable() {
+        String gtidStr = taskContext.source().gtidSet();
+        if ( gtidStr != null) {
+            if ( gtidStr.trim().isEmpty() ) return true; // start at beginning ...
+            String availableGtidStr = knownGtidSet();
+            if ( availableGtidStr == null || availableGtidStr.trim().isEmpty() ) {
+                // Last offsets had GTIDs but the server does not use them ...
+                logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
+                return false;
+            }
+            // GTIDs are enabled, and we used them previously ...
+            GtidSet gtidSet = new GtidSet(gtidStr);
+            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+                return true;
+            }
+            logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);
+            return false;
+        }
+        
         String binlogFilename = taskContext.source().binlogFilename();
         if (binlogFilename == null) return true; // start at current position
         if (binlogFilename.equals("""")) return true; // start at beginning
 
         // Accumulate the available binlog filenames ...
         List<String> logNames = new ArrayList<>();
         try {
+            logger.info(""Stop 0: Get all known binlogs from MySQL"");
             taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
                 while (rs.next()) {
                     logNames.add(rs.getString(1));
@@ -191,6 +219,50 @@ protected boolean isBinlogAvailable() {
         }
 
         // And compare with the one we're supposed to use ...
-        return logNames.stream().anyMatch(binlogFilename::equals);
+        boolean found = logNames.stream().anyMatch(binlogFilename::equals);
+        if ( !found ) {
+            logger.info(""Connector requires binlog file '{}', but MySQL only has {}"",binlogFilename,String.join("", "",logNames));
+        }
+        return found;
+    }
+
+    /**
+     * Determine whether the MySQL server has GTIDs enabled.
+     * 
+     * @return {@code false} if the server's {@code gtid_mode} is set and is {@code OFF}, or {@code true} otherwise
+     */
+    protected boolean isGtidModeEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>(""off"");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'GTID_MODE'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return !""OFF"".equalsIgnoreCase(mode.get());
+    }
+
+    /**
+     * Determine the available GTID set for MySQL.
+     * 
+     * @return the string representation of MySQL's GTID sets.
+     */
+    protected String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return gtidSetStr.get();
     }
 }",2016-06-07T17:01:51Z,10
"@@ -188,7 +188,7 @@ protected void execute() {
                 if (rs.next()) {
                     source.setBinlogFilename(rs.getString(1));
                     source.setBinlogPosition(rs.getLong(2));
-                    source.setGtids(rs.getString(5));// GTIDs
+                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
                     source.startSnapshot();
                 }
             });",2016-06-07T17:01:51Z,15
"@@ -14,6 +14,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.data.Envelope;
 import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
@@ -26,7 +27,7 @@
  * 
  * <pre>
  * {
- *     ""db"" : ""myDatabase""
+ *     ""server"" : ""production-server""
  * }
  * </pre>
  * 
@@ -36,14 +37,40 @@
  * 
  * <pre>
  * {
- *         ""gtids"" = ""3E11FA47-71CA-11E1-9E33-C80AA9429562:1-5"",
- *         ""file"" = ""mysql-bin.000003"",
- *         ""pos"" = 105586,
- *         ""row"" = 0
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
+ *     ""file"" = ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"" = 0,
+ *     ""snapshot"": true
  * }
  * </pre>
  * 
- * Note that the ""{@code gtid}"" field is present only when GTIDs are enabled.
+ * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
+ * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
+ * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
+ * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
+ * 
+ * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
+ * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
+ * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
+ * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
+ * an event that corresponds to the above partition and offset:
+ * 
+ * <pre>
+ * {
+ *     ""name"": ""production-server"",
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"": 0,
+ *     ""snapshot"": true
+ * }
+ * </pre>
  * 
  * @author Randall Hauch
  */
@@ -56,11 +83,12 @@ final class SourceInfo {
 
     public static final String SERVER_NAME_KEY = ""name"";
     public static final String SERVER_PARTITION_KEY = ""server"";
-    public static final String GTID_KEY = ""gtids"";
+    public static final String GTID_SET_KEY = ""gtids"";
+    public static final String GTID_KEY = ""gtid"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
     public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
-    public static final String TIMESTAMP_KEY = ""ts"";
+    public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
     /**
@@ -78,13 +106,14 @@ final class SourceInfo {
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private GtidSet binlogGtids;
+    private String gtidSet;
+    private String binlogGtid;
     private String binlogFilename;
     private long binlogPosition = 4;
     private int eventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
-    private long binlogTs = 0;
+    private long binlogTimestampSeconds = 0;
     private Map<String, String> sourcePartition;
     private boolean snapshot = false;
 
@@ -122,11 +151,11 @@ public Map<String, String> partition() {
      */
     public Map<String, ?> offset() {
         Map<String, Object> map = new HashMap<>();
-        if (binlogGtids != null) {
-            map.put(GTID_KEY, binlogGtids.toString());
+        if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
+        if (gtidSet != null) {
+            map.put(GTID_SET_KEY, gtidSet);
         }
-        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTs != 0 ) map.put(TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -158,13 +187,14 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        if (binlogGtids != null) {
-            result.put(GTID_KEY, binlogGtids.toString());
+        // Don't put the GTID Set into the struct; only the current GTID is fine ...
+        if (binlogGtid != null) {
+            result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
-        result.put(TIMESTAMP_KEY, binlogTs);
+        result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
@@ -193,12 +223,23 @@ public boolean isSnapshotInEffect() {
     }
 
     /**
-     * Set the GTID range for the MySQL binary log file.
+     * Set the latest GTID from the MySQL binary log file.
+     * 
+     * @param gtid the string representation of a specific GTID; may not be null
+     */
+    public void setGtid(String gtid) {
+        this.binlogGtid = gtid;
+    }
+
+    /**
+     * Set the set of GTIDs known to the MySQL server.
      * 
-     * @param gtids the string representation of the binlog GTIDs; may not be null
+     * @param gtidSet the string representation of GTID set; may not be null
      */
-    public void setGtids(String gtids) {
-        this.binlogGtids = gtids != null && !gtids.trim().isEmpty() ? new GtidSet(gtids) : null;
+    public void setGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            this.gtidSet = gtidSet;
+        }
     }
 
     /**
@@ -239,12 +280,12 @@ public void setBinlogServerId(long serverId) {
     }
 
     /**
-     * Set the timestamp as found within the MySQL binary log file.
+     * Set the number of <em>seconds</em> since Unix epoch (January 1, 1970) as found within the MySQL binary log file.
      * 
-     * @param timestamp the timestamp found within the binary log file
+     * @param timestampInSeconds the timestamp in <em>seconds</em> found within the binary log file
      */
-    public void setBinlogTimestamp(long timestamp) {
-        this.binlogTs = timestamp;
+    public void setBinlogTimestampSeconds(long timestampInSeconds) {
+        this.binlogTimestampSeconds = timestampInSeconds / 1000;
     }
 
     /**
@@ -270,7 +311,7 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtids((String) sourceOffset.get(GTID_KEY)); // may be null
+            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
             binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
@@ -297,7 +338,7 @@ private long longOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.binlogGtids != null ? this.binlogGtids.toString() : null;
+        return this.gtidSet != null ? this.gtidSet.toString() : null;
     }
 
     /**
@@ -340,9 +381,9 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (binlogGtids != null) {
+        if (gtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(binlogGtids);
+            sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
             sb.append("", pos="").append(binlogPosition());
             sb.append("", row="").append(eventRowNumber());
@@ -379,17 +420,17 @@ public String toString() {
      * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
      */
     public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
-        String recordedGtidSetStr = recorded.getString(GTID_KEY);
-        String desiredGtidSetStr = desired.getString(GTID_KEY);
+        String recordedGtidSetStr = recorded.getString(GTID_SET_KEY);
+        String desiredGtidSetStr = desired.getString(GTID_SET_KEY);
         if (desiredGtidSetStr != null) {
             // The desired position uses GTIDs, so we ideally compare using GTIDs ...
             if (recordedGtidSetStr != null) {
                 // Both have GTIDs, so base the comparison entirely on the GTID sets.
                 GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
                 GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
-                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                if (recordedGtidSet.equals(desiredGtidSet)) {
                     // They are exactly the same, which means the recorded position exactly matches the desired ...
-                    if ( !recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
+                    if (!recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
@@ -414,35 +455,35 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         }
 
         // Both positions are missing GTIDs. Look at the servers ...
-        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        if ( recordedServerId != desiredServerId ) {
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        if (recordedServerId != desiredServerId) {
             // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
             // is compare timestamps, and we have to assume that the server timestamps can be compared ...
-            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
-            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
+            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
+            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
             return recordedTimestamp <= desiredTimestamp;
         }
-        
+
         // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
         // comparable ...
         String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
         String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
-        if ( diff > 0 ) return false;
-        
+        if (diff > 0) return false;
+
         // The positions are the same, so compare the row number ...
         int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The binlog coordinates are the same ...
         return true;",2016-06-07T17:01:51Z,11
"@@ -0,0 +1,52 @@
+# For advice on how to change settings please see
+# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
+
+[mysqld]
+#
+# Remove leading # and set to the amount of RAM for the most important data
+# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
+# innodb_buffer_pool_size = 128M
+#
+# Remove leading # to turn on a very important data integrity option: logging
+# changes to the binary log between backups.
+# log_bin
+#
+# Remove leading # to set options mainly useful for reporting servers.
+# The server defaults are faster for transactions and fast SELECTs.
+# Adjust sizes as needed, experiment to find the optimal values.
+# join_buffer_size = 128M
+# sort_buffer_size = 2M
+# read_rnd_buffer_size = 2M
+skip-host-cache
+skip-name-resolve
+datadir=/var/lib/mysql
+socket=/var/lib/mysql/mysql.sock
+secure-file-priv=/var/lib/mysql-files
+user=mysql
+
+# Disabling symbolic-links is recommended to prevent assorted security risks
+symbolic-links=0
+
+log-error=/var/log/mysqld.log
+pid-file=/var/run/mysqld/mysqld.pid
+
+# ----------------------------------------------
+# Enable GTIDs on this master
+# ----------------------------------------------
+gtid_mode                 = on
+enforce_gtid_consistency  = on
+
+# ----------------------------------------------
+# Debezium ingest
+# ----------------------------------------------
+
+# Enable binary replication log and set the prefix, expiration, and log format.
+# The prefix is arbitrary, expiration can be short for integration tests but would
+# be longer on a production system. Row-level info is required for ingest to work.
+# Server ID is required, but this will vary on production systems
+server-id         = 112233
+log_bin           = mysql-bin
+expire_logs_days  = 1
+binlog_format     = row
+
+",2016-06-07T17:01:51Z,117
"@@ -0,0 +1,107 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.connector.mysql.GtidSet.Interval;
+import io.debezium.connector.mysql.GtidSet.UUIDSet;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class GtidSetTest {
+    
+    private static final String UUID1 = ""24bc7850-2c16-11e6-a073-0242ac110002"";
+
+    private GtidSet gtids;
+    
+    @Test
+    public void shouldCreateSetWithSingleInterval() {
+        gtids = new GtidSet(UUID1 + "":1-191"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,191);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,1,191);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191"");
+    }
+    
+    @Test
+    public void shouldCollapseAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,199);
+        asertFirstInterval(UUID1,1,199);
+        asertLastInterval(UUID1,1,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199"");
+    }
+
+    
+    @Test
+    public void shouldNotCollapseNonAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199"");
+        asertIntervalCount(UUID1,2);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,193,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199:1000-1033"");
+        asertIntervalCount(UUID1,3);
+        asertFirstInterval(UUID1,1,191);
+        asertIntervalExists(UUID1,193,199);
+        asertLastInterval(UUID1,1000,1033);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199:1000-1033"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervalsThatMayBeAdjacent() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199:1000-1033:1035-1036:1038-1039"");
+        asertIntervalCount(UUID1, 4);
+        asertFirstInterval(UUID1, 1, 199);
+        asertIntervalExists(UUID1, 1000, 1033);
+        asertIntervalExists(UUID1, 1035, 1036);
+        asertLastInterval(UUID1, 1038, 1039);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199:1000-1033:1035-1036:1038-1039""); // ??
+    }
+    
+    protected void asertIntervalCount( String uuid, int count) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        assertThat(set.getIntervals().size()).isEqualTo(count);
+    }
+    
+    protected void asertIntervalExists( String uuid, int start, int end) {
+        assertThat(hasInterval(uuid,start,end)).isTrue();
+    }
+    
+    protected void asertFirstInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getFirstInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected void asertLastInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getLastInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected boolean hasInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        for ( Interval interval : set.getIntervals() ) {
+            if ( interval.getStart() == start && interval.getEnd() == end ) return true;
+        }
+        return false;
+    }
+
+}",2016-06-07T17:01:51Z,118
"@@ -86,9 +86,9 @@ protected Document positionWithGtids(String gtids) {
 
     protected Document positionWithGtids(String gtids, boolean snapshot) {
         if (snapshot) {
-            return Document.create(SourceInfo.GTID_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
+            return Document.create(SourceInfo.GTID_SET_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
         }
-        return Document.create(SourceInfo.GTID_KEY, gtids);
+        return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
     protected Document positionWithoutGtids(String filename, int position, int row) {",2016-06-07T17:01:51Z,11
"@@ -45,15 +45,18 @@ public static enum Operation {
          */
         DELETE(""d"");
         private final String code;
+
         private Operation(String code) {
             this.code = code;
         }
-        public static Operation forCode( String code ) {
-            for ( Operation op : Operation.values()) {
-                if ( op.code().equalsIgnoreCase(code)) return op;
+
+        public static Operation forCode(String code) {
+            for (Operation op : Operation.values()) {
+                if (op.code().equalsIgnoreCase(code)) return op;
             }
             return null;
         }
+
         public String code() {
             return code;
         }
@@ -81,11 +84,13 @@ public static final class FieldName {
          */
         public static final String SOURCE = ""source"";
         /**
-         * The {@code ts} field is used to store the information about the local time at which the connector processed/generated
-         * the event. Note that the accuracy of the timestamp is not defined, and the values may not always be monotonically
-         * increasing.
+         * The {@code ts_ms} field is used to store the information about the local time at which the connector
+         * processed/generated the event. The timestamp values are the number of milliseconds past epoch (January 1, 1970), and
+         * determined by the {@link System#currentTimeMillis() JVM current time in milliseconds}. Note that the <em>accuracy</em>
+         * of the timestamp value depends on the JVM's system clock and all of its assumptions, limitations, conditions, and
+         * variations.
          */
-        public static final String TIMESTAMP = ""ts"";
+        public static final String TIMESTAMP = ""ts_ms"";
     }
 
     /**
@@ -304,6 +309,7 @@ public Struct delete(Struct before, Struct source, Long timestamp) {
 
     /**
      * Obtain the operation for the given source record.
+     * 
      * @param record the source record; may not be null
      * @return the operation, or null if no valid operation was found in the record
      */",2016-06-07T17:01:51Z,119
"@@ -0,0 +1,268 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.TreeMap;
+
+import io.debezium.annotation.Immutable;
+
+/**
+ * A set of MySQL GTIDs. This is an improvement of {@link com.github.shyiko.mysql.binlog.GtidSet} that is immutable,
+ * and more properly supports comparisons.
+ * 
+ * @author Randall Hauch
+ */
+@Immutable
+public final class GtidSet {
+
+    private final String orderedString;
+    private final Map<String, UUIDSet> uuidSetsByServerId = new TreeMap<>(); // sorts on keys
+
+    /**
+     * @param gtids the string representation of the GTIDs.
+     */
+    public GtidSet(String gtids) {
+        new com.github.shyiko.mysql.binlog.GtidSet(gtids).getUUIDSets().forEach(uuidSet -> {
+            uuidSetsByServerId.put(uuidSet.getUUID(), new UUIDSet(uuidSet));
+        });
+        StringBuilder sb = new StringBuilder();
+        uuidSetsByServerId.values().forEach(uuidSet -> {
+            if (sb.length() != 0) sb.append(',');
+            sb.append(uuidSet.toString());
+        });
+        orderedString = sb.toString();
+    }
+
+    /**
+     * Get an immutable collection of the {@link UUIDSet range of GTIDs for a single server}.
+     * 
+     * @return the {@link UUIDSet GTID ranges for each server}; never null
+     */
+    public Collection<UUIDSet> getUUIDSets() {
+        return Collections.unmodifiableCollection(uuidSetsByServerId.values());
+    }
+
+    /**
+     * Find the {@link UUIDSet} for the server with the specified UUID.
+     * 
+     * @param uuid the UUID of the server
+     * @return the {@link UUIDSet} for the identified server, or {@code null} if there are no GTIDs from that server.
+     */
+    public UUIDSet forServerWithId(String uuid) {
+        return uuidSetsByServerId.get(uuid);
+    }
+
+    /**
+     * Determine if the GTIDs represented by this object are contained completely within the supplied set of GTIDs.
+     * 
+     * @param other the other set of GTIDs; may be null
+     * @return {@code true} if all of the GTIDs in this set are completely contained within the supplied set of GTIDs, or
+     *         {@code false} otherwise
+     */
+    public boolean isSubsetOf(GtidSet other) {
+        if (other == null) return false;
+        if (this.equals(other)) return true;
+        for (UUIDSet uuidSet : uuidSetsByServerId.values()) {
+            UUIDSet thatSet = other.forServerWithId(uuidSet.getUUID());
+            if (!uuidSet.isSubsetOf(thatSet)) return false;
+        }
+        return true;
+    }
+
+    @Override
+    public int hashCode() {
+        return orderedString.hashCode();
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (obj == this) return true;
+        if (obj instanceof GtidSet) {
+            GtidSet that = (GtidSet) obj;
+            return this.orderedString.equalsIgnoreCase(that.orderedString);
+        }
+        return false;
+    }
+
+    @Override
+    public String toString() {
+        return orderedString;
+    }
+
+    /**
+     * A range of GTIDs for a single server with a specific UUID.
+     */
+    @Immutable
+    public static class UUIDSet {
+
+        private String uuid;
+        private LinkedList<Interval> intervals = new LinkedList<>();
+
+        protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
+            this.uuid = uuidSet.getUUID();
+            uuidSet.getIntervals().forEach(interval -> {
+                intervals.add(new Interval(interval.getStart(), interval.getEnd()));
+            });
+            Collections.sort(this.intervals);
+        }
+
+        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
+            this.uuid = uuid;
+            this.intervals = intervals;
+        }
+
+        /**
+         * Get the UUID for the server that generated the GTIDs.
+         * 
+         * @return the server's UUID; never null
+         */
+        public String getUUID() {
+            return uuid;
+        }
+
+        /**
+         * Get the intervals of transaction numbers.
+         * 
+         * @return the immutable transaction intervals; never null
+         */
+        public Collection<Interval> getIntervals() {
+            return Collections.unmodifiableCollection(intervals);
+        }
+
+        /**
+         * Get the first interval of transaction numbers for this server.
+         * 
+         * @return the first interval, or {@code null} if there is none
+         */
+        public Interval getFirstInterval() {
+            return intervals.isEmpty() ? null : intervals.getFirst();
+        }
+
+        /**
+         * Get the last interval of transaction numbers for this server.
+         * 
+         * @return the last interval, or {@code null} if there is none
+         */
+        public Interval getLastInterval() {
+            return intervals.isEmpty() ? null : intervals.getLast();
+        }
+
+        /**
+         * Get the interval that contains the full range (and possibly more) of all of the individual intervals for this server.
+         * 
+         * @return the complete interval comprised of the {@link Interval#getStart() start} of the {@link #getFirstInterval()
+         *         first interval} and the {@link Interval#getEnd() end} of the {@link #getLastInterval()}, or {@code null} if
+         *         this server has no intervals at all
+         */
+        public Interval getCompleteInterval() {
+            return intervals.isEmpty() ? null : new Interval(getFirstInterval().getStart(), getLastInterval().getEnd());
+        }
+
+        /**
+         * Determine if the set of transaction numbers from this server is completely within the set of transaction numbers from
+         * the set of transaction numbers in the supplied set.
+         * 
+         * @param other the set to compare with this set
+         * @return {@code true} if this server's transaction numbers are a subset of the transaction numbers of the supplied set,
+         *         or false otherwise
+         */
+        public boolean isSubsetOf(UUIDSet other) {
+            if (other == null) return false;
+            if (!this.getUUID().equalsIgnoreCase(other.getUUID())) {
+                // Not even the same server ...
+                return false;
+            }
+            if (this.intervals.isEmpty()) return true;
+            if (other.intervals.isEmpty()) return false;
+            assert this.intervals.size() > 0;
+            assert other.intervals.size() > 0;
+
+            // Every interval in this must be within an interval of the other ...
+            for (Interval thisInterval : this.intervals) {
+                boolean found = false;
+                for (Interval otherInterval : other.intervals) {
+                    if (thisInterval.isSubsetOf(otherInterval)) {
+                        found = true;
+                        break;
+                    }
+                }
+                if (!found) return false; // didn't find a match
+            }
+            return true;
+        }
+
+        @Override
+        public int hashCode() {
+            return uuid.hashCode();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (obj == this) return true;
+            if (obj instanceof UUIDSet) {
+                UUIDSet that = (UUIDSet) obj;
+                return this.getUUID().equalsIgnoreCase(that.getUUID()) && this.getIntervals().equals(that.getIntervals());
+            }
+            return super.equals(obj);
+        }
+
+        @Override
+        public String toString() {
+            StringBuilder sb = new StringBuilder();
+            if (sb.length() != 0) sb.append(',');
+            sb.append(uuid).append(':');
+            sb.append(intervals.getFirst().getStart());
+            sb.append(intervals.getLast().getEnd());
+            return sb.toString();
+        }
+    }
+
+    @Immutable
+    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+
+        public Interval(long start, long end) {
+            super(start, end);
+        }
+
+        /**
+         * Determine if this interval is completely within the supplied interval.
+         * 
+         * @param other the interval to compare with
+         * @return {@code true} if the {@link #getStart() start} is greater than or equal to the supplied interval's
+         *         {@link #getStart() start} and the {@link #getEnd() end} is less than or equal to the supplied interval's
+         *         {@link #getEnd() end}, or {@code false} otherwise
+         */
+        public boolean isSubsetOf(Interval other) {
+            if (other == this) return true;
+            if (other == null) return false;
+            return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
+        }
+
+        @Override
+        public int hashCode() {
+            return (int) getStart();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (this == obj) return true;
+            if (obj instanceof com.github.shyiko.mysql.binlog.GtidSet.Interval) {
+                com.github.shyiko.mysql.binlog.GtidSet.Interval that = (com.github.shyiko.mysql.binlog.GtidSet.Interval) obj;
+                return this.getStart() == that.getStart() && this.getEnd() == that.getEnd();
+            }
+            return false;
+        }
+
+        @Override
+        public String toString() {
+            return getStart() == getEnd() ? Long.toString(getStart()) : """" + getStart() + ""-"" + getEnd();
+        }
+    }
+}",2016-06-04T21:20:26Z,101
"@@ -76,7 +76,8 @@ public class MySqlConnectorConfig {
                                                       .withDescription(""The name of the DatabaseHistory class that should be used to store and recover database schema changes. ""
                                                               + ""The configuration properties for the history are prefixed with the '""
                                                               + DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING + ""' string."")
-                                                      .withDefault(KafkaDatabaseHistory.class.getName());
+                                                      .withDefault(KafkaDatabaseHistory.class.getName())
+                                                      .withValidation(Field::isClassName);
 
     public static final Field INCLUDE_SCHEMA_CHANGES = Field.create(""include.schema.changes"")
                                                             .withDescription(""Whether the connector should publish changes in the database schema to a Kafka topic with """,2016-06-04T21:20:26Z,65
"@@ -27,6 +27,7 @@
 import io.debezium.relational.ddl.DdlChanges;
 import io.debezium.relational.ddl.DdlChanges.DatabaseStatementStringConsumer;
 import io.debezium.relational.history.DatabaseHistory;
+import io.debezium.relational.history.HistoryRecordComparator;
 import io.debezium.text.ParsingException;
 import io.debezium.util.Collect;
 
@@ -51,6 +52,8 @@
 @NotThreadSafe
 public class MySqlSchema {
 
+    private static final HistoryRecordComparator HISTORY_COMPARATOR = HistoryRecordComparator.usingPositions(SourceInfo::isPositionAtOrBefore);
+
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final Set<String> ignoredQueryStatements = Collect.unmodifiableSet(""BEGIN"", ""END"", ""FLUSH PRIVILEGES"");
     private final MySqlDdlParser ddlParser;
@@ -85,7 +88,7 @@ public MySqlSchema(Configuration config, String serverName) {
         }
         // Do not remove the prefix from the subset of config properties ...
         Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false);
-        this.dbHistory.configure(dbHistoryConfig); // validates
+        this.dbHistory.configure(dbHistoryConfig,HISTORY_COMPARATOR); // validates
     }
 
     /**",2016-06-04T21:20:26Z,16
"@@ -13,9 +13,8 @@
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.errors.ConnectException;
 
-import com.github.shyiko.mysql.binlog.GtidSet;
-
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
 /**
@@ -126,6 +125,8 @@ public Map<String, String> partition() {
         if (binlogGtids != null) {
             map.put(BINLOG_GTID_KEY, binlogGtids.toString());
         }
+        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTs != 0 ) map.put(BINLOG_EVENT_TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -360,4 +361,90 @@ public String toString() {
         }
         return sb.toString();
     }
+
+    /**
+     * Determine whether the first {@link #offset() offset} is at or before the point in time of the second
+     * offset, where the offsets are given in JSON representation of the maps returned by {@link #offset()}.
+     * <p>
+     * This logic makes a significant assumption: once a MySQL server/cluster has GTIDs enabled, they will
+     * never be disabled. This is the only way to compare a position with a GTID to a position without a GTID,
+     * and we conclude that any position with a GTID is *after* the position without.
+     * <p>
+     * When both positions have GTIDs, then we compare the positions by using only the GTIDs. Of course, if the
+     * GTIDs are the same, then we also look at whether they have snapshots enabled.
+     * 
+     * @param recorded the position obtained from recorded history; never null
+     * @param desired the desired position that we want to obtain, which should be after some recorded positions,
+     *            at some recorded positions, and before other recorded positions; never null
+     * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
+     */
+    public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
+        String recordedGtidSetStr = recorded.getString(BINLOG_GTID_KEY);
+        String desiredGtidSetStr = desired.getString(BINLOG_GTID_KEY);
+        if (desiredGtidSetStr != null) {
+            // The desired position uses GTIDs, so we ideally compare using GTIDs ...
+            if (recordedGtidSetStr != null) {
+                // Both have GTIDs, so base the comparison entirely on the GTID sets.
+                GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
+                GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
+                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                    // They are exactly the same, which means the recorded position exactly matches the desired ...
+                    if ( !recorded.has(BINLOG_SNAPSHOT_KEY) && desired.has(BINLOG_SNAPSHOT_KEY)) {
+                        // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
+                        return false;
+                    }
+                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired ...
+                    return true;
+                }
+                // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
+                return recordedGtidSet.isSubsetOf(desiredGtidSet);
+            }
+            // The desired position did use GTIDs while the recorded did not use GTIDs. So, we assume that the
+            // recorded position is older since GTIDs are often enabled but rarely disabled. And if they are disabled,
+            // it is likely that the desired position would not include GTIDs as we would be trying to read the binlog of a
+            // server that no longer has GTIDs. And if they are enabled, disabled, and re-enabled, per
+            // https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-failover.html all properly configured slaves that
+            // use GTIDs should always have the complete set of GTIDs copied from the master, in which case
+            // again we know that recorded not having GTIDs is before the desired position ...
+            return true;
+        } else if (recordedGtidSetStr != null) {
+            // The recorded has a GTID but the desired does not, so per the previous paragraph we assume that previous
+            // is not at or before ...
+            return false;
+        }
+
+        // Both positions are missing GTIDs. Look at the servers ...
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
+        if ( recordedServerId != desiredServerId ) {
+            // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
+            // is compare timestamps, and we have to assume that the server timestamps can be compared ...
+            long recordedTimestamp = recorded.getLong(BINLOG_EVENT_TIMESTAMP_KEY,0);
+            long desiredTimestamp = recorded.getLong(BINLOG_EVENT_TIMESTAMP_KEY,0);
+            return recordedTimestamp <= desiredTimestamp;
+        }
+        
+        // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
+        // comparable ...
+        String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
+        String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
+        assert recordedFilename != null;
+        int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
+        if ( diff > 0 ) return false;
+
+        // The filenames are the same, so compare the positions ...
+        int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
+        int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
+        diff = recordedPosition - desiredPosition;
+        if ( diff > 0 ) return false;
+        
+        // The positions are the same, so compare the row number ...
+        int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        diff = recordedRow - desiredRow;
+        if ( diff > 0 ) return false;
+
+        // The binlog coordinates are the same ...
+        return true;
+    }
 }",2016-06-04T21:20:26Z,11
"@@ -5,33 +5,153 @@
  */
 package io.debezium.connector.mysql;
 
-import io.confluent.connect.avro.AvroData;
+import static org.junit.Assert.assertTrue;
 
 import org.apache.avro.Schema;
-
-
+import org.fest.assertions.GenericAssert;
 import org.junit.Test;
 
-import static org.junit.Assert.assertTrue;
+import io.confluent.connect.avro.AvroData;
+import io.debezium.document.Document;
 
 public class SourceInfoTest {
-    private static final AvroData avroData;
-    private static int avroSchemaCacheSize = 1000;
 
-    static {
-        avroData = new AvroData(avroSchemaCacheSize);
-    }
+    private static int avroSchemaCacheSize = 1000;
+    private static final AvroData avroData = new AvroData(avroSchemaCacheSize);
 
     /**
      * When we want to consume SinkRecord which generated by debezium-connector-mysql, it should not
      * throw error ""org.apache.avro.SchemaParseException: Illegal character in: server-id""
      */
     @Test
-    public void testValidateSourceInfoSchema() {
+    public void shouldValidateSourceInfoSchema() {
         org.apache.kafka.connect.data.Schema kafkaSchema = SourceInfo.SCHEMA;
         Schema avroSchema = avroData.fromConnectSchema(kafkaSchema);
         assertTrue(avroSchema != null);
     }
 
-}
+    @Test
+    public void shouldConsiderPositionsWithSameGtidSetsAsSame() {
+        assertPositionWithGtids(""IdA:1-5"").isAtOrBefore(positionWithGtids(""IdA:1-5""));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"").isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"")); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"").isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"")); // equivalent
+    }
+
+    @Test
+    public void shouldConsiderPositionsWithSameGtidSetsAndSnapshotAsSame() {
+        assertPositionWithGtids(""IdA:1-5"",true).isAtOrBefore(positionWithGtids(""IdA:1-5"",true));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"",true)); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"",true)); // equivalent
+    }
 
+    @Test
+    public void shouldOrderPositionWithGtidAndSnapshotBeforePositionWithSameGtidButNoSnapshot() {
+        assertPositionWithGtids(""IdA:1-5"",true).isAtOrBefore(positionWithGtids(""IdA:1-5""));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"")); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"")); // equivalent
+    }
+
+    @Test
+    public void shouldOrderPositionWithoutGtidAndSnapshotAfterPositionWithSameGtidAndSnapshot() {
+        assertPositionWithGtids(""IdA:1-5"",false).isAfter(positionWithGtids(""IdA:1-5"",true));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",false).isAfter(positionWithGtids(""IdA:1-5,IdB:1-20"",true)); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",false).isAfter(positionWithGtids(""IdB:1-20,IdA:1-5"",true)); // equivalent
+    }
+
+    @Test
+    public void shouldOrderPositionWithGtidsAsBeforePositionWithExtraServerUuidInGtids() {
+        assertPositionWithGtids(""IdA:1-5"").isBefore(positionWithGtids(""IdA:1-5,IdB:1-20""));
+    }
+
+    @Test
+    public void shouldOrderPositionsWithSameServerButLowerUpperLimitAsBeforePositionWithSameServerUuidInGtids() {
+        assertPositionWithGtids(""IdA:1-5"").isBefore(positionWithGtids(""IdA:1-6""));
+        assertPositionWithGtids(""IdA:1-5:7-9"").isBefore(positionWithGtids(""IdA:1-10""));
+        assertPositionWithGtids(""IdA:2-5:8-9"").isBefore(positionWithGtids(""IdA:1-10""));
+    }
+
+    @Test
+    public void shouldOrderPositionWithoutGtidAsBeforePositionWithGtid() {
+        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0).isBefore(positionWithGtids(""IdA:1-5""));
+    }
+
+    @Test
+    public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
+        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
+    }
+
+    protected Document positionWithGtids(String gtids) {
+        return positionWithGtids(gtids, false);
+    }
+
+    protected Document positionWithGtids(String gtids, boolean snapshot) {
+        if (snapshot) {
+            return Document.create(SourceInfo.BINLOG_GTID_KEY, gtids, SourceInfo.BINLOG_SNAPSHOT_KEY, true);
+        }
+        return Document.create(SourceInfo.BINLOG_GTID_KEY, gtids);
+    }
+
+    protected Document positionWithoutGtids(String filename, int position, int row) {
+        return positionWithoutGtids(filename, position, row, false);
+    }
+
+    protected Document positionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+        if (snapshot) {
+            return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
+                                   SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
+                                   SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row,
+                                   SourceInfo.BINLOG_SNAPSHOT_KEY, true);
+        }
+        return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
+                               SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
+                               SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row);
+    }
+
+    protected PositionAssert assertThat(Document position) {
+        return new PositionAssert(position);
+    }
+
+    protected PositionAssert assertPositionWithGtids(String gtids) {
+        return assertThat(positionWithGtids(gtids));
+    }
+
+    protected PositionAssert assertPositionWithGtids(String gtids, boolean snapshot) {
+        return assertThat(positionWithGtids(gtids, snapshot));
+    }
+
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row) {
+        return assertPositionWithoutGtids(filename, position, row, false);
+    }
+
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+        return assertThat(positionWithoutGtids(filename, position, row, snapshot));
+    }
+
+    protected static class PositionAssert extends GenericAssert<PositionAssert, Document> {
+        public PositionAssert(Document position) {
+            super(PositionAssert.class, position);
+        }
+
+        public PositionAssert isAt(Document otherPosition) {
+            if (SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider same position as "" + otherPosition);
+        }
+
+        public PositionAssert isBefore(Document otherPosition) {
+            return isAtOrBefore(otherPosition);
+        }
+
+        public PositionAssert isAtOrBefore(Document otherPosition) {
+            if (SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider same position as or before "" + otherPosition);
+        }
+
+        public PositionAssert isAfter(Document otherPosition) {
+            if (!SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider after "" + otherPosition);
+        }
+    }
+}",2016-06-04T21:20:26Z,11
"@@ -14,6 +14,8 @@
 import java.util.function.Predicate;
 import java.util.function.Supplier;
 
+import javax.lang.model.SourceVersion;
+
 import io.debezium.annotation.Immutable;
 
 /**
@@ -395,6 +397,13 @@ public String toString() {
         return name();
     }
 
+    public static int isClassName(Configuration config, Field field, Consumer<String> problems) {
+        String value = config.getString(field);
+        if (value == null || SourceVersion.isName(value)) return 0;
+        problems.accept(""The '"" + field.name() + ""' field must contain a valid name of a Java class."");
+        return 1;
+    }
+
     public static int isRequired(Configuration config, Field field, Consumer<String> problems) {
         String value = config.getString(field);
         if (value != null && value.trim().length() > 0) return 0;",2016-06-04T21:20:26Z,5
"@@ -21,15 +21,17 @@
  */
 public abstract class AbstractDatabaseHistory implements DatabaseHistory {
 
-    protected Configuration config;
     protected final Logger logger = LoggerFactory.getLogger(getClass());
+    protected Configuration config;
+    private HistoryRecordComparator comparator = HistoryRecordComparator.INSTANCE;
 
     protected AbstractDatabaseHistory() {
     }
-
+    
     @Override
-    public void configure(Configuration config) {
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
         this.config = config;
+        this.comparator = comparator != null ? comparator : HistoryRecordComparator.INSTANCE;
     }
     
     @Override
@@ -46,7 +48,7 @@ public final void record(Map<String, ?> source, Map<String, ?> position, String
     public final void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser) {
         HistoryRecord stopPoint = new HistoryRecord(source, position, null, null);
         recoverRecords(schema,ddlParser,recovered->{
-            if (recovered.isAtOrBefore(stopPoint)) {
+            if (comparator.isAtOrBefore(recovered,stopPoint)) {
                 String ddl = recovered.ddl();
                 if (ddl != null) {
                     ddlParser.setCurrentSchema(recovered.databaseName()); // may be null",2016-06-04T21:20:26Z,3
"@@ -26,8 +26,11 @@ public interface DatabaseHistory {
      * Configure this instance.
      * 
      * @param config the configuration for this history store
+     * @param comparator the function that should be used to compare history records during
+     *            {@link #recover(Map, Map, Tables, DdlParser) recovery}; may be null if the
+     *            {@link HistoryRecordComparator#INSTANCE default comparator} is to be used
      */
-    void configure(Configuration config);
+    void configure(Configuration config, HistoryRecordComparator comparator);
 
     /**
      * Start the history.
@@ -62,7 +65,7 @@ public interface DatabaseHistory {
     void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser);
 
     /**
-     * Stop recording history and release any resources acquired since {@link #configure(Configuration)}.
+     * Stop recording history and release any resources acquired since {@link #configure(Configuration, HistoryRecordComparator)}.
      */
     void stop();
 }",2016-06-04T21:20:26Z,14
"@@ -49,15 +49,14 @@ public final class FileDatabaseHistory extends AbstractDatabaseHistory {
     private Path path;
 
     @Override
-    public void configure(Configuration config) {
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
         lock.write(() -> {
-            super.configure(config);
             if (!config.validate(ALL_FIELDS, logger::error)) {
                 throw new ConnectException(
                         ""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
             }
             config.validate(ALL_FIELDS, logger::error);
-            super.configure(config);
+            super.configure(config,comparator);
             path = Paths.get(config.getString(FILE_PATH));
         });
     }",2016-06-04T21:20:26Z,4
"@@ -0,0 +1,62 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.relational.history;
+
+import java.util.function.BiFunction;
+
+import io.debezium.document.Document;
+
+/**
+ * Compares HistoryRecord instances to determine which came first.
+ * 
+ * @author Randall Hauch
+ * @since 0.2
+ */
+public class HistoryRecordComparator {
+
+    /**
+     * A comparator instance that requires the {@link HistoryRecord#source() records' sources} to be the same and considers only
+     * those fields that are in both records' {@link HistoryRecord#position() positions}.
+     */
+    public static final HistoryRecordComparator INSTANCE = new HistoryRecordComparator();
+
+    /**
+     * Create a {@link HistoryRecordComparator} that requires identical sources but will use the supplied function to compare
+     * positions.
+     * 
+     * @param positionComparator the non-null function that returns {@code true} if the first position is at or before
+     *            the second position or {@code false} otherwise
+     * @return the comparator instance; never null
+     */
+    public static HistoryRecordComparator usingPositions(BiFunction<Document, Document, Boolean> positionComparator) {
+        return new HistoryRecordComparator() {
+            @Override
+            protected boolean isPositionAtOrBefore(Document position1, Document position2) {
+                return positionComparator.apply(position1, position2);
+            }
+        };
+    }
+
+    /**
+     * Determine if the first {@link HistoryRecord} is at the same or earlier point in time than the second {@link HistoryRecord}.
+     * 
+     * @param record1 the first record; never null
+     * @param record2 the second record; never null
+     * @return {@code true} if the first record is at the same or earlier point in time than the second record, or {@code false}
+     *         otherwise
+     */
+    public boolean isAtOrBefore(HistoryRecord record1, HistoryRecord record2) {
+        return isSameSource(record1.source(), record2.source()) && isPositionAtOrBefore(record1.position(), record2.position());
+    }
+
+    protected boolean isPositionAtOrBefore(Document position1, Document position2) {
+        return position1.compareToUsingSimilarFields(position2) <= 0;
+    }
+
+    protected boolean isSameSource(Document source1, Document source2) {
+        return source1.equals(source2);
+    }
+}
\ No newline at end of file",2016-06-04T21:20:26Z,136
"@@ -83,8 +83,8 @@ public class KafkaDatabaseHistory extends AbstractDatabaseHistory {
     private int pollIntervalMs = -1;
 
     @Override
-    public void configure(Configuration config) {
-        super.configure(config);
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
+        super.configure(config,comparator);
         if (!config.validate(ALL_FIELDS, logger::error)) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }",2016-06-04T21:20:26Z,30
"@@ -31,7 +31,7 @@ protected DatabaseHistory createHistory() {
         DatabaseHistory history = new FileDatabaseHistory();
         history.configure(Configuration.create()
                                        .with(FileDatabaseHistory.FILE_PATH, TEST_FILE_PATH.toAbsolutePath().toString())
-                                       .build());
+                                       .build(),null);
         return history;
     }
 }",2016-06-04T21:20:26Z,19
"@@ -76,7 +76,7 @@ public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exc
                               .with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS, kafka.brokerList())
                               .with(KafkaDatabaseHistory.TOPIC, topicName)
                               .build();
-        history.configure(config);
+        history.configure(config,null);
         history.start();
 
         DdlParser recoveryParser = new DdlParserSql2003();
@@ -123,7 +123,7 @@ public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exc
         // Stop the history (which should stop the producer) ...
         history.stop();
         history = new KafkaDatabaseHistory();
-        history.configure(config);
+        history.configure(config, null);
         // no need to start
 
         // Recover from the very beginning to just past the first change ...",2016-06-04T21:20:26Z,22
"@@ -18,7 +18,6 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import com.github.shyiko.mysql.binlog.BinaryLogClient;
-import com.github.shyiko.mysql.binlog.BinaryLogClient.AbstractLifecycleListener;
 import com.github.shyiko.mysql.binlog.BinaryLogClient.LifecycleListener;
 import com.github.shyiko.mysql.binlog.event.DeleteRowsEventData;
 import com.github.shyiko.mysql.binlog.event.Event;
@@ -68,17 +67,7 @@ public BinlogReader(MySqlTaskContext context) {
         client.setServerId(context.serverId());
         client.setKeepAlive(context.config().getBoolean(MySqlConnectorConfig.KEEP_ALIVE));
         client.registerEventListener(this::handleEvent);
-        client.registerLifecycleListener(new AbstractLifecycleListener(){
-            @Override
-            public void onCommunicationFailure(BinaryLogClient client, Exception ex) {
-                failed(ex,""Stopped reading binlog due to error: "" + ex.getMessage());
-            }
-            @Override
-            public void onEventDeserializationFailure(BinaryLogClient client, Exception ex) {
-                failed(ex,""Stopped reading binlog due to error: "" + ex.getMessage());
-            }
-        });
-        client.registerLifecycleListener(new TraceLifecycleListener());
+        client.registerLifecycleListener(new ReaderThreadLifecycleListener());
         if (logger.isDebugEnabled()) client.registerEventListener(this::logEvent);
 
         // Set up the event deserializer with additional type(s) ...
@@ -104,34 +93,31 @@ protected void doStart() {
         client.setBinlogFilename(source.binlogFilename());
         client.setBinlogPosition(source.binlogPosition());
         // The event row number will be used when processing the first event ...
-        logger.info(""Reading from MySQL {} starting at {}"",context.serverName(), source);
 
         // Start the log reader, which starts background threads ...
         long timeoutInMilliseconds = context.timeoutInMilliseconds();
         try {
-            logger.debug(""Binlog reader connecting to MySQL server '{}'"", context.serverName());
+            logger.debug(""Attempting to establish binlog reader connection with timeout of {} ms"", timeoutInMilliseconds);
             client.connect(context.timeoutInMilliseconds());
-            logger.info(""Successfully started reading MySQL binlog"");
         } catch (TimeoutException e) {
             double seconds = TimeUnit.MILLISECONDS.toSeconds(timeoutInMilliseconds);
-            throw new ConnectException(""Timed out after "" + seconds + "" seconds while waiting to connect to the MySQL database at "" +
-                    context.username() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
+            throw new ConnectException(""Timed out after "" + seconds + "" seconds while waiting to connect to MySQL at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
         } catch (AuthenticationException e) {
-            throw new ConnectException(""Failed to authenticate to the MySQL database at "" + context.hostname() + "":"" +
-                    context.port() + "" with user '"" + context.username() + ""'"", e);
+            throw new ConnectException(""Failed to authenticate to the MySQL database at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
         } catch (Throwable e) {
-            throw new ConnectException(""Unable to connect to the MySQL database at "" + context.hostname() + "":"" + context.port() +
-                    "" with user '"" + context.username() + ""': "" + e.getMessage(), e);
+            throw new ConnectException(""Unable to connect to the MySQL database at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""': "" + e.getMessage(), e);
         }
 
     }
 
     @Override
     protected void doStop() {
         try {
-            logger.debug(""Binlog reader disconnecting from MySQL server '{}'"", context.serverName());
+            logger.debug(""Stopping binlog reader"");
             client.disconnect();
-            logger.info(""Stopped connector to MySQL server '{}'"", context.serverName());
         } catch (IOException e) {
             logger.error(""Unexpected error when disconnecting from the MySQL binary log reader"", e);
         }
@@ -142,7 +128,7 @@ protected void doCleanup() {
     }
 
     protected void logEvent(Event event) {
-        //logger.debug(""Received event: {}"", event);
+        logger.trace(""Received event: {}"", event);
     }
 
     protected void ignoreEvent(Event event) {
@@ -344,25 +330,31 @@ protected void handleDelete(Event event) throws InterruptedException {
         }
     }
 
-    protected final class TraceLifecycleListener implements LifecycleListener {
+    protected final class ReaderThreadLifecycleListener implements LifecycleListener {
         @Override
         public void onDisconnect(BinaryLogClient client) {
-            logger.debug(""MySQL Connector disconnected"");
+            context.temporaryLoggingContext(""binlog"", () -> {
+                logger.info(""Stopped reading binlog and closed connection"");
+            });
         }
 
         @Override
         public void onConnect(BinaryLogClient client) {
-            logger.info(""MySQL Connector connected"");
+            // Set up the MDC logging context for this thread ...
+            context.configureLoggingContext(""binlog"");
+
+            // The event row number will be used when processing the first event ...
+            logger.info(""Connected to MySQL binlog at {}:{}, starting at {}"", context.hostname(), context.port(), source);
         }
 
         @Override
         public void onCommunicationFailure(BinaryLogClient client, Exception ex) {
-            logger.error(""MySQL Connector communication failure"", ex);
+            BinlogReader.this.failed(ex);
         }
 
         @Override
         public void onEventDeserializationFailure(BinaryLogClient client, Exception ex) {
-            logger.error(""MySQL Connector received event deserialization failure"", ex);
+            BinlogReader.this.failed(ex);
         }
     }
 }",2016-06-02T19:05:06Z,24
"@@ -60,7 +60,7 @@ public void start(Map<String, String> props) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }
 
-        // Create the task and set our running flag ...
+        // Create and start the task context ...
         this.taskContext = new MySqlTaskContext(config);
         this.taskContext.start();
 
@@ -133,13 +133,13 @@ public void start(Map<String, String> props) {
 
     @Override
     public List<SourceRecord> poll() throws InterruptedException {
-        logger.trace(""Polling for events from MySQL connector"");
+        logger.trace(""Polling for events"");
         return currentReader.poll();
     }
 
     @Override
     public void stop() {
-        logger.info(""Stopping MySQL Connector"");
+        logger.info(""Stopping MySQL connector task"");
         // We need to explicitly stop both readers, in this order. If we were to instead call 'currentReader.stop()', there
         // is a chance without synchronization that we'd miss the transition and stop only the snapshot reader. And stopping both
         // is far simpler and more efficient than synchronizing ...
@@ -155,7 +155,7 @@ public void stop() {
                 } catch (Throwable e) {
                     logger.error(""Unexpected error shutting down the database history and/or closing JDBC connections"", e);
                 } finally {
-                    logger.info(""Stopped connector to MySQL server '{}'"", taskContext.serverName());
+                    logger.info(""Connector task successfully stopped"");
                 }
             }
         }",2016-06-02T19:05:06Z,10
"@@ -15,6 +15,8 @@
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
 import io.debezium.util.Clock;
+import io.debezium.util.LoggingContext;
+import io.debezium.util.LoggingContext.PreviousContext;
 
 /**
  * A Kafka Connect source task reads the MySQL binary log and generate the corresponding data change events.
@@ -96,7 +98,7 @@ public void loadHistory(SourceInfo startingPoint) {
         dbSchema.loadHistory(startingPoint);
         recordProcessor.regenerate();
     }
-    
+
     public Clock clock() {
         return clock;
     }
@@ -157,21 +159,23 @@ protected SnapshotMode snapshotMode() {
         String value = config.getString(MySqlConnectorConfig.SNAPSHOT_MODE);
         return SnapshotMode.parse(value, MySqlConnectorConfig.SNAPSHOT_MODE.defaultValue());
     }
-    
+
     public boolean useMinimalSnapshotLocking() {
         return config.getBoolean(MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
     }
 
     public void start() {
-        // Start the MySQL database history, which simply starts up resources but does not recover the history to a specific
-        // point.
+        // First, configure the logging context for the thread that created this context object ...
+        this.configureLoggingContext(""task"");
+
+        // Start the MySQL database history, which simply starts up resources but does not recover the history to a specific point
         dbSchema().start();
     }
 
     public void shutdown() {
         try {
             // Flush and stop the database history ...
-            logger.debug(""Stopping database history for MySQL server '{}'"", serverName());
+            logger.debug(""Stopping database history"");
             dbSchema.shutdown();
         } catch (Throwable e) {
             logger.error(""Unexpected error shutting down the database history"", e);
@@ -184,4 +188,26 @@ public void shutdown() {
         }
     }
 
+    /**
+     * Configure the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * @param contextName the name of the context; may not be null
+     * @return the previous MDC context; never null
+     * @throws IllegalArgumentException if {@code contextName} is null
+     */
+    public PreviousContext configureLoggingContext(String contextName) {
+        return LoggingContext.forConnector(""MySQL"", serverName(), contextName);
+    }
+    
+    /**
+     * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
+     * state before this method was called.
+     * 
+     * @param contextName the name of the context; may not be null
+     * @param operation the function to run in the new MDC context; may not be null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public void temporaryLoggingContext(String contextName, Runnable operation) {
+        LoggingContext.temporarilyForConnector(""MySQL"", serverName(), contextName, operation);
+    }
+
 }",2016-06-02T19:05:06Z,87
"@@ -19,6 +19,8 @@
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
+import io.debezium.util.Clock;
+import io.debezium.util.Strings;
 
 /**
  * A component that performs a snapshot of a MySQL server, and records the schema changes in {@link MySqlSchema}.
@@ -127,13 +129,15 @@ protected void doCleanup() {
      * Perform the snapshot using the same logic as the ""mysqldump"" utility.
      */
     protected void execute() {
-        logger.info(""Starting snapshot for MySQL server {}"", context.serverName());
+        context.configureLoggingContext(""snapshot"");
+        logger.info(""Starting snapshot"");
         final AtomicReference<String> sql = new AtomicReference<>();
         final JdbcConnection mysql = context.jdbc();
         final MySqlSchema schema = context.dbSchema();
         final Filters filters = schema.filters();
         final SourceInfo source = context.source();
-        final long ts = context.clock().currentTimeInMillis();
+        final Clock clock = context.clock();
+        final long ts = clock.currentTimeInMillis();
         try {
             // ------
             // STEP 0
@@ -148,6 +152,7 @@ protected void execute() {
             // See: https://dev.mysql.com/doc/refman/5.7/en/set-transaction.html
             // See: https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html
             // See: https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html
+            logger.info(""Step 0: disabling autocommit and enabling repeatable read transactions"");
             mysql.setAutoCommit(false);
             sql.set(""SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"");
             mysql.execute(sql.get());
@@ -157,6 +162,7 @@ protected void execute() {
             // ------
             // First, start a transaction and request that a consistent MVCC snapshot is obtained immediately.
             // See http://dev.mysql.com/doc/refman/5.7/en/commit.html
+            logger.info(""Step 1: start transaction with consistent snapshot"");
             sql.set(""START TRANSACTION WITH CONSISTENT SNAPSHOT"");
             mysql.execute(sql.get());
 
@@ -166,6 +172,8 @@ protected void execute() {
             // Obtain read lock on all tables. This statement closes all open tables and locks all tables
             // for all databases with a global read lock, and it prevents ALL updates while we have this lock.
             // It also ensures that everything we do while we have this lock will be consistent.
+            long lockAcquired = clock.currentTimeInMillis();
+            logger.info(""Step 2: flush and obtain global read lock (preventing writes to database)"");
             sql.set(""FLUSH TABLES WITH READ LOCK"");
             mysql.execute(sql.get());
 
@@ -174,6 +182,7 @@ protected void execute() {
             // ------
             // Obtain the binlog position and update the SourceInfo in the context. This means that all source records generated
             // as part of the snapshot will contain the binlog position of the snapshot.
+            logger.info(""Step 3: read binlog position of MySQL master"");
             sql.set(""SHOW MASTER STATUS"");
             mysql.query(sql.get(), rs -> {
                 if (rs.next()) {
@@ -191,6 +200,7 @@ protected void execute() {
             // STEP 4
             // ------
             // Get the list of databases ...
+            logger.info(""Step 4: read list of available databases"");
             final List<String> databaseNames = new ArrayList<>();
             sql.set(""SHOW DATABASES"");
             mysql.query(sql.get(), rs -> {
@@ -205,6 +215,7 @@ protected void execute() {
             // Get the list of table IDs for each database. We can't use a prepared statement with MySQL, so we have to
             // build the SQL statement each time. Although in other cases this might lead to SQL injection, in our case
             // we are reading the database names from the database and not taking them from the user ...
+            logger.info(""Step 5: read list of available tables in each database"");
             final List<TableId> tableIds = new ArrayList<>();
             final Map<String,List<TableId>> tableIdsByDbName = new HashMap<>();
             for (String dbName : databaseNames) {
@@ -225,6 +236,7 @@ protected void execute() {
             // ------
             // Transform the current schema so that it reflects the *current* state of the MySQL server's contents.
             // First, get the DROP TABLE and CREATE TABLE statement (with keys and constraint definitions) for our tables ...
+            logger.info(""Step 6: generating DROP and CREATE statements to reflect current database schemas"");
             final List<String> ddlStatements = new ArrayList<>();
             // Add DROP TABLE statements for all tables that we knew about AND those tables found in the databases ...
             Set<TableId> allTableIds = new HashSet<>(schema.tables().tableIds());
@@ -253,6 +265,7 @@ protected void execute() {
                 }
             }
             // Finally, apply the DDL statements to the schema and then update the record maker...
+            logger.debug(""Step 6b: applying DROP and CREATE statements to connector's table model"");
             String ddlStatementsStr = String.join("";"" + System.lineSeparator(), ddlStatements);
             schema.applyDdl(source, null, ddlStatementsStr, this::enqueueSchemaChanges);
             context.makeRecord().regenerate();
@@ -266,17 +279,25 @@ protected void execute() {
                 // should still use the MVCC snapshot obtained when we started our transaction (since we started it
                 // ""...with consistent snapshot""). So, since we're only doing very simple SELECT without WHERE predicates,
                 // we can release the lock now ...
+                logger.info(""Step 7: releasing global read lock to enable MySQL writes"");
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
+                long lockReleased = clock.currentTimeInMillis();
+                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased-lockAcquired));
             }
 
             // ------
             // STEP 8
             // ------
             // Dump all of the tables and generate source records ...
+            logger.info(""Step 8: scanning contents of {} tables"",tableIds.size());
+            long startScan = clock.currentTimeInMillis();
             AtomicBoolean interrupted = new AtomicBoolean(false);
+            int counter = 0;
             for (TableId tableId : tableIds) {
+                long start = clock.currentTimeInMillis();
+                logger.debug(""Step 8.{}: scanning table '{}'; {} tables remain"",++counter,tableId,tableIds.size()-counter);
                 sql.set(""SELECT * FROM "" + tableId);
                 mysql.query(sql.get(), rs -> {
                     RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, super::enqueueRecord);
@@ -300,32 +321,42 @@ protected void execute() {
                     }
                 });
                 if ( interrupted.get() ) break;
+                long stop = clock.currentTimeInMillis();
+                logger.info(""Step 8.{}: scanned table '{}' in {}"",counter,tableId,Strings.duration(stop-start));
             }
+            long stop = clock.currentTimeInMillis();
+            logger.info(""Step 8: scanned contents of {} tables in {}"",tableIds.size(),Strings.duration(stop-startScan));
 
             // ------
             // STEP 9
             // ------
             // Release the read lock if we have not yet done so ...
             if (!unlocked) {
+                logger.info(""Step 9: releasing global read lock to enable MySQL writes"");
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
+                long lockReleased = clock.currentTimeInMillis();
+                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased-lockAcquired));
             }
 
             // -------
             // STEP 10
             // -------
             if (interrupted.get()) {
                 // We were interrupted while reading the tables, so roll back the transaction and return immediately ...
+                logger.info(""Step 10: rolling back transaction after request to stop"");
                 sql.set(""ROLLBACK"");
                 mysql.execute(sql.get());
                 return;
             }
             // Otherwise, commit our transaction
+            logger.info(""Step 10: committing transaction"");
             sql.set(""COMMIT"");
             mysql.execute(sql.get());
 
             try {
+                logger.info(""Step 11: recording completion of snapshot"");
                 // Mark the source as having completed the snapshot. Because of this, **subsequent** source records
                 // produced by the connector (to any topic) will have a normal (not snapshot) offset ...
                 source.completeSnapshot();
@@ -338,7 +369,8 @@ protected void execute() {
             } finally {
                 // Set the completion flag ...
                 super.completeSuccessfully();
-                logger.info(""Completed snapshot for MySQL server {}"", context.serverName());
+                stop = clock.currentTimeInMillis();
+                logger.info(""Completed snapshot in {}"", Strings.duration(stop-ts));
             }
         } catch (Throwable e) {
             failed(e, ""Aborting snapshot after running '"" + sql.get() + ""': "" + e.getMessage());",2016-06-02T19:05:06Z,15
"@@ -2,7 +2,7 @@
 log4j.appender.stdout=org.apache.log4j.ConsoleAppender
 log4j.appender.stdout.Target=System.out
 log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
-log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p     %m (%c)%n
+log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n
 
 # Root logger option
 log4j.rootLogger=INFO, stdout",2016-06-02T19:05:06Z,89
"@@ -0,0 +1,100 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.util;
+
+import java.util.Map;
+
+import org.slf4j.MDC;
+
+/**
+ * A utility that provides a consistent set of properties for the Mapped Diagnostic Context (MDC) properties used by Debezium
+ * components.
+ * 
+ * @author Randall Hauch
+ * @since 0.2
+ */
+public class LoggingContext {
+
+    /**
+     * The key for the connector type MDC property.
+     */
+    public static final String CONNECTOR_TYPE = ""dbz.connectorType"";
+    /**
+     * The key for the connector logical name MDC property.
+     */
+    public static final String CONNECTOR_NAME = ""dbz.connectorName"";
+    /**
+     * The key for the connector context name MDC property.
+     */
+    public static final String CONNECTOR_CONTEXT = ""dbz.connectorContext"";
+
+    private LoggingContext() {
+    }
+    
+    /**
+     * A snapshot of an MDC context that can be {@link #restore()}.
+     */
+    public static final class PreviousContext {
+        private final Map<String,String> context;
+        @SuppressWarnings(""unchecked"")
+        protected PreviousContext() {
+            context = MDC.getCopyOfContextMap();
+        }
+        /**
+         * Restore this logging context.
+         */
+        public void restore() {
+            for ( Map.Entry<String, String> entry : context.entrySet() ) {
+                MDC.put(entry.getKey(), entry.getValue());
+            }
+        }
+    }
+
+    /**
+     * Configure for a connector the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * 
+     * @param connectorType the type of connector; may not be null
+     * @param connectorName the name of the connector; may not be null
+     * @param contextName the name of the context; may not be null
+     * @return the previous MDC context; never null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public static PreviousContext forConnector(String connectorType, String connectorName, String contextName) {
+        if (connectorType == null) throw new IllegalArgumentException(""The MDC value for the connector type may not be null"");
+        if (connectorName == null) throw new IllegalArgumentException(""The MDC value for the connector name may not be null"");
+        if (contextName == null) throw new IllegalArgumentException(""The MDC value for the connector context may not be null"");
+        PreviousContext previous = new PreviousContext();
+        MDC.put(CONNECTOR_TYPE, connectorType);
+        MDC.put(CONNECTOR_NAME, connectorName);
+        MDC.put(CONNECTOR_CONTEXT, contextName);
+        return previous;
+    }
+    
+    /**
+     * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
+     * state before this method was called.
+     * 
+     * @param connectorType the type of connector; may not be null
+     * @param connectorName the logical name of the connector; may not be null
+     * @param contextName the name of the context; may not be null
+     * @param operation the function to run in the new MDC context; may not be null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public static void temporarilyForConnector(String connectorType, String connectorName, String contextName, Runnable operation) {
+        if (connectorType == null) throw new IllegalArgumentException(""The MDC value for the connector type may not be null"");
+        if (connectorName == null) throw new IllegalArgumentException(""The MDC value for the connector name may not be null"");
+        if (contextName == null) throw new IllegalArgumentException(""The MDC value for the connector context may not be null"");
+        if (operation == null) throw new IllegalArgumentException(""The operation may not be null"");
+        PreviousContext previous = new PreviousContext();
+        try {
+            forConnector(connectorType,connectorName,contextName);
+            operation.run();
+        } finally {
+            previous.restore();
+        }
+    }
+    
+}",2016-06-02T19:05:06Z,137
"@@ -7,6 +7,8 @@
 
 import java.io.ByteArrayOutputStream;
 import java.io.PrintWriter;
+import java.math.BigDecimal;
+import java.text.DecimalFormat;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashSet;
@@ -36,11 +38,11 @@ public final class Strings {
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, Function<String, String[]> splitter, Function<String, T> factory) {
-        if ( input == null ) return Collections.emptySet();
+        if (input == null) return Collections.emptySet();
         Set<T> matches = new HashSet<>();
         for (String item : splitter.apply(input)) {
             T obj = factory.apply(item);
-            if ( obj != null ) matches.add(obj);
+            if (obj != null) matches.add(obj);
         }
         return matches;
     }
@@ -54,7 +56,7 @@ public static <T> Set<T> listOf(String input, Function<String, String[]> splitte
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, char delimiter, Function<String, T> factory) {
-        return listOf(input,(str) -> str.split(""["" + delimiter + ""]""),factory);
+        return listOf(input, (str) -> str.split(""["" + delimiter + ""]""), factory);
     }
 
     /**
@@ -65,7 +67,7 @@ public static <T> Set<T> listOf(String input, char delimiter, Function<String, T
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, Function<String, T> factory) {
-        return listOf(input,',',factory);
+        return listOf(input, ',', factory);
     }
 
     /**
@@ -76,7 +78,7 @@ public static <T> Set<T> listOf(String input, Function<String, T> factory) {
      * @return the list of regular expression {@link Pattern}s included in the list; never null
      */
     public static Set<Pattern> listOfRegex(String input) {
-        return listOf(input,',',Pattern::compile);
+        return listOf(input, ',', Pattern::compile);
     }
 
     /**
@@ -88,7 +90,7 @@ public static Set<Pattern> listOfRegex(String input) {
      * @return the list of regular expression {@link Pattern}s included in the list; never null
      */
     public static Set<Pattern> listOfRegex(String input, int regexFlags) {
-        return listOf(input,',',(str)->Pattern.compile(str,regexFlags));
+        return listOf(input, ',', (str) -> Pattern.compile(str, regexFlags));
     }
 
     /**
@@ -111,7 +113,7 @@ public static interface CharacterPredicate {
      * @param content the string content that is to be split
      * @return the list of lines; never null but may be an empty (unmodifiable) list if the supplied content is null or empty
      */
-    public static List<String> splitLines( final String content ) {
+    public static List<String> splitLines(final String content) {
         if (content == null || content.length() == 0) return Collections.emptyList();
         String[] lines = content.split(""[\\r]?\\n"");
         return Arrays.asList(lines);
@@ -410,8 +412,7 @@ public static int asInt(String value, int defaultValue) {
         if (value != null) {
             try {
                 return Integer.parseInt(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -427,8 +428,7 @@ public static long asLong(String value, long defaultValue) {
         if (value != null) {
             try {
                 return Long.parseLong(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -444,8 +444,7 @@ public static double asDouble(String value, double defaultValue) {
         if (value != null) {
             try {
                 return Double.parseDouble(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -461,12 +460,46 @@ public static boolean asBoolean(String value, boolean defaultValue) {
         if (value != null) {
             try {
                 return Boolean.parseBoolean(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
 
+    /**
+     * For the given duration in milliseconds, obtain a readable representation of the form {@code HHH:MM:SS.mmm}, where
+     * <dl>
+     * <dt>HHH</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""03"")</dd>
+     * <dt>MM</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""05"")</dd>
+     * <dt>SS</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""09"")</dd>
+     * <dt>mmm</dt>
+     * <dd>is the fractional part of seconds, written with 1-3 digits (any trailing zeros are dropped)</dd>
+     * </dl>
+     * 
+     * @param durationInMillis the duration in milliseconds
+     * @return the readable duration.
+     */
+    public static String duration(long durationInMillis) {
+        // Calculate how many seconds, and don't lose any information ...
+        BigDecimal bigSeconds = new BigDecimal(Math.abs(durationInMillis)).divide(new BigDecimal(1000));
+        // Calculate the minutes, and round to lose the seconds
+        int minutes = bigSeconds.intValue() / 60;
+        // Remove the minutes from the seconds, to just have the remainder of seconds
+        double dMinutes = minutes;
+        double seconds = bigSeconds.doubleValue() - dMinutes * 60;
+        // Now compute the number of full hours, and change 'minutes' to hold the remaining minutes
+        int hours = minutes / 60;
+        minutes = minutes - (hours * 60);
+
+        // Format the string, and have at least 2 digits for the hours, minutes and whole seconds,
+        // and between 3 and 6 digits for the fractional part of the seconds...
+        String result = new DecimalFormat(""######00"").format(hours) + ':' + new DecimalFormat(""00"").format(minutes) + ':'
+                + new DecimalFormat(""00.0##"").format(seconds);
+        return result;
+    }
+
     private Strings() {
     }
 }",2016-06-02T19:05:06Z,115
"@@ -25,12 +25,6 @@
             <groupId>org.apache.kafka</groupId>
             <artifactId>connect-api</artifactId>
         </dependency>
-
-        <dependency>
-            <groupId>io.confluent</groupId>
-            <artifactId>kafka-connect-avro-converter</artifactId>
-        </dependency>
-
         <dependency>
             <groupId>org.slf4j</groupId>
             <artifactId>slf4j-api</artifactId>
@@ -74,6 +68,10 @@
             <groupId>org.easytesting</groupId>
             <artifactId>fest-assert</artifactId>
         </dependency>
+        <dependency>
+            <groupId>io.confluent</groupId>
+            <artifactId>kafka-connect-avro-converter</artifactId>
+        </dependency>
     </dependencies>
     <properties>
         <!-- ",2016-06-02T21:54:21Z,97
"@@ -59,20 +59,23 @@ public class MySqlSchema {
     private final DatabaseHistory dbHistory;
     private final TableSchemaBuilder schemaBuilder;
     private final DdlChanges ddlChanges;
+    private final String serverName;
     private Tables tables;
 
     /**
      * Create a schema component given the supplied {@link MySqlConnectorConfig MySQL connector configuration}.
      * 
      * @param config the connector configuration, which is presumed to be valid
+     * @param serverName the name of the server
      */
-    public MySqlSchema(Configuration config) {
+    public MySqlSchema(Configuration config, String serverName) {
         this.filters = new Filters(config);
         this.ddlParser = new MySqlDdlParser(false);
         this.tables = new Tables();
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
         this.schemaBuilder = new TableSchemaBuilder();
+        this.serverName = serverName;
 
         // Create and configure the database history ...
         this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
@@ -247,7 +250,7 @@ protected void refreshSchemas() {
         // Create TableSchema instances for any existing table ...
         this.tables.tableIds().forEach(id -> {
             Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(table, filters.columnFilter(), filters.columnMappers());
+            TableSchema schema = schemaBuilder.create(serverName,table, filters.columnFilter(), filters.columnMappers());
             tableSchemaByTableId.put(id, schema);
         });
     }
@@ -317,7 +320,7 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
             if (table == null) { // removed
                 tableSchemaByTableId.remove(tableId);
             } else {
-                TableSchema schema = schemaBuilder.create(table, filters.columnFilter(), filters.columnMappers());
+                TableSchema schema = schemaBuilder.create(serverName, table, filters.columnFilter(), filters.columnMappers());
                 tableSchemaByTableId.put(tableId, schema);
             }
         });",2016-06-02T21:54:21Z,16
"@@ -48,7 +48,7 @@ public MySqlTaskContext(Configuration config) {
         this.source.setServerName(serverName());
 
         // Set up the MySQL schema ...
-        this.dbSchema = new MySqlSchema(config);
+        this.dbSchema = new MySqlSchema(config, serverName());
         this.dbSchema.start();
 
         // Set up the record processor ...
@@ -108,7 +108,11 @@ public long serverId() {
     }
 
     public String serverName() {
-        return config.getString(MySqlConnectorConfig.SERVER_NAME);
+        String serverName = config.getString(MySqlConnectorConfig.SERVER_NAME);
+        if ( serverName == null ) {
+            serverName = hostname() + "":"" + port();
+        }
+        return serverName;
     }
 
     public String username() {",2016-06-02T21:54:21Z,87
"@@ -161,7 +161,7 @@ public boolean assign(long tableNumber, TableId id) {
 
         String topicName = topicSelector.getTopic(id);
         Envelope envelope = Envelope.defineSchema()
-                                    .withName(topicName)
+                                    .withName(topicName + "".Envelope"")
                                     .withRecord(schema.schemaFor(id).valueSchema())
                                     .withSource(SourceInfo.SCHEMA)
                                     .build();",2016-06-02T21:54:21Z,128
"@@ -77,7 +77,8 @@ public Filters createFilters() {
     }
 
     public MySqlSchema createSchemas() {
-        return new MySqlSchema(configBuilder.build());
+        Configuration config = configBuilder.build();
+        return new MySqlSchema(config,config.getString(MySqlConnectorConfig.SERVER_NAME));
     }
 
 }
\ No newline at end of file",2016-06-02T21:54:21Z,35
"@@ -71,6 +71,11 @@
             <artifactId>connect-file</artifactId>
             <scope>test</scope>
         </dependency>
+        <dependency>
+            <groupId>io.confluent</groupId>
+            <artifactId>kafka-connect-avro-converter</artifactId>
+            <scope>test</scope>
+        </dependency>
     </dependencies>
     <build>
         <resources>",2016-06-02T21:54:21Z,138
"@@ -24,6 +24,16 @@ public class SchemaUtil {
     private SchemaUtil() {
     }
 
+    /**
+     * Obtain a JSON string representation of the specified field.
+     * 
+     * @param field the field; may not be null
+     * @return the JSON string representation
+     */
+    public static String asString(Object field) {
+        return new RecordWriter().append(field).toString();
+    }
+
     /**
      * Obtain a JSON string representation of the specified field.
      * ",2016-06-02T21:54:21Z,133
"@@ -105,11 +105,12 @@ public TableSchema create(ResultSet resultSet, String name) throws SQLException
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
      * @param table the table definition; may not be null
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
-    public TableSchema create(Table table) {
-        return create(table, null, null);
+    public TableSchema create(String schemaPrefix, Table table) {
+        return create(schemaPrefix, table, null, null);
     }
 
     /**
@@ -120,18 +121,20 @@ public TableSchema create(Table table) {
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
      * @param table the table definition; may not be null
      * @param filter the filter that specifies whether columns in the table should be included; may be null if all columns
      *            are to be included
      * @param mappers the mapping functions for columns; may be null if none of the columns are to be mapped to different values
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
-    public TableSchema create(Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
+    public TableSchema create(String schemaPrefix, Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
+        if ( schemaPrefix == null ) schemaPrefix = """";
         // Build the schemas ...
         final TableId tableId = table.id();
         final String tableIdStr = tableId.toString();
-        SchemaBuilder valSchemaBuilder = SchemaBuilder.struct().name(tableIdStr);
-        SchemaBuilder keySchemaBuilder = SchemaBuilder.struct().name(tableIdStr + ""/pk"");
+        SchemaBuilder valSchemaBuilder = SchemaBuilder.struct().name(schemaPrefix + tableIdStr + "".Value"");
+        SchemaBuilder keySchemaBuilder = SchemaBuilder.struct().name(schemaPrefix + tableIdStr + "".Key"");
         AtomicBoolean hasPrimaryKey = new AtomicBoolean(false);
         table.columns().forEach(column -> {
             if (table.isPrimaryKeyColumn(column.name())) {",2016-06-02T21:54:21Z,135
"@@ -26,6 +26,8 @@
 
 import static org.fest.assertions.Assertions.assertThat;
 
+import io.confluent.connect.avro.AvroConverter;
+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;
 import io.debezium.data.Envelope.FieldName;
 import io.debezium.data.Envelope.Operation;
 import io.debezium.util.Testing;
@@ -42,6 +44,10 @@ public class VerifyRecord {
     private static final JsonDeserializer keyJsonDeserializer = new JsonDeserializer();
     private static final JsonDeserializer valueJsonDeserializer = new JsonDeserializer();
 
+    private static final MockSchemaRegistryClient schemaRegistry = new MockSchemaRegistryClient();
+    private static final AvroConverter avroKeyConverter = new AvroConverter(schemaRegistry);
+    private static final AvroConverter avroValueConverter = new AvroConverter(schemaRegistry);
+    
     static {
         Map<String,Object> config = new HashMap<>();
         config.put(""schemas.enable"",Boolean.TRUE.toString());
@@ -50,6 +56,11 @@ public class VerifyRecord {
         keyJsonDeserializer.configure(config, true);
         valueJsonConverter.configure(config, false);
         valueJsonDeserializer.configure(config, false);
+
+        config = new HashMap<>();
+        config.put(""schema.registry.url"",""http://fake-url"");
+        avroKeyConverter.configure(config, false);
+        avroValueConverter.configure(config, false);
     }
 
     /**
@@ -263,6 +274,8 @@ public static void isValid(SourceRecord record) {
         JsonNode valueJson = null;
         SchemaAndValue keyWithSchema = null;
         SchemaAndValue valueWithSchema = null;
+        SchemaAndValue avroKeyWithSchema = null;
+        SchemaAndValue avroValueWithSchema = null;
         try {
             // The key should never be null ...
             assertThat(record.key()).isNotNull();
@@ -290,22 +303,43 @@ public static void isValid(SourceRecord record) {
             assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
             assertThat(valueWithSchema.value()).isEqualTo(record.value());
             schemaMatchesStruct(valueWithSchema);
+            
+            // Serialize and deserialize the key using the Avro converter, and check that we got the same result ...
+            byte[] avroKeyBytes = avroValueConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            avroKeyWithSchema = avroValueConverter.toConnectData(record.topic(), avroKeyBytes);
+            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            schemaMatchesStruct(keyWithSchema);
+
+            // Serialize and deserialize the value using the Avro converter, and check that we got the same result ...
+            byte[] avroValueBytes = avroValueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            avroValueWithSchema = avroValueConverter.toConnectData(record.topic(), avroValueBytes);
+            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
+            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            schemaMatchesStruct(valueWithSchema);
+            
         } catch (Throwable t) {
-            Testing.printError(t);
+            Testing.Print.enable();
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
-            if (keyJson != null) {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
-            } else if (keyWithSchema != null) {
-                Testing.print(""valid key with schema = "" + keyWithSchema);
+            Testing.printError(t);
+            if (keyJson == null ){
+                Testing.print(""error deserializing key from JSON: "" + SchemaUtil.asString(record.key()));
+            } else if (keyWithSchema == null ){
+                Testing.print(""error using JSON converter on key: "" + prettyJson(keyJson));
+            } else if (avroKeyWithSchema == null ){
+                Testing.print(""error using Avro converter on key: "" + prettyJson(keyJson));
             } else {
-                Testing.print(""invalid key"");
+                Testing.print(""valid key = "" + prettyJson(keyJson));
             }
-            if (valueJson != null) {
-                Testing.print(""valid value = "" + prettyJson(valueJson));
-            } else if (valueWithSchema != null) {
-                Testing.print(""valid value with schema = "" + valueWithSchema);
+
+            if (valueJson == null ){
+                Testing.print(""error deserializing value from JSON: "" + SchemaUtil.asString(record.value()));
+            } else if (valueWithSchema == null ){
+                Testing.print(""error using JSON converter on value: "" + prettyJson(valueJson));
+            } else if (avroValueWithSchema == null ){
+                Testing.print(""error using Avro converter on value: "" + prettyJson(valueJson));
             } else {
-                Testing.print(""invalid value"");
+                Testing.print(""valid key = "" + prettyJson(keyJson));
             }
             if (t instanceof AssertionError) throw t;
             fail(t.getMessage());",2016-06-02T21:54:21Z,92
"@@ -19,6 +19,7 @@
 
 public class TableSchemaBuilderTest {
 
+    private final String prefix = """";
     private final TableId id = new TableId(""catalog"", ""schema"", ""table"");
     private final Object[] data = new Object[] { ""c1value"", 3.142d, java.sql.Date.valueOf(""2001-10-31""), 4 };
     private Table table;
@@ -68,19 +69,19 @@ public void checkPreconditions() {
 
     @Test(expected = NullPointerException.class)
     public void shouldFailToBuildTableSchemaFromNullTable() {
-        new TableSchemaBuilder().create(null);
+        new TableSchemaBuilder().create(prefix,null);
     }
 
     @Test
     public void shouldBuildTableSchemaFromTable() {
-        schema = new TableSchemaBuilder().create(table);
+        schema = new TableSchemaBuilder().create(prefix,table);
         assertThat(schema).isNotNull();
     }
 
     @Test
     public void shouldBuildTableSchemaFromTableWithoutPrimaryKey() {
         table = table.edit().setPrimaryKeyNames().create();
-        schema = new TableSchemaBuilder().create(table);
+        schema = new TableSchemaBuilder().create(prefix,table);
         assertThat(schema).isNotNull();
         // Check the keys ...
         assertThat(schema.keySchema()).isNull();",2016-06-02T21:54:21Z,139
"@@ -50,13 +50,14 @@
         <!-- Kafka and it's dependencies MUST reflect what the Kafka version uses -->
         <version.kafka>0.9.0.1</version.kafka>
         <version.kafka.scala>2.11</version.kafka.scala>
-        <version.confluent.platform>2.1.0-alpha1</version.confluent.platform>
         <version.scala>2.11.7</version.scala>
         <version.curator>2.4.0</version.curator>
         <version.zookeeper>3.4.6</version.zookeeper>
         <version.jackson>2.5.4</version.jackson>
         <version.org.slf4j>1.7.6</version.org.slf4j>
         <version.log4j>1.2.17</version.log4j>
+        <!-- check new release version at https://github.com/confluentinc/schema-registry/releases -->
+        <version.confluent.platform>3.0.0</version.confluent.platform>
 
         <!-- Databases -->
         <version.postgresql.driver>9.4-1205-jdbc42</version.postgresql.driver>
@@ -185,7 +186,6 @@
             <dependency>
                 <groupId>io.confluent</groupId>
                 <artifactId>kafka-connect-avro-converter</artifactId>
-                <!-- check new release version at https://github.com/confluentinc/schema-registry/releases -->
                 <version>${version.confluent.platform}</version>
                 <scope>test</scope>
             </dependency>",2016-06-02T21:54:21Z,82
"@@ -16,6 +16,9 @@
           <excludes>
               <!-- Exclude all Kafka APIs and their dependencies, since they will be available in the runtime -->
               <exclude>org.apache.kafka:*</exclude>
+              <exclude>org.slf4j:*</exclude>
+              <exclude>log4j:log4j:*</exclude>
+              <exclude>com.fasterxml.jackson.core:jackson-core:*</exclude>
               <exclude>org.xerial.snappy:*</exclude>
               <exclude>net.jpountz.lz4:*</exclude>
           </excludes>",2016-03-17T16:03:27Z,96
"@@ -16,10 +16,6 @@
             <groupId>io.debezium</groupId>
             <artifactId>debezium-core</artifactId>
         </dependency>
-        <dependency>
-            <groupId>mysql</groupId>
-            <artifactId>mysql-connector-java</artifactId>
-        </dependency>
         <dependency>
             <groupId>com.github.shyiko</groupId>
             <artifactId>mysql-binlog-connector-java</artifactId>
@@ -50,6 +46,11 @@
             <artifactId>debezium-embedded</artifactId>
             <scope>test</scope>
         </dependency>
+        <dependency>
+            <groupId>mysql</groupId>
+            <artifactId>mysql-connector-java</artifactId>
+            <scope>test</scope>
+        </dependency>
         <dependency>
             <groupId>org.slf4j</groupId>
             <artifactId>slf4j-log4j12</artifactId>",2016-03-17T16:03:27Z,97
"@@ -45,18 +45,21 @@
         <maven.compiler.source>1.8</maven.compiler.source>
         <maven.compiler.target>1.8</maven.compiler.target>
 
-        <!-- Major dependencies -->
-        <version.jackson>2.4.0</version.jackson>
-
-        <!-- Logging -->
-        <version.org.slf4j>1.7.2</version.org.slf4j>
+        <!-- Kafka and it's dependencies MUST reflect what the Kafka version uses -->
+        <version.kafka>0.9.0.1</version.kafka>
+        <version.kafka.scala>2.11</version.kafka.scala>
+        <version.scala>2.11.7</version.scala>
+        <version.curator>2.4.0</version.curator>
+        <version.zookeeper>3.4.6</version.zookeeper>
+        <version.jackson>2.5.4</version.jackson>
+        <version.org.slf4j>1.7.6</version.org.slf4j>
         <version.log4j>1.2.17</version.log4j>
 
         <!-- Databases -->
         <version.postgresql.driver>9.4-1205-jdbc42</version.postgresql.driver>
         <version.postgresql.server>9.4</version.postgresql.server>
         <version.mysql.server>5.7</version.mysql.server>
-        <version.mysql.driver>5.1.37</version.mysql.driver>
+        <version.mysql.driver>5.1.38</version.mysql.driver>
         <version.mysql.binlog>0.2.4</version.mysql.binlog>
 
         <!-- Testing -->
@@ -75,13 +78,6 @@
         <!-- Dockerfiles -->
         <docker.maintainer>Debezium community</docker.maintainer>
 
-        <!-- Kafka -->
-        <version.kafka>0.9.0.1</version.kafka>
-        <version.kafka.scala>2.11</version.kafka.scala>
-        <version.scala>2.11.7</version.scala>
-        <version.curator>2.4.0</version.curator>
-        <version.zookeeper>3.4.6</version.zookeeper>
-
         <!--Skip long running tests by default-->
         <skipLongRunningTests>true</skipLongRunningTests>
 ",2016-03-17T16:03:27Z,82
"@@ -102,7 +102,9 @@ public void shouldConsumeAllEventsFromDatabase() throws SQLException, Interrupte
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         
         records.ddlRecordsForDatabase(""connector_test"").forEach(this::print);
-        //records.ddlRecordsForDatabase(""readbinlog_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
         
         // Make sure there are no more ...
         Testing.Print.disable();
@@ -200,6 +202,9 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
         assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.customers"").size()).isEqualTo(4);
         assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.orders"").size()).isEqualTo(5);
         assertThat(records.topics().size()).isEqualTo(4);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
         
         // More records may have been written (if this method were run after the others), but we don't care ...
         stopConnector();",2016-05-19T22:06:22Z,88
"@@ -24,7 +24,9 @@
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.DataException;
 import org.apache.kafka.connect.json.JsonConverter;
 import org.apache.kafka.connect.json.JsonDeserializer;
 import org.apache.kafka.connect.source.SourceConnector;
@@ -202,13 +204,13 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
-                               .notifying((record)->{
+                               .notifying((record) -> {
                                    try {
                                        consumedLines.put(record);
                                    } catch ( InterruptedException e ) {
                                        Thread.interrupted();
                                    }
-                                })
+                               })
                                .using(this.getClass().getClassLoader())
                                .using(wrapperCallback)
                                .build();
@@ -320,6 +322,12 @@ public List<SourceRecord> recordsForTopic( String topicName ) {
         public Set<String> topics() {
             return recordsByTopic.keySet();
         }
+        public void forEachInTopic(String topic, Consumer<SourceRecord> consumer) {
+            recordsForTopic(topic).forEach(consumer);
+        }
+        public void forEach( Consumer<SourceRecord> consumer) {
+            records.forEach(consumer);
+        }
         public void print() {
             Testing.print("""" + topics().size() + "" topics: "" + topics());
             recordsByTopic.forEach((k,v)->{
@@ -436,6 +444,100 @@ protected void assertTombstone(SourceRecord record) {
         assertThat(record.valueSchema()).isNull();
     }
 
+    /**
+     * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
+     * matches that of the supplied {@code schema}.
+     * 
+     * @param value the value with a schema; may not be null
+     */
+    protected void assertSchemaMatchesStruct(SchemaAndValue value) {
+        Object val = value.value();
+        assertThat(val).isInstanceOf(Struct.class);
+        assertSchemaMatchesStruct((Struct) val, value.schema());
+    }
+
+    /**
+     * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
+     * matches that of the supplied {@code schema}.
+     * 
+     * @param struct the {@link Struct} to validate; may not be null
+     * @param schema the expected schema of the {@link Struct}; may not be null
+     */
+    protected void assertSchemaMatchesStruct(Struct struct, Schema schema) {
+        // First validate the struct itself ...
+        try {
+            struct.validate();
+        } catch (DataException e) {
+            throw new AssertionError(""The struct '"" + struct + ""' failed to validate"", e);
+        }
+
+        Schema actualSchema = struct.schema();
+        assertThat(actualSchema).isEqualTo(schema);
+        assertFieldsInSchema(struct,schema);
+    }
+
+    private void assertFieldsInSchema(Struct struct, Schema schema ) {
+        schema.fields().forEach(field->{
+            Object val1 = struct.get(field);
+            Object val2 = struct.get(field.name());
+            assertThat(val1).isSameAs(val2);
+            if ( val1 instanceof Struct ) {
+                assertFieldsInSchema((Struct)val1,field.schema());
+            }
+        });
+    }
+    
+    /**
+     * Validate that a {@link SourceRecord}'s key and value can each be converted to a byte[] and then back to an equivalent
+     * {@link SourceRecord}.
+     * 
+     * @param record the record to validate; may not be null
+     */
+    protected void validate(SourceRecord record) {
+        print(record);
+
+        JsonNode keyJson = null;
+        JsonNode valueJson = null;
+        SchemaAndValue keyWithSchema = null;
+        SchemaAndValue valueWithSchema = null;
+        try {
+            // First serialize and deserialize the key ...
+            byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            assertSchemaMatchesStruct(keyWithSchema);
+
+            // then the value ...
+            byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
+            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
+            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            assertSchemaMatchesStruct(valueWithSchema);
+        } catch (Throwable t) {
+            Testing.printError(t);
+            Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
+            if (keyJson != null) {
+                Testing.print(""valid key = "" + prettyJson(keyJson));
+            } else if (keyWithSchema != null) {
+                Testing.print(""valid key with schema = "" + keyWithSchema);
+            } else {
+                Testing.print(""invalid key"");
+            }
+            if (valueJson != null) {
+                Testing.print(""valid value = "" + prettyJson(valueJson));
+            } else if (valueWithSchema != null) {
+                Testing.print(""valid value with schema = "" + valueWithSchema);
+            } else {
+                Testing.print(""invalid value"");
+            }
+            if (t instanceof AssertionError) throw t;
+            fail(t.getMessage());
+        }
+    }
+
     protected void print(SourceRecord record) {
         StringBuilder sb = new StringBuilder(""SourceRecord{"");
         sb.append(""sourcePartition="").append(record.sourcePartition());
@@ -470,12 +572,12 @@ protected void printJson(SourceRecord record) {
         } catch (Throwable t) {
             Testing.printError(t);
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
-            if ( keyJson != null ) {
+            if (keyJson != null) {
                 Testing.print(""valid key = "" + prettyJson(keyJson));
             } else {
                 Testing.print(""invalid key"");
             }
-            if ( valueJson != null ) {
+            if (valueJson != null) {
                 Testing.print(""valid value = "" + prettyJson(valueJson));
             } else {
                 Testing.print(""invalid value"");",2016-05-19T22:06:22Z,61
"@@ -102,7 +102,9 @@ public void shouldConsumeAllEventsFromDatabase() throws SQLException, Interrupte
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         
         records.ddlRecordsForDatabase(""connector_test"").forEach(this::print);
-        //records.ddlRecordsForDatabase(""readbinlog_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
         
         // Make sure there are no more ...
         Testing.Print.disable();
@@ -200,6 +202,9 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
         assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.customers"").size()).isEqualTo(4);
         assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.orders"").size()).isEqualTo(5);
         assertThat(records.topics().size()).isEqualTo(4);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
         
         // More records may have been written (if this method were run after the others), but we don't care ...
         stopConnector();",2016-05-19T22:06:22Z,88
"@@ -24,7 +24,9 @@
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.DataException;
 import org.apache.kafka.connect.json.JsonConverter;
 import org.apache.kafka.connect.json.JsonDeserializer;
 import org.apache.kafka.connect.source.SourceConnector;
@@ -202,13 +204,13 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
-                               .notifying((record)->{
+                               .notifying((record) -> {
                                    try {
                                        consumedLines.put(record);
                                    } catch ( InterruptedException e ) {
                                        Thread.interrupted();
                                    }
-                                })
+                               })
                                .using(this.getClass().getClassLoader())
                                .using(wrapperCallback)
                                .build();
@@ -320,6 +322,12 @@ public List<SourceRecord> recordsForTopic( String topicName ) {
         public Set<String> topics() {
             return recordsByTopic.keySet();
         }
+        public void forEachInTopic(String topic, Consumer<SourceRecord> consumer) {
+            recordsForTopic(topic).forEach(consumer);
+        }
+        public void forEach( Consumer<SourceRecord> consumer) {
+            records.forEach(consumer);
+        }
         public void print() {
             Testing.print("""" + topics().size() + "" topics: "" + topics());
             recordsByTopic.forEach((k,v)->{
@@ -436,6 +444,100 @@ protected void assertTombstone(SourceRecord record) {
         assertThat(record.valueSchema()).isNull();
     }
 
+    /**
+     * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
+     * matches that of the supplied {@code schema}.
+     * 
+     * @param value the value with a schema; may not be null
+     */
+    protected void assertSchemaMatchesStruct(SchemaAndValue value) {
+        Object val = value.value();
+        assertThat(val).isInstanceOf(Struct.class);
+        assertSchemaMatchesStruct((Struct) val, value.schema());
+    }
+
+    /**
+     * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
+     * matches that of the supplied {@code schema}.
+     * 
+     * @param struct the {@link Struct} to validate; may not be null
+     * @param schema the expected schema of the {@link Struct}; may not be null
+     */
+    protected void assertSchemaMatchesStruct(Struct struct, Schema schema) {
+        // First validate the struct itself ...
+        try {
+            struct.validate();
+        } catch (DataException e) {
+            throw new AssertionError(""The struct '"" + struct + ""' failed to validate"", e);
+        }
+
+        Schema actualSchema = struct.schema();
+        assertThat(actualSchema).isEqualTo(schema);
+        assertFieldsInSchema(struct,schema);
+    }
+
+    private void assertFieldsInSchema(Struct struct, Schema schema ) {
+        schema.fields().forEach(field->{
+            Object val1 = struct.get(field);
+            Object val2 = struct.get(field.name());
+            assertThat(val1).isSameAs(val2);
+            if ( val1 instanceof Struct ) {
+                assertFieldsInSchema((Struct)val1,field.schema());
+            }
+        });
+    }
+    
+    /**
+     * Validate that a {@link SourceRecord}'s key and value can each be converted to a byte[] and then back to an equivalent
+     * {@link SourceRecord}.
+     * 
+     * @param record the record to validate; may not be null
+     */
+    protected void validate(SourceRecord record) {
+        print(record);
+
+        JsonNode keyJson = null;
+        JsonNode valueJson = null;
+        SchemaAndValue keyWithSchema = null;
+        SchemaAndValue valueWithSchema = null;
+        try {
+            // First serialize and deserialize the key ...
+            byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            assertSchemaMatchesStruct(keyWithSchema);
+
+            // then the value ...
+            byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
+            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
+            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            assertSchemaMatchesStruct(valueWithSchema);
+        } catch (Throwable t) {
+            Testing.printError(t);
+            Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
+            if (keyJson != null) {
+                Testing.print(""valid key = "" + prettyJson(keyJson));
+            } else if (keyWithSchema != null) {
+                Testing.print(""valid key with schema = "" + keyWithSchema);
+            } else {
+                Testing.print(""invalid key"");
+            }
+            if (valueJson != null) {
+                Testing.print(""valid value = "" + prettyJson(valueJson));
+            } else if (valueWithSchema != null) {
+                Testing.print(""valid value with schema = "" + valueWithSchema);
+            } else {
+                Testing.print(""invalid value"");
+            }
+            if (t instanceof AssertionError) throw t;
+            fail(t.getMessage());
+        }
+    }
+
     protected void print(SourceRecord record) {
         StringBuilder sb = new StringBuilder(""SourceRecord{"");
         sb.append(""sourcePartition="").append(record.sourcePartition());
@@ -470,12 +572,12 @@ protected void printJson(SourceRecord record) {
         } catch (Throwable t) {
             Testing.printError(t);
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
-            if ( keyJson != null ) {
+            if (keyJson != null) {
                 Testing.print(""valid key = "" + prettyJson(keyJson));
             } else {
                 Testing.print(""invalid key"");
             }
-            if ( valueJson != null ) {
+            if (valueJson != null) {
                 Testing.print(""valid value = "" + prettyJson(valueJson));
             } else {
                 Testing.print(""invalid value"");",2016-05-19T22:06:22Z,61
"@@ -578,7 +578,9 @@ protected void parseIndexOptions(Marker start) {
     protected void parseReferenceDefinition(Marker start) {
         tokens.consume(""REFERENCES"");
         parseSchemaQualifiedName(start); // table name
-        parseColumnNameList(start);
+        if (tokens.matches('(')) {
+            parseColumnNameList(start);
+        }
         if (tokens.canConsume(""MATCH"")) {
             tokens.consumeAnyOf(""FULL"", ""PARTIAL"", ""SIMPLE"");
             if (tokens.canConsume(""ON"")) {",2016-05-13T14:32:47Z,33
"@@ -150,28 +150,44 @@ public void shouldParseGrantStatement() {
     public void shouldParseCreateStatements() {
         parser.parse(readFile(""ddl/mysql-test-create.ddl""), tables);
         Testing.print(tables);
-        assertThat(tables.size()).isEqualTo(57); // no tables
+        assertThat(tables.size()).isEqualTo(57);
         assertThat(listener.total()).isEqualTo(144);
     }
 
     @Test
     public void shouldParseTestStatements() {
         parser.parse(readFile(""ddl/mysql-test-statements.ddl""), tables);
         Testing.print(tables);
-        assertThat(tables.size()).isEqualTo(6); // no tables
+        assertThat(tables.size()).isEqualTo(6);
         assertThat(listener.total()).isEqualTo(49);
         //listener.forEach(this::printEvent);
     }
 
     @Test
     public void shouldParseSomeLinesFromCreateStatements() {
         parser.parse(readLines(189,""ddl/mysql-test-create.ddl""), tables);
-        assertThat(tables.size()).isEqualTo(39); // no tables
+        assertThat(tables.size()).isEqualTo(39);
         assertThat(listener.total()).isEqualTo(120);
     }
+
+    @Test
+    public void shouldParseMySql56InitializationStatements() {
+        parser.parse(readLines(1,""ddl/mysql-test-init-5.6.ddl""), tables);
+        assertThat(tables.size()).isEqualTo(85); // 1 table
+        assertThat(listener.total()).isEqualTo(112);
+        listener.forEach(this::printEvent);
+    }
+
+    @Test
+    public void shouldParseMySql57InitializationStatements() {
+        parser.parse(readLines(1,""ddl/mysql-test-init-5.7.ddl""), tables);
+        assertThat(tables.size()).isEqualTo(123);
+        assertThat(listener.total()).isEqualTo(125);
+        listener.forEach(this::printEvent);
+    }
     
     protected void printEvent( Event event ) {
-        System.out.println(event);
+        Testing.print(event);
     }
 
     protected String readFile( String classpathResource ) {",2016-05-13T14:32:47Z,36
"@@ -239,6 +239,29 @@ public void shouldCaptureMultipleWriteUpdateDeletesInSingleEvents() throws Excep
                                                  .removedRow(""Jamie"", 19, any(), any()));
     }
 
+    /**
+     * Test case that is normally commented out since it is only useful to print out the DDL statements recorded by
+     * the binlog during a MySQL server initialization and startup.
+     * 
+     * @throws Exception if there are problems
+     */
+    @Ignore
+    @Test
+    public void shouldCaptureQueryEventData() throws Exception {
+        // Testing.Print.enable();
+        startClient(client -> {
+            client.setBinlogFilename(""mysql-bin.000001"");
+            client.setBinlogPosition(4);
+        });
+        counters.consumeAll(5, TimeUnit.SECONDS);
+        List<QueryEventData> allQueryEvents = recordedEventData(QueryEventData.class, -1);
+        allQueryEvents.forEach(event -> {
+            String sql = event.getSql();
+            if (sql.equalsIgnoreCase(""BEGIN"") || sql.equalsIgnoreCase(""COMMIT"")) return;
+            System.out.println(event.getSql());
+        });
+    }
+
     @Test
     public void shouldQueryInformationSchema() throws Exception {
         // long tableId = writeRows.getTableId();",2016-05-13T14:32:47Z,12
"@@ -0,0 +1,227 @@
+--
+-- Statements recorded by binlog during MySQL 5.6 initialization with Debezium scripts.
+--
+CREATE TABLE IF NOT EXISTS db (   Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Select_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Insert_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Update_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Delete_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Drop_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Grant_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, References_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Index_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tmp_table_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Lock_tables_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Execute_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Event_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Trigger_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY Host (Host,Db,User), KEY User (User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='Database privileges';
+CREATE TABLE IF NOT EXISTS user (   Host char(60) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Password char(41) character set latin1 collate latin1_bin DEFAULT '' NOT NULL, Select_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Insert_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Update_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Delete_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Drop_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Reload_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Shutdown_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Process_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, File_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Grant_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, References_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Index_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_db_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Super_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tmp_table_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Lock_tables_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Execute_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Repl_slave_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Repl_client_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_user_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Event_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Trigger_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tablespace_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, ssl_type enum('','ANY','X509', 'SPECIFIED') COLLATE utf8_general_ci DEFAULT '' NOT NULL, ssl_cipher BLOB NOT NULL, x509_issuer BLOB NOT NULL, x509_subject BLOB NOT NULL, max_questions int(11) unsigned DEFAULT 0  NOT NULL, max_updates int(11) unsigned DEFAULT 0  NOT NULL, max_connections int(11) unsigned DEFAULT 0  NOT NULL, max_user_connections int(11) unsigned DEFAULT 0  NOT NULL, plugin char(64) DEFAULT 'mysql_native_password', authentication_string TEXT, password_expired ENUM('N', 'Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY Host (Host,User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='Users and global privileges';
+CREATE TABLE IF NOT EXISTS func (  name char(64) binary DEFAULT '' NOT NULL, ret tinyint(1) DEFAULT '0' NOT NULL, dl char(128) DEFAULT '' NOT NULL, type enum ('function','aggregate') COLLATE utf8_general_ci NOT NULL, PRIMARY KEY (name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='User defined functions';
+CREATE TABLE IF NOT EXISTS plugin ( name varchar(64) DEFAULT '' NOT NULL, dl varchar(128) DEFAULT '' NOT NULL, PRIMARY KEY (name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_general_ci comment='MySQL plugins';
+CREATE TABLE IF NOT EXISTS servers ( Server_name char(64) NOT NULL DEFAULT '', Host char(64) NOT NULL DEFAULT '', Db char(64) NOT NULL DEFAULT '', Username char(64) NOT NULL DEFAULT '', Password char(64) NOT NULL DEFAULT '', Port INT(4) NOT NULL DEFAULT '0', Socket char(64) NOT NULL DEFAULT '', Wrapper char(64) NOT NULL DEFAULT '', Owner char(64) NOT NULL DEFAULT '', PRIMARY KEY (Server_name)) CHARACTER SET utf8 comment='MySQL Foreign Servers table';
+CREATE TABLE IF NOT EXISTS tables_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Table_name char(64) binary DEFAULT '' NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, Table_priv set('Select','Insert','Update','Delete','Create','Drop','Grant','References','Index','Alter','Create View','Show view','Trigger') COLLATE utf8_general_ci DEFAULT '' NOT NULL, Column_priv set('Select','Insert','Update','References') COLLATE utf8_general_ci DEFAULT '' NOT NULL, PRIMARY KEY (Host,Db,User,Table_name), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Table privileges';
+CREATE TABLE IF NOT EXISTS columns_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Table_name char(64) binary DEFAULT '' NOT NULL, Column_name char(64) binary DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, Column_priv set('Select','Insert','Update','References') COLLATE utf8_general_ci DEFAULT '' NOT NULL, PRIMARY KEY (Host,Db,User,Table_name,Column_name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Column privileges';
+CREATE TABLE IF NOT EXISTS help_topic ( help_topic_id int unsigned not null, name char(64) not null, help_category_id smallint unsigned not null, description text not null, example text not null, url text not null, primary key (help_topic_id), unique index (name) ) engine=MyISAM CHARACTER SET utf8 comment='help topics';
+CREATE TABLE IF NOT EXISTS help_category ( help_category_id smallint unsigned not null, name  char(64) not null, parent_category_id smallint unsigned null, url text not null, primary key (help_category_id), unique index (name) ) engine=MyISAM CHARACTER SET utf8 comment='help categories';
+CREATE TABLE IF NOT EXISTS help_relation ( help_topic_id int unsigned not null references help_topic, help_keyword_id  int unsigned not null references help_keyword, primary key (help_keyword_id, help_topic_id) ) engine=MyISAM CHARACTER SET utf8 comment='keyword-topic relation';
+CREATE TABLE IF NOT EXISTS help_keyword (   help_keyword_id  int unsigned not null, name char(64) not null, primary key (help_keyword_id), unique index (name) ) engine=MyISAM CHARACTER SET utf8 comment='help keywords';
+CREATE TABLE IF NOT EXISTS time_zone_name (   Name char(64) NOT NULL, Time_zone_id int unsigned NOT NULL, PRIMARY KEY Name (Name) ) engine=MyISAM CHARACTER SET utf8   comment='Time zone names';
+CREATE TABLE IF NOT EXISTS time_zone (   Time_zone_id int unsigned NOT NULL auto_increment, Use_leap_seconds enum('Y','N') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY TzId (Time_zone_id) ) engine=MyISAM CHARACTER SET utf8   comment='Time zones';
+CREATE TABLE IF NOT EXISTS time_zone_transition (   Time_zone_id int unsigned NOT NULL, Transition_time bigint signed NOT NULL, Transition_type_id int unsigned NOT NULL, PRIMARY KEY TzIdTranTime (Time_zone_id, Transition_time) ) engine=MyISAM CHARACTER SET utf8   comment='Time zone transitions';
+CREATE TABLE IF NOT EXISTS time_zone_transition_type (   Time_zone_id int unsigned NOT NULL, Transition_type_id int unsigned NOT NULL, Offset int signed DEFAULT 0 NOT NULL, Is_DST tinyint unsigned DEFAULT 0 NOT NULL, Abbreviation char(8) DEFAULT '' NOT NULL, PRIMARY KEY TzIdTrTId (Time_zone_id, Transition_type_id) ) engine=MyISAM CHARACTER SET utf8   comment='Time zone transition types';
+CREATE TABLE IF NOT EXISTS time_zone_leap_second (   Transition_time bigint signed NOT NULL, Correction int signed NOT NULL, PRIMARY KEY TranTime (Transition_time) ) engine=MyISAM CHARACTER SET utf8   comment='Leap seconds information for time zones';
+CREATE TABLE IF NOT EXISTS proc (db char(64) collate utf8_bin DEFAULT '' NOT NULL, name char(64) DEFAULT '' NOT NULL, type enum('FUNCTION','PROCEDURE') NOT NULL, specific_name char(64) DEFAULT '' NOT NULL, language enum('SQL') DEFAULT 'SQL' NOT NULL, sql_data_access enum( 'CONTAINS_SQL', 'NO_SQL', 'READS_SQL_DATA', 'MODIFIES_SQL_DATA') DEFAULT 'CONTAINS_SQL' NOT NULL, is_deterministic enum('YES','NO') DEFAULT 'NO' NOT NULL, security_type enum('INVOKER','DEFINER') DEFAULT 'DEFINER' NOT NULL, param_list blob NOT NULL, returns longblob DEFAULT '' NOT NULL, body longblob NOT NULL, definer char(77) collate utf8_bin DEFAULT '' NOT NULL, created timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, modified timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', sql_mode set( 'REAL_AS_FLOAT', 'PIPES_AS_CONCAT', 'ANSI_QUOTES', 'IGNORE_SPACE', 'NOT_USED', 'ONLY_FULL_GROUP_BY', 'NO_UNSIGNED_SUBTRACTION', 'NO_DIR_IN_CREATE', 'POSTGRESQL', 'ORACLE', 'MSSQL', 'DB2', 'MAXDB', 'NO_KEY_OPTIONS', 'NO_TABLE_OPTIONS', 'NO_FIELD_OPTIONS', 'MYSQL323', 'MYSQL40', 'ANSI', 'NO_AUTO_VALUE_ON_ZERO', 'NO_BACKSLASH_ESCAPES', 'STRICT_TRANS_TABLES', 'STRICT_ALL_TABLES', 'NO_ZERO_IN_DATE', 'NO_ZERO_DATE', 'INVALID_DATES', 'ERROR_FOR_DIVISION_BY_ZERO', 'TRADITIONAL', 'NO_AUTO_CREATE_USER', 'HIGH_NOT_PRECEDENCE', 'NO_ENGINE_SUBSTITUTION', 'PAD_CHAR_TO_FULL_LENGTH') DEFAULT '' NOT NULL, comment text collate utf8_bin NOT NULL, character_set_client char(32) collate utf8_bin, collation_connection char(32) collate utf8_bin, db_collation char(32) collate utf8_bin, body_utf8 longblob, PRIMARY KEY (db,name,type)) engine=MyISAM character set utf8 comment='Stored Procedures';
+CREATE TABLE IF NOT EXISTS procs_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Routine_name char(64) COLLATE utf8_general_ci DEFAULT '' NOT NULL, Routine_type enum('FUNCTION','PROCEDURE') NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Proc_priv set('Execute','Alter Routine','Grant') COLLATE utf8_general_ci DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (Host,Db,User,Routine_name,Routine_type), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Procedure privileges';
+CREATE TABLE IF NOT EXISTS general_log (event_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, user_host MEDIUMTEXT NOT NULL, thread_id BIGINT(21) UNSIGNED NOT NULL, server_id INTEGER UNSIGNED NOT NULL, command_type VARCHAR(64) NOT NULL, argument MEDIUMTEXT NOT NULL) engine=CSV CHARACTER SET utf8 comment=""General log""
+CREATE TABLE IF NOT EXISTS slow_log (start_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, user_host MEDIUMTEXT NOT NULL, query_time TIME NOT NULL, lock_time TIME NOT NULL, rows_sent INTEGER NOT NULL, rows_examined INTEGER NOT NULL, db VARCHAR(512) NOT NULL, last_insert_id INTEGER NOT NULL, insert_id INTEGER NOT NULL, server_id INTEGER UNSIGNED NOT NULL, sql_text MEDIUMTEXT NOT NULL, thread_id BIGINT(21) UNSIGNED NOT NULL) engine=CSV CHARACTER SET utf8 comment=""Slow log""
+CREATE TABLE IF NOT EXISTS event ( db char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', name char(64) CHARACTER SET utf8 NOT NULL default '', body longblob NOT NULL, definer char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', execute_at DATETIME default NULL, interval_value int(11) default NULL, interval_field ENUM('YEAR','QUARTER','MONTH','DAY','HOUR','MINUTE','WEEK','SECOND','MICROSECOND','YEAR_MONTH','DAY_HOUR','DAY_MINUTE','DAY_SECOND','HOUR_MINUTE','HOUR_SECOND','MINUTE_SECOND','DAY_MICROSECOND','HOUR_MICROSECOND','MINUTE_MICROSECOND','SECOND_MICROSECOND') default NULL, created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, modified TIMESTAMP NOT NULL DEFAULT '0000-00-00 00:00:00', last_executed DATETIME default NULL, starts DATETIME default NULL, ends DATETIME default NULL, status ENUM('ENABLED','DISABLED','SLAVESIDE_DISABLED') NOT NULL default 'ENABLED', on_completion ENUM('DROP','PRESERVE') NOT NULL default 'DROP', sql_mode  set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','NOT_USED','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_ALL_TABLES','NO_ZERO_IN_DATE','NO_ZERO_DATE','INVALID_DATES','ERROR_FOR_DIVISION_BY_ZERO','TRADITIONAL','NO_AUTO_CREATE_USER','HIGH_NOT_PRECEDENCE','NO_ENGINE_SUBSTITUTION','PAD_CHAR_TO_FULL_LENGTH') DEFAULT '' NOT NULL, comment char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', originator INTEGER UNSIGNED NOT NULL, time_zone char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM', character_set_client char(32) collate utf8_bin, collation_connection char(32) collate utf8_bin, db_collation char(32) collate utf8_bin, body_utf8 longblob, PRIMARY KEY (db, name) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT 'Events';
+CREATE TABLE IF NOT EXISTS ndb_binlog_index (Position BIGINT UNSIGNED NOT NULL, File VARCHAR(255) NOT NULL, epoch BIGINT UNSIGNED NOT NULL, inserts INT UNSIGNED NOT NULL, updates INT UNSIGNED NOT NULL, deletes INT UNSIGNED NOT NULL, schemaops INT UNSIGNED NOT NULL, orig_server_id INT UNSIGNED NOT NULL, orig_epoch BIGINT UNSIGNED NOT NULL, gci INT UNSIGNED NOT NULL, PRIMARY KEY(epoch, orig_server_id, orig_epoch)) ENGINE=MYISAM;
+CREATE TABLE IF NOT EXISTS innodb_table_stats (
+    database_name           VARCHAR(64) NOT NULL,
+    table_name          VARCHAR(64) NOT NULL,
+    last_update         TIMESTAMP NOT NULL NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+    n_rows              BIGINT UNSIGNED NOT NULL,
+    clustered_index_size        BIGINT UNSIGNED NOT NULL,
+    sum_of_other_index_sizes    BIGINT UNSIGNED NOT NULL,
+    PRIMARY KEY (database_name, table_name)
+) ENGINE=INNODB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0
+CREATE TABLE IF NOT EXISTS innodb_index_stats (
+    database_name           VARCHAR(64) NOT NULL,
+    table_name          VARCHAR(64) NOT NULL,
+    index_name          VARCHAR(64) NOT NULL,
+    last_update         TIMESTAMP NOT NULL NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+    /* there are at least:
+    stat_name='size'
+    stat_name='n_leaf_pages'
+    stat_name='n_diff_pfx%' */
+    stat_name           VARCHAR(64) NOT NULL,
+    stat_value          BIGINT UNSIGNED NOT NULL,
+    sample_size         BIGINT UNSIGNED,
+    stat_description        VARCHAR(1024) NOT NULL,
+    PRIMARY KEY (database_name, table_name, index_name, stat_name)
+) ENGINE=INNODB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0
+CREATE TABLE IF NOT EXISTS slave_relay_log_info (
+  Number_of_lines INTEGER UNSIGNED NOT NULL COMMENT 'Number of lines in the file or rows in the table. Used to version table definitions.',
+  Relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the current relay log file.',
+  Relay_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The relay log position of the last executed event.',
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log file from which the events in the relay log file were read.',
+  Master_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The master log position of the last executed event.',
+  Sql_delay INTEGER NOT NULL COMMENT 'The number of seconds that the slave must lag behind the master.',
+  Number_of_workers INTEGER UNSIGNED NOT NULL,
+  Id INTEGER UNSIGNED NOT NULL COMMENT 'Internal Id that uniquely identifies this record.',
+  PRIMARY KEY(Id)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Relay Log Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS slave_master_info (
+  Number_of_lines INTEGER UNSIGNED NOT NULL COMMENT 'Number of lines in the file.',
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log currently being read from the master.',
+  Master_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The master log position of the last read event.',
+  Host CHAR(64) CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The host name of the master.',
+  User_name TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The user name used to connect to the master.',
+  User_password TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The password used to connect to the master.',
+  Port INTEGER UNSIGNED NOT NULL COMMENT 'The network port used to connect to the master.',
+  Connect_retry INTEGER UNSIGNED NOT NULL COMMENT 'The period (in seconds) that the slave will wait before trying to reconnect to the master.',
+  Enabled_ssl BOOLEAN NOT NULL COMMENT 'Indicates whether the server supports SSL connections.',
+  Ssl_ca TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Authority (CA) certificate.',
+  Ssl_capath TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path to the Certificate Authority (CA) certificates.',
+  Ssl_cert TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL certificate file.',
+  Ssl_cipher TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the cipher in use for the SSL connection.',
+  Ssl_key TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL key file.',
+  Ssl_verify_server_cert BOOLEAN NOT NULL COMMENT 'Whether to verify the server certificate.',
+  Heartbeat FLOAT NOT NULL COMMENT '',
+  Bind TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'Displays which interface is employed when connecting to the MySQL server',
+  Ignored_server_ids TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The number of server IDs to be ignored, followed by the actual server IDs',
+  Uuid TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The master server uuid.',
+  Retry_count BIGINT UNSIGNED NOT NULL COMMENT 'Number of reconnect attempts, to the master, before giving up.',
+  Ssl_crl TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Revocation List (CRL)',
+  Ssl_crlpath TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path used for Certificate Revocation List (CRL) files',
+  Enabled_auto_position BOOLEAN NOT NULL COMMENT 'Indicates whether GTIDs will be used to retrieve events from the master.',
+  PRIMARY KEY(Host, Port)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Master Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS slave_worker_info (
+  Id INTEGER UNSIGNED NOT NULL,
+  Relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Relay_log_pos BIGINT UNSIGNED NOT NULL,
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Master_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Checkpoint_relay_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Checkpoint_master_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_seqno INT UNSIGNED NOT NULL,
+  Checkpoint_group_size INTEGER UNSIGNED NOT NULL,
+  Checkpoint_group_bitmap BLOB NOT NULL,
+  PRIMARY KEY(Id)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Worker Information' ENGINE= INNODB
+DROP DATABASE IF EXISTS performance_schema
+CREATE DATABASE performance_schema character set utf8
+CREATE TABLE performance_schema.cond_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_instance(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_user_by_event_name(USER CHAR(16) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_account_by_event_name(USER CHAR(16) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_instances(FILE_NAME VARCHAR(512) not null,EVENT_NAME VARCHAR(128) not null,OPEN_COUNT INTEGER unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_summary_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_summary_by_instance(FILE_NAME VARCHAR(512) not null,EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_instances(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,THREAD_ID BIGINT unsigned,SOCKET_ID INTEGER not null,IP VARCHAR(64) not null,PORT INTEGER not null,STATE ENUM('IDLE','ACTIVE') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_summary_by_instance(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT unsigned not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_summary_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT unsigned not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.host_cache(IP VARCHAR(64) not null,HOST VARCHAR(255) collate utf8_bin,HOST_VALIDATED ENUM ('YES', 'NO') not null,SUM_CONNECT_ERRORS BIGINT not null,COUNT_HOST_BLOCKED_ERRORS BIGINT not null,COUNT_NAMEINFO_TRANSIENT_ERRORS BIGINT not null,COUNT_NAMEINFO_PERMANENT_ERRORS BIGINT not null,COUNT_FORMAT_ERRORS BIGINT not null,COUNT_ADDRINFO_TRANSIENT_ERRORS BIGINT not null,COUNT_ADDRINFO_PERMANENT_ERRORS BIGINT not null,COUNT_FCRDNS_ERRORS BIGINT not null,COUNT_HOST_ACL_ERRORS BIGINT not null,COUNT_NO_AUTH_PLUGIN_ERRORS BIGINT not null,COUNT_AUTH_PLUGIN_ERRORS BIGINT not null,COUNT_HANDSHAKE_ERRORS BIGINT not null,COUNT_PROXY_USER_ERRORS BIGINT not null,COUNT_PROXY_USER_ACL_ERRORS BIGINT not null,COUNT_AUTHENTICATION_ERRORS BIGINT not null,COUNT_SSL_ERRORS BIGINT not null,COUNT_MAX_USER_CONNECTIONS_ERRORS BIGINT not null,COUNT_MAX_USER_CONNECTIONS_PER_HOUR_ERRORS BIGINT not null,COUNT_DEFAULT_DATABASE_ERRORS BIGINT not null,COUNT_INIT_CONNECT_ERRORS BIGINT not null,COUNT_LOCAL_ERRORS BIGINT not null,COUNT_UNKNOWN_ERRORS BIGINT not null,FIRST_SEEN TIMESTAMP(0) NOT NULL default 0,LAST_SEEN TIMESTAMP(0) NOT NULL default 0,FIRST_ERROR_SEEN TIMESTAMP(0) null default 0,LAST_ERROR_SEEN TIMESTAMP(0) null default 0)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.mutex_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,LOCKED_BY_THREAD_ID BIGINT unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.objects_summary_global_by_type(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.performance_timers(TIMER_NAME ENUM ('CYCLE', 'NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'TICK') not null,TIMER_FREQUENCY BIGINT,TIMER_RESOLUTION BIGINT,TIMER_OVERHEAD BIGINT) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.rwlock_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,WRITE_LOCKED_BY_THREAD_ID BIGINT unsigned,READ_LOCKED_BY_COUNT INTEGER unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_actors(HOST CHAR(60) collate utf8_bin default '%' not null,USER CHAR(16) collate utf8_bin default '%' not null,ROLE CHAR(16) collate utf8_bin default '%' not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_consumers(NAME VARCHAR(64) not null,ENABLED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_instruments(NAME VARCHAR(128) not null,ENABLED ENUM ('YES', 'NO') not null,TIMED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_objects(OBJECT_TYPE ENUM ('TABLE') not null default 'TABLE',OBJECT_SCHEMA VARCHAR(64) default '%',OBJECT_NAME VARCHAR(64) not null default '%',ENABLED ENUM ('YES', 'NO') not null default 'YES',TIMED ENUM ('YES', 'NO') not null default 'YES')ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_timers(NAME VARCHAR(64) not null,TIMER_NAME ENUM ('CYCLE', 'NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'TICK') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_io_waits_summary_by_index_usage(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),INDEX_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_FETCH BIGINT unsigned not null,SUM_TIMER_FETCH BIGINT unsigned not null,MIN_TIMER_FETCH BIGINT unsigned not null,AVG_TIMER_FETCH BIGINT unsigned not null,MAX_TIMER_FETCH BIGINT unsigned not null,COUNT_INSERT BIGINT unsigned not null,SUM_TIMER_INSERT BIGINT unsigned not null,MIN_TIMER_INSERT BIGINT unsigned not null,AVG_TIMER_INSERT BIGINT unsigned not null,MAX_TIMER_INSERT BIGINT unsigned not null,COUNT_UPDATE BIGINT unsigned not null,SUM_TIMER_UPDATE BIGINT unsigned not null,MIN_TIMER_UPDATE BIGINT unsigned not null,AVG_TIMER_UPDATE BIGINT unsigned not null,MAX_TIMER_UPDATE BIGINT unsigned not null,COUNT_DELETE BIGINT unsigned not null,SUM_TIMER_DELETE BIGINT unsigned not null,MIN_TIMER_DELETE BIGINT unsigned not null,AVG_TIMER_DELETE BIGINT unsigned not null,MAX_TIMER_DELETE BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_io_waits_summary_by_table(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_FETCH BIGINT unsigned not null,SUM_TIMER_FETCH BIGINT unsigned not null,MIN_TIMER_FETCH BIGINT unsigned not null,AVG_TIMER_FETCH BIGINT unsigned not null,MAX_TIMER_FETCH BIGINT unsigned not null,COUNT_INSERT BIGINT unsigned not null,SUM_TIMER_INSERT BIGINT unsigned not null,MIN_TIMER_INSERT BIGINT unsigned not null,AVG_TIMER_INSERT BIGINT unsigned not null,MAX_TIMER_INSERT BIGINT unsigned not null,COUNT_UPDATE BIGINT unsigned not null,SUM_TIMER_UPDATE BIGINT unsigned not null,MIN_TIMER_UPDATE BIGINT unsigned not null,AVG_TIMER_UPDATE BIGINT unsigned not null,MAX_TIMER_UPDATE BIGINT unsigned not null,COUNT_DELETE BIGINT unsigned not null,SUM_TIMER_DELETE BIGINT unsigned not null,MIN_TIMER_DELETE BIGINT unsigned not null,AVG_TIMER_DELETE BIGINT unsigned not null,MAX_TIMER_DELETE BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_lock_waits_summary_by_table(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_READ_NORMAL BIGINT unsigned not null,SUM_TIMER_READ_NORMAL BIGINT unsigned not null,MIN_TIMER_READ_NORMAL BIGINT unsigned not null,AVG_TIMER_READ_NORMAL BIGINT unsigned not null,MAX_TIMER_READ_NORMAL BIGINT unsigned not null,COUNT_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,SUM_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,MIN_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,AVG_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,MAX_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,COUNT_READ_HIGH_PRIORITY BIGINT unsigned not null,SUM_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,MIN_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,AVG_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,MAX_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,COUNT_READ_NO_INSERT BIGINT unsigned not null,SUM_TIMER_READ_NO_INSERT BIGINT unsigned not null,MIN_TIMER_READ_NO_INSERT BIGINT unsigned not null,AVG_TIMER_READ_NO_INSERT BIGINT unsigned not null,MAX_TIMER_READ_NO_INSERT BIGINT unsigned not null,COUNT_READ_EXTERNAL BIGINT unsigned not null,SUM_TIMER_READ_EXTERNAL BIGINT unsigned not null,MIN_TIMER_READ_EXTERNAL BIGINT unsigned not null,AVG_TIMER_READ_EXTERNAL BIGINT unsigned not null,MAX_TIMER_READ_EXTERNAL BIGINT unsigned not null,COUNT_WRITE_ALLOW_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,COUNT_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,SUM_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,MIN_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,AVG_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,MAX_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,COUNT_WRITE_DELAYED BIGINT unsigned not null,SUM_TIMER_WRITE_DELAYED BIGINT unsigned not null,MIN_TIMER_WRITE_DELAYED BIGINT unsigned not null,AVG_TIMER_WRITE_DELAYED BIGINT unsigned not null,MAX_TIMER_WRITE_DELAYED BIGINT unsigned not null,COUNT_WRITE_LOW_PRIORITY BIGINT unsigned not null,SUM_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,MIN_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,AVG_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,MAX_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,COUNT_WRITE_NORMAL BIGINT unsigned not null,SUM_TIMER_WRITE_NORMAL BIGINT unsigned not null,MIN_TIMER_WRITE_NORMAL BIGINT unsigned not null,AVG_TIMER_WRITE_NORMAL BIGINT unsigned not null,MAX_TIMER_WRITE_NORMAL BIGINT unsigned not null,COUNT_WRITE_EXTERNAL BIGINT unsigned not null,SUM_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,MIN_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,AVG_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,MAX_TIMER_WRITE_EXTERNAL BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.threads(THREAD_ID BIGINT unsigned not null,NAME VARCHAR(128) not null,TYPE VARCHAR(10) not null,PROCESSLIST_ID BIGINT unsigned,PROCESSLIST_USER VARCHAR(16),PROCESSLIST_HOST VARCHAR(60),PROCESSLIST_DB VARCHAR(64),PROCESSLIST_COMMAND VARCHAR(16),PROCESSLIST_TIME BIGINT,PROCESSLIST_STATE VARCHAR(64),PROCESSLIST_INFO LONGTEXT,PARENT_THREAD_ID BIGINT unsigned,ROLE VARCHAR(64),INSTRUMENTED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_user_by_event_name(USER CHAR(16) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_account_by_event_name(USER CHAR(16) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_user_by_event_name(USER CHAR(16) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_account_by_event_name(USER CHAR(16) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.hosts(HOST CHAR(60) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.users(USER CHAR(16) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.accounts(USER CHAR(16) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_digest(SCHEMA_NAME VARCHAR(64),DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null,FIRST_SEEN TIMESTAMP(0) NOT NULL default 0,LAST_SEEN TIMESTAMP(0) NOT NULL default 0)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.session_connect_attrs(PROCESSLIST_ID INT NOT NULL,ATTR_NAME VARCHAR(32) NOT NULL,ATTR_VALUE VARCHAR(1024),ORDINAL_POSITION INT)ENGINE=PERFORMANCE_SCHEMA CHARACTER SET utf8 COLLATE utf8_bin
+CREATE TABLE performance_schema.session_account_connect_attrs  LIKE performance_schema.session_connect_attrs
+CREATE TABLE IF NOT EXISTS proxies_priv (Host char(60) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Proxied_host char(60) binary DEFAULT '' NOT NULL, Proxied_user char(16) binary DEFAULT '' NOT NULL, With_grant BOOL DEFAULT 0 NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY Host (Host,User,Proxied_host,Proxied_user), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='User proxy privileges';
+DROP TEMPORARY TABLE IF EXISTS `tmp_db` /* generated by server */
+DROP TEMPORARY TABLE IF EXISTS `tmp_user` /* generated by server */
+DROP TEMPORARY TABLE IF EXISTS `tmp_proxies_priv` /* generated by server */
+TRUNCATE TABLE help_topic;
+TRUNCATE TABLE help_category;
+TRUNCATE TABLE help_keyword;
+TRUNCATE TABLE help_relation;
+TRUNCATE TABLE time_zone
+TRUNCATE TABLE time_zone_name
+TRUNCATE TABLE time_zone_transition
+TRUNCATE TABLE time_zone_transition_type
+ALTER TABLE time_zone_transition ORDER BY Time_zone_id, Transition_time
+ALTER TABLE time_zone_transition_type ORDER BY Time_zone_id, Transition_type_id
+CREATE DATABASE IF NOT EXISTS `mysql`
+CREATE USER 'mysqluser'@'%' IDENTIFIED BY PASSWORD '*FBC02A898D66B9181D6F8826C045C11FD2B364A4'
+GRANT ALL ON `mysql`.* TO 'mysqluser'@'%'
+FLUSH PRIVILEGES
+GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replicator'@'%' IDENTIFIED BY PASSWORD '*D98280F03D0F78162EBDBB9C883FC01395DEA2BF'
+CREATE DATABASE connector_test
+GRANT ALL PRIVILEGES ON connector_test.* TO 'mysqluser'@'%'
+CREATE TABLE products (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  name VARCHAR(255) NOT NULL,
+  description VARCHAR(512),
+  weight FLOAT
+)
+ALTER TABLE products AUTO_INCREMENT = 101
+CREATE TABLE products_on_hand (
+  product_id INTEGER NOT NULL PRIMARY KEY,
+  quantity INTEGER NOT NULL,
+  FOREIGN KEY (product_id) REFERENCES products(id)
+)
+CREATE TABLE customers (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  first_name VARCHAR(255) NOT NULL,
+  last_name VARCHAR(255) NOT NULL,
+  email VARCHAR(255) NOT NULL UNIQUE KEY
+) AUTO_INCREMENT=1001
+CREATE TABLE orders (
+  order_number INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  order_date DATE NOT NULL,
+  purchaser INTEGER NOT NULL,
+  quantity INTEGER NOT NULL,
+  product_id INTEGER NOT NULL,
+  FOREIGN KEY order_customer (purchaser) REFERENCES customers(id),
+  FOREIGN KEY ordered_product (product_id) REFERENCES products(id)
+) AUTO_INCREMENT = 10001
+CREATE DATABASE emptydb
+GRANT ALL PRIVILEGES ON emptydb.* TO 'mysqluser'@'%'
+GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replicator'@'%' IDENTIFIED BY PASSWORD '*D98280F03D0F78162EBDBB9C883FC01395DEA2BF'
+CREATE DATABASE readbinlog_test
+GRANT ALL PRIVILEGES ON readbinlog_test.* TO 'mysqluser'@'%'
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)",2016-05-13T14:32:47Z,140
"@@ -0,0 +1,289 @@
+--
+-- Statements recorded by binlog during MySQL 5.6 initialization with Debezium scripts.
+--
+CREATE DATABASE mysql;
+
+CREATE TABLE IF NOT EXISTS db (   Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Select_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Insert_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Update_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Delete_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Drop_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Grant_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, References_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Index_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tmp_table_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Lock_tables_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Execute_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Event_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Trigger_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY Host (Host,Db,User), KEY User (User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='Database privileges';
+
+CREATE TABLE IF NOT EXISTS user (   Host char(60) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Select_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Insert_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Update_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Delete_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Drop_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Reload_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Shutdown_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Process_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, File_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Grant_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, References_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Index_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_db_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Super_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tmp_table_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Lock_tables_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Execute_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Repl_slave_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Repl_client_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_user_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Event_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Trigger_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tablespace_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, ssl_type enum('','ANY','X509', 'SPECIFIED') COLLATE utf8_general_ci DEFAULT '' NOT NULL, ssl_cipher BLOB NOT NULL, x509_issuer BLOB NOT NULL, x509_subject BLOB NOT NULL, max_questions int(11) unsigned DEFAULT 0  NOT NULL, max_updates int(11) unsigned DEFAULT 0  NOT NULL, max_connections int(11) unsigned DEFAULT 0  NOT NULL, max_user_connections int(11) unsigned DEFAULT 0  NOT NULL, plugin char(64) DEFAULT 'mysql_native_password' NOT NULL, authentication_string TEXT, password_expired ENUM('N', 'Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, password_last_changed timestamp NULL DEFAULT NULL, password_lifetime smallint unsigned NULL DEFAULT NULL, account_locked ENUM('N', 'Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY Host (Host,User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='Users and global privileges';
+
+CREATE TABLE IF NOT EXISTS func (  name char(64) binary DEFAULT '' NOT NULL, ret tinyint(1) DEFAULT '0' NOT NULL, dl char(128) DEFAULT '' NOT NULL, type enum ('function','aggregate') COLLATE utf8_general_ci NOT NULL, PRIMARY KEY (name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='User defined functions';
+
+CREATE TABLE IF NOT EXISTS plugin ( name varchar(64) DEFAULT '' NOT NULL, dl varchar(128) DEFAULT '' NOT NULL, PRIMARY KEY (name) ) engine=InnoDB STATS_PERSISTENT=0 CHARACTER SET utf8 COLLATE utf8_general_ci comment='MySQL plugins';
+
+CREATE TABLE IF NOT EXISTS servers ( Server_name char(64) NOT NULL DEFAULT '', Host char(64) NOT NULL DEFAULT '', Db char(64) NOT NULL DEFAULT '', Username char(64) NOT NULL DEFAULT '', Password char(64) NOT NULL DEFAULT '', Port INT(4) NOT NULL DEFAULT '0', Socket char(64) NOT NULL DEFAULT '', Wrapper char(64) NOT NULL DEFAULT '', Owner char(64) NOT NULL DEFAULT '', PRIMARY KEY (Server_name)) engine=InnoDB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='MySQL Foreign Servers table';
+
+CREATE TABLE IF NOT EXISTS tables_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Table_name char(64) binary DEFAULT '' NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, Table_priv set('Select','Insert','Update','Delete','Create','Drop','Grant','References','Index','Alter','Create View','Show view','Trigger') COLLATE utf8_general_ci DEFAULT '' NOT NULL, Column_priv set('Select','Insert','Update','References') COLLATE utf8_general_ci DEFAULT '' NOT NULL, PRIMARY KEY (Host,Db,User,Table_name), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Table privileges';
+
+CREATE TABLE IF NOT EXISTS columns_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Table_name char(64) binary DEFAULT '' NOT NULL, Column_name char(64) binary DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, Column_priv set('Select','Insert','Update','References') COLLATE utf8_general_ci DEFAULT '' NOT NULL, PRIMARY KEY (Host,Db,User,Table_name,Column_name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Column privileges';
+
+CREATE TABLE IF NOT EXISTS help_topic ( help_topic_id int unsigned not null, name char(64) not null, help_category_id smallint unsigned not null, description text not null, example text not null, url text not null, primary key (help_topic_id), unique index (name) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='help topics';
+
+CREATE TABLE IF NOT EXISTS help_category ( help_category_id smallint unsigned not null, name  char(64) not null, parent_category_id smallint unsigned null, url text not null, primary key (help_category_id), unique index (name) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='help categories';
+
+CREATE TABLE IF NOT EXISTS help_relation ( help_topic_id int unsigned not null, help_keyword_id  int unsigned not null, primary key (help_keyword_id, help_topic_id) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='keyword-topic relation';
+
+CREATE TABLE IF NOT EXISTS help_keyword (   help_keyword_id  int unsigned not null, name char(64) not null, primary key (help_keyword_id), unique index (name) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='help keywords';
+
+CREATE TABLE IF NOT EXISTS time_zone_name (   Name char(64) NOT NULL, Time_zone_id int unsigned NOT NULL, PRIMARY KEY Name (Name) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Time zone names';
+
+CREATE TABLE IF NOT EXISTS time_zone (   Time_zone_id int unsigned NOT NULL auto_increment, Use_leap_seconds enum('Y','N') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY TzId (Time_zone_id) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Time zones';
+
+CREATE TABLE IF NOT EXISTS time_zone_transition (   Time_zone_id int unsigned NOT NULL, Transition_time bigint signed NOT NULL, Transition_type_id int unsigned NOT NULL, PRIMARY KEY TzIdTranTime (Time_zone_id, Transition_time) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Time zone transitions';
+
+CREATE TABLE IF NOT EXISTS time_zone_transition_type (   Time_zone_id int unsigned NOT NULL, Transition_type_id int unsigned NOT NULL, Offset int signed DEFAULT 0 NOT NULL, Is_DST tinyint unsigned DEFAULT 0 NOT NULL, Abbreviation char(8) DEFAULT '' NOT NULL, PRIMARY KEY TzIdTrTId (Time_zone_id, Transition_type_id) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Time zone transition types';
+
+CREATE TABLE IF NOT EXISTS time_zone_leap_second (   Transition_time bigint signed NOT NULL, Correction int signed NOT NULL, PRIMARY KEY TranTime (Transition_time) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Leap seconds information for time zones';
+
+CREATE TABLE IF NOT EXISTS proc (db char(64) collate utf8_bin DEFAULT '' NOT NULL, name char(64) DEFAULT '' NOT NULL, type enum('FUNCTION','PROCEDURE') NOT NULL, specific_name char(64) DEFAULT '' NOT NULL, language enum('SQL') DEFAULT 'SQL' NOT NULL, sql_data_access enum( 'CONTAINS_SQL', 'NO_SQL', 'READS_SQL_DATA', 'MODIFIES_SQL_DATA') DEFAULT 'CONTAINS_SQL' NOT NULL, is_deterministic enum('YES','NO') DEFAULT 'NO' NOT NULL, security_type enum('INVOKER','DEFINER') DEFAULT 'DEFINER' NOT NULL, param_list blob NOT NULL, returns longblob DEFAULT '' NOT NULL, body longblob NOT NULL, definer char(77) collate utf8_bin DEFAULT '' NOT NULL, created timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, modified timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', sql_mode set( 'REAL_AS_FLOAT', 'PIPES_AS_CONCAT', 'ANSI_QUOTES', 'IGNORE_SPACE', 'NOT_USED', 'ONLY_FULL_GROUP_BY', 'NO_UNSIGNED_SUBTRACTION', 'NO_DIR_IN_CREATE', 'POSTGRESQL', 'ORACLE', 'MSSQL', 'DB2', 'MAXDB', 'NO_KEY_OPTIONS', 'NO_TABLE_OPTIONS', 'NO_FIELD_OPTIONS', 'MYSQL323', 'MYSQL40', 'ANSI', 'NO_AUTO_VALUE_ON_ZERO', 'NO_BACKSLASH_ESCAPES', 'STRICT_TRANS_TABLES', 'STRICT_ALL_TABLES', 'NO_ZERO_IN_DATE', 'NO_ZERO_DATE', 'INVALID_DATES', 'ERROR_FOR_DIVISION_BY_ZERO', 'TRADITIONAL', 'NO_AUTO_CREATE_USER', 'HIGH_NOT_PRECEDENCE', 'NO_ENGINE_SUBSTITUTION', 'PAD_CHAR_TO_FULL_LENGTH') DEFAULT '' NOT NULL, comment text collate utf8_bin NOT NULL, character_set_client char(32) collate utf8_bin, collation_connection char(32) collate utf8_bin, db_collation char(32) collate utf8_bin, body_utf8 longblob, PRIMARY KEY (db,name,type)) engine=MyISAM character set utf8 comment='Stored Procedures';
+
+CREATE TABLE IF NOT EXISTS procs_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Routine_name char(64) COLLATE utf8_general_ci DEFAULT '' NOT NULL, Routine_type enum('FUNCTION','PROCEDURE') NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Proc_priv set('Execute','Alter Routine','Grant') COLLATE utf8_general_ci DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (Host,Db,User,Routine_name,Routine_type), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Procedure privileges';
+
+CREATE TABLE IF NOT EXISTS general_log (event_time TIMESTAMP(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), user_host MEDIUMTEXT NOT NULL, thread_id BIGINT(21) UNSIGNED NOT NULL, server_id INTEGER UNSIGNED NOT NULL, command_type VARCHAR(64) NOT NULL, argument MEDIUMBLOB NOT NULL) engine=CSV CHARACTER SET utf8 comment=""General log"";
+
+CREATE TABLE IF NOT EXISTS slow_log (start_time TIMESTAMP(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), user_host MEDIUMTEXT NOT NULL, query_time TIME(6) NOT NULL, lock_time TIME(6) NOT NULL, rows_sent INTEGER NOT NULL, rows_examined INTEGER NOT NULL, db VARCHAR(512) NOT NULL, last_insert_id INTEGER NOT NULL, insert_id INTEGER NOT NULL, server_id INTEGER UNSIGNED NOT NULL, sql_text MEDIUMBLOB NOT NULL, thread_id BIGINT(21) UNSIGNED NOT NULL) engine=CSV CHARACTER SET utf8 comment=""Slow log"";
+
+CREATE TABLE IF NOT EXISTS event ( db char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', name char(64) CHARACTER SET utf8 NOT NULL default '', body longblob NOT NULL, definer char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', execute_at DATETIME default NULL, interval_value int(11) default NULL, interval_field ENUM('YEAR','QUARTER','MONTH','DAY','HOUR','MINUTE','WEEK','SECOND','MICROSECOND','YEAR_MONTH','DAY_HOUR','DAY_MINUTE','DAY_SECOND','HOUR_MINUTE','HOUR_SECOND','MINUTE_SECOND','DAY_MICROSECOND','HOUR_MICROSECOND','MINUTE_MICROSECOND','SECOND_MICROSECOND') default NULL, created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, modified TIMESTAMP NOT NULL DEFAULT '0000-00-00 00:00:00', last_executed DATETIME default NULL, starts DATETIME default NULL, ends DATETIME default NULL, status ENUM('ENABLED','DISABLED','SLAVESIDE_DISABLED') NOT NULL default 'ENABLED', on_completion ENUM('DROP','PRESERVE') NOT NULL default 'DROP', sql_mode  set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','NOT_USED','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_ALL_TABLES','NO_ZERO_IN_DATE','NO_ZERO_DATE','INVALID_DATES','ERROR_FOR_DIVISION_BY_ZERO','TRADITIONAL','NO_AUTO_CREATE_USER','HIGH_NOT_PRECEDENCE','NO_ENGINE_SUBSTITUTION','PAD_CHAR_TO_FULL_LENGTH') DEFAULT '' NOT NULL, comment char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', originator INTEGER UNSIGNED NOT NULL, time_zone char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM', character_set_client char(32) collate utf8_bin, collation_connection char(32) collate utf8_bin, db_collation char(32) collate utf8_bin, body_utf8 longblob, PRIMARY KEY (db, name) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT 'Events';
+
+CREATE TABLE IF NOT EXISTS ndb_binlog_index (Position BIGINT UNSIGNED NOT NULL, File VARCHAR(255) NOT NULL, epoch BIGINT UNSIGNED NOT NULL, inserts INT UNSIGNED NOT NULL, updates INT UNSIGNED NOT NULL, deletes INT UNSIGNED NOT NULL, schemaops INT UNSIGNED NOT NULL, orig_server_id INT UNSIGNED NOT NULL, orig_epoch BIGINT UNSIGNED NOT NULL, gci INT UNSIGNED NOT NULL, next_position BIGINT UNSIGNED NOT NULL, next_file VARCHAR(255) NOT NULL, PRIMARY KEY(epoch, orig_server_id, orig_epoch)) ENGINE=MYISAM;
+
+CREATE TABLE IF NOT EXISTS innodb_table_stats (
+	database_name			VARCHAR(64) NOT NULL,
+	table_name			VARCHAR(64) NOT NULL,
+	last_update			TIMESTAMP NOT NULL NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+	n_rows				BIGINT UNSIGNED NOT NULL,
+	clustered_index_size		BIGINT UNSIGNED NOT NULL,
+	sum_of_other_index_sizes	BIGINT UNSIGNED NOT NULL,
+	PRIMARY KEY (database_name, table_name)
+) ENGINE=INNODB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0
+CREATE TABLE IF NOT EXISTS innodb_index_stats (
+	database_name			VARCHAR(64) NOT NULL,
+	table_name			VARCHAR(64) NOT NULL,
+	index_name			VARCHAR(64) NOT NULL,
+	last_update			TIMESTAMP NOT NULL NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+	/* there are at least:
+	stat_name='size'
+	stat_name='n_leaf_pages'
+	stat_name='n_diff_pfx%' */
+	stat_name			VARCHAR(64) NOT NULL,
+	stat_value			BIGINT UNSIGNED NOT NULL,
+	sample_size			BIGINT UNSIGNED,
+	stat_description		VARCHAR(1024) NOT NULL,
+	PRIMARY KEY (database_name, table_name, index_name, stat_name)
+) ENGINE=INNODB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0
+CREATE TABLE IF NOT EXISTS slave_relay_log_info (
+  Number_of_lines INTEGER UNSIGNED NOT NULL COMMENT 'Number of lines in the file or rows in the table. Used to version table definitions.',
+  Relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the current relay log file.',
+  Relay_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The relay log position of the last executed event.',
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log file from which the events in the relay log file were read.',
+  Master_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The master log position of the last executed event.',
+  Sql_delay INTEGER NOT NULL COMMENT 'The number of seconds that the slave must lag behind the master.',
+  Number_of_workers INTEGER UNSIGNED NOT NULL,
+  Id INTEGER UNSIGNED NOT NULL COMMENT 'Internal Id that uniquely identifies this record.',
+  Channel_name CHAR(64) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT 'The channel on which the slave is connected to a source. Used in Multisource Replication',
+  PRIMARY KEY(Channel_name)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Relay Log Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS slave_master_info (
+  Number_of_lines INTEGER UNSIGNED NOT NULL COMMENT 'Number of lines in the file.',
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log currently being read from the master.',
+  Master_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The master log position of the last read event.',
+  Host CHAR(64) CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The host name of the master.',
+  User_name TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The user name used to connect to the master.',
+  User_password TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The password used to connect to the master.',
+  Port INTEGER UNSIGNED NOT NULL COMMENT 'The network port used to connect to the master.',
+  Connect_retry INTEGER UNSIGNED NOT NULL COMMENT 'The period (in seconds) that the slave will wait before trying to reconnect to the master.',
+  Enabled_ssl BOOLEAN NOT NULL COMMENT 'Indicates whether the server supports SSL connections.',
+  Ssl_ca TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Authority (CA) certificate.',
+  Ssl_capath TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path to the Certificate Authority (CA) certificates.',
+  Ssl_cert TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL certificate file.',
+  Ssl_cipher TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the cipher in use for the SSL connection.',
+  Ssl_key TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL key file.',
+  Ssl_verify_server_cert BOOLEAN NOT NULL COMMENT 'Whether to verify the server certificate.',
+  Heartbeat FLOAT NOT NULL COMMENT '',
+  Bind TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'Displays which interface is employed when connecting to the MySQL server',
+  Ignored_server_ids TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The number of server IDs to be ignored, followed by the actual server IDs',
+  Uuid TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The master server uuid.',
+  Retry_count BIGINT UNSIGNED NOT NULL COMMENT 'Number of reconnect attempts, to the master, before giving up.',
+  Ssl_crl TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Revocation List (CRL)',
+  Ssl_crlpath TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path used for Certificate Revocation List (CRL) files',
+  Enabled_auto_position BOOLEAN NOT NULL COMMENT 'Indicates whether GTIDs will be used to retrieve events from the master.',
+  Channel_name CHAR(64) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT 'The channel on which the slave is connected to a source. Used in Multisource Replication',
+  Tls_version TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'Tls version',
+  PRIMARY KEY(Channel_name)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Master Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS slave_worker_info (
+  Id INTEGER UNSIGNED NOT NULL,
+  Relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Relay_log_pos BIGINT UNSIGNED NOT NULL,
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Master_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Checkpoint_relay_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Checkpoint_master_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_seqno INT UNSIGNED NOT NULL,
+  Checkpoint_group_size INTEGER UNSIGNED NOT NULL,
+  Checkpoint_group_bitmap BLOB NOT NULL,
+  Channel_name CHAR(64) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT 'The channel on which the slave is connected to a source. Used in Multisource Replication',
+  PRIMARY KEY(Channel_name, Id)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Worker Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS gtid_executed (
+    source_uuid CHAR(36) NOT NULL COMMENT 'uuid of the source where the transaction was originally executed.',
+    interval_start BIGINT NOT NULL COMMENT 'First number of interval.',
+    interval_end BIGINT NOT NULL COMMENT 'Last number of interval.',
+    PRIMARY KEY(source_uuid, interval_start)) ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS server_cost (
+  cost_name   VARCHAR(64) NOT NULL,
+  cost_value  FLOAT DEFAULT NULL,
+  last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+  comment     VARCHAR(1024) DEFAULT NULL,
+  PRIMARY KEY (cost_name)
+) ENGINE=InnoDB CHARACTER SET=utf8 COLLATE=utf8_general_ci STATS_PERSISTENT=0;
+
+CREATE TABLE IF NOT EXISTS engine_cost (
+  engine_name VARCHAR(64) NOT NULL,
+  device_type INTEGER NOT NULL,
+  cost_name   VARCHAR(64) NOT NULL,
+  cost_value  FLOAT DEFAULT NULL,
+  last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+  comment     VARCHAR(1024) DEFAULT NULL,
+  PRIMARY KEY (cost_name, engine_name, device_type)
+) ENGINE=InnoDB CHARACTER SET=utf8 COLLATE=utf8_general_ci STATS_PERSISTENT=0;
+
+DROP DATABASE IF EXISTS performance_schema
+CREATE DATABASE performance_schema character set utf8
+CREATE TABLE performance_schema.cond_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_instance(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_instances(FILE_NAME VARCHAR(512) not null,EVENT_NAME VARCHAR(128) not null,OPEN_COUNT INTEGER unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_summary_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_summary_by_instance(FILE_NAME VARCHAR(512) not null,EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_instances(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,THREAD_ID BIGINT unsigned,SOCKET_ID INTEGER not null,IP VARCHAR(64) not null,PORT INTEGER not null,STATE ENUM('IDLE','ACTIVE') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_summary_by_instance(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT unsigned not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_summary_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT unsigned not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.host_cache(IP VARCHAR(64) not null,HOST VARCHAR(255) collate utf8_bin,HOST_VALIDATED ENUM ('YES', 'NO') not null,SUM_CONNECT_ERRORS BIGINT not null,COUNT_HOST_BLOCKED_ERRORS BIGINT not null,COUNT_NAMEINFO_TRANSIENT_ERRORS BIGINT not null,COUNT_NAMEINFO_PERMANENT_ERRORS BIGINT not null,COUNT_FORMAT_ERRORS BIGINT not null,COUNT_ADDRINFO_TRANSIENT_ERRORS BIGINT not null,COUNT_ADDRINFO_PERMANENT_ERRORS BIGINT not null,COUNT_FCRDNS_ERRORS BIGINT not null,COUNT_HOST_ACL_ERRORS BIGINT not null,COUNT_NO_AUTH_PLUGIN_ERRORS BIGINT not null,COUNT_AUTH_PLUGIN_ERRORS BIGINT not null,COUNT_HANDSHAKE_ERRORS BIGINT not null,COUNT_PROXY_USER_ERRORS BIGINT not null,COUNT_PROXY_USER_ACL_ERRORS BIGINT not null,COUNT_AUTHENTICATION_ERRORS BIGINT not null,COUNT_SSL_ERRORS BIGINT not null,COUNT_MAX_USER_CONNECTIONS_ERRORS BIGINT not null,COUNT_MAX_USER_CONNECTIONS_PER_HOUR_ERRORS BIGINT not null,COUNT_DEFAULT_DATABASE_ERRORS BIGINT not null,COUNT_INIT_CONNECT_ERRORS BIGINT not null,COUNT_LOCAL_ERRORS BIGINT not null,COUNT_UNKNOWN_ERRORS BIGINT not null,FIRST_SEEN TIMESTAMP(0) NOT NULL default 0,LAST_SEEN TIMESTAMP(0) NOT NULL default 0,FIRST_ERROR_SEEN TIMESTAMP(0) null default 0,LAST_ERROR_SEEN TIMESTAMP(0) null default 0)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.mutex_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,LOCKED_BY_THREAD_ID BIGINT unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.objects_summary_global_by_type(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.performance_timers(TIMER_NAME ENUM ('CYCLE', 'NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'TICK') not null,TIMER_FREQUENCY BIGINT,TIMER_RESOLUTION BIGINT,TIMER_OVERHEAD BIGINT) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.rwlock_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,WRITE_LOCKED_BY_THREAD_ID BIGINT unsigned,READ_LOCKED_BY_COUNT INTEGER unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_actors(HOST CHAR(60) collate utf8_bin default '%' not null,USER CHAR(32) collate utf8_bin default '%' not null,ROLE CHAR(16) collate utf8_bin default '%' not null,ENABLED ENUM ('YES', 'NO') not null default 'YES',HISTORY ENUM ('YES', 'NO') not null default 'YES')ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_consumers(NAME VARCHAR(64) not null,ENABLED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_instruments(NAME VARCHAR(128) not null,ENABLED ENUM ('YES', 'NO') not null,TIMED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_objects(OBJECT_TYPE ENUM ('EVENT', 'FUNCTION', 'PROCEDURE', 'TABLE', 'TRIGGER') not null default 'TABLE',OBJECT_SCHEMA VARCHAR(64) default '%',OBJECT_NAME VARCHAR(64) not null default '%',ENABLED ENUM ('YES', 'NO') not null default 'YES',TIMED ENUM ('YES', 'NO') not null default 'YES')ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_timers(NAME VARCHAR(64) not null,TIMER_NAME ENUM ('CYCLE', 'NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'TICK') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_io_waits_summary_by_index_usage(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),INDEX_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_FETCH BIGINT unsigned not null,SUM_TIMER_FETCH BIGINT unsigned not null,MIN_TIMER_FETCH BIGINT unsigned not null,AVG_TIMER_FETCH BIGINT unsigned not null,MAX_TIMER_FETCH BIGINT unsigned not null,COUNT_INSERT BIGINT unsigned not null,SUM_TIMER_INSERT BIGINT unsigned not null,MIN_TIMER_INSERT BIGINT unsigned not null,AVG_TIMER_INSERT BIGINT unsigned not null,MAX_TIMER_INSERT BIGINT unsigned not null,COUNT_UPDATE BIGINT unsigned not null,SUM_TIMER_UPDATE BIGINT unsigned not null,MIN_TIMER_UPDATE BIGINT unsigned not null,AVG_TIMER_UPDATE BIGINT unsigned not null,MAX_TIMER_UPDATE BIGINT unsigned not null,COUNT_DELETE BIGINT unsigned not null,SUM_TIMER_DELETE BIGINT unsigned not null,MIN_TIMER_DELETE BIGINT unsigned not null,AVG_TIMER_DELETE BIGINT unsigned not null,MAX_TIMER_DELETE BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_io_waits_summary_by_table(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_FETCH BIGINT unsigned not null,SUM_TIMER_FETCH BIGINT unsigned not null,MIN_TIMER_FETCH BIGINT unsigned not null,AVG_TIMER_FETCH BIGINT unsigned not null,MAX_TIMER_FETCH BIGINT unsigned not null,COUNT_INSERT BIGINT unsigned not null,SUM_TIMER_INSERT BIGINT unsigned not null,MIN_TIMER_INSERT BIGINT unsigned not null,AVG_TIMER_INSERT BIGINT unsigned not null,MAX_TIMER_INSERT BIGINT unsigned not null,COUNT_UPDATE BIGINT unsigned not null,SUM_TIMER_UPDATE BIGINT unsigned not null,MIN_TIMER_UPDATE BIGINT unsigned not null,AVG_TIMER_UPDATE BIGINT unsigned not null,MAX_TIMER_UPDATE BIGINT unsigned not null,COUNT_DELETE BIGINT unsigned not null,SUM_TIMER_DELETE BIGINT unsigned not null,MIN_TIMER_DELETE BIGINT unsigned not null,AVG_TIMER_DELETE BIGINT unsigned not null,MAX_TIMER_DELETE BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_lock_waits_summary_by_table(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_READ_NORMAL BIGINT unsigned not null,SUM_TIMER_READ_NORMAL BIGINT unsigned not null,MIN_TIMER_READ_NORMAL BIGINT unsigned not null,AVG_TIMER_READ_NORMAL BIGINT unsigned not null,MAX_TIMER_READ_NORMAL BIGINT unsigned not null,COUNT_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,SUM_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,MIN_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,AVG_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,MAX_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,COUNT_READ_HIGH_PRIORITY BIGINT unsigned not null,SUM_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,MIN_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,AVG_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,MAX_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,COUNT_READ_NO_INSERT BIGINT unsigned not null,SUM_TIMER_READ_NO_INSERT BIGINT unsigned not null,MIN_TIMER_READ_NO_INSERT BIGINT unsigned not null,AVG_TIMER_READ_NO_INSERT BIGINT unsigned not null,MAX_TIMER_READ_NO_INSERT BIGINT unsigned not null,COUNT_READ_EXTERNAL BIGINT unsigned not null,SUM_TIMER_READ_EXTERNAL BIGINT unsigned not null,MIN_TIMER_READ_EXTERNAL BIGINT unsigned not null,AVG_TIMER_READ_EXTERNAL BIGINT unsigned not null,MAX_TIMER_READ_EXTERNAL BIGINT unsigned not null,COUNT_WRITE_ALLOW_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,COUNT_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,SUM_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,MIN_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,AVG_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,MAX_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,COUNT_WRITE_LOW_PRIORITY BIGINT unsigned not null,SUM_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,MIN_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,AVG_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,MAX_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,COUNT_WRITE_NORMAL BIGINT unsigned not null,SUM_TIMER_WRITE_NORMAL BIGINT unsigned not null,MIN_TIMER_WRITE_NORMAL BIGINT unsigned not null,AVG_TIMER_WRITE_NORMAL BIGINT unsigned not null,MAX_TIMER_WRITE_NORMAL BIGINT unsigned not null,COUNT_WRITE_EXTERNAL BIGINT unsigned not null,SUM_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,MIN_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,AVG_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,MAX_TIMER_WRITE_EXTERNAL BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.threads(THREAD_ID BIGINT unsigned not null,NAME VARCHAR(128) not null,TYPE VARCHAR(10) not null,PROCESSLIST_ID BIGINT unsigned,PROCESSLIST_USER VARCHAR(32),PROCESSLIST_HOST VARCHAR(60),PROCESSLIST_DB VARCHAR(64),PROCESSLIST_COMMAND VARCHAR(16),PROCESSLIST_TIME BIGINT,PROCESSLIST_STATE VARCHAR(64),PROCESSLIST_INFO LONGTEXT,PARENT_THREAD_ID BIGINT unsigned,ROLE VARCHAR(64),INSTRUMENTED ENUM ('YES', 'NO') not null,HISTORY ENUM ('YES', 'NO') not null,CONNECTION_TYPE VARCHAR(16),THREAD_OS_ID BIGINT unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,WORK_COMPLETED BIGINT unsigned,WORK_ESTIMATED BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,WORK_COMPLETED BIGINT unsigned,WORK_ESTIMATED BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,WORK_COMPLETED BIGINT unsigned,WORK_ESTIMATED BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),NESTING_EVENT_LEVEL INTEGER)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),NESTING_EVENT_LEVEL INTEGER)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),NESTING_EVENT_LEVEL INTEGER)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,STATE ENUM('ACTIVE', 'COMMITTED', 'ROLLED BACK'),TRX_ID BIGINT unsigned,GTID VARCHAR(64),XID_FORMAT_ID INTEGER,XID_GTRID VARCHAR(130),XID_BQUAL VARCHAR(130),XA_STATE VARCHAR(64),SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,ACCESS_MODE ENUM('READ ONLY', 'READ WRITE'),ISOLATION_LEVEL VARCHAR(64),AUTOCOMMIT ENUM('YES','NO') not null,NUMBER_OF_SAVEPOINTS BIGINT unsigned,NUMBER_OF_ROLLBACK_TO_SAVEPOINT BIGINT unsigned,NUMBER_OF_RELEASE_SAVEPOINT BIGINT unsigned,OBJECT_INSTANCE_BEGIN BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,STATE ENUM('ACTIVE', 'COMMITTED', 'ROLLED BACK'),TRX_ID BIGINT unsigned,GTID VARCHAR(64),XID_FORMAT_ID INTEGER,XID_GTRID VARCHAR(130),XID_BQUAL VARCHAR(130),XA_STATE VARCHAR(64),SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,ACCESS_MODE ENUM('READ ONLY', 'READ WRITE'),ISOLATION_LEVEL VARCHAR(64),AUTOCOMMIT ENUM('YES','NO') not null,NUMBER_OF_SAVEPOINTS BIGINT unsigned,NUMBER_OF_ROLLBACK_TO_SAVEPOINT BIGINT unsigned,NUMBER_OF_RELEASE_SAVEPOINT BIGINT unsigned,OBJECT_INSTANCE_BEGIN BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,STATE ENUM('ACTIVE', 'COMMITTED', 'ROLLED BACK'),TRX_ID BIGINT unsigned,GTID VARCHAR(64),XID_FORMAT_ID INTEGER,XID_GTRID VARCHAR(130),XID_BQUAL VARCHAR(130),XA_STATE VARCHAR(64),SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,ACCESS_MODE ENUM('READ ONLY', 'READ WRITE'),ISOLATION_LEVEL VARCHAR(64),AUTOCOMMIT ENUM('YES','NO') not null,NUMBER_OF_SAVEPOINTS BIGINT unsigned,NUMBER_OF_ROLLBACK_TO_SAVEPOINT BIGINT unsigned,NUMBER_OF_RELEASE_SAVEPOINT BIGINT unsigned,OBJECT_INSTANCE_BEGIN BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.hosts(HOST CHAR(60) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.users(USER CHAR(32) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.accounts(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_digest(SCHEMA_NAME VARCHAR(64),DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null,FIRST_SEEN TIMESTAMP(0) NOT NULL default 0,LAST_SEEN TIMESTAMP(0) NOT NULL default 0)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_program(OBJECT_TYPE enum('EVENT', 'FUNCTION', 'PROCEDURE', 'TABLE', 'TRIGGER'),OBJECT_SCHEMA varchar(64) NOT NULL,OBJECT_NAME varchar(64) NOT NULL,COUNT_STAR bigint(20) unsigned NOT NULL,SUM_TIMER_WAIT bigint(20) unsigned NOT NULL,MIN_TIMER_WAIT bigint(20) unsigned NOT NULL,AVG_TIMER_WAIT bigint(20) unsigned NOT NULL,MAX_TIMER_WAIT bigint(20) unsigned NOT NULL,COUNT_STATEMENTS bigint(20) unsigned NOT NULL,SUM_STATEMENTS_WAIT bigint(20) unsigned NOT NULL,MIN_STATEMENTS_WAIT bigint(20) unsigned NOT NULL,AVG_STATEMENTS_WAIT bigint(20) unsigned NOT NULL,MAX_STATEMENTS_WAIT bigint(20) unsigned NOT NULL,SUM_LOCK_TIME bigint(20) unsigned NOT NULL,SUM_ERRORS bigint(20) unsigned NOT NULL,SUM_WARNINGS bigint(20) unsigned NOT NULL,SUM_ROWS_AFFECTED bigint(20) unsigned NOT NULL,SUM_ROWS_SENT bigint(20) unsigned NOT NULL,SUM_ROWS_EXAMINED bigint(20) unsigned NOT NULL,SUM_CREATED_TMP_DISK_TABLES bigint(20) unsigned NOT NULL,SUM_CREATED_TMP_TABLES bigint(20) unsigned NOT NULL,SUM_SELECT_FULL_JOIN bigint(20) unsigned NOT NULL,SUM_SELECT_FULL_RANGE_JOIN bigint(20) unsigned NOT NULL,SUM_SELECT_RANGE bigint(20) unsigned NOT NULL,SUM_SELECT_RANGE_CHECK bigint(20) unsigned NOT NULL,SUM_SELECT_SCAN bigint(20) unsigned NOT NULL,SUM_SORT_MERGE_PASSES bigint(20) unsigned NOT NULL,SUM_SORT_RANGE bigint(20) unsigned NOT NULL,SUM_SORT_ROWS bigint(20) unsigned NOT NULL,SUM_SORT_SCAN bigint(20) unsigned NOT NULL,SUM_NO_INDEX_USED bigint(20) unsigned NOT NULL,SUM_NO_GOOD_INDEX_USED bigint(20) unsigned NOT NULL)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.prepared_statements_instances(OBJECT_INSTANCE_BEGIN bigint(20) unsigned NOT NULL,STATEMENT_ID bigint(20) unsigned NOT NULL,STATEMENT_NAME varchar(64) default NULL,SQL_TEXT longtext NOT NULL,OWNER_THREAD_ID bigint(20) unsigned NOT NULL,OWNER_EVENT_ID bigint(20) unsigned NOT NULL,OWNER_OBJECT_TYPE enum('EVENT','FUNCTION','PROCEDURE','TABLE','TRIGGER') DEFAULT NULL,OWNER_OBJECT_SCHEMA varchar(64) DEFAULT NULL,OWNER_OBJECT_NAME varchar(64) DEFAULT NULL,TIMER_PREPARE bigint(20) unsigned NOT NULL,COUNT_REPREPARE bigint(20) unsigned NOT NULL,COUNT_EXECUTE bigint(20) unsigned NOT NULL,SUM_TIMER_EXECUTE bigint(20) unsigned NOT NULL,MIN_TIMER_EXECUTE bigint(20) unsigned NOT NULL,AVG_TIMER_EXECUTE bigint(20) unsigned NOT NULL,MAX_TIMER_EXECUTE bigint(20) unsigned NOT NULL,SUM_LOCK_TIME bigint(20) unsigned NOT NULL,SUM_ERRORS bigint(20) unsigned NOT NULL,SUM_WARNINGS bigint(20) unsigned NOT NULL,SUM_ROWS_AFFECTED bigint(20) unsigned NOT NULL,SUM_ROWS_SENT bigint(20) unsigned NOT NULL,SUM_ROWS_EXAMINED bigint(20) unsigned NOT NULL,SUM_CREATED_TMP_DISK_TABLES bigint(20) unsigned NOT NULL,SUM_CREATED_TMP_TABLES bigint(20) unsigned NOT NULL,SUM_SELECT_FULL_JOIN bigint(20) unsigned NOT NULL,SUM_SELECT_FULL_RANGE_JOIN bigint(20) unsigned NOT NULL,SUM_SELECT_RANGE bigint(20) unsigned NOT NULL,SUM_SELECT_RANGE_CHECK bigint(20) unsigned NOT NULL,SUM_SELECT_SCAN bigint(20) unsigned NOT NULL,SUM_SORT_MERGE_PASSES bigint(20) unsigned NOT NULL,SUM_SORT_RANGE bigint(20) unsigned NOT NULL,SUM_SORT_ROWS bigint(20) unsigned NOT NULL,SUM_SORT_SCAN bigint(20) unsigned NOT NULL,SUM_NO_INDEX_USED bigint(20) unsigned NOT NULL,SUM_NO_GOOD_INDEX_USED bigint(20) unsigned NOT NULL)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_connection_configuration(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,HOST CHAR(60) collate utf8_bin not null,PORT INTEGER not null,USER CHAR(32) collate utf8_bin not null,NETWORK_INTERFACE CHAR(60) collate utf8_bin not null,AUTO_POSITION ENUM('1','0') not null,SSL_ALLOWED ENUM('YES','NO','IGNORED') not null,SSL_CA_FILE VARCHAR(512) not null,SSL_CA_PATH VARCHAR(512) not null,SSL_CERTIFICATE VARCHAR(512) not null,SSL_CIPHER VARCHAR(512) not null,SSL_KEY VARCHAR(512) not null,SSL_VERIFY_SERVER_CERTIFICATE ENUM('YES','NO') not null,SSL_CRL_FILE VARCHAR(255) not null,SSL_CRL_PATH VARCHAR(255) not null,CONNECTION_RETRY_INTERVAL INTEGER not null,CONNECTION_RETRY_COUNT BIGINT unsigned not null,HEARTBEAT_INTERVAL DOUBLE(10,3) unsigned not null COMMENT 'Number of seconds after which a heartbeat will be sent .',TLS_VERSION VARCHAR(255) not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_group_member_stats(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,VIEW_ID CHAR(60) collate utf8_bin not null,MEMBER_ID CHAR(36) collate utf8_bin not null,COUNT_TRANSACTIONS_IN_QUEUE BIGINT unsigned not null,COUNT_TRANSACTIONS_CHECKED BIGINT unsigned not null,COUNT_CONFLICTS_DETECTED BIGINT unsigned not null,COUNT_TRANSACTIONS_VALIDATING BIGINT unsigned not null,TRANSACTIONS_COMMITTED_ALL_MEMBERS LONGTEXT not null,LAST_CONFLICT_FREE_TRANSACTION TEXT not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_group_members(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,MEMBER_ID CHAR(36) collate utf8_bin not null,MEMBER_HOST CHAR(60) collate utf8_bin not null,MEMBER_PORT INTEGER,MEMBER_STATE CHAR(64) collate utf8_bin not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_connection_status(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,GROUP_NAME CHAR(36) collate utf8_bin not null,SOURCE_UUID CHAR(36) collate utf8_bin not null,THREAD_ID BIGINT unsigned,SERVICE_STATE ENUM('ON','OFF','CONNECTING') not null,COUNT_RECEIVED_HEARTBEATS bigint unsigned NOT NULL DEFAULT 0,LAST_HEARTBEAT_TIMESTAMP TIMESTAMP(0) not null COMMENT 'Shows when the most recent heartbeat signal was received.',RECEIVED_TRANSACTION_SET TEXT not null,LAST_ERROR_NUMBER INTEGER not null,LAST_ERROR_MESSAGE VARCHAR(1024) not null,LAST_ERROR_TIMESTAMP TIMESTAMP(0) not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_applier_configuration(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,DESIRED_DELAY INTEGER not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_applier_status(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,SERVICE_STATE ENUM('ON','OFF') not null,REMAINING_DELAY INTEGER unsigned,COUNT_TRANSACTIONS_RETRIES BIGINT unsigned not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_applier_status_by_coordinator(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,THREAD_ID BIGINT UNSIGNED,SERVICE_STATE ENUM('ON','OFF') not null,LAST_ERROR_NUMBER INTEGER not null,LAST_ERROR_MESSAGE VARCHAR(1024) not null,LAST_ERROR_TIMESTAMP TIMESTAMP(0) not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_applier_status_by_worker(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,WORKER_ID BIGINT UNSIGNED not null,THREAD_ID BIGINT UNSIGNED,SERVICE_STATE ENUM('ON','OFF') not null,LAST_SEEN_TRANSACTION CHAR(57) not null,LAST_ERROR_NUMBER INTEGER not null,LAST_ERROR_MESSAGE VARCHAR(1024) not null,LAST_ERROR_TIMESTAMP TIMESTAMP(0) not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.session_connect_attrs(PROCESSLIST_ID INT NOT NULL,ATTR_NAME VARCHAR(32) NOT NULL,ATTR_VALUE VARCHAR(1024),ORDINAL_POSITION INT)ENGINE=PERFORMANCE_SCHEMA CHARACTER SET utf8 COLLATE utf8_bin
+CREATE TABLE performance_schema.session_account_connect_attrs  LIKE performance_schema.session_connect_attrs
+CREATE TABLE performance_schema.table_handles(OBJECT_TYPE VARCHAR(64) not null,OBJECT_SCHEMA VARCHAR(64) not null,OBJECT_NAME VARCHAR(64) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,OWNER_THREAD_ID BIGINT unsigned,OWNER_EVENT_ID BIGINT unsigned,INTERNAL_LOCK VARCHAR(64),EXTERNAL_LOCK VARCHAR(64))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.metadata_locks(OBJECT_TYPE VARCHAR(64) not null,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,LOCK_TYPE VARCHAR(32) not null,LOCK_DURATION VARCHAR(32) not null,LOCK_STATUS VARCHAR(32) not null,SOURCE VARCHAR(64),OWNER_THREAD_ID BIGINT unsigned,OWNER_EVENT_ID BIGINT unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.user_variables_by_thread(THREAD_ID BIGINT unsigned not null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE LONGBLOB)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.variables_by_thread(THREAD_ID BIGINT unsigned not null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.global_variables(VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.session_variables(VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.status_by_thread(THREAD_ID BIGINT unsigned not null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.status_by_user(USER CHAR(32) collate utf8_bin default null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.status_by_host(HOST CHAR(60) collate utf8_bin default null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.status_by_account(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.global_status(VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.session_status(VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE IF NOT EXISTS proxies_priv (Host char(60) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Proxied_host char(60) binary DEFAULT '' NOT NULL, Proxied_user char(32) binary DEFAULT '' NOT NULL, With_grant BOOL DEFAULT 0 NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY Host (Host,User,Proxied_host,Proxied_user), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='User proxy privileges';
+
+FLUSH PRIVILEGES
+CREATE USER 'root'@'localhost' IDENTIFIED WITH 'mysql_native_password'
+GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' WITH GRANT OPTION
+GRANT USAGE ON *.* TO ''@'','root'@'localhost' WITH GRANT OPTION
+TRUNCATE TABLE time_zone
+TRUNCATE TABLE time_zone_name
+TRUNCATE TABLE time_zone_transition
+TRUNCATE TABLE time_zone_transition_type
+CREATE DATABASE IF NOT EXISTS `mysql`
+CREATE USER 'mysqluser'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*FBC02A898D66B9181D6F8826C045C11FD2B364A4'
+GRANT ALL PRIVILEGES ON `mysql`.* TO 'mysqluser'@'%'
+FLUSH PRIVILEGES
+GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replicator'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*D98280F03D0F78162EBDBB9C883FC01395DEA2BF'
+CREATE DATABASE connector_test
+GRANT ALL PRIVILEGES ON `connector_test`.* TO 'mysqluser'@'%'
+CREATE TABLE products (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  name VARCHAR(255) NOT NULL,
+  description VARCHAR(512),
+  weight FLOAT
+)
+ALTER TABLE products AUTO_INCREMENT = 101
+CREATE TABLE products_on_hand (
+  product_id INTEGER NOT NULL PRIMARY KEY,
+  quantity INTEGER NOT NULL,
+  FOREIGN KEY (product_id) REFERENCES products(id)
+)
+CREATE TABLE customers (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  first_name VARCHAR(255) NOT NULL,
+  last_name VARCHAR(255) NOT NULL,
+  email VARCHAR(255) NOT NULL UNIQUE KEY
+) AUTO_INCREMENT=1001
+CREATE TABLE orders (
+  order_number INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  order_date DATE NOT NULL,
+  purchaser INTEGER NOT NULL,
+  quantity INTEGER NOT NULL,
+  product_id INTEGER NOT NULL,
+  FOREIGN KEY order_customer (purchaser) REFERENCES customers(id),
+  FOREIGN KEY ordered_product (product_id) REFERENCES products(id)
+) AUTO_INCREMENT = 10001
+CREATE DATABASE emptydb
+GRANT ALL PRIVILEGES ON `emptydb`.* TO 'mysqluser'@'%'
+GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replicator'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*D98280F03D0F78162EBDBB9C883FC01395DEA2BF'
+CREATE DATABASE readbinlog_test
+GRANT ALL PRIVILEGES ON `readbinlog_test`.* TO 'mysqluser'@'%'
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)",2016-05-13T14:32:47Z,141
"@@ -148,6 +148,12 @@ public void start(Map<String, String> props) {
         maxBatchSize = config.getInteger(MySqlConnectorConfig.MAX_BATCH_SIZE);
         metronome = Metronome.parker(pollIntervalMs, TimeUnit.MILLISECONDS, Clock.SYSTEM);
 
+        // Define the filter used for database names ...
+        Predicate<String> dbFilter = Selectors.databaseSelector()
+                                              .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
+                                              .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
+                                              .build();
+
         // Define the filter using the whitelists and blacklists for tables and database names ...
         Predicate<TableId> tableFilter = Selectors.tableSelector()
                                                   .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
@@ -178,7 +184,7 @@ public void start(Map<String, String> props) {
         // Set up our handlers for specific kinds of events ...
         tables = new Tables();
         tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, clock,
-                                              tables, tableFilter, columnFilter, columnMappers);
+                                              dbFilter, tables, tableFilter, columnFilter, columnMappers);
         eventHandlers.put(EventType.ROTATE, tableConverters::rotateLogs);
         eventHandlers.put(EventType.TABLE_MAP, tableConverters::updateTableMetadata);
         eventHandlers.put(EventType.QUERY, tableConverters::updateTableCommand);",2016-05-19T21:54:22Z,10
"@@ -143,7 +143,7 @@ public Struct struct() {
         setRowInEvent(eventRowNumber);
         return offset();
     }
-
+    
     /**
      * Set the name of the MySQL binary log file.
      * ",2016-05-19T21:54:22Z,11
"@@ -18,6 +18,7 @@
 import java.util.function.Predicate;
 
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.SchemaBuilder;
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.slf4j.Logger;
@@ -40,7 +41,7 @@
 import io.debezium.relational.TableSchemaBuilder;
 import io.debezium.relational.Tables;
 import io.debezium.relational.history.DatabaseHistory;
-import io.debezium.relational.history.HistoryRecord;
+import io.debezium.relational.history.HistoryRecord.Fields;
 import io.debezium.relational.mapping.ColumnMappers;
 import io.debezium.text.ParsingException;
 import io.debezium.util.Clock;
@@ -53,6 +54,32 @@
 @NotThreadSafe
 final class TableConverters {
 
+    protected static final Schema SCHEMA_CHANGE_RECORD_KEY_SCHEMA = SchemaBuilder.struct()
+                                                                                 .name(""io.debezium.connector.mysql.SchemaRecordKey"")
+                                                                                 .field(Fields.DATABASE_NAME, Schema.STRING_SCHEMA)
+                                                                                 .build();
+
+    protected static final Schema SCHEMA_CHANGE_RECORD_VALUE_SCHEMA = SchemaBuilder.struct()
+                                                                                   .name(""io.debezium.connector.mysql.SchemaRecordKey"")
+                                                                                   .field(Fields.SOURCE, SourceInfo.SCHEMA)
+                                                                                   .field(Fields.DATABASE_NAME, Schema.STRING_SCHEMA)
+                                                                                   .field(Fields.DDL_STATEMENTS, Schema.STRING_SCHEMA)
+                                                                                   .build();
+
+    public Struct schemaChangeRecordKey(String databaseName) {
+        Struct result = new Struct(SCHEMA_CHANGE_RECORD_KEY_SCHEMA);
+        result.put(Fields.DATABASE_NAME, databaseName);
+        return result;
+    }
+
+    public Struct schemaChangeRecordValue(SourceInfo source, String databaseName, String ddlStatements) {
+        Struct result = new Struct(SCHEMA_CHANGE_RECORD_VALUE_SCHEMA);
+        result.put(Fields.SOURCE, source.struct());
+        result.put(Fields.DATABASE_NAME, databaseName);
+        result.put(Fields.DDL_STATEMENTS, ddlStatements);
+        return result;
+    }
+
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final DatabaseHistory dbHistory;
     private final TopicSelector topicSelector;
@@ -63,6 +90,7 @@ final class TableConverters {
     private final Map<Long, Converter> convertersByTableId = new HashMap<>();
     private final Map<String, Long> tableNumbersByTableName = new HashMap<>();
     private final boolean recordSchemaChangesInSourceRecords;
+    private final Predicate<String> dbFilter;
     private final Predicate<TableId> tableFilter;
     private final Predicate<ColumnId> columnFilter;
     private final ColumnMappers columnMappers;
@@ -71,15 +99,17 @@ final class TableConverters {
     private final Clock clock;
 
     public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
-            boolean recordSchemaChangesInSourceRecords, Clock clock, Tables tables,
+            boolean recordSchemaChangesInSourceRecords, Clock clock, Predicate<String> dbFilter, Tables tables,
             Predicate<TableId> tableFilter, Predicate<ColumnId> columnFilter, ColumnMappers columnSelectors) {
         Objects.requireNonNull(topicSelector, ""A topic selector is required"");
         Objects.requireNonNull(dbHistory, ""Database history storage is required"");
         Objects.requireNonNull(tables, ""A Tables object is required"");
         Objects.requireNonNull(clock, ""A Clock object is required"");
+        Objects.requireNonNull(dbFilter, ""A database filter object is required"");
         this.topicSelector = topicSelector;
         this.dbHistory = dbHistory;
         this.clock = clock;
+        this.dbFilter = dbFilter;
         this.tables = tables;
         this.columnFilter = columnFilter;
         this.columnMappers = columnSelectors;
@@ -124,12 +154,17 @@ public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRe
             // Record the DDL statement so that we can later recover them if needed ...
             dbHistory.record(source.partition(), source.offset(), databaseName, tables, ddlStatements);
 
-            if (recordSchemaChangesInSourceRecords) {
+            if (recordSchemaChangesInSourceRecords && dbFilter.test(databaseName)) {
                 String serverName = source.serverName();
                 String topicName = topicSelector.getTopic(serverName);
-                HistoryRecord historyRecord = new HistoryRecord(source.partition(), source.offset(), databaseName, ddlStatements);
-                recorder.accept(new SourceRecord(source.partition(), source.offset(), topicName, 0,
-                        Schema.STRING_SCHEMA, databaseName, Schema.STRING_SCHEMA, historyRecord.document().toString()));
+                Integer partition = 0;
+                Struct key = schemaChangeRecordKey(databaseName);
+                Struct value = schemaChangeRecordValue(source, databaseName, ddlStatements);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(),
+                        topicName, partition,
+                        SCHEMA_CHANGE_RECORD_KEY_SCHEMA, key,
+                        SCHEMA_CHANGE_RECORD_VALUE_SCHEMA, value);
+                recorder.accept(record);
             }
         }
 ",2016-05-19T21:54:22Z,28
"@@ -9,7 +9,6 @@
 
 import java.nio.file.Path;
 import java.sql.SQLException;
-import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
 
@@ -49,7 +48,7 @@ public void afterEach() {
         stopConnector();
         Testing.Files.delete(DB_HISTORY_PATH);
     }
-    
+
     /**
      * Verifies that the connector doesn't run with an invalid configuration. This does not actually connect to the MySQL server.
      */
@@ -70,15 +69,7 @@ public void shouldNotStartWithInvalidConfiguration() {
     }
 
     @Test
-    public void shouldStartAndPollShouldReturnSourceRecordsFromDatabase() throws SQLException {
-        try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
-            try (JdbcConnection connection = db.connect()) {
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
-                connection.execute(""INSERT INTO products VALUES (default,'robot','Toy robot',1.304);"");
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
-            }
-        }
-
+    public void shouldConsumeAllEventsFromDatabase() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()
                               .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
@@ -90,70 +81,97 @@ public void shouldStartAndPollShouldReturnSourceRecordsFromDatabase() throws SQL
                               .with(MySqlConnectorConfig.INITIAL_BINLOG_FILENAME, ""mysql-bin.000001"")
                               .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test"")
                               .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
+        //waitForAvailableRecords(10, TimeUnit.SECONDS);
 
-        waitForAvailableRecords(10, TimeUnit.SECONDS);
-
+        // Consume the first records due to startup and initialization of the database ...
+        //Testing.Print.enable();
+        SourceRecords records = consumeRecordsByTopic(6+9+9+4+5);
+        assertThat(records.recordsForTopic(""kafka-connect"").size()).isEqualTo(6);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.products"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.products_on_hand"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.customers"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.orders"").size()).isEqualTo(5);
+        assertThat(records.topics().size()).isEqualTo(5);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"").size()).isEqualTo(6);
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        
+        records.ddlRecordsForDatabase(""connector_test"").forEach(this::print);
+        //records.ddlRecordsForDatabase(""readbinlog_test"").forEach(this::print);
+        
+        // Make sure there are no more ...
+        Testing.Print.disable();
+        waitForAvailableRecords(3, TimeUnit.SECONDS);
+        int totalConsumed = consumeAvailableRecords(this::print);
+        assertThat(totalConsumed).isEqualTo(0);
+        stopConnector();
+        
+        // Make some changes to data only ...
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
-                connection.execute(""INSERT INTO products VALUES (default,'harrison','real robot',134.82);"");
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
+                connection.execute(""INSERT INTO products VALUES (default,'robot','Toy robot',1.304);"");
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
             }
         }
-        
-        //Testing.Print.enable();
-        int totalConsumed = consumeAvailableRecords(this::print);  // expecting at least 1
-        stopConnector();
-        
-        // Restart the connector and wait for a few seconds (at most) for records that will never arrive ...
+
+        // Restart the connector and read the insert record ...
         start(MySqlConnector.class, config);
-        waitForAvailableRecords(2, TimeUnit.SECONDS);
-        totalConsumed += consumeAvailableRecords(this::print);
-        stopConnector();
+        records = consumeRecordsByTopic(1);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.products"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(1);
         
         // Create an additional few records ...
-        Testing.Print.disable();
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
                 connection.execute(""INSERT INTO products VALUES (1001,'roy','old robot',1234.56);"");
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
             }
         }
-
-        // Restart the connector and wait for a few seconds (at most) for the new record ...
-        //Testing.Print.enable();
-        start(MySqlConnector.class, config);
-        waitForAvailableRecords(5, TimeUnit.SECONDS);
-        totalConsumed += consumeAvailableRecords(this::print);
-
+        
+        // And consume the one insert ...
+        records = consumeRecordsByTopic(1);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.products"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(1);
+        List<SourceRecord> inserts = records.recordsForTopic(""kafka-connect.connector_test.products"");
+        assertInsert(inserts.get(0), ""id"", 1001);
+
+        // Update one of the records by changing its primary key ...
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
                 connection.execute(""UPDATE products SET id=2001, description='really old robot' WHERE id=1001"");
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
             }
         }
-        waitForAvailableRecords(5, TimeUnit.SECONDS);
-        List<SourceRecord> deletes = new ArrayList<>();
-        totalConsumed += consumeAvailableRecords(deletes::add);
+        // And consume the update of the PK, which is one insert followed by a delete followed by a tombstone ...
+        records = consumeRecordsByTopic(3);
+        List<SourceRecord> updates = records.recordsForTopic(""kafka-connect.connector_test.products"");
+        assertThat(updates.size()).isEqualTo(3);
+        assertInsert(updates.get(0), ""id"", 2001);
+        assertDelete(updates.get(1), ""id"", 1001);
+        assertTombstone(updates.get(2), ""id"", 1001);
+
+        // Stop the connector ...
         stopConnector();
-        
-        // Verify that the update of a record where the pk changes results in
-        // 1 update, 1 delete, and 1 tombstone event ...
-        assertThat(deletes.size()).isEqualTo(3);
-        assertInsert(deletes.get(0),""id"",2001);
-        assertDelete(deletes.get(1),""id"",1001);
-        assertTombstone(deletes.get(2),""id"",1001);
-
-        // We should have seen a total of 33 events, though when they appear may vary ...
-        assertThat(totalConsumed).isEqualTo(33);
     }
-    
+
     @Test
-    public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLException {
+    public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLException, InterruptedException {
         Testing.Files.delete(DB_HISTORY_PATH);
+        
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()
                               .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
@@ -167,42 +185,49 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
                               .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test"")
                               .with(MySqlConnectorConfig.COLUMN_BLACKLIST, ""connector_test.orders.order_number"")
                               .with(MySqlConnectorConfig.MASK_COLUMN(12), ""connector_test.customers.email"")
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, false)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
                               .build();
+
         // Start the connector ...
         start(MySqlConnector.class, config);
 
-        // Wait for records to become available ...
-        //Testing.Print.enable();
-        waitForAvailableRecords(15, TimeUnit.SECONDS);
+        // Consume the first records due to startup and initialization of the database ...
+        // Testing.Print.enable();
+        SourceRecords records = consumeRecordsByTopic(9+9+4+5);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.products"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.products_on_hand"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.customers"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.orders"").size()).isEqualTo(5);
+        assertThat(records.topics().size()).isEqualTo(4);
         
-        // Now consume the records ...
-        int totalConsumed = consumeAvailableRecords((record)->{
+        // More records may have been written (if this method were run after the others), but we don't care ...
+        stopConnector();
+
+        // Check that the orders.order_number is not present ...
+        records.recordsForTopic(""kafka-connect-2.connector_test.orders"").forEach(record->{
             print(record);
-            if ( record.topic().endsWith("".orders"")) {
-                Struct value = (Struct) record.value();
-                try {
-                    value.get(""order_number"");
-                    fail(""The 'order_number' field was found but should not exist"");
-                } catch ( DataException e ) {
-                    // expected
-                    printJson(record);
-                }
-            } else if ( record.topic().endsWith("".customers"")) {
-                Struct value = (Struct) record.value();
-                if ( value.getStruct(""after"") != null ) {
-                    assertThat(value.getStruct(""after"").getString(""email"")).isEqualTo(""************"");
-                }
-                if ( value.getStruct(""before"") != null ) {
-                    assertThat(value.getStruct(""before"").getString(""email"")).isEqualTo(""************"");
-                }
+            Struct value = (Struct) record.value();
+            try {
+                value.get(""order_number"");
+                fail(""The 'order_number' field was found but should not exist"");
+            } catch (DataException e) {
+                // expected
                 printJson(record);
             }
         });
-        stopConnector();
-
-        // We should have seen a total of 27 events, though when they appear may vary ...
-        assertThat(totalConsumed).isEqualTo(27);
+        
+        // Check that the customer.email is masked ...
+        records.recordsForTopic(""kafka-connect-2.connector_test.customers"").forEach(record->{
+            Struct value = (Struct) record.value();
+            if (value.getStruct(""after"") != null) {
+                assertThat(value.getStruct(""after"").getString(""email"")).isEqualTo(""************"");
+            }
+            if (value.getStruct(""before"") != null) {
+                assertThat(value.getStruct(""before"").getString(""email"")).isEqualTo(""************"");
+            }
+            printJson(record);
+        });
     }
-    
+
 }",2016-05-19T21:54:22Z,88
"@@ -28,6 +28,68 @@
 @Immutable
 public class Selectors {
     
+    /**
+     * Obtain a new {@link TableSelectionPredicateBuilder builder} for a table selection predicate.
+     * 
+     * @return the builder; never null
+     */
+    public static DatabaseSelectionPredicateBuilder databaseSelector() {
+        return new DatabaseSelectionPredicateBuilder();
+    }
+
+    /**
+     * A builder of a database predicate.
+     */
+    public static class DatabaseSelectionPredicateBuilder {
+        private Predicate<String> dbInclusions;
+        private Predicate<String> dbExclusions;
+
+        /**
+         * Specify the names of the databases that should be included. This method will override previously included and
+         * {@link #excludeDatabases(String) excluded} databases.
+         * 
+         * @param databaseNames the comma-separated list of database names to include; may be null or empty
+         * @return this builder so that methods can be chained together; never null
+         */
+        public DatabaseSelectionPredicateBuilder includeDatabases(String databaseNames) {
+            if (databaseNames == null || databaseNames.trim().isEmpty()) {
+                dbInclusions = null;
+            } else {
+                dbInclusions = Predicates.includes(databaseNames);
+            }
+            return this;
+        }
+
+        /**
+         * Specify the names of the databases that should be excluded. This method will override previously {@link
+         * #excludeDatabases(String) excluded} databases, although {@link #includeDatabases(String) including databases} overrides
+         * exclusions.
+         * 
+         * @param databaseNames the comma-separated list of database names to exclude; may be null or empty
+         * @return this builder so that methods can be chained together; never null
+         */
+        public DatabaseSelectionPredicateBuilder excludeDatabases(String databaseNames) {
+            if (databaseNames == null || databaseNames.trim().isEmpty()) {
+                dbExclusions = null;
+            } else {
+                dbExclusions = Predicates.excludes(databaseNames);
+            }
+            return this;
+        }
+
+        /**
+         * Build the {@link Predicate} that determines whether a database identified by its name is to be included.
+         * 
+         * @return the table selection predicate; never null
+         * @see #includeDatabases(String)
+         * @see #excludeDatabases(String)
+         */
+        public Predicate<String> build() {
+            Predicate<String> dbFilter = dbInclusions != null ? dbInclusions : dbExclusions;
+            return dbFilter != null ? dbFilter : (id) -> true;
+        }
+    }
+
     /**
      * Obtain a new {@link TableSelectionPredicateBuilder builder} for a table selection predicate.
      * 
@@ -38,9 +100,7 @@ public static TableSelectionPredicateBuilder tableSelector() {
     }
 
     /**
-     * A builder of {@link Selectors}.
-     * 
-     * @author Randall Hauch
+     * A builder of a table predicate.
      */
     public static class TableSelectionPredicateBuilder {
         private Predicate<String> dbInclusions;",2016-05-19T21:54:22Z,142
"@@ -10,20 +10,28 @@
 import io.debezium.document.Document;
 
 public class HistoryRecord {
+
+    public static final class Fields {
+        public static final String SOURCE = ""source"";
+        public static final String POSITION = ""position"";
+        public static final String DATABASE_NAME = ""databaseName"";
+        public static final String DDL_STATEMENTS = ""ddl"";
+    }
+
     private final Document doc;
-    
+
     public HistoryRecord(Document document) {
         this.doc = document;
     }
 
     public HistoryRecord(Map<String, ?> source, Map<String, ?> position, String databaseName, String ddl) {
         this.doc = Document.create();
-        Document src = doc.setDocument(""source"");
+        Document src = doc.setDocument(Fields.SOURCE);
         if (source != null) source.forEach(src::set);
-        Document pos = doc.setDocument(""position"");
+        Document pos = doc.setDocument(Fields.POSITION);
         if (position != null) position.forEach(pos::set);
-        if (databaseName != null) doc.setString(""databaseName"", databaseName);
-        if (ddl != null) doc.setString(""ddl"", ddl);
+        if (databaseName != null) doc.setString(Fields.DATABASE_NAME, databaseName);
+        if (ddl != null) doc.setString(Fields.DDL_STATEMENTS, ddl);
     }
 
     public Document document() {
@@ -61,7 +69,7 @@ protected boolean hasSameDatabase(HistoryRecord other) {
         if (this == other) return true;
         return other != null && databaseName().equals(other.databaseName());
     }
-    
+
     @Override
     public String toString() {
         return doc.toString();",2016-05-19T21:54:22Z,143
"@@ -8,9 +8,12 @@
 import static org.junit.Assert.fail;
 
 import java.nio.file.Path;
+import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.ArrayBlockingQueue;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.CountDownLatch;
@@ -199,7 +202,13 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
-                               .notifying(consumedLines::add)
+                               .notifying((record)->{
+                                   try {
+                                       consumedLines.put(record);
+                                   } catch ( InterruptedException e ) {
+                                       Thread.interrupted();
+                                   }
+                                })
                                .using(this.getClass().getClassLoader())
                                .using(wrapperCallback)
                                .build();
@@ -255,7 +264,7 @@ protected int consumeRecords(int numberOfRecords) throws InterruptedException {
      */
     protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordConsumer) throws InterruptedException {
         int recordsConsumed = 0;
-        for (int i = 0; i != numberOfRecords; ++i) {
+        while ( recordsConsumed < numberOfRecords ) {
             SourceRecord record = consumedLines.poll(pollTimeoutInMs, TimeUnit.MILLISECONDS);
             if (record != null) {
                 ++recordsConsumed;
@@ -266,6 +275,60 @@ protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordC
         }
         return recordsConsumed;
     }
+    
+    protected SourceRecords consumeRecordsByTopic(int numRecords) throws InterruptedException {
+        return consumeRecordsByTopic(numRecords, new SourceRecords());
+    }
+    
+    protected SourceRecords consumeRecordsByTopic(int numRecords, SourceRecords records) throws InterruptedException {
+        consumeRecords(numRecords,records::add);
+        return records;
+    }
+    
+    protected class SourceRecords {
+        private final List<SourceRecord> records = new ArrayList<>();
+        private final Map<String,List<SourceRecord>> recordsByTopic = new HashMap<>();
+        private final Map<String,List<SourceRecord>> ddlRecordsByDbName = new HashMap<>();
+        public void add( SourceRecord record ) {
+            records.add(record);
+            recordsByTopic.compute(record.topic(), (topicName,list)->{
+                if ( list == null ) list = new ArrayList<SourceRecord>();
+                list.add(record);
+                return list;
+            });
+            if ( record.key() instanceof Struct ) {
+                Struct key = (Struct)record.key();
+                if ( key.schema().field(""databaseName"") != null) {
+                    String dbName = key.getString(""databaseName"");
+                    ddlRecordsByDbName.compute(dbName, (databaseName,list)->{
+                        if ( list == null ) list = new ArrayList<SourceRecord>();
+                        list.add(record);
+                        return list;
+                    });
+                }
+            }
+        }
+        public List<SourceRecord> ddlRecordsForDatabase( String dbName ) {
+            return ddlRecordsByDbName.get(dbName);
+        }
+        public Set<String> databaseNames() {
+            return ddlRecordsByDbName.keySet();
+        }
+        public List<SourceRecord> recordsForTopic( String topicName ) {
+            return recordsByTopic.get(topicName);
+        }
+        public Set<String> topics() {
+            return recordsByTopic.keySet();
+        }
+        public void print() {
+            Testing.print("""" + topics().size() + "" topics: "" + topics());
+            recordsByTopic.forEach((k,v)->{
+                Testing.print("" - topic:'"" + k + ""'; # of events = "" + v.size());
+            });
+            Testing.print(""Records:"" );
+            records.forEach(record->AbstractConnectorTest.this.print(record));
+        }
+    }
 
     /**
      * Try to consume all of the messages that have already been returned by the connector.
@@ -281,7 +344,7 @@ protected int consumeAvailableRecords(Consumer<SourceRecord> recordConsumer) {
         }
         return records.size();
     }
-
+    
     /**
      * Wait for a maximum amount of time until the first record is available.
      * ",2016-05-19T21:54:22Z,61
"@@ -969,7 +969,9 @@ protected void parseRenameTable(Marker start) {
         tokens.consume(""TO"");
         TableId to = parseQualifiedTableName(start);
         databaseTables.renameTable(from, to);
-        signalAlterTable(from, to, start);
+        // Signal a separate statement for this table rename action, even though multiple renames might be
+        // performed by a single DDL statement on the token stream ...
+        signalAlterTable(from,to,""RENAME TABLE "" + from + "" TO "" + to);
     }
 
     protected List<String> parseColumnNameList(Marker start) {",2016-04-12T22:58:07Z,33
"@@ -22,6 +22,7 @@
 import io.debezium.relational.TableId;
 import io.debezium.relational.Tables;
 import io.debezium.relational.ddl.DdlParser;
+import io.debezium.relational.ddl.DdlParserListener.Event;
 import io.debezium.relational.ddl.SimpleDdlParserListener;
 import io.debezium.util.IoUtil;
 import io.debezium.util.Testing;
@@ -159,6 +160,7 @@ public void shouldParseTestStatements() {
         Testing.print(tables);
         assertThat(tables.size()).isEqualTo(6); // no tables
         assertThat(listener.total()).isEqualTo(49);
+        // listener.forEach(this::printEvent);
     }
 
     @Test
@@ -167,6 +169,10 @@ public void shouldParseSomeLinesFromCreateStatements() {
         assertThat(tables.size()).isEqualTo(39); // no tables
         assertThat(listener.total()).isEqualTo(120);
     }
+    
+    protected void printEvent( Event event ) {
+        System.out.println(event);
+    }
 
     protected String readFile( String classpathResource ) {
         try ( InputStream stream = getClass().getClassLoader().getResourceAsStream(classpathResource); ) {",2016-04-12T22:58:07Z,36
"@@ -391,6 +391,17 @@ protected void signalAlterTable(TableId id, TableId previousId, Marker statement
         signalEvent(new TableAlteredEvent(id, previousId, statement(statementStart), false));
     }
 
+    /**
+     * Signal an alter table event to all listeners.
+     * 
+     * @param id the table identifier; may not be null
+     * @param previousId the previous name of the view if it was renamed, or null if it was not renamed
+     * @param statement the DDL statement; may not be null
+     */
+    protected void signalAlterTable(TableId id, TableId previousId, String statement) {
+        signalEvent(new TableAlteredEvent(id, previousId, statement, false));
+    }
+
     /**
      * Signal a drop table event to all listeners.
      * ",2016-04-12T22:58:07Z,85
"@@ -8,6 +8,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicLong;
+import java.util.function.Consumer;
 
 import static org.fest.assertions.Assertions.assertThat;
 
@@ -167,4 +168,12 @@ public EventAssert assertNext() {
         assertThat( events.isEmpty()).isFalse();
         return new EventAssert(events.remove(0));
     }
+
+    /**
+     * Perform an operation on each of the events.
+     * @param eventConsumer the event consumer function; may not be null
+     */
+    public void forEach( Consumer<Event> eventConsumer ) {
+        events.forEach(eventConsumer);
+    }
 }",2016-04-12T22:58:07Z,144
"@@ -17,6 +17,7 @@
 import java.util.concurrent.LinkedBlockingDeque;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Consumer;
 import java.util.function.Predicate;
 
@@ -67,6 +68,8 @@ public final class MySqlConnectorTask extends SourceTask {
                                                                              ""slave_relay_log_info"", ""slave_master_info"",
                                                                              ""slave_worker_info"", ""gtid_executed"",
                                                                              ""server_cost"", ""engine_cost"");
+    private final Set<String> BUILT_IN_DB_NAMES = Collect.unmodifiableSet(""mysql"", ""performance_schema"");
+
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final TopicSelector topicSelector;
 
@@ -81,6 +84,7 @@ public final class MySqlConnectorTask extends SourceTask {
     private int maxBatchSize;
     private String serverName;
     private Metronome metronome;
+    private final AtomicBoolean running = new AtomicBoolean(false);
 
     // Used in the methods that process events ...
     private final SourceInfo source = new SourceInfo();
@@ -122,6 +126,7 @@ public void start(Map<String, String> props) {
                                                                                                                  // prefix
         this.dbHistory.configure(dbHistoryConfig); // validates
         this.dbHistory.start();
+        this.running.set(true);
 
         // Read the configuration ...
         final String user = config.getString(MySqlConnectorConfig.USER);
@@ -145,7 +150,9 @@ public void start(Map<String, String> props) {
                                                         config.getString(MySqlConnectorConfig.TABLE_WHITELIST),
                                                         config.getString(MySqlConnectorConfig.TABLE_BLACKLIST));
         if (config.getBoolean(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN)) {
-            Predicate<TableId> ignoreBuiltins = (id) -> !BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase());
+            Predicate<TableId> ignoreBuiltins = (id) -> {
+                return !BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase()) && !BUILT_IN_DB_NAMES.contains(id.catalog().toLowerCase());
+            };
             tableFilter = ignoreBuiltins.or(tableFilter);
         }
 
@@ -197,7 +204,7 @@ public void start(Map<String, String> props) {
                 logger.info(""Recovering MySQL connector '{}' database schemas from history stored in {}"", serverName, dbHistory);
                 DdlParser ddlParser = new MySqlDdlParser();
                 dbHistory.recover(source.partition(), source.offset(), tables, ddlParser);
-                logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, tables);
+                logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, tables.subset(tableFilter));
             } catch (Throwable t) {
                 throw new ConnectException(""Failure while recovering database schemas"", t);
             }
@@ -229,7 +236,7 @@ public void start(Map<String, String> props) {
     @Override
     public List<SourceRecord> poll() throws InterruptedException {
         logger.trace(""Polling for events from MySQL server '{}'"", serverName);
-        while (events.drainTo(batchEvents, maxBatchSize - batchEvents.size()) == 0 || batchEvents.isEmpty()) {
+        while (running.get() && (events.drainTo(batchEvents, maxBatchSize - batchEvents.size()) == 0 || batchEvents.isEmpty())) {
             // No events to process, so sleep for a bit ...
             metronome.pause();
         }
@@ -263,6 +270,8 @@ public List<SourceRecord> poll() throws InterruptedException {
                     source.setRowInEvent(0);
                 }
             }
+            
+            if ( !running.get()) break;
 
             // If there is a handler for this event, forward the event to it ...
             EventHandler handler = eventHandlers.get(eventType);
@@ -272,6 +281,12 @@ public List<SourceRecord> poll() throws InterruptedException {
         }
         logger.trace(""Completed processing {} events from MySQL server '{}'"", serverName);
 
+        if (!this.running.get()) {
+            // We're supposed to stop, so return nothing that we might have already processed
+            // so that no records get persisted if DB history has already been stopped ...
+            return null;
+        }
+
         // We've processed them all, so clear the batch and return the records ...
         assert batchEvents.isEmpty();
         return records;
@@ -280,6 +295,10 @@ public List<SourceRecord> poll() throws InterruptedException {
     @Override
     public void stop() {
         try {
+            // Signal to the 'poll()' method that it should stop what its doing ...
+            this.running.set(false);
+
+            // Flush and stop the database history ...
             logger.debug(""Stopping database history for MySQL server '{}'"", serverName);
             dbHistory.stop();
         } catch (Throwable e) {",2016-03-03T21:27:11Z,10
"@@ -95,7 +95,32 @@ public void shouldStartAndPollShouldReturnSourceRecordsFromDatabase() throws SQL
         }
         
         Testing.Print.enable();
-        assertThat(consumeAvailableRecords(this::print)).isGreaterThan(0); // expecting at least 1
+        int totalConsumed = consumeAvailableRecords(this::print);  // expecting at least 1
         stopConnector();
+        
+        // Restart the connector and wait for a few seconds (at most) for records that will never arrive ...
+        start(MySqlConnector.class, config);
+        waitForAvailableRecords(2, TimeUnit.SECONDS);
+        totalConsumed += consumeAvailableRecords(this::print);
+        stopConnector();
+        
+        // Create an additional few records ...
+        Testing.Print.disable();
+        try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
+            try (JdbcConnection connection = db.connect()) {
+                connection.execute(""INSERT INTO products VALUES (default,'roy','old robot',1234.56);"");
+                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
+            }
+        }
+
+        // Restart the connector and wait for a few seconds (at most) for the new record ...
+        Testing.Print.enable();
+        start(MySqlConnector.class, config);
+        waitForAvailableRecords(5, TimeUnit.SECONDS);
+        totalConsumed += consumeAvailableRecords(this::print);
+        stopConnector();
+
+        // We should have seen a total of 30 events, though when they appear may vary ...
+        assertThat(totalConsumed).isEqualTo(30);
     }
 }",2016-03-03T21:27:11Z,88
"@@ -12,6 +12,7 @@
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.function.Function;
+import java.util.function.Predicate;
 
 import org.apache.kafka.connect.data.Schema;
 
@@ -314,6 +315,19 @@ public boolean equals(Object obj) {
         return false;
     }
 
+    public Tables subset(Predicate<TableId> filter) {
+        if (filter == null) return this;
+        return lock.read(() -> {
+            Tables result = new Tables();
+            tablesByTableId.forEach((tableId, table) -> {
+                if (filter.test(tableId)) {
+                    result.overwriteTable(table);
+                }
+            });
+            return result;
+        });
+    }
+
     @Override
     public String toString() {
         return lock.read(() -> {",2016-03-03T21:27:11Z,111
"@@ -47,7 +47,7 @@ public static void enable() {
         }
 
         public static void disable() {
-            enabled = true;
+            enabled = false;
         }
 
         public static boolean isEnabled() {",2016-03-03T21:27:11Z,145
"@@ -469,7 +469,7 @@ protected void maybeFlush(OffsetStorageWriter offsetWriter, OffsetCommitPolicy p
 
             long started = clock.currentTimeInMillis();
             long timeout = started + commitTimeoutMs;
-            offsetWriter.beginFlush();
+            if ( !offsetWriter.beginFlush() ) return;
             Future<Void> flush = offsetWriter.doFlush(this::completedFlush);
             if (flush == null) return; // no offsets to commit ...
 ",2016-03-03T21:27:11Z,2
"@@ -83,9 +83,7 @@ public void stopConnector(BooleanConsumer callback) {
             if (engine != null && engine.isRunning()) {
                 engine.stop();
                 try {
-                    while (!engine.await(5, TimeUnit.SECONDS)) {
-                        // Wait for connector to stop completely ...
-                    }
+                    engine.await(5, TimeUnit.SECONDS);
                 } catch (InterruptedException e) {
                     Thread.interrupted();
                 }
@@ -101,6 +99,15 @@ public void stopConnector(BooleanConsumer callback) {
                     Thread.interrupted();
                 }
             }
+            if (engine != null && engine.isRunning()) {
+                try {
+                    while (!engine.await(5, TimeUnit.SECONDS)) {
+                        // Wait for connector to stop completely ...
+                    }
+                } catch (InterruptedException e) {
+                    Thread.interrupted();
+                }
+            }
             if (callback != null) callback.accept(engine != null ? engine.isRunning() : false);
         } finally {
             engine = null;",2016-03-03T21:27:11Z,61
"@@ -204,6 +204,7 @@ public void start(Map<String, String> props) {
                 logger.info(""Recovering MySQL connector '{}' database schemas from history stored in {}"", serverName, dbHistory);
                 DdlParser ddlParser = new MySqlDdlParser();
                 dbHistory.recover(source.partition(), source.offset(), tables, ddlParser);
+                tableConverters.loadTables();
                 logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, tables.subset(tableFilter));
             } catch (Throwable t) {
                 throw new ConnectException(""Failure while recovering database schemas"", t);",2016-03-03T21:27:39Z,10
"@@ -76,6 +76,15 @@ public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
         Predicate<TableId> knownTables = (id) -> !unknownTableIds.contains(id); // known if not unknown
         this.tableFilter = tableFilter != null ? tableFilter.and(knownTables) : knownTables;
     }
+    
+    public void loadTables() {
+        // Create TableSchema instances for any existing table ...
+        this.tables.tableIds().forEach(id->{
+            Table table = this.tables.forTable(id);
+            TableSchema schema = schemaBuilder.create(table, false);
+            tableSchemaByTableId.put(id, schema);
+        });
+    }
 
     public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
         QueryEventData command = event.getData();",2016-03-03T21:27:39Z,28
"@@ -16,6 +16,7 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
 import java.util.function.Consumer;
+import java.util.function.Predicate;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -36,6 +37,7 @@
 
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
+import io.debezium.relational.TableId;
 import io.debezium.relational.Tables;
 import io.debezium.relational.ddl.DdlParser;
 import io.debezium.relational.history.DatabaseHistory;
@@ -133,13 +135,19 @@ public void start(Map<String, String> props) {
                          MySqlConnectorConfig.MAX_BATCH_SIZE, MySqlConnectorConfig.MAX_QUEUE_SIZE, maxBatchSize);
         }
 
+        // Define the filter using the whitelists and blacklists for tables and database names ...
+        Predicate<TableId> tableFilter = TableId.filter(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST),
+                                                        config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST),
+                                                        config.getString(MySqlConnectorConfig.TABLE_WHITELIST),
+                                                        config.getString(MySqlConnectorConfig.TABLE_BLACKLIST));
+
         // Create the queue ...
         events = new LinkedBlockingDeque<>(maxQueueSize);
         batchEvents = new ArrayList<>(maxBatchSize);
 
-        // Set up our handlers ...
+        // Set up our handlers for specific kinds of events ...
         tables = new Tables();
-        tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, tables);
+        tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, tables, tableFilter);
         eventHandlers.put(EventType.TABLE_MAP, tableConverters::updateTableMetadata);
         eventHandlers.put(EventType.QUERY, tableConverters::updateTableCommand);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, tableConverters::handleInsert);",2016-02-04T13:56:13Z,10
"@@ -84,10 +84,26 @@ public class MySqlConnectorConfig {
                                                             .withDefault(false)
                                                             .withValidation(Field::isBoolean);
 
+    public static final Field TABLE_WHITELIST = Field.create(""table.whitelist"")
+                                                     .withDescription(""A comma-separated list of table identifiers to be monitored, where each identifer consists ""
+                                                             + ""of the '<databaseName>.<tableName>'. A whitelist takes precedence over any blacklist."");
+
+    public static final Field TABLE_BLACKLIST = Field.create(""table.blacklist"")
+                                                     .withDescription(""A comma-separated list of table identifiers to not be monitored, where each identifer consists ""
+                                                             + ""of the '<databaseName>.<tableName>'. Any whitelist takes precedence over this blacklist."");
+
+    public static final Field DATABASE_WHITELIST = Field.create(""database.whitelist"")
+                                                        .withDescription(""A comma-separated list of database names to be monitored. A database whitelist takes precedence over any database blacklist and supersedes a table whitelist or table blacklist."");
+
+    public static final Field DATABASE_BLACKLIST = Field.create(""database.blacklist"")
+                                                        .withDescription(""A comma-separated list of database names to not be monitored. Any database whitelist takes precedence over this blacklist, and supersedes a table whitelist or table blacklist."");
+
     public static Collection<Field> ALL_FIELDS = Collect.arrayListOf(USER, PASSWORD, HOSTNAME, PORT, SERVER_ID,
                                                                      SERVER_NAME, CONNECTION_TIMEOUT_MS, KEEP_ALIVE,
                                                                      MAX_QUEUE_SIZE, MAX_BATCH_SIZE, POLL_INTERVAL_MS,
-                                                                     DATABASE_HISTORY, INCLUDE_SCHEMA_CHANGES);
+                                                                     DATABASE_HISTORY, INCLUDE_SCHEMA_CHANGES,
+                                                                     TABLE_WHITELIST, TABLE_BLACKLIST,
+                                                                     DATABASE_WHITELIST, DATABASE_BLACKLIST);
 
     private static int validateMaxQueueSize(Configuration config, Field field, Consumer<String> problems) {
         int maxQueueSize = config.getInteger(field);",2016-02-04T13:56:13Z,65
"@@ -12,6 +12,7 @@
 import java.util.Objects;
 import java.util.Set;
 import java.util.function.Consumer;
+import java.util.function.Predicate;
 
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.Struct;
@@ -53,9 +54,11 @@ final class TableConverters {
     private final Map<Long, Converter> convertersByTableId = new HashMap<>();
     private final Map<String, Long> tableNumbersByTableName = new HashMap<>();
     private final boolean recordSchemaChangesInSourceRecords;
+    private final Predicate<TableId> tableFilter;
 
     public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
-            boolean recordSchemaChangesInSourceRecords, Tables tables) {
+            boolean recordSchemaChangesInSourceRecords, Tables tables,
+            Predicate<TableId> tableFilter) {
         Objects.requireNonNull(topicSelector, ""A topic selector is required"");
         Objects.requireNonNull(dbHistory, ""Database history storage is required"");
         Objects.requireNonNull(tables, ""A Tables object is required"");
@@ -64,6 +67,7 @@ public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
         this.tables = tables;
         this.ddlParser = new MySqlDdlParser(false); // don't include views
         this.recordSchemaChangesInSourceRecords = recordSchemaChangesInSourceRecords;
+        this.tableFilter = tableFilter != null ? tableFilter : (id) -> true;
     }
 
     public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
@@ -133,6 +137,11 @@ public void updateTableMetadata(Event event, SourceInfo source, Consumer<SourceR
 
             // Generate this table's insert, update, and delete converters ...
             Converter converter = new Converter() {
+                @Override
+                public TableId tableId() {
+                    return tableId;
+                }
+
                 @Override
                 public String topic() {
                     return topicName;
@@ -191,17 +200,19 @@ public void handleInsert(Event event, SourceInfo source, Consumer<SourceRecord>
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         Converter converter = convertersByTableId.get(tableNumber);
-        String topic = converter.topic();
-        Integer partition = converter.partition();
-        for (int row = 0; row <= source.eventRowNumber(); ++row) {
-            Serializable[] values = write.getRows().get(row);
-            Schema keySchema = converter.keySchema();
-            Object key = converter.createKey(values, includedColumns);
-            Schema valueSchema = converter.valueSchema();
-            Struct value = converter.inserted(values, includedColumns);
-            SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                    keySchema, key, valueSchema, value);
-            recorder.accept(record);
+        if (tableFilter.test(converter.tableId())) {
+            String topic = converter.topic();
+            Integer partition = converter.partition();
+            for (int row = 0; row <= source.eventRowNumber(); ++row) {
+                Serializable[] values = write.getRows().get(row);
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(values, includedColumns);
+                Schema valueSchema = converter.valueSchema();
+                Struct value = converter.inserted(values, includedColumns);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                        keySchema, key, valueSchema, value);
+                recorder.accept(record);
+            }
         }
     }
 
@@ -218,19 +229,21 @@ public void handleUpdate(Event event, SourceInfo source, Consumer<SourceRecord>
         BitSet includedColumns = update.getIncludedColumns();
         BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
         Converter converter = convertersByTableId.get(tableNumber);
-        String topic = converter.topic();
-        Integer partition = converter.partition();
-        for (int row = 0; row <= source.eventRowNumber(); ++row) {
-            Map.Entry<Serializable[], Serializable[]> changes = update.getRows().get(row);
-            Serializable[] before = changes.getKey();
-            Serializable[] after = changes.getValue();
-            Schema keySchema = converter.keySchema();
-            Object key = converter.createKey(after, includedColumns);
-            Schema valueSchema = converter.valueSchema();
-            Struct value = converter.updated(before, includedColumnsBefore, after, includedColumns);
-            SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                    keySchema, key, valueSchema, value);
-            recorder.accept(record);
+        if (tableFilter.test(converter.tableId())) {
+            String topic = converter.topic();
+            Integer partition = converter.partition();
+            for (int row = 0; row <= source.eventRowNumber(); ++row) {
+                Map.Entry<Serializable[], Serializable[]> changes = update.getRows().get(row);
+                Serializable[] before = changes.getKey();
+                Serializable[] after = changes.getValue();
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(after, includedColumns);
+                Schema valueSchema = converter.valueSchema();
+                Struct value = converter.updated(before, includedColumnsBefore, after, includedColumns);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                        keySchema, key, valueSchema, value);
+                recorder.accept(record);
+            }
         }
     }
 
@@ -239,21 +252,25 @@ public void handleDelete(Event event, SourceInfo source, Consumer<SourceRecord>
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         Converter converter = convertersByTableId.get(tableNumber);
-        String topic = converter.topic();
-        Integer partition = converter.partition();
-        for (int row = 0; row <= source.eventRowNumber(); ++row) {
-            Serializable[] values = deleted.getRows().get(row);
-            Schema keySchema = converter.keySchema();
-            Object key = converter.createKey(values, includedColumns);
-            Schema valueSchema = converter.valueSchema();
-            Struct value = converter.inserted(values, includedColumns);
-            SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                    keySchema, key, valueSchema, value);
-            recorder.accept(record);
+        if (tableFilter.test(converter.tableId())) {
+            String topic = converter.topic();
+            Integer partition = converter.partition();
+            for (int row = 0; row <= source.eventRowNumber(); ++row) {
+                Serializable[] values = deleted.getRows().get(row);
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(values, includedColumns);
+                Schema valueSchema = converter.valueSchema();
+                Struct value = converter.inserted(values, includedColumns);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                        keySchema, key, valueSchema, value);
+                recorder.accept(record);
+            }
         }
     }
 
     protected static interface Converter {
+        TableId tableId();
+
         String topic();
 
         Integer partition();",2016-02-04T13:56:13Z,28
"@@ -5,6 +5,9 @@
  */
 package io.debezium.function;
 
+import java.util.HashSet;
+import java.util.Set;
+import java.util.function.Function;
 import java.util.function.Predicate;
 
 /**
@@ -13,6 +16,112 @@
  */
 public class Predicates {
 
+
+    /**
+     * Generate a whitelist filter/predicate that allows only those values that <em>are</em> included in the supplied input.
+     * 
+     * @param input the input string
+     * @param splitter the function that splits the input into multiple items; may not be null
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> whitelist(String input, Function<String, String[]> splitter, Function<String, T> factory) {
+        if ( input == null ) return (str)->false;
+        Set<T> matches = new HashSet<>();
+        for (String item : splitter.apply(input)) {
+            T obj = factory.apply(item);
+            if ( obj != null ) matches.add(obj);
+        }
+        return matches::contains;
+    }
+
+    /**
+     * Generate a whitelist filter/predicate that allows only those values that <em>are</em> included in the supplied input.
+     * 
+     * @param input the input string
+     * @param delimiter the character used to delimit the items in the input
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> whitelist(String input, char delimiter, Function<String, T> factory) {
+        return whitelist(input, (str) -> str.split(""["" + delimiter + ""]""), factory);
+    }
+
+    /**
+     * Generate a whitelist filter/predicate that allows only those values that <em>are</em> included in the supplied
+     * comma-separated input.
+     * 
+     * @param input the input string
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> whitelist(String input, Function<String, T> factory) {
+        return whitelist(input, (str) -> str.split(""[\\,]""), factory);
+    }
+
+    /**
+     * Generate a whitelist filter/predicate that allows only those values that <em>are</em> included in the supplied
+     * comma-separated input.
+     * 
+     * @param input the input string
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static Predicate<String> whitelist(String input) {
+        return whitelist(input, (str) -> str);
+    }
+
+    /**
+     * Generate a blacklist filter/predicate that allows only those values that are <em>not</em> included in the supplied input.
+     * 
+     * @param input the input string
+     * @param splitter the function that splits the input into multiple items; may not be null
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> blacklist(String input, Function<String, String[]> splitter, Function<String, T> factory) {
+        return whitelist(input, splitter, factory).negate();
+    }
+
+    /**
+     * Generate a blacklist filter/predicate that allows only those values that are <em>not</em> included in the supplied input.
+     * 
+     * @param input the input string
+     * @param delimiter the character used to delimit the items in the input
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> blacklist(String input, char delimiter, Function<String, T> factory) {
+        return whitelist(input, delimiter, factory).negate();
+    }
+
+    /**
+     * Generate a blacklist filter/predicate that allows only those values that are <em>not</em> included in the supplied comma-separated input.
+     * 
+     * @param input the input string
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> blacklist(String input, Function<String, T> factory) {
+        return whitelist(input, factory).negate();
+    }
+    /**
+     * Generate a blacklist filter/predicate that allows only those values that are <em>not</em> included in the supplied comma-separated input.
+     * 
+     * @param input the input string
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static Predicate<String> blacklist(String input) {
+        return whitelist(input).negate();
+    }
+
     public static <R> Predicate<R> not(Predicate<R> predicate) {
         return predicate.negate();
     }",2016-02-04T13:56:13Z,122
"@@ -5,89 +5,167 @@
  */
 package io.debezium.relational;
 
+import java.util.function.Predicate;
+
 import io.debezium.annotation.Immutable;
+import io.debezium.function.Predicates;
 
 /**
  * Unique identifier for a database table.
+ * 
  * @author Randall Hauch
  */
 @Immutable
 public final class TableId implements Comparable<TableId> {
 
+    /**
+     * Create a predicate function that allows only those {@link TableId}s that are allowed by the database whitelist (or
+     * not disallowed by the database blacklist) and allowed by the table whitelist (or not disallowed by the table blacklist).
+     * Therefore, blacklists are only used if there is no corresponding whitelist.
+     * <p>
+     * Qualified table names are comma-separated strings that are each {@link #parse(String) parsed} into {@link TableId} objects.
+     * 
+     * @param dbWhitelist the comma-separated string listing the names of the databases to be explicitly allowed;
+     *            may be null
+     * @param dbBlacklist the comma-separated string listing the names of the databases to be explicitly disallowed;
+     *            may be null
+     * @param tableWhitelist the comma-separated string listing the qualified names of the tables to be explicitly allowed;
+     *            may be null
+     * @param tableBlacklist the comma-separated string listing the qualified names of the tables to be explicitly disallowed;
+     *            may be null
+     * @return the predicate function; never null
+     */
+    public static Predicate<TableId> filter(String dbWhitelist, String dbBlacklist, String tableWhitelist, String tableBlacklist) {
+        Predicate<TableId> tableExclusions = tableBlacklist == null ? null : Predicates.blacklist(tableBlacklist, TableId::parse);
+        Predicate<TableId> tableInclusions = tableWhitelist == null ? null : Predicates.whitelist(tableWhitelist, TableId::parse);
+        Predicate<TableId> tableFilter = tableInclusions != null ? tableInclusions : tableExclusions;
+        Predicate<String> dbExclusions = dbBlacklist == null ? null : Predicates.blacklist(dbBlacklist);
+        Predicate<String> dbInclusions = dbWhitelist == null ? null : Predicates.whitelist(dbWhitelist);
+        Predicate<String> dbFilter = dbInclusions != null ? dbInclusions : dbExclusions;
+        if (dbFilter != null) {
+            if (tableFilter != null) {
+                return (id) -> dbFilter.test(id.catalog()) && tableFilter.test(id);
+            }
+            return (id) -> dbFilter.test(id.catalog());
+        }
+        if (tableFilter != null) {
+            return tableFilter;
+        }
+        return (id) -> true;
+    }
+
+    /**
+     * Parse the supplied string delimited with a period ({@code .}) character, extracting up to the first 3 parts into a TableID.
+     * If the input contains only two parts, then the first part will be used as the catalog name and the second as the table
+     * name.
+     * 
+     * @param str the input string
+     * @return the table ID, or null if it could not be parsed
+     */
+    public static TableId parse(String str) {
+        return parse(str, '.', true);
+    }
+
+    /**
+     * Parse the supplied string, extracting up to the first 3 parts into a TableID.
+     * 
+     * @param str the input string
+     * @param delimiter the delimiter between parts
+     * @param useCatalogBeforeSchema {@code true} if the parsed string contains only 2 items and the first should be used as
+     *            the catalog and the second as the table name, or {@code false} if the first should be used as the schema and the
+     *            second
+     *            as the table name
+     * @return the table ID, or null if it could not be parsed
+     */
+    public static TableId parse(String str, char delimiter, boolean useCatalogBeforeSchema) {
+        String[] parts = str.split(""[\\"" + delimiter + ""]"");
+        if (parts.length == 0) return null;
+        if (parts.length == 1) return new TableId(null, null, parts[0]); // table only
+        if (parts.length == 2) {
+            if (useCatalogBeforeSchema) return new TableId(parts[0], null, parts[1]); // catalog & table only
+            return new TableId(null, parts[0], parts[1]); // catalog & table only
+        }
+        return new TableId(parts[0], parts[1], parts[2]); // catalog & table only
+    }
+
     private final String catalogName;
     private final String schemaName;
     private final String tableName;
     private final String id;
-    
+
     /**
      * Create a new table identifier.
+     * 
      * @param catalogName the name of the database catalog that contains the table; may be null if the JDBC driver does not
      *            show a schema for this table
      * @param schemaName the name of the database schema that contains the table; may be null if the JDBC driver does not
      *            show a schema for this table
      * @param tableName the name of the table; may not be null
      */
-    public TableId( String catalogName, String schemaName, String tableName ) {
+    public TableId(String catalogName, String schemaName, String tableName) {
         this.catalogName = catalogName;
         this.schemaName = schemaName;
         this.tableName = tableName;
         assert this.tableName != null;
         this.id = tableId(this.catalogName, this.schemaName, this.tableName);
     }
-    
+
     /**
      * Get the name of the JDBC catalog.
+     * 
      * @return the catalog name, or null if the table does not belong to a catalog
      */
     public String catalog() {
         return catalogName;
     }
-    
+
     /**
      * Get the name of the JDBC schema.
+     * 
      * @return the JDBC schema name, or null if the table does not belong to a JDBC schema
      */
     public String schema() {
         return schemaName;
     }
-    
+
     /**
      * Get the name of the table.
+     * 
      * @return the table name; never null
      */
     public String table() {
         return tableName;
     }
-    
+
     @Override
     public int compareTo(TableId that) {
-        if ( this == that ) return 0;
+        if (this == that) return 0;
         return this.id.compareTo(that.id);
     }
-    
+
     public int compareToIgnoreCase(TableId that) {
-        if ( this == that ) return 0;
+        if (this == that) return 0;
         return this.id.compareToIgnoreCase(that.id);
     }
-    
+
     @Override
     public int hashCode() {
         return id.hashCode();
     }
-    
+
     @Override
     public boolean equals(Object obj) {
-        if ( obj instanceof TableId ) {
-            return this.compareTo((TableId)obj) == 0;
+        if (obj instanceof TableId) {
+            return this.compareTo((TableId) obj) == 0;
         }
         return false;
     }
-    
+
     @Override
     public String toString() {
         return id;
     }
-    
+
     private static String tableId(String catalog, String schema, String table) {
         if (catalog == null || catalog.length() == 0) {
             if (schema == null || schema.length() == 0) {",2016-02-04T13:56:13Z,123
"@@ -0,0 +1,45 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.function;
+
+import java.util.function.Predicate;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author Randall Hauch
+ */
+public class PredicatesTest {
+
+    @Test
+    public void shouldWhitelistCommaSeparatedIntegers() {
+        Predicate<Integer> p = Predicates.whitelist(""1,2,3,4,5"",',', Integer::parseInt);
+        assertThat(p.test(1)).isTrue();
+        assertThat(p.test(2)).isTrue();
+        assertThat(p.test(3)).isTrue();
+        assertThat(p.test(4)).isTrue();
+        assertThat(p.test(5)).isTrue();
+        assertThat(p.test(0)).isFalse();
+        assertThat(p.test(6)).isFalse();
+        assertThat(p.test(-1)).isFalse();
+    }
+
+    @Test
+    public void shouldBlacklistCommaSeparatedIntegers() {
+        Predicate<Integer> p = Predicates.blacklist(""1,2,3,4,5"",',', Integer::parseInt);
+        assertThat(p.test(1)).isFalse();
+        assertThat(p.test(2)).isFalse();
+        assertThat(p.test(3)).isFalse();
+        assertThat(p.test(4)).isFalse();
+        assertThat(p.test(5)).isFalse();
+        assertThat(p.test(0)).isTrue();
+        assertThat(p.test(6)).isTrue();
+        assertThat(p.test(-1)).isTrue();
+    }
+
+}",2016-02-04T13:56:13Z,146
"@@ -0,0 +1,159 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.relational;
+
+import java.util.function.Predicate;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author Randall Hauch
+ */
+public class TableIdTest {
+
+    private Predicate<TableId> filter;
+
+    @Test
+    public void shouldCreateFilterWithDatabaseWhitelistAndTableWhitelist() {
+        filter = TableId.filter(""db1,db2"", null, ""db1.A,db1.B,db2.C"", null);
+
+        assertAllowed(filter, ""db1"", ""A"");
+        assertAllowed(filter, ""db1"", ""B"");
+        assertNotAllowed(filter, ""db1"", ""D"");
+        assertNotAllowed(filter, ""db1"", ""E"");
+        assertNotAllowed(filter, ""db1"", ""F"");
+
+        assertAllowed(filter, ""db2"", ""C"");
+        assertNotAllowed(filter, ""db2"", ""G"");
+        assertNotAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseWhitelistAndTableBlacklist() {
+        filter = TableId.filter(""db1,db2"", null, null, ""db1.A,db1.B,db2.C"");
+
+        assertNotAllowed(filter, ""db1"", ""A"");
+        assertNotAllowed(filter, ""db1"", ""B"");
+        assertAllowed(filter, ""db1"", ""D"");
+        assertAllowed(filter, ""db1"", ""E"");
+        assertAllowed(filter, ""db1"", ""F"");
+
+        assertNotAllowed(filter, ""db2"", ""C"");
+        assertAllowed(filter, ""db2"", ""G"");
+        assertAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseBlacklistAndTableWhitelist() {
+        filter = TableId.filter(null,""db3,db4"", ""db1.A,db1.B,db2.C"", null);
+
+        assertAllowed(filter, ""db1"", ""A"");
+        assertAllowed(filter, ""db1"", ""B"");
+        assertNotAllowed(filter, ""db1"", ""D"");
+        assertNotAllowed(filter, ""db1"", ""E"");
+        assertNotAllowed(filter, ""db1"", ""F"");
+
+        assertAllowed(filter, ""db2"", ""C"");
+        assertNotAllowed(filter, ""db2"", ""G"");
+        assertNotAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseBlacklistAndTableBlacklist() {
+        filter = TableId.filter(null,""db3,db4"", null, ""db1.A,db1.B,db2.C"");
+
+        assertNotAllowed(filter, ""db1"", ""A"");
+        assertNotAllowed(filter, ""db1"", ""B"");
+        assertAllowed(filter, ""db1"", ""D"");
+        assertAllowed(filter, ""db1"", ""E"");
+        assertAllowed(filter, ""db1"", ""F"");
+
+        assertNotAllowed(filter, ""db2"", ""C"");
+        assertAllowed(filter, ""db2"", ""G"");
+        assertAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithNoDatabaseFilterAndTableWhitelist() {
+        filter = TableId.filter(null, null, ""db1.A,db1.B,db2.C"", null);
+
+        assertAllowed(filter, ""db1"", ""A"");
+        assertAllowed(filter, ""db1"", ""B"");
+        assertNotAllowed(filter, ""db1"", ""D"");
+        assertNotAllowed(filter, ""db1"", ""E"");
+        assertNotAllowed(filter, ""db1"", ""F"");
+
+        assertAllowed(filter, ""db2"", ""C"");
+        assertNotAllowed(filter, ""db2"", ""G"");
+        assertNotAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithNoDatabaseFilterAndTableBlacklist() {
+        filter = TableId.filter(null, null, null, ""db1.A,db1.B,db2.C"");
+
+        assertNotAllowed(filter, ""db1"", ""A"");
+        assertNotAllowed(filter, ""db1"", ""B"");
+        assertAllowed(filter, ""db1"", ""D"");
+        assertAllowed(filter, ""db1"", ""E"");
+        assertAllowed(filter, ""db1"", ""F"");
+
+        assertNotAllowed(filter, ""db2"", ""C"");
+        assertAllowed(filter, ""db2"", ""G"");
+        assertAllowed(filter, ""db2"", ""H"");
+
+        assertAllowed(filter, ""db3"", ""A"");
+        assertAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseWhitelistAndNoTableFilter() {
+        filter = TableId.filter(""db1,db2"", null, null, null);
+
+        assertAllowed(filter, ""db1"", ""A"");
+        assertAllowed(filter, ""db2"", ""A"");
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseBlacklistAndNoTableFilter() {
+        filter = TableId.filter(null, ""db1,db2"", null, null);
+
+        assertNotAllowed(filter, ""db1"", ""A"");
+        assertNotAllowed(filter, ""db2"", ""A"");
+        assertAllowed(filter, ""db3"", ""A"");
+        assertAllowed(filter, ""db4"", ""A"");
+    }
+
+    protected void assertAllowed(Predicate<TableId> filter, String dbName, String tableName) {
+        TableId id = new TableId(dbName, null, tableName);
+        assertThat(filter.test(id)).isTrue();
+    }
+
+    protected void assertNotAllowed(Predicate<TableId> filter, String dbName, String tableName) {
+        TableId id = new TableId(dbName, null, tableName);
+        assertThat(filter.test(id)).isFalse();
+    }
+
+}",2016-02-04T13:56:13Z,23
