file_diff,commit_time,file_diff_id
"@@ -1,5 +1,5 @@
 [id=""debezium-connector-for-sql-server""]
-= {prodname} Connector for SQL Server
+= {prodname} connector for SQL Server
 
 ifdef::community[]
 
@@ -210,19 +210,19 @@ A message to the schema change topic contains a logical representation of the ta
       ""commit_lsn"": ""00000025:00000d98:00a2"",
       ""event_serial_no"": null
     },
-    ""databaseName"": ""testDB"", <1>
+    ""databaseName"": ""testDB"", // <1>
     ""schemaName"": ""dbo"",
-    ""ddl"": null, <2>
-    ""tableChanges"": [ <3>
+    ""ddl"": null, // <2>
+    ""tableChanges"": [ // <3>
       {
-        ""type"": ""CREATE"", <4>
-        ""id"": ""\""testDB\"".\""dbo\"".\""customers\"""", <5>
-        ""table"": { <6>
+        ""type"": ""CREATE"", // <4>
+        ""id"": ""\""testDB\"".\""dbo\"".\""customers\"""", // <5>
+        ""table"": { // <6>
           ""defaultCharsetName"": null,
-          ""primaryKeyColumnNames"": [ <7>
+          ""primaryKeyColumnNames"": [ // <7>
             ""id""
           ],
-          ""columns"": [ <8>
+          ""columns"": [ // <8>
             {
               ""name"": ""id"",
               ""jdbcType"": 4,
@@ -288,7 +288,7 @@ A message to the schema change topic contains a logical representation of the ta
 ----
 
 .Descriptions of fields in messages emitted to the schema change topic
-[cols=""1,3,6"",options=""header""]
+[cols=""1,4,5"",options=""header""]
 |===
 |Item |Field name |Description
 
@@ -333,7 +333,7 @@ a|Describes the kind of change. The value is one of the following:
 
 In messages to the schema change topic, the key is the name of the database that contains the schema change. In the following example, the `payload` field contains the key:
 
-[source,json,indent=0,subs=""attributes""]
+[source,json,indent=0,subs=""+attributes""]
 ----
 {
   ""schema"": {
@@ -365,16 +365,16 @@ The following skeleton JSON shows the basic four parts of a change event. Howeve
 [source,json,index=0]
 ----
 {
- ""schema"": { //<1>
+ ""schema"": { // <1>
    ...
   },
- ""payload"": { //<2>
+ ""payload"": { // <2>
    ...
  },
- ""schema"": { //<3> 
+ ""schema"": { // <3> 
    ...
  },
- ""payload"": { //<4>
+ ""payload"": { // <4>
    ...
  },
 }
@@ -438,19 +438,19 @@ Every change event that captures a change to the `customers` table has the same
 [source,json,indent=0]
 ----
 {
-    ""schema"": { <1>
+    ""schema"": { // <1>
         ""type"": ""struct"",
-        ""fields"": [ <2>
+        ""fields"": [ // <2>
             {
                 ""type"": ""int32"",
                 ""optional"": false,
                 ""field"": ""id""
             }
         ],
-        ""optional"": false, <3>
-        ""name"": ""server1.dbo.customers.Key"" <4>
+        ""optional"": false, // <3>
+        ""name"": ""server1.dbo.customers.Key"" // <4>
     },
-    ""payload"": { <5>
+    ""payload"": { // <5>
         ""id"": 1004
     }
 }
@@ -474,7 +474,7 @@ Every change event that captures a change to the `customers` table has the same
 |Indicates whether the event key must contain a value in its `payload` field. In this example, a value in the key's payload is required. A value in the key's payload field is optional when a table does not have a primary key.
 
 |4
-|`server1.dbo.customers.Key`
+|`server1.dbo{zwsp}.customers{zwsp}.Key`
 a|Name of the schema that defines the structure of the key's payload. This schema describes the structure of the primary key for the table that was changed. Key schema names have the format _connector-name_._database-schema-name_._table-name_.`Key`. In this example: + 
 
 * `server1` is the name of the connector that generated this event. + 
@@ -774,19 +774,19 @@ The value of a change event for an update in the sample `customers` table has th
 {
   ""schema"": { ... },
   ""payload"": {
-    ""before"": { <1>
+    ""before"": { // <1>
       ""id"": 1005,
       ""first_name"": ""john"",
       ""last_name"": ""doe"",
       ""email"": ""john.doe@example.org""
     },
-    ""after"": { <2>
+    ""after"": { // <2>
       ""id"": 1005,
       ""first_name"": ""john"",
       ""last_name"": ""doe"",
       ""email"": ""noreply@example.org""
     },
-    ""source"": { <3>
+    ""source"": { // <3>
       ""version"": ""{debezium-version}"",
       ""connector"": ""sqlserver"",
       ""name"": ""server1"",
@@ -799,7 +799,7 @@ The value of a change event for an update in the sample `customers` table has th
       ""commit_lsn"": ""00000027:00000ac0:0007"",
       ""event_serial_no"": ""2""
     },
-    ""op"": ""u"", <4>
+    ""op"": ""u"", // <4>
     ""ts_ms"": 1559729998706
   }
 }
@@ -943,7 +943,7 @@ Every event contains
 
 Following is an example of what a message looks like:
 
-[source,json,indent=0,subs=""attributes""]
+[source,json,indent=0,subs=""+attributes""]
 ----
 {
   ""status"": ""BEGIN"",
@@ -981,7 +981,7 @@ This field provides information about every event in the form of a composite of
 
 Following is an example of what a message looks like:
 
-[source,json,indent=0,subs=""attributes""]
+[source,json,indent=0,subs=""+attributes""]
 ----
 {
   ""before"": null,
@@ -1140,87 +1140,75 @@ The following table describes how the connector maps each of the SQL Server data
 Here, the _literal type_ describes how the value is literally represented using Kafka Connect schema types, namely `INT8`, `INT16`, `INT32`, `INT64`, `FLOAT32`, `FLOAT64`, `BOOLEAN`, `STRING`, `BYTES`, `ARRAY`, `MAP`, and `STRUCT`.
 The _semantic type_ describes how the Kafka Connect schema captures the _meaning_ of the field using the name of the Kafka Connect schema for the field.
 
-[cols=""20%a,15%a,30%a,35%a""]
+[cols=""30%a,25%a,45%a"",options=""header""]
 |===
-|SQL Server Data Type
+|SQL Server data type
 |Literal type (schema type)
-|Semantic type (schema name)
-|Notes
+|Semantic type (schema name) and Notes
 
 |`BIT`
 |`BOOLEAN`
 |n/a
-|
 
 |`TINYINT`
 |`INT16`
 |n/a
-|
 
 |`SMALLINT`
 |`INT16`
 |n/a
-|
 
 |`INT`
 |`INT32`
 |n/a
-|
 
 |`BIGINT`
 |`INT64`
 |n/a
-|
 
 |`REAL`
 |`FLOAT32`
 |n/a
-|
 
 |`FLOAT[(N)]`
 |`FLOAT64`
 |n/a
-|
 
 |`CHAR[(N)]`
 |`STRING`
 |n/a
-|
 
 |`VARCHAR[(N)]`
 |`STRING`
 |n/a
-|
 
 |`TEXT`
 |`STRING`
 |n/a
-|
 
 |`NCHAR[(N)]`
 |`STRING`
 |n/a
-|
 
 |`NVARCHAR[(N)]`
 |`STRING`
 |n/a
-|
 
 |`NTEXT`
 |`STRING`
 |n/a
-|
 
 |`XML`
 |`STRING`
-|`io.debezium.data.Xml`
-|Contains the string representation of a XML document
+|`io.debezium.data.Xml` +
+ +
+Contains the string representation of an XML document
 
 |`DATETIMEOFFSET[(P)]`
 |`STRING`
-|`io.debezium.time.ZonedTimestamp`
-| A string representation of a timestamp with timezone information, where the timezone is GMT
+|`io.debezium.time.ZonedTimestamp` +
+ +
+A string representation of a timestamp with timezone information, where the timezone is GMT
 
 |===
 
@@ -1238,93 +1226,105 @@ endif::community[]
 
 Other than SQL Server's `DATETIMEOFFSET` data type (which contain time zone information), the other temporal types depend on the value of the `time.precision.mode` configuration property.  When the `time.precision.mode` configuration property is set to `adaptive` (the default), then the connector will determine the literal type and semantic type for the temporal types based on the column's data type definition so that events _exactly_ represent the values in the database:
 
-[cols=""20%a,15%a,30%a,35%a""]
+[cols=""30%a,25%a,45%a"",options=""header""]
 |===
-|SQL Server Data Type
+|SQL Server data type
 |Literal type (schema type)
-|Semantic type (schema name)
-|Notes
+|Semantic type (schema name) and Notes
 
 |`DATE`
 |`INT32`
-|`io.debezium.time.Date`
-| Represents the number of days since epoch.
+|`io.debezium.time.Date` +
+ +
+Represents the number of days since the epoch.
 
 |`TIME(0)`, `TIME(1)`, `TIME(2)`, `TIME(3)`
 |`INT32`
-|`io.debezium.time.Time`
-| Represents the number of milliseconds past midnight, and does not include timezone information.
+|`io.debezium.time.Time` +
+ +
+Represents the number of milliseconds past midnight, and does not include timezone information.
 
 |`TIME(4)`, `TIME(5)`, `TIME(6)`
 |`INT64`
-|`io.debezium.time.MicroTime`
-| Represents the number of microseconds past midnight, and does not include timezone information.
+|`io.debezium.time.MicroTime` +
+ +
+Represents the number of microseconds past midnight, and does not include timezone information.
 
 |`TIME(7)`
 |`INT64`
-|`io.debezium.time.NanoTime`
-| Represents the number of nanoseconds past midnight, and does not include timezone information.
+|`io.debezium.time.NanoTime` +
+ +
+Represents the number of nanoseconds past midnight, and does not include timezone information.
 
 |`DATETIME`
 |`INT64`
-|`io.debezium.time.Timestamp`
-| Represents the number of milliseconds past epoch, and does not include timezone information.
+|`io.debezium.time.Timestamp` +
+ +
+Represents the number of milliseconds past the epoch, and does not include timezone information.
 
 |`SMALLDATETIME`
 |`INT64`
-|`io.debezium.time.Timestamp`
-| Represents the number of milliseconds past epoch, and does not include timezone information.
+|`io.debezium.time.Timestamp` +
+ +
+Represents the number of milliseconds past the epoch, and does not include timezone information.
 
 |`DATETIME2(0)`, `DATETIME2(1)`, `DATETIME2(2)`, `DATETIME2(3)`
 |`INT64`
-|`io.debezium.time.Timestamp`
-| Represents the number of milliseconds past epoch, and does not include timezone information.
+|`io.debezium.time.Timestamp` +
+ +
+Represents the number of milliseconds past the epoch, and does not include timezone information.
 
 |`DATETIME2(4)`, `DATETIME2(5)`, `DATETIME2(6)`
 |`INT64`
-|`io.debezium.time.MicroTimestamp`
-| Represents the number of microseconds past epoch, and does not include timezone information.
+|`io.debezium.time.MicroTimestamp` +
+ +
+Represents the number of microseconds past the epoch, and does not include timezone information.
 
 |`DATETIME2(7)`
 |`INT64`
-|`io.debezium.time.NanoTimestamp`
-| Represents the number of nanoseconds past epoch, and does not include timezone information.
+|`io.debezium.time.NanoTimestamp` +
+ +
+Represents the number of nanoseconds past the epoch, and does not include timezone information.
 
 |===
 
 When the `time.precision.mode` configuration property is set to `connect`, then the connector will use the predefined Kafka Connect logical types. This may be useful when consumers only know about the built-in Kafka Connect logical types and are unable to handle variable-precision time values. On the other hand, since SQL Server supports tenth of microsecond precision, the events generated by a connector with the `connect` time precision mode will *result in a loss of precision* when the database column has a _fractional second precision_ value greater than 3:
 
-[cols=""20%a,15%a,30%a,35%a""]
+[cols=""25%a,20%a,55%a"",options=""header""]
 |===
-|SQL Server Data Type
+|SQL Server data type
 |Literal type (schema type)
-|Semantic type (schema name)
-|Notes
+|Semantic type (schema name) and Notes
 
 |`DATE`
 |`INT32`
-|`org.apache.kafka.connect.data.Date`
-| Represents the number of days since epoch.
+|`org.apache.kafka.connect.data.Date` +
+ +
+Represents the number of days since the epoch.
 
 |`TIME([P])`
 |`INT64`
-|`org.apache.kafka.connect.data.Time`
-| Represents the number of milliseconds since midnight, and does not include timezone information. SQL Server allows `P` to be in the range 0-7 to store up to tenth of microsecond precision, though this mode results in a loss of precision when `P` > 3.
+|`org.apache.kafka.connect.data.Time` +
+ +
+Represents the number of milliseconds since midnight, and does not include timezone information. SQL Server allows `P` to be in the range 0-7 to store up to tenth of a microsecond precision, though this mode results in a loss of precision when `P` > 3.
 
 |`DATETIME`
 |`INT64`
-|`org.apache.kafka.connect.data.Timestamp`
-| Represents the number of milliseconds since epoch, and does not include timezone information.
+|`org.apache.kafka.connect.data.Timestamp` +
+ +
+Represents the number of milliseconds since the epoch, and does not include timezone information.
 
 |`SMALLDATETIME`
 |`INT64`
-|`org.apache.kafka.connect.data.Timestamp`
-| Represents the number of milliseconds past epoch, and does not include timezone information.
+|`org.apache.kafka.connect.data.Timestamp` +
+ +
+Represents the number of milliseconds past the epoch, and does not include timezone information.
 
 |`DATETIME2`
 |`INT64`
-|`org.apache.kafka.connect.data.Timestamp`
-| Represents the number of milliseconds since epoch, and does not include timezone information. SQL Server allows `P` to be in the range 0-7 to store up to tenth of microsecond precision, though this mode results in a loss of precision when `P` > 3.
+|`org.apache.kafka.connect.data.Timestamp` +
+ +
+Represents the number of milliseconds since the epoch, and does not include timezone information. SQL Server allows `P` to be in the range 0-7 to store up to tenth of a microsecond precision, though this mode results in a loss of precision when `P` > 3.
 
 |===
 
@@ -1339,39 +1339,33 @@ Note that the timezone of the JVM running Kafka Connect and {prodname} does not
 
 ==== Decimal values
 
-[cols=""15%a,15%a,35%a,35%a""]
+[cols=""30%a,17%a,53%a"",options=""header""]
 |===
-|SQL Server Data Type
+|SQL Server data type
 |Literal type (schema type)
 |Semantic type (schema name)
-|Notes
 
 |`NUMERIC[(P[,S])]`
 |`BYTES`
-|`org.apache.kafka.connect.data.Decimal`
-|The `scale` schema parameter contains an integer representing how many digits the decimal point was shifted.
-The `connect.decimal.precision` schema parameter contains an integer representing the precision of the given decimal value.
+|`org.apache.kafka.connect.data.Decimal` 
 
 |`DECIMAL[(P[,S])]`
 |`BYTES`
-|`org.apache.kafka.connect.data.Decimal`
-|The `scale` schema parameter contains an integer representing how many digits the decimal point was shifted.
-The `connect.decimal.precision` schema parameter contains an integer representing the precision of the given decimal value.
+|`org.apache.kafka.connect.data.Decimal` 
 
 |`SMALLMONEY`
 |`BYTES`
 |`org.apache.kafka.connect.data.Decimal`
-|The `scale` schema parameter contains an integer representing how many digits the decimal point was shifted.
-The `connect.decimal.precision` schema parameter contains an integer representing the precision of the given decimal value.
 
 |`MONEY`
 |`BYTES`
 |`org.apache.kafka.connect.data.Decimal`
-|The `scale` schema parameter contains an integer representing how many digits the decimal point was shifted.
-The `connect.decimal.precision` schema parameter contains an integer representing the precision of the given decimal value.
 
 |===
 
+The `scale` schema parameter contains an integer that represents how many digits the decimal point was shifted.
+The `connect.decimal.precision` schema parameter contains an integer that represents the precision of the given decimal value.
+
 [[sqlserver-deploying-a-connector]]
 == Deployment
 
@@ -1432,18 +1426,18 @@ Typically, you configure the {prodname} SQL Server connector in a `.json` file u
 [source,json]
 ----
 {
-  ""name"": ""inventory-connector"", <1>
+  ""name"": ""inventory-connector"", // <1>
   ""config"": {
-    ""connector.class"": ""io.debezium.connector.sqlserver.SqlServerConnector"", <2>
-    ""database.hostname"": ""192.168.99.100"", <3>
-    ""database.port"": ""1433"", <4>
-    ""database.user"": ""sa"", <5>
-    ""database.password"": ""Password!"", <6>
-    ""database.dbname"": ""testDB"", <7>
-    ""database.server.name"": ""fullfillment"", <8>
-    ""table.include.list"": ""dbo.customers"", <9>
-    ""database.history.kafka.bootstrap.servers"": ""kafka:9092"", <10>
-    ""database.history.kafka.topic"": ""dbhistory.fullfillment"" <11>
+    ""connector.class"": ""io.debezium.connector.sqlserver.SqlServerConnector"", // <2>
+    ""database.hostname"": ""192.168.99.100"", // <3>
+    ""database.port"": ""1433"", // <4>
+    ""database.user"": ""sa"", // <5>
+    ""database.password"": ""Password!"", // <6>
+    ""database.dbname"": ""testDB"", // <7>
+    ""database.server.name"": ""fullfillment"", // <8>
+    ""table.include.list"": ""dbo.customers"", // <9>
+    ""database.history.kafka.bootstrap.servers"": ""kafka:9092"", // <10>
+    ""database.history.kafka.topic"": ""dbhistory.fullfillment"" // <11>
   }
 }
 ----
@@ -1578,7 +1572,7 @@ pass:quotes[*tree ./my-plugins/*]
 
 .. Create a new `Dockerfile` by using `{DockerKafkaConnect}` as the base image. In the following example, you would replace _my-plugins_ with the name of your plug-ins directory:
 +
-[subs=+macros]
+[subs=""+macros,+attributes""]
 ----
 FROM {DockerKafkaConnect}
 USER root:root
@@ -1681,7 +1675,7 @@ include::{partialsdir}/modules/all-connectors/ref-connector-monitoring-schema-hi
 
 The following configuration properties are _required_ unless a default value is available.
 
-[cols=""30%a,25%a,45%a""]
+[cols=""30%a,25%a,45%a"",options=""header""]
 |===
 |Property
 |Default
@@ -1719,16 +1713,16 @@ The following configuration properties are _required_ unless a default value is
 |
 |The name of the SQL Server database from which to stream the changes
 
-|[[sqlserver-property-database-server-name]]<<sqlserver-property-database-server-name, `database.server.name`>>
+|[[sqlserver-property-database-server-name]]<<sqlserver-property-database-server-name, `database.server{zwsp}.name`>>
 |
 |Logical name that identifies and provides a namespace for the particular SQL Server database server being monitored. The logical name should be unique across all other connectors, since it is used as a prefix for all Kafka topic names emanating from this connector.
 Only alphanumeric characters and underscores should be used.
 
-|[[sqlserver-property-database-history-kafka-topic]]<<sqlserver-property-database-history-kafka-topic, `database.history.kafka.topic`>>
+|[[sqlserver-property-database-history-kafka-topic]]<<sqlserver-property-database-history-kafka-topic, `database.history{zwsp}.kafka.topic`>>
 |
 |The full name of the Kafka topic where the connector will store the database schema history.
 
-|[[sqlserver-property-database-history-kafka-bootstrap-servers]]<<sqlserver-property-database-history-kafka-bootstrap-servers, `database.history{zwsp}.kafka.bootstrap.servers`>>
+|[[sqlserver-property-database-history-kafka-bootstrap-servers]]<<sqlserver-property-database-history-kafka-bootstrap-servers, `database.history{zwsp}.kafka.bootstrap{zwsp}.servers`>>
 |
 |A list of host/port pairs that the connector will use for establishing an initial connection to the Kafka cluster.
 This connection is used for retrieving database schema history previously stored by the connector, and for writing each DDL statement read from the source database. This should point to the same Kafka cluster used by the Kafka Connect process.
@@ -1762,7 +1756,7 @@ Note that primary key columns are always included in the event's key, also if ex
 Do not also set the `column.include.list` property.
 
 
-|[[sqlserver-property-column-mask-hash]]<<sqlserver-property-column-mask-hash, `column.mask.hash._hashAlgorithm_.with.salt._salt_`>>
+|[[sqlserver-property-column-mask-hash]]<<sqlserver-property-column-mask-hash, `column.mask{zwsp}.hash._hashAlgorithm_{zwsp}.with.salt._salt_`>>
 |_n/a_
 |An optional comma-separated list of regular expressions that match the fully-qualified names of character-based columns whose values should be pseudonyms in the change event message values with a field value consisting of the hashed value using the algorithm `_hashAlgorithm_` and salt `_salt_`.
 Based on the used hash function referential integrity is kept while data is pseudonymized. Supported hash functions are described in the {link-java7-standard-names}[MessageDigest section] of the Java Cryptography Architecture Standard Algorithm Name Documentation.
@@ -1778,36 +1772,36 @@ where `CzQMA0cB5K` is a randomly selected salt.
 
 Note: Depending on the `_hashAlgorithm_` used, the `_salt_` selected and the actual data set, the resulting masked data set may not be completely anonymized.
 
-|[[sqlserver-property-time-precision-mode]]<<sqlserver-property-time-precision-mode, `time.precision.mode`>>
+|[[sqlserver-property-time-precision-mode]]<<sqlserver-property-time-precision-mode, `time.precision{zwsp}.mode`>>
 |`adaptive`
 | Time, date, and timestamps can be represented with different kinds of precision, including: `adaptive` (the default) captures the time and timestamp values exactly as in the database using either millisecond, microsecond, or nanosecond precision values based on the database column's type; or `connect` always represents time and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, which uses millisecond precision regardless of the database columns' precision. See {link-prefix}:{link-sqlserver-connector}#sqlserver-temporal-values[temporal values].
 
-|[[sqlserver-property-include-schema-changes]]<<sqlserver-property-include-schema-changes, `include.schema.changes`>>
+|[[sqlserver-property-include-schema-changes]]<<sqlserver-property-include-schema-changes, `include.schema{zwsp}.changes`>>
 |`true`
 |Boolean value that specifies whether the connector should publish changes in the database schema to a Kafka topic with the same name as the database server ID. Each schema change is recorded with a key that contains the database name and a value that is a JSON structure that describes the schema update. This is independent of how the connector internally records database history. The default is `true`.
 
-|[[sqlserver-property-tombstones-on-delete]]<<sqlserver-property-tombstones-on-delete, `tombstones.on.delete`>>
+|[[sqlserver-property-tombstones-on-delete]]<<sqlserver-property-tombstones-on-delete, `tombstones.on{zwsp}.delete`>>
 |`true`
 | Controls whether a tombstone event should be generated after a delete event. +
 When `true` the delete operations are represented by a delete event and a subsequent tombstone event. When `false` only a delete event is sent. +
 Emitting the tombstone event (the default behavior) allows Kafka to completely delete all events pertaining to the given key once the source record got deleted.
 
-|[[sqlserver-property-column-truncate-to-length-chars]]<<sqlserver-property-column-truncate-to-length-chars, `column.truncate.to._length_.chars`>>
+|[[sqlserver-property-column-truncate-to-length-chars]]<<sqlserver-property-column-truncate-to-length-chars, `column.truncate.to{zwsp}._length_.chars`>>
 |_n/a_
 |An optional comma-separated list of regular expressions that match the fully-qualified names of character-based columns whose values should be truncated in the change event message values if the field values are longer than the specified number of characters. Multiple properties with different lengths can be used in a single configuration, although in each the length must be a positive integer. Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_.
 
-|[[sqlserver-property-column-mask-with-length-chars]]<<sqlserver-property-column-mask-with-length-chars, `column.mask.with._length_.chars`>>
+|[[sqlserver-property-column-mask-with-length-chars]]<<sqlserver-property-column-mask-with-length-chars, `column.mask.with{zwsp}._length_.chars`>>
 |_n/a_
 |An optional comma-separated list of regular expressions that match the fully-qualified names of character-based columns whose values should be replaced in the change event message values with a field value consisting of the specified number of asterisk (`*`) characters. Multiple properties with different lengths can be used in a single configuration, although in each the length must be a positive integer or zero. Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_.
 
-|[[sqlserver-property-column-propagate-source-type]]<<sqlserver-property-column-propagate-source-type, `column.propagate.source.type`>>
+|[[sqlserver-property-column-propagate-source-type]]<<sqlserver-property-column-propagate-source-type, `column.propagate{zwsp}.source.type`>>
 |_n/a_
 |An optional comma-separated list of regular expressions that match the fully-qualified names of columns whose original type and length should be added as a parameter to the corresponding field schemas in the emitted change messages.
 The schema parameters `pass:[_]pass:[_]debezium.source.column.type`, `pass:[_]pass:[_]debezium.source.column.length` and `pass:[_]pass:[_]debezium.source.column.scale` is used to propagate the original type name and length (for variable-width types), respectively.
 Useful to properly size corresponding columns in sink databases.
 Fully-qualified names for columns are of the form _schemaName_._tableName_._columnName_.
 
-|[[sqlserver-property-datatype-propagate-source-type]]<<sqlserver-property-datatype-propagate-source-type,`datatype.propagate.source.type`>>
+|[[sqlserver-property-datatype-propagate-source-type]]<<sqlserver-property-datatype-propagate-source-type,`datatype.propagate{zwsp}.source.type`>>
 |_n/a_
 |An optional comma-separated list of regular expressions that match the database-specific data type name of columns whose original type and length should be added as a parameter to the corresponding field schemas in the emitted change messages.
 The schema parameters `pass:[_]pass:[_]debezium.source.column.type`, `pass:[_]pass:[_]debezium.source.column.length` and `pass:[_]pass:[_]debezium.source.column.scale` will be used to propagate the original type name and length (for variable-width types), respectively.
@@ -1824,7 +1818,7 @@ Fully-qualified tables could be defined as _schemaName_._tableName_.
 
 The following _advanced_ configuration properties have good defaults that will work in most situations and therefore rarely need to be specified in the connector's configuration.
 
-[cols=""30%a,25%a,45%a""]
+[cols=""30%a,25%a,45%a"",options=""header""]
 |===
 |Property
 |Default
@@ -1840,7 +1834,7 @@ Supported values are: +
 `initial_only`: Takes a snapshot of structure and data like `initial` but instead does not transition into streaming changes once the snapshot has completed. +
 `schema_only`: Takes a snapshot of the structure of captured tables only; useful if only changes happening from now onwards should be propagated to topics.
 
-|[[sqlserver-property-snapshot-isolation-mode]]<<sqlserver-property-snapshot-isolation-mode, `snapshot.isolation.mode`>>
+|[[sqlserver-property-snapshot-isolation-mode]]<<sqlserver-property-snapshot-isolation-mode, `snapshot.isolation{zwsp}.mode`>>
 |_repeatable_read_
 |Mode to control which transaction isolation level is used and how long the connector locks the monitored tables.
 There are five possible values: `read_uncommitted`, `read_committed`, `repeatable_read`, `snapshot`, and `exclusive` (
@@ -1857,13 +1851,13 @@ twice - once in initial snapshot and once in streaming phase. Nonetheless, that
 data mirroring.
 For `read_uncommitted` there are no data consistency guarantees at all (some data might be lost or corrupted).
 
-|[[sqlserver-property-source-timestamp-mode]]<<sqlserver-property-source-timestamp-mode, `source.timestamp.mode`>>
+|[[sqlserver-property-source-timestamp-mode]]<<sqlserver-property-source-timestamp-mode, `source.timestamp{zwsp}.mode`>>
 |_commit_
 |String representing the criteria of the attached timestamp within the source record (ts_ms).
 `commit` will set the source timestamp to the instant where the record was committed in the database (default and current behavior).
 `processing` will set the source timestamp to the instant where the record was processed by {prodname}. This option could be used when either we want to set the top level `ts_ms` value here or when we want to skip the query to extract the timestamp of that LSN.
 
-|[[sqlserver-property-event-processing-failure-handling-mode]]<<sqlserver-property-event-processing-failure-handling-mode, `event.processing{zwsp}.failure.handling.mode`>>
+|[[sqlserver-property-event-processing-failure-handling-mode]]<<sqlserver-property-event-processing-failure-handling-mode, `event.processing{zwsp}.failure.handling{zwsp}.mode`>>
 |`fail`
 | Specifies how the connector should react to exceptions during processing of events.
 `fail` will propagate the exception (indicating the offset of the problematic event), causing the connector to stop. +
@@ -1882,7 +1876,7 @@ For `read_uncommitted` there are no data consistency guarantees at all (some dat
 |`2048`
 |Positive integer value that specifies the maximum size of each batch of events that should be processed during each iteration of this connector. Defaults to 2048.
 
-|[[sqlserver-property-heartbeat-interval-ms]]<<sqlserver-property-heartbeat-interval-ms, `heartbeat.interval.ms`>>
+|[[sqlserver-property-heartbeat-interval-ms]]<<sqlserver-property-heartbeat-interval-ms, `heartbeat.interval{zwsp}.ms`>>
 |`0`
 |Controls how frequently heartbeat messages are sent. +
 This property contains an interval in milli-seconds that defines how frequently the connector sends messages into a heartbeat topic.
@@ -1894,7 +1888,7 @@ This may result in more change events to be re-sent after a connector restart.
 Set this parameter to `0` to not send heartbeat messages at all. +
 Disabled by default.
 
-|[[sqlserver-property-heartbeat-topics-prefix]]<<sqlserver-property-heartbeat-topics-prefix, `heartbeat.topics.prefix`>>
+|[[sqlserver-property-heartbeat-topics-prefix]]<<sqlserver-property-heartbeat-topics-prefix, `heartbeat.topics{zwsp}.prefix`>>
 |`__debezium-heartbeat`
 |Controls the naming of the topic to which heartbeat messages are sent. +
 The topic is named according to the pattern `<heartbeat.topics.prefix>.<server.name>`.
@@ -1914,12 +1908,12 @@ The connector will read the table contents in multiple batches of this size. Def
 |Specifies the number of rows that will be fetched for each database round-trip of a given query.
 Defaults to the JDBC driver's default fetch size.
 
-|[[sqlserver-property-snapshot-lock-timeout-ms]]<<sqlserver-property-snapshot-lock-timeout-ms, `snapshot.lock.timeout.ms`>>
+|[[sqlserver-property-snapshot-lock-timeout-ms]]<<sqlserver-property-snapshot-lock-timeout-ms, `snapshot.lock{zwsp}.timeout.ms`>>
 |`10000`
 |An integer value that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot. If table locks cannot be acquired in this time interval, the snapshot will fail (also see {link-prefix}:{link-sqlserver-connector}#sqlserver-snapshots[snapshots]). +
 When set to `0` the connector will fail immediately when it cannot obtain the lock. Value `-1` indicates infinite waiting.
 
-|[[sqlserver-property-snapshot-select-statement-overrides]]<<sqlserver-property-snapshot-select-statement-overrides, `snapshot.select.statement.overrides`>>
+|[[sqlserver-property-snapshot-select-statement-overrides]]<<sqlserver-property-snapshot-select-statement-overrides, `snapshot.select{zwsp}.statement{zwsp}.overrides`>>
 |
 |Controls which rows from tables are included in snapshot. +
 This property contains a comma-separated list of fully-qualified tables _(SCHEMA_NAME.TABLE_NAME)_. Select statements for the individual tables are specified in further configuration properties, one for each table, identified by the id `snapshot.select.statement.overrides.[SCHEMA_NAME].[TABLE_NAME]`. The value of those properties is the SELECT statement to use when retrieving data from the specific table during snapshotting. _A possible use case for large append-only tables is setting a specific point where to start (resume) snapshotting, in case a previous snapshotting was interrupted._ +
@@ -1935,22 +1929,22 @@ By setting this option to `v1` the structure used in earlier versions can be pro
 Note that this setting is not recommended and is planned for removal in a future {prodname} version.
 endif::community[]
 
-|[[sqlserver-property-sanitize-field-names]]<<sqlserver-property-sanitize-field-names, `sanitize.field.names`>>
+|[[sqlserver-property-sanitize-field-names]]<<sqlserver-property-sanitize-field-names, `sanitize.field{zwsp}.names`>>
 |`true` when connector configuration explicitly specifies the `key.converter` or `value.converter` parameters to use Avro, otherwise defaults to `false`.
 |Whether field names are sanitized to adhere to Avro naming requirements.
 ifdef::community[]
 See {link-prefix}:{link-avro-serialization}#avro-naming[Avro naming] for more details.
 endif::community[]
 
-|[[sqlserver-property-database-server-timezone]]<<sqlserver-property-database-server-timezone, `database.server.timezone`>>
+|[[sqlserver-property-database-server-timezone]]<<sqlserver-property-database-server-timezone, `database.server{zwsp}.timezone`>>
 |
 | Timezone of the server.
 
 This is used to define the timezone of the transaction timestamp (ts_ms) retrieved from the server (which is actually not zoned). Default value is unset. Should only be specified when running on SQL Server 2014 or older and using different timezones for the database server and the JVM running the {prodname} connector. +
 When unset, default behavior is to use the timezone of the VM running the {prodname} connector. In this case, when running on on SQL Server 2014 or older and using different timezones on server and the connector, incorrect ts_ms values may be produced. +
 Possible values include ""Z"", ""UTC"", offset values like ""+02:00"", short zone ids like ""CET"", and long zone ids like ""Europe/Paris"".
 
-|[[sqlserver-property-provide-transaction-metadata]]<<sqlserver-property-provide-transaction-metadata, `provide.transaction.metadata`>>
+|[[sqlserver-property-provide-transaction-metadata]]<<sqlserver-property-provide-transaction-metadata, `provide.transaction{zwsp}.metadata`>>
 |`false`
 |When set to `true` {prodname} generates events with transaction boundaries and enriches data events envelope with transaction metadata.
 ",2020-09-23T08:25:48Z,1
"@@ -6,7 +6,6 @@
 
 package io.debezium.connector.sqlserver;
 
-import java.math.BigDecimal;
 import java.sql.DatabaseMetaData;
 import java.sql.ResultSet;
 import java.sql.SQLException;
@@ -16,18 +15,12 @@
 import java.time.ZoneOffset;
 import java.util.ArrayList;
 import java.util.Collections;
-import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Optional;
-import java.util.Map;
-import java.util.Optional;
 import java.util.Set;
 import java.util.stream.Collectors;
 
-import org.apache.kafka.connect.data.Field;
-import org.apache.kafka.connect.data.Schema;
-import org.apache.kafka.connect.data.SchemaBuilder;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -40,11 +33,8 @@
 import io.debezium.relational.ColumnEditor;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
-import io.debezium.relational.ValueConverter;
 import io.debezium.util.BoundedConcurrentHashMap;
 import io.debezium.util.Clock;
-import io.debezium.util.HexConverter;
-import microsoft.sql.DateTimeOffset;
 
 /**
  * {@link JdbcConnection} extension to be used with Microsoft SQL Server
@@ -87,30 +77,13 @@ public class SqlServerConnection extends JdbcConnection {
     private final ZoneId transactionTimezone;
     private final SourceTimestampMode sourceTimestampMode;
     private final Clock clock;
-    private final SqlServerValueConverters valueConverters;
 
     public static interface ResultSetExtractor<T> {
         T apply(ResultSet rs) throws SQLException;
     }
 
-    /**
-     * Converts JDBC string representation of a default column value to an object.
-     */
-    @FunctionalInterface
-    public interface DefaultValueMapper {
-
-        /**
-         * Parses string to an object.
-         * @param value string representation
-         * @return value
-         * @throws Exception if there is an parsing error
-         */
-        Object parse(String value) throws Exception;
-    }
-
     private final BoundedConcurrentHashMap<Lsn, Instant> lsnToInstantCache;
-
-    private final Map<String, DefaultValueMapper> defaultValueMappers;
+    private final SqlServerDefaultValueConverter defaultValueConverter;
 
     /**
      * Creates a new connection using the supplied configuration.
@@ -124,15 +97,14 @@ public interface DefaultValueMapper {
      */
     public SqlServerConnection(Configuration config, Clock clock, SourceTimestampMode sourceTimestampMode, SqlServerValueConverters valueConverters) {
         super(config, FACTORY);
-        this.valueConverters = valueConverters;
         lsnToInstantCache = new BoundedConcurrentHashMap<>(100);
         realDatabaseName = retrieveRealDatabaseName();
         boolean supportsAtTimeZone = supportsAtTimeZone();
         transactionTimezone = retrieveTransactionTimezone(supportsAtTimeZone);
         lsnToTimestamp = getLsnToTimestamp(supportsAtTimeZone);
         this.clock = clock;
         this.sourceTimestampMode = sourceTimestampMode;
-        defaultValueMappers = createDefaultValueMappers();
+        defaultValueConverter = new SqlServerDefaultValueConverter(this::connection, valueConverters);
     }
 
     /**
@@ -153,75 +125,6 @@ private static String getLsnToTimestamp(boolean supportsAtTimeZone) {
         return lsnToTimestamp;
     }
 
-    private Map<String, DefaultValueMapper> createDefaultValueMappers() {
-        Map<String, DefaultValueMapper> result = new HashMap<>();
-
-        // Exact numbers
-        result.put(""bigint"", v -> Long.parseLong(v.substring(2, v.length() - 3)));      // Sample value: ((3147483648.))
-        result.put(""int"", v -> Integer.parseInt(v.substring(2, v.length() - 2)));       // Sample value: ((2147483647))
-        result.put(""smallint"", v -> Short.parseShort(v.substring(2, v.length() - 2)));  // Sample value: ((32767))
-        result.put(""tinyint"", v -> Short.parseShort(v.substring(2, v.length() - 2)));   // Sample value: ((255))
-        result.put(""bit"", v -> v.equals(""((1))""));                                      // Either ((1)) or ((0))
-        result.put(""decimal"", v -> new BigDecimal(v.substring(2, v.length() - 2)));     // Sample value: ((100.12345))
-        result.put(""numeric"", v -> new BigDecimal(v.substring(2, v.length() - 2)));     // Sample value: ((100.12345))
-        result.put(""money"", v -> new BigDecimal(v.substring(2, v.length() - 2)));       // Sample value: ((922337203685477.58))
-        result.put(""smallmoney"", v -> new BigDecimal(v.substring(2, v.length() - 2)));  // Sample value: ((214748.3647))
-
-        // Approximate numerics
-        result.put(""float"", v -> Double.parseDouble(v.substring(2, v.length() - 2)));   // Sample value: ((1.2345000000000000e+003))
-        result.put(""real"", v -> Float.parseFloat(v.substring(2, v.length() - 2)));      // Sample value: ((1.2345000000000000e+003))
-
-        // Date and time
-        result.put(""date"", v -> { // Sample value: ('2019-02-03')
-            String rawValue = v.substring(2, v.length() - 2);
-            return prepareQueryAndMap(""SELECT PARSE(? AS date)"", st -> st.setString(1, rawValue),
-                    singleResultMapper(rs -> rs.getDate(1), ""Parse() should return exactly one result.""));
-        });
-        result.put(""datetime"", v -> { // Sample value: ('2019-01-01 00:00:00.000')
-            String rawValue = v.substring(2, v.length() - 2);
-            return prepareQueryAndMap(""SELECT PARSE(? AS datetime)"", st -> st.setString(1, rawValue),
-                    singleResultMapper(rs -> rs.getTimestamp(1), ""Parse() should return exactly one result.""));
-        });
-        result.put(""datetime2"", v -> { // Sample value: ('2019-01-01 00:00:00.1234567')
-            String rawValue = v.substring(2, v.length() - 2);
-            return prepareQueryAndMap(""SELECT PARSE(? AS datetime2)"", st -> st.setString(1, rawValue),
-                    singleResultMapper(rs -> rs.getTimestamp(1), ""Parse() should return exactly one result.""));
-        });
-        result.put(""datetimeoffset"", v -> { // Sample value: ('2019-01-01 00:00:00.1234567+02:00')
-            String rawValue = v.substring(2, v.length() - 2);
-            return prepareQueryAndMap(""SELECT PARSE(? AS datetimeoffset)"", st -> st.setString(1, rawValue),
-                    singleResultMapper(rs -> (DateTimeOffset) rs.getObject(1), ""Parse() should return exactly one result.""));
-        });
-        result.put(""smalldatetime"", v -> { // Sample value: ('2019-01-01 00:00:00')
-            String rawValue = v.substring(2, v.length() - 2);
-            return prepareQueryAndMap(""SELECT PARSE(? AS smalldatetime)"", st -> st.setString(1, rawValue),
-                    singleResultMapper(rs -> rs.getTimestamp(1), ""Parse() should return exactly one result.""));
-        });
-        result.put(""time"", v -> { // Sample value: ('2019-01-01 00:00:00')
-            String rawValue = v.substring(2, v.length() - 2);
-            return prepareQueryAndMap(""SELECT PARSE(? AS time)"", st -> st.setString(1, rawValue),
-                    singleResultMapper(rs -> rs.getTime(1), ""Parse() should return exactly one result.""));
-        });
-
-        // Character strings
-        result.put(""char"", v -> v.substring(2, v.length() - 2));    // Sample value: ('aaa')
-        result.put(""text"", v -> v.substring(2, v.length() - 2));    // Sample value: ('aaa')
-        result.put(""varchar"", v -> v.substring(2, v.length() - 2)); // Sample value: ('aaa')
-
-        // Unicode character strings
-        result.put(""nchar"", v -> v.substring(2, v.length() - 2));       // Sample value: ('aaa')
-        result.put(""ntext"", v -> v.substring(2, v.length() - 2));       // Sample value: ('aaa')
-        result.put(""nvarchar"", v -> v.substring(2, v.length() - 2));    // Sample value: ('aaa')
-
-        // Binary strings
-        result.put(""binary"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1)));     // Sample value: (0x0102030405)
-        result.put(""image"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1)));      // Sample value: (0x0102030405)
-        result.put(""varbinary"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1)));  // Sample value: (0x0102030405)
-
-        // Other data types, such as cursor, xml or uniqueidentifier, have been omitted.
-        return result;
-    }
-
     /**
      * @return the current largest log sequence number
      */
@@ -480,52 +383,6 @@ public Table getTableSchemaFromTable(SqlServerChangeTable changeTable) throws SQ
                 .create();
     }
 
-    @Override
-    protected void setDefaultValue(ColumnEditor columnEditor, String defaultValue) {
-        if (defaultValue == null) {
-            return;
-        }
-        parseDefaultValue(columnEditor.typeName(), defaultValue)
-                .map(rawDefaultValue -> convertDefaultValue(rawDefaultValue, columnEditor))
-                .ifPresent(columnEditor::defaultValue);
-    }
-
-    private Optional<Object> parseDefaultValue(String dataType, String defaultValue) {
-        DefaultValueMapper mapper = defaultValueMappers.get(dataType);
-        if (mapper == null) {
-            LOGGER.warn(""Mapper for type '{}' not found."", dataType);
-            return Optional.empty();
-        }
-
-        try {
-            return Optional.of(mapper.parse(defaultValue));
-        }
-        catch (Exception e) {
-            LOGGER.warn(""Cannot parse column default value '{}' to type '{}'."", defaultValue, dataType, e);
-            return Optional.empty();
-        }
-    }
-
-    private Object convertDefaultValue(Object defaultValue, ColumnEditor columnEditor) {
-        final Column column = columnEditor.create();
-
-        // if converters is not null and the default value is not null, we need to convert default value
-        if (valueConverters != null && defaultValue != null) {
-            final SchemaBuilder schemaBuilder = valueConverters.schemaBuilder(column);
-            if (schemaBuilder == null) {
-                return defaultValue;
-            }
-            final Schema schema = schemaBuilder.build();
-            // In order to get the valueConverter for this column, we have to create a field;
-            // The index value -1 in the field will never used when converting default value;
-            // So we can set any number here;
-            final Field field = new Field(columnEditor.name(), -1, schema);
-            final ValueConverter valueConverter = valueConverters.converter(columnEditor.create(), field);
-            return valueConverter.convert(defaultValue);
-        }
-        return defaultValue;
-    }
-
     public Table getTableSchemaFromChangeTable(SqlServerChangeTable changeTable) throws SQLException {
         final DatabaseMetaData metadata = connection().getMetaData();
         final TableId changeTableId = changeTable.getChangeTableId();
@@ -633,4 +490,13 @@ private Optional<Integer> getSqlServerVersion() {
             throw new RuntimeException(""Couldn't obtain database server version"", e);
         }
     }
+
+    @Override
+    protected void setDefaultValue(ColumnEditor columnEditor, String defaultValue) {
+        if (defaultValue != null) {
+            defaultValueConverter
+                    .parseDefaultValue(columnEditor, defaultValue)
+                    .ifPresent(columnEditor::defaultValue);
+        }
+    }
 }",2020-07-24T07:07:07Z,2
"@@ -29,7 +29,8 @@ public class SqlServerDatabaseSchema extends HistorizedRelationalDatabaseSchema
 
     private static final Logger LOGGER = LoggerFactory.getLogger(SqlServerDatabaseSchema.class);
 
-    public SqlServerDatabaseSchema(SqlServerConnectorConfig connectorConfig, ValueConverterProvider valueConverter, TopicSelector<TableId> topicSelector, SchemaNameAdjuster schemaNameAdjuster) {
+    public SqlServerDatabaseSchema(SqlServerConnectorConfig connectorConfig, ValueConverterProvider valueConverter, TopicSelector<TableId> topicSelector,
+                                   SchemaNameAdjuster schemaNameAdjuster) {
         super(connectorConfig, topicSelector, connectorConfig.getTableFilters().dataCollectionFilter(), connectorConfig.getColumnFilter(),
                 new TableSchemaBuilder(
                         valueConverter,",2020-07-24T07:07:07Z,3
"@@ -0,0 +1,190 @@
+/*
+ * Copyright Debezium Authors.
+ *
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+
+package io.debezium.connector.sqlserver;
+
+import java.math.BigDecimal;
+import java.sql.Connection;
+import java.sql.PreparedStatement;
+import java.sql.ResultSet;
+import java.sql.SQLException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Optional;
+
+import org.apache.kafka.connect.data.Field;
+import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.SchemaBuilder;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import io.debezium.connector.sqlserver.SqlServerConnection.ResultSetExtractor;
+import io.debezium.jdbc.JdbcConnection.StatementPreparer;
+import io.debezium.relational.Column;
+import io.debezium.relational.ColumnEditor;
+import io.debezium.relational.ValueConverter;
+import io.debezium.util.HexConverter;
+
+import microsoft.sql.DateTimeOffset;
+
+/**
+ * Parses and converts column default values.
+ */
+class SqlServerDefaultValueConverter {
+
+    private static Logger LOGGER = LoggerFactory.getLogger(SqlServerDefaultValueConverter.class);
+
+    /**
+     * Provides SQL connection instance.
+     */
+    @FunctionalInterface
+    interface ConnectionProvider {
+        Connection get() throws SQLException;
+    }
+
+    /**
+     * Converts JDBC string representation of a default column value to an object.
+     */
+    @FunctionalInterface
+    private interface DefaultValueMapper {
+
+        /**
+         * Parses string to an object.
+         *
+         * @param value string representation
+         * @return value
+         * @throws Exception if there is an parsing error
+         */
+        Object parse(String value) throws Exception;
+    }
+
+    private final ConnectionProvider connectionProvider;
+    private final SqlServerValueConverters valueConverters;
+    private final Map<String, DefaultValueMapper> defaultValueMappers;
+
+    SqlServerDefaultValueConverter(ConnectionProvider connectionProvider, SqlServerValueConverters valueConverters) {
+        this.connectionProvider = connectionProvider;
+        this.valueConverters = valueConverters;
+        this.defaultValueMappers = createDefaultValueMappers();
+    }
+
+    Optional<Object> parseDefaultValue(ColumnEditor columnEditor, String defaultValue) {
+        final String dataType = columnEditor.typeName();
+        final DefaultValueMapper mapper = defaultValueMappers.get(dataType);
+        if (mapper == null) {
+            LOGGER.warn(""Mapper for type '{}' not found."", dataType);
+            return Optional.empty();
+        }
+
+        try {
+            Object rawDefaultValue = mapper.parse(defaultValue);
+            Object convertedDefaultValue = convertDefaultValue(rawDefaultValue, columnEditor);
+            return Optional.of(convertedDefaultValue);
+        }
+        catch (Exception e) {
+            LOGGER.warn(""Cannot parse column default value '{}' to type '{}'."", defaultValue, dataType, e);
+            return Optional.empty();
+        }
+    }
+
+    private Object convertDefaultValue(Object defaultValue, ColumnEditor columnEditor) {
+        final Column column = columnEditor.create();
+
+        // if converters is not null and the default value is not null, we need to convert default value
+        if (valueConverters != null && defaultValue != null) {
+            final SchemaBuilder schemaBuilder = valueConverters.schemaBuilder(column);
+            if (schemaBuilder == null) {
+                return defaultValue;
+            }
+            final Schema schema = schemaBuilder.build();
+            // In order to get the valueConverter for this column, we have to create a field;
+            // The index value -1 in the field will never used when converting default value;
+            // So we can set any number here;
+            final Field field = new Field(columnEditor.name(), -1, schema);
+            final ValueConverter valueConverter = valueConverters.converter(columnEditor.create(), field);
+            return valueConverter.convert(defaultValue);
+        }
+        return defaultValue;
+    }
+
+    private Map<String, DefaultValueMapper> createDefaultValueMappers() {
+        final Map<String, DefaultValueMapper> result = new HashMap<>();
+
+        // Exact numbers
+        result.put(""bigint"", v -> Long.parseLong(v.substring(2, v.length() - 3))); // Sample value: ((3147483648.))
+        result.put(""int"", v -> Integer.parseInt(v.substring(2, v.length() - 2))); // Sample value: ((2147483647))
+        result.put(""smallint"", v -> Short.parseShort(v.substring(2, v.length() - 2))); // Sample value: ((32767))
+        result.put(""tinyint"", v -> Short.parseShort(v.substring(2, v.length() - 2))); // Sample value: ((255))
+        result.put(""bit"", v -> v.equals(""((1))"")); // Either ((1)) or ((0))
+        result.put(""decimal"", v -> new BigDecimal(v.substring(2, v.length() - 2))); // Sample value: ((100.12345))
+        result.put(""numeric"", v -> new BigDecimal(v.substring(2, v.length() - 2))); // Sample value: ((100.12345))
+        result.put(""money"", v -> new BigDecimal(v.substring(2, v.length() - 2))); // Sample value: ((922337203685477.58))
+        result.put(""smallmoney"", v -> new BigDecimal(v.substring(2, v.length() - 2))); // Sample value: ((214748.3647))
+
+        // Approximate numerics
+        result.put(""float"", v -> Double.parseDouble(v.substring(2, v.length() - 2))); // Sample value: ((1.2345000000000000e+003))
+        result.put(""real"", v -> Float.parseFloat(v.substring(2, v.length() - 2))); // Sample value: ((1.2345000000000000e+003))
+
+        // Date and time
+        result.put(""date"", v -> { // Sample value: ('2019-02-03')
+            String rawValue = v.substring(2, v.length() - 2);
+            return querySingleValue(""SELECT PARSE(? AS date)"", st -> st.setString(1, rawValue), rs -> rs.getDate(1));
+        });
+        result.put(""datetime"", v -> { // Sample value: ('2019-01-01 00:00:00.000')
+            String rawValue = v.substring(2, v.length() - 2);
+            return querySingleValue(""SELECT PARSE(? AS datetime)"", st -> st.setString(1, rawValue), rs -> rs.getTimestamp(1));
+        });
+        result.put(""datetime2"", v -> { // Sample value: ('2019-01-01 00:00:00.1234567')
+            String rawValue = v.substring(2, v.length() - 2);
+            return querySingleValue(""SELECT PARSE(? AS datetime2)"", st -> st.setString(1, rawValue), rs -> rs.getTimestamp(1));
+        });
+        result.put(""datetimeoffset"", v -> { // Sample value: ('2019-01-01 00:00:00.1234567+02:00')
+            String rawValue = v.substring(2, v.length() - 2);
+            return querySingleValue(""SELECT PARSE(? AS datetimeoffset)"", st -> st.setString(1, rawValue), rs -> (DateTimeOffset) rs.getObject(1));
+        });
+        result.put(""smalldatetime"", v -> { // Sample value: ('2019-01-01 00:00:00')
+            String rawValue = v.substring(2, v.length() - 2);
+            return querySingleValue(""SELECT PARSE(? AS smalldatetime)"", st -> st.setString(1, rawValue), rs -> rs.getTimestamp(1));
+        });
+        result.put(""time"", v -> { // Sample value: ('2019-01-01 00:00:00')
+            String rawValue = v.substring(2, v.length() - 2);
+            return querySingleValue(""SELECT PARSE(? AS time)"", st -> st.setString(1, rawValue), rs -> rs.getTime(1));
+        });
+
+        // Character strings
+        result.put(""char"", v -> v.substring(2, v.length() - 2)); // Sample value: ('aaa')
+        result.put(""text"", v -> v.substring(2, v.length() - 2)); // Sample value: ('aaa')
+        result.put(""varchar"", v -> v.substring(2, v.length() - 2)); // Sample value: ('aaa')
+
+        // Unicode character strings
+        result.put(""nchar"", v -> v.substring(2, v.length() - 2)); // Sample value: ('aaa')
+        result.put(""ntext"", v -> v.substring(2, v.length() - 2)); // Sample value: ('aaa')
+        result.put(""nvarchar"", v -> v.substring(2, v.length() - 2)); // Sample value: ('aaa')
+
+        // Binary strings
+        result.put(""binary"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1))); // Sample value: (0x0102030405)
+        result.put(""image"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1))); // Sample value: (0x0102030405)
+        result.put(""varbinary"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1))); // Sample value: (0x0102030405)
+
+        // Other data types, such as cursor, xml or uniqueidentifier, have been omitted.
+        return result;
+    }
+
+    private <T> T querySingleValue(String queryString, StatementPreparer preparer, ResultSetExtractor<T> extractor) throws SQLException {
+        PreparedStatement preparedStatement = connectionProvider.get().prepareStatement(queryString);
+        preparer.accept(preparedStatement);
+        try (ResultSet resultSet = preparedStatement.executeQuery()) {
+            if (resultSet.next()) {
+                final T result = extractor.apply(resultSet);
+                if (!resultSet.next()) {
+                    return result;
+                }
+            }
+            throw new IllegalStateException(""Exactly one result expected."");
+        }
+    }
+
+}",2020-07-24T07:07:07Z,4
"@@ -754,7 +754,7 @@ public void changeColumn() throws Exception {
     @Test
     public void addDefaultValue() throws Exception {
         final Configuration config = TestHelper.defaultConfig()
-                .with(SqlServerConnectorConfig.SNAPSHOT_MODE, SnapshotMode.INITIAL_SCHEMA_ONLY)
+                .with(SqlServerConnectorConfig.SNAPSHOT_MODE, SnapshotMode.SCHEMA_ONLY)
                 .build();
 
         start(SqlServerConnector.class, config);
@@ -780,7 +780,7 @@ public void alterDefaultValue() throws Exception {
         TestHelper.enableTableCdc(connection, ""table_dv"");
 
         final Configuration config = TestHelper.defaultConfig()
-                .with(SqlServerConnectorConfig.SNAPSHOT_MODE, SnapshotMode.INITIAL_SCHEMA_ONLY)
+                .with(SqlServerConnectorConfig.SNAPSHOT_MODE, SnapshotMode.SCHEMA_ONLY)
                 .build();
 
         start(SqlServerConnector.class, config);",2020-07-24T07:07:07Z,5
"@@ -19,6 +19,7 @@
 import org.junit.Test;
 
 import io.debezium.connector.sqlserver.util.TestHelper;
+import io.debezium.doc.FixFor;
 import io.debezium.relational.Column;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
@@ -105,6 +106,7 @@ public void shouldEnableCdcWithWrapperFunctionsForTable() throws Exception {
     }
 
     @Test
+    @FixFor(""DBZ-1015"")
     public void shouldProperlyGetDefaultColumnValues() throws Exception {
         try (SqlServerConnection connection = TestHelper.adminConnection()) {
             connection.connect();
@@ -176,7 +178,7 @@ public void shouldProperlyGetDefaultColumnValues() throws Exception {
             // and issue a test call to a CDC wrapper function
             Thread.sleep(5_000); // Need to wait to make sure the min_lsn is available
 
-            ChangeTable changeTable = new ChangeTable(new TableId(""testDB"", ""dbo"", ""table_with_defaults""),
+            SqlServerChangeTable changeTable = new SqlServerChangeTable(new TableId(""testDB"", ""dbo"", ""table_with_defaults""),
                     null, 0, null, null);
             Table table = connection.getTableSchemaFromTable(changeTable);
 ",2020-07-24T07:07:07Z,6
"@@ -1431,7 +1431,8 @@ public void shouldCaptureTableSchema() throws SQLException, InterruptedException
         assertConnectorIsRunning();
         TestHelper.waitForSnapshotToBeCompleted();
 
-        connection.execute(""INSERT INTO table_schema_test (key_cola, key_colb, cola, colb, colc, cold) VALUES(1, 'a', 100, '2019-01-01 10:20:39.1234567 +02:00', 'some_value', 100.20)"");
+        connection.execute(
+                ""INSERT INTO table_schema_test (key_cola, key_colb, cola, colb, colc, cold) VALUES(1, 'a', 100, '2019-01-01 10:20:39.1234567 +02:00', 'some_value', 100.20)"");
 
         List<SourceRecord> records = consumeRecordsByTopic(1).recordsForTopic(""server1.dbo.table_schema_test"");
         assertThat(records).hasSize(1);
@@ -1440,19 +1441,19 @@ public void shouldCaptureTableSchema() throws SQLException, InterruptedException
                         .name(""server1.dbo.table_schema_test.Key"")
                         .field(""key_cola"", Schema.INT32_SCHEMA)
                         .field(""key_colb"", Schema.STRING_SCHEMA)
-                        .build()
-                )
+                        .build())
                 .valueAfterFieldSchemaIsEqualTo(SchemaBuilder.struct()
                         .optional()
                         .name(""server1.dbo.table_schema_test.Value"")
                         .field(""key_cola"", Schema.INT32_SCHEMA)
                         .field(""key_colb"", Schema.STRING_SCHEMA)
                         .field(""cola"", Schema.INT32_SCHEMA)
-                        .field(""colb"", SchemaBuilder.string().name(""io.debezium.time.ZonedTimestamp"").required().defaultValue(""2019-01-01T12:34:56.1234567+04:00"").version(1).build())
+                        .field(""colb"",
+                                SchemaBuilder.string().name(""io.debezium.time.ZonedTimestamp"").required().defaultValue(""2019-01-01T12:34:56.1234567+04:00"").version(1)
+                                        .build())
                         .field(""colc"", SchemaBuilder.string().optional().defaultValue(""default_value"").build())
                         .field(""cold"", Schema.OPTIONAL_FLOAT64_SCHEMA)
-                        .build()
-                );
+                        .build());
 
         stopConnector();
     }",2020-07-24T07:07:07Z,7
"@@ -208,11 +208,13 @@ private static void dropTestDatabase(SqlServerConnection connection) throws SQLE
     }
 
     public static SqlServerConnection adminConnection() {
-        return new SqlServerConnection(TestHelper.adminJdbcConfig(), Clock.system(), SourceTimestampMode.getDefaultMode(), new SqlServerValueConverters(JdbcValueConverters.DecimalMode.PRECISE, TemporalPrecisionMode.ADAPTIVE));
+        return new SqlServerConnection(TestHelper.adminJdbcConfig(), Clock.system(), SourceTimestampMode.getDefaultMode(),
+                new SqlServerValueConverters(JdbcValueConverters.DecimalMode.PRECISE, TemporalPrecisionMode.ADAPTIVE));
     }
 
     public static SqlServerConnection testConnection() {
-        return new SqlServerConnection(TestHelper.defaultJdbcConfig(), Clock.system(), SourceTimestampMode.getDefaultMode(), new SqlServerValueConverters(JdbcValueConverters.DecimalMode.PRECISE, TemporalPrecisionMode.ADAPTIVE));
+        return new SqlServerConnection(TestHelper.defaultJdbcConfig(), Clock.system(), SourceTimestampMode.getDefaultMode(),
+                new SqlServerValueConverters(JdbcValueConverters.DecimalMode.PRECISE, TemporalPrecisionMode.ADAPTIVE));
     }
 
     /**",2020-07-24T07:07:07Z,8
"@@ -1112,12 +1112,8 @@ protected Optional<ColumnEditor> readTableColumn(ResultSet columnMetadata, Table
             column.generated(""YES"".equalsIgnoreCase(autogenerated));
 
             column.nativeType(resolveNativeType(column.typeName()));
-<<<<<<< HEAD
             column.jdbcType(resolveJdbcType(columnMetadata.getInt(5), column.nativeType()));
-            parseDefaultValue(columnType, columnMetadata.getString(13)).ifPresent(column::defaultValue);
-=======
             setDefaultValue(column, columnMetadata.getString(13));
->>>>>>> DBZ-1491 Parse temporal values using queries on database
             return Optional.of(column);
         }
 ",2020-07-24T07:07:07Z,9
"@@ -6,6 +6,7 @@
 
 package io.debezium.connector.sqlserver;
 
+import java.math.BigDecimal;
 import java.sql.DatabaseMetaData;
 import java.sql.ResultSet;
 import java.sql.SQLException;
@@ -15,9 +16,11 @@
 import java.time.ZoneOffset;
 import java.util.ArrayList;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.HashSet;
 import java.util.List;
 import java.util.Optional;
+import java.util.Map;
 import java.util.Set;
 import java.util.stream.Collectors;
 
@@ -35,6 +38,8 @@
 import io.debezium.relational.TableId;
 import io.debezium.util.BoundedConcurrentHashMap;
 import io.debezium.util.Clock;
+import io.debezium.util.HexConverter;
+import microsoft.sql.DateTimeOffset;
 
 /**
  * {@link JdbcConnection} extension to be used with Microsoft SQL Server
@@ -84,6 +89,8 @@ public static interface ResultSetExtractor<T> {
 
     private final BoundedConcurrentHashMap<Lsn, Instant> lsnToInstantCache;
 
+    private final Map<String, DefaultValueMapper> defaultValueMappers;
+
     /**
      * Creates a new connection using the supplied configuration.
      *
@@ -99,6 +106,7 @@ public SqlServerConnection(Configuration config, Clock clock, SourceTimestampMod
         lsnToTimestamp = getLsnToTimestamp(supportsAtTimeZone);
         this.clock = clock;
         this.sourceTimestampMode = sourceTimestampMode;
+        defaultValueMappers = createDefaultValueMappers();
     }
 
     /**
@@ -119,6 +127,75 @@ private static String getLsnToTimestamp(boolean supportsAtTimeZone) {
         return lsnToTimestamp;
     }
 
+    private Map<String, DefaultValueMapper> createDefaultValueMappers() {
+        Map<String, DefaultValueMapper> result = new HashMap<>();
+
+        // Exact numbers
+        result.put(""bigint"", v -> Long.parseLong(v.substring(2, v.length() - 3)));       // Sample value: ((3147483648.))
+        result.put(""int"", v -> Integer.parseInt(v.substring(2, v.length() - 2)));        // Sample value: ((2147483647))
+        result.put(""smallint"", v -> Short.parseShort(v.substring(2, v.length() - 2)));   // Sample value: ((32767))
+        result.put(""tinyint"", v -> Short.parseShort(v.substring(2, v.length() - 2)));    // Sample value: ((255))
+        result.put(""bit"", v -> v.equals(""((1))""));                                       // Either ((1)) or ((0))
+        result.put(""decimal"", v -> new BigDecimal(v.substring(2, v.length() - 2)));      // Sample value: ((100.12345))
+        result.put(""numeric"", v -> new BigDecimal(v.substring(2, v.length() - 2)));      // Sample value: ((100.12345))
+        result.put(""money"", v -> new BigDecimal(v.substring(2, v.length() - 2)));        // Sample value: ((922337203685477.58))
+        result.put(""smallmoney"", v -> new BigDecimal(v.substring(2, v.length() - 2)));   // Sample value: ((214748.3647))
+
+        // Approximate numerics
+        result.put(""float"", v -> Double.parseDouble(v.substring(2, v.length() - 2)));    // Sample value: ((1.2345000000000000e+003))
+        result.put(""real"", v -> Float.parseFloat(v.substring(2, v.length() - 2)));       // Sample value: ((1.2345000000000000e+003))
+
+        // Date and time
+        result.put(""date"", v -> {                                                        // Sample value: ('2019-02-03')
+            String rawValue = v.substring(2, v.length() - 2);
+            return prepareQueryAndMap(""SELECT PARSE(? AS date)"", st -> st.setString(1, rawValue),
+                    singleResultMapper(rs -> rs.getDate(1), ""Parse() should return exactly one result.""));
+        });
+        result.put(""datetime"", v -> {                                                    // Sample value: ('2019-01-01 00:00:00.000')
+            String rawValue = v.substring(2, v.length() - 2);
+            return prepareQueryAndMap(""SELECT PARSE(? AS datetime)"", st -> st.setString(1, rawValue),
+                    singleResultMapper(rs -> rs.getTimestamp(1), ""Parse() should return exactly one result.""));
+        });
+        result.put(""datetime2"", v -> {                                                    // Sample value: ('2019-01-01 00:00:00.1234567')
+            String rawValue = v.substring(2, v.length() - 2);
+            return prepareQueryAndMap(""SELECT PARSE(? AS datetime2)"", st -> st.setString(1, rawValue),
+                    singleResultMapper(rs -> rs.getTimestamp(1), ""Parse() should return exactly one result.""));
+        });
+        result.put(""datetimeoffset"", v -> {                                              // Sample value: ('2019-01-01 00:00:00.1234567+02:00')
+            String rawValue = v.substring(2, v.length() - 2);
+            return prepareQueryAndMap(""SELECT PARSE(? AS datetimeoffset)"", st -> st.setString(1, rawValue),
+                    singleResultMapper(rs -> (DateTimeOffset) rs.getObject(1), ""Parse() should return exactly one result.""));
+        });
+        result.put(""smalldatetime"", v -> {                                               // Sample value: ('2019-01-01 00:00:00')
+            String rawValue = v.substring(2, v.length() - 2);
+            return prepareQueryAndMap(""SELECT PARSE(? AS smalldatetime)"", st -> st.setString(1, rawValue),
+                    singleResultMapper(rs -> rs.getTimestamp(1), ""Parse() should return exactly one result.""));
+        });
+        result.put(""time"", v -> {                                                        // Sample value: ('2019-01-01 00:00:00')
+            String rawValue = v.substring(2, v.length() - 2);
+            return prepareQueryAndMap(""SELECT PARSE(? AS time)"", st -> st.setString(1, rawValue),
+                    singleResultMapper(rs -> rs.getTime(1), ""Parse() should return exactly one result.""));
+        });
+
+        // Character strings
+        result.put(""char"", v -> v.substring(2, v.length() - 2));         // Sample value: ('aaa')
+        result.put(""text"", v -> v.substring(2, v.length() - 2));         // Sample value: ('aaa')
+        result.put(""varchar"", v -> v.substring(2, v.length() - 2));      // Sample value: ('aaa')
+
+        // Unicode character strings
+        result.put(""nchar"", v -> v.substring(2, v.length() - 2));        // Sample value: ('aaa')
+        result.put(""ntext"", v -> v.substring(2, v.length() - 2));        // Sample value: ('aaa')
+        result.put(""nvarchar"", v -> v.substring(2, v.length() - 2));     // Sample value: ('aaa')
+
+        // Binary strings
+        result.put(""binary"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1)));      // Sample value: (0x0102030405)
+        result.put(""image"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1)));       // Sample value: (0x0102030405)
+        result.put(""varbinary"", v -> HexConverter.convertFromHex(v.substring(3, v.length() - 1)));   // Sample value: (0x0102030405)
+
+        // Other data types, such as cursor, xml or uniqueidentifier, have been omitted.
+        return result;
+    }
+
     /**
      * @return the current largest log sequence number
      */
@@ -377,6 +454,11 @@ public Table getTableSchemaFromTable(SqlServerChangeTable changeTable) throws SQ
                 .create();
     }
 
+    @Override
+    protected Map<String, DefaultValueMapper> getDefaultValueMappers() {
+        return defaultValueMappers;
+    }
+
     public Table getTableSchemaFromChangeTable(SqlServerChangeTable changeTable) throws SQLException {
         final DatabaseMetaData metadata = connection().getMetaData();
         final TableId changeTableId = changeTable.getChangeTableId();",2020-07-24T07:05:58Z,2
"@@ -264,7 +264,7 @@ private void addColumnToTable(boolean pauseAfterCaptureChange) throws Exception
                             .name(""server1.dbo.tableb.Value"")
                             .field(""id"", Schema.INT32_SCHEMA)
                             .field(""colb"", Schema.OPTIONAL_STRING_SCHEMA)
-                            .field(""newcol"", Schema.INT32_SCHEMA)
+                            .field(""newcol"", SchemaBuilder.int32().defaultValue(0).build())
                             .build());
         });
 
@@ -286,7 +286,7 @@ private void addColumnToTable(boolean pauseAfterCaptureChange) throws Exception
                             .name(""server1.dbo.tableb.Value"")
                             .field(""id"", Schema.INT32_SCHEMA)
                             .field(""colb"", Schema.OPTIONAL_STRING_SCHEMA)
-                            .field(""newcol"", Schema.INT32_SCHEMA)
+                            .field(""newcol"", SchemaBuilder.int32().defaultValue(0).build())
                             .build());
         });
     }",2020-07-24T07:05:58Z,5
"@@ -6,13 +6,22 @@
 
 package io.debezium.connector.sqlserver;
 
+import java.math.BigDecimal;
 import java.math.BigInteger;
+import java.sql.Date;
 import java.sql.SQLException;
+import java.sql.Time;
+import java.sql.Timestamp;
 
+import microsoft.sql.DateTimeOffset;
+import org.fest.assertions.Assertions;
 import org.junit.Before;
 import org.junit.Test;
 
 import io.debezium.connector.sqlserver.util.TestHelper;
+import io.debezium.relational.Column;
+import io.debezium.relational.Table;
+import io.debezium.relational.TableId;
 import io.debezium.util.Testing;
 
 /**
@@ -81,4 +90,114 @@ public void shouldEnableCdcWithWrapperFunctionsForTable() throws Exception {
 
     }
 
+    @Test
+    public void shouldProperlyGetDefaultColumnValues() throws Exception {
+        try (SqlServerConnection connection = TestHelper.adminConnection()) {
+            connection.connect();
+            connection.execute(""CREATE DATABASE testDB"");
+            connection.execute(""USE testDB"");
+        }
+
+        try (SqlServerConnection connection = TestHelper.testConnection()) {
+            connection.connect();
+            // NOTE: you cannot enable CDC on master
+            TestHelper.enableDbCdc(connection, ""testDB"");
+
+            // create table if exists
+            String sql = ""IF EXISTS (select 1 from sys.objects where name = 'table_with_defaults' and type = 'u')\n""
+                    + ""DROP TABLE testTable\n""
+                    + ""CREATE TABLE testDB.dbo.table_with_defaults (""
+                    + ""    int_no_default_not_null int not null,""
+                    + ""    int_no_default int,""
+                    + ""    bigint_column bigint default (3147483648),""
+                    + ""    int_column int default (2147483647),""
+                    + ""    smallint_column smallint default (32767),""
+                    + ""    tinyint_column tinyint default (255),""
+                    + ""    bit_column bit default(1),""
+                    + ""    decimal_column decimal(20,5) default (100.12345),""
+                    + ""    numeric_column numeric(10,3) default (200.123),""
+                    + ""    money_column money default (922337203685477.58),""
+                    + ""    smallmoney_column smallmoney default (214748.3647),""
+                    + ""    float_column float default (1.2345e2),""
+                    + ""    real_column real default (1.2345e3),""
+                    + ""    date_column date default ('2019-02-03'),""
+                    + ""    datetime_column datetime default ('2019-01-01 00:00:00.000'),""
+                    + ""    datetime2_column datetime2 default ('2019-01-01 00:00:00.1234567'),""
+                    + ""    datetimeoffset_column datetimeoffset default ('2019-01-01 00:00:00.1234567+02:00'),""
+                    + ""    smalldatetime_column smalldatetime default ('2019-01-01 00:00:00'),""
+                    + ""    time_column time default ('00:00:56.123'),""
+                    + ""    char_column char(3) default ('aaa'),""
+                    + ""    varchar_column varchar(20) default ('bbb'),""
+                    + ""    text_column text default ('ccc'),""
+                    + ""    nchar_column nchar(3) default ('ddd'),""
+                    + ""    nvarchar_column nvarchar(20) default ('eee'),""
+                    + ""    ntext_column ntext default ('fff'),""
+                    + ""    binary_column binary(5) default (0x0102030405),""
+                    + ""    varbinary_column varbinary(10) default (0x010203040506),""
+                    + ""    image_column image default (0x01020304050607)""
+                    + "");"";
+
+            connection.execute(sql);
+
+            // then enable CDC and wrapper functions
+            TestHelper.enableTableCdc(connection, ""table_with_defaults"");
+            // insert some data
+
+            // and issue a test call to a CDC wrapper function
+            Thread.sleep(5_000); // Need to wait to make sure the min_lsn is available
+
+            ChangeTable changeTable = new ChangeTable(new TableId(""testDB"", ""dbo"", ""table_with_defaults""),
+                    null, 0, null, null);
+            Table table = connection.getTableSchemaFromTable(changeTable);
+
+            assertColumnHasNotDefaultValue(table, ""int_no_default_not_null"");
+            assertColumnHasDefaultValue(table, ""int_no_default"", null);
+
+            assertColumnHasDefaultValue(table, ""bigint_column"", 3147483648L);
+            assertColumnHasDefaultValue(table, ""int_column"", 2147483647);
+            assertColumnHasDefaultValue(table, ""smallint_column"", (short) 32767);
+            assertColumnHasDefaultValue(table, ""tinyint_column"", (short) 255);
+            assertColumnHasDefaultValue(table, ""bit_column"", true);
+            assertColumnHasDefaultValue(table, ""decimal_column"", new BigDecimal(""100.12345""));
+            assertColumnHasDefaultValue(table, ""numeric_column"", new BigDecimal(""200.123""));
+            assertColumnHasDefaultValue(table, ""money_column"", new BigDecimal(""922337203685477.58""));
+            assertColumnHasDefaultValue(table, ""smallmoney_column"", new BigDecimal(""214748.3647""));
+            assertColumnHasDefaultValue(table, ""float_column"", 123.45);
+            assertColumnHasDefaultValue(table, ""real_column"", 1234.5f);
+            assertColumnHasDefaultValue(table, ""date_column"", Date.valueOf(""2019-02-03""));
+            assertColumnHasDefaultValue(table, ""datetime_column"", Timestamp.valueOf(""2019-01-01 00:00:00.000""));
+            assertColumnHasDefaultValue(table, ""datetime2_column"", Timestamp.valueOf(""2019-01-01 00:00:00.1234567""));
+            assertColumnHasDefaultValue(table, ""datetimeoffset_column"", nanosToDatetimeoffset(1546293600123456700L, 120));
+            assertColumnHasDefaultValue(table, ""smalldatetime_column"", Timestamp.valueOf(""2019-01-01 00:00:00""));
+            // JDBC connector provides accuracy limited to milliseconds only.
+            assertColumnHasDefaultValue(table, ""time_column"", new Time(56123));
+            assertColumnHasDefaultValue(table, ""char_column"", ""aaa"");
+            assertColumnHasDefaultValue(table, ""varchar_column"", ""bbb"");
+            assertColumnHasDefaultValue(table, ""text_column"", ""ccc"");
+            assertColumnHasDefaultValue(table, ""nchar_column"", ""ddd"");
+            assertColumnHasDefaultValue(table, ""nvarchar_column"", ""eee"");
+            assertColumnHasDefaultValue(table, ""ntext_column"", ""fff"");
+            assertColumnHasDefaultValue(table, ""binary_column"", new byte[]{ 1, 2, 3, 4, 5 });
+            assertColumnHasDefaultValue(table, ""varbinary_column"", new byte[]{ 1, 2, 3, 4, 5, 6 });
+            assertColumnHasDefaultValue(table, ""image_column"", new byte[]{ 1, 2, 3, 4, 5, 6, 7 });
+        }
+    }
+
+    private DateTimeOffset nanosToDatetimeoffset(long nanos, int offset) {
+        Timestamp dateTimeOffsetPart = new Timestamp(nanos / 1_000_000);
+        dateTimeOffsetPart.setNanos((int) (nanos % 1_000_000_000L));
+        return DateTimeOffset.valueOf(dateTimeOffsetPart, offset);
+    }
+
+    private void assertColumnHasNotDefaultValue(Table table, String columnName) {
+        Column column = table.columnWithName(columnName);
+        Assertions.assertThat(column.hasDefaultValue()).isFalse();
+    }
+
+    private void assertColumnHasDefaultValue(Table table, String columnName, Object expectedValue) {
+        Column column = table.columnWithName(columnName);
+        Assertions.assertThat(column.hasDefaultValue()).isTrue();
+        Assertions.assertThat(column.defaultValue()).isEqualTo(expectedValue);
+    }
+
 }",2020-07-24T07:05:58Z,6
"@@ -74,6 +74,21 @@ public void onEntryChosenForEviction(PreparedStatement statement) {
                 }
             });
 
+    /**
+     * Converts JDBC string representation of a default column value to an object.
+     */
+    @FunctionalInterface
+    public interface DefaultValueMapper {
+
+        /**
+         * Parses string to an object.
+         * @param value string representation
+         * @return value
+         * @throws Exception if there is an parsing error
+         */
+        Object parse(String value) throws Exception;
+    }
+
     /**
      * Establishes JDBC connections.
      */
@@ -1093,7 +1108,8 @@ protected Optional<ColumnEditor> readTableColumn(ResultSet columnMetadata, Table
         final String columnName = columnMetadata.getString(4);
         if (columnFilter == null || columnFilter.matches(tableId.catalog(), tableId.schema(), tableId.table(), columnName)) {
             final ColumnEditor column = Column.editor().name(columnName);
-            column.type(columnMetadata.getString(6));
+            String columnType = columnMetadata.getString(6);
+            column.type(columnType);
             column.length(columnMetadata.getInt(7));
             if (columnMetadata.getObject(9) != null) {
                 column.scale(columnMetadata.getInt(9));
@@ -1112,13 +1128,37 @@ protected Optional<ColumnEditor> readTableColumn(ResultSet columnMetadata, Table
 
             column.nativeType(resolveNativeType(column.typeName()));
             column.jdbcType(resolveJdbcType(columnMetadata.getInt(5), column.nativeType()));
-
+            parseDefaultValue(columnType, columnMetadata.getString(13)).ifPresent(column::defaultValue);
             return Optional.of(column);
         }
 
         return Optional.empty();
     }
 
+    private Optional<Object> parseDefaultValue(String dataType, String defaultValue) {
+        if (defaultValue == null) {
+            return Optional.empty();
+        }
+
+        DefaultValueMapper mapper = getDefaultValueMappers().get(dataType);
+        if (mapper == null) {
+            LOGGER.warn(""Mapper for type '{}' not found."", dataType);
+            return Optional.empty();
+        }
+
+        try {
+            return Optional.of(mapper.parse(defaultValue));
+        }
+        catch (Exception e) {
+            LOGGER.warn(""Cannot parse column default value '{}' to type '{}'."", defaultValue, dataType, e);
+            return Optional.empty();
+        }
+    }
+
+    protected Map<String, DefaultValueMapper> getDefaultValueMappers() {
+        return Collections.emptyMap();
+    }
+
     protected List<String> readPrimaryKeyNames(DatabaseMetaData metadata, TableId id) throws SQLException {
         final List<String> pkColumnNames = new ArrayList<>();
         try (ResultSet rs = metadata.getPrimaryKeys(id.catalog(), id.schema(), id.table())) {",2020-07-24T07:05:58Z,9
"@@ -591,7 +591,10 @@ To use the connector to produce change events for a particular MongoDB replica s
 When the connector starts, it will perform a snapshot of the collections in your MongoDB replica sets and start reading the replica sets' oplogs, producing events for every inserted, updated, and deleted row.
 Optionally filter out collections that are not needed.
 
-Here is an example of the configuration for a MongoDB connector that monitors a MongoDB replica set `rs0` at port 27017 on 192.168.99.100, which we logically name `fullfillment`:
+ifndef::cdc-product[]
+
+Following is an example of the configuration for a MongoDB connector that monitors a MongoDB replica set `rs0` at port 27017 on 192.168.99.100, which we logically name `fullfillment`.
+Typically, you configure the {prodname} MongoDB connector in a `.json` file using the configuration properties available for the connector.
 
 [source,json]
 ----
@@ -607,9 +610,37 @@ Here is an example of the configuration for a MongoDB connector that monitors a
 ----
 <1> The name of our connector when we register it with a Kafka Connect service.
 <2> The name of the MongoDB connector class.
-<3> The host addresses to use to connect to the MongoDB replica set
+<3> The host addresses to use to connect to the MongoDB replica set.
+<4> The _logical name_ of the MongoDB replica set, which forms a namespace for generated events and is used in all the names of the Kafka topics to which the connector writes, the Kafka Connect schema names, and the namespaces of the corresponding Avro schema when the Avro Connector is used.
+<5> A list of regular expressions that match the collection namespaces (for example, <dbName>.<collectionName>) of all collections to be monitored. This is optional.
+
+endif::[]
+
+ifdef::cdc-product[]
+Following is an example of the configuration for a MongoDB connector that monitors a MongoDB replica set `rs0` at port 27017 on 192.168.99.100, which we logically name `fullfillment`.
+Typically, you configure the {prodname} MongoDB connector in a `.yaml` file using the configuration properties available for the connector.
+
+[source,yaml,options=""nowrap""]
+----
+apiVersion: 
+  kind: MongoDbConnector
+  metadata:
+    name: inventory-connector  // <1>
+    labels:  
+  spec:
+    class: io.debezium.connector.mongodb.MongoDbConnector // <2>
+    config:  
+     mongodb.hosts: rs0/192.168.99.100:27017 // <3>
+     mongodb.name: fulfillment // <4>
+     collection.whitelist: inventory[.]* // <5>
+----
+<1> The name of our connector when we register it with a Kafka Connect service.
+<2> The name of the MongoDB connector class.
+<3> The host addresses to use to connect to the MongoDB replica set.
 <4> The _logical name_ of the MongoDB replica set, which forms a namespace for generated events and is used in all the names of the Kafka topics to which the connector writes, the Kafka Connect schema names, and the namespaces of the corresponding Avro schema when the Avro Connector is used.
-<5> A list of regular expressions that match the collection namespaces (e.g., <dbName>.<collectionName>) of all collections to be monitored. This is optional.
+<5> A list of regular expressions that match the collection namespaces (for example, <dbName>.<collectionName>) of all collections to be monitored. This is optional.
+
+endif::[]
 
 See the link:#mongodb-connector-properties[complete list of connector properties] that can be specified in these configurations.
 ",2020-04-28T16:27:43Z,10
"@@ -1595,12 +1595,15 @@ To use the connector to produce change events for a particular PostgreSQL server
 
 . Install the link:#output-plugin[logical decoding plugin]
 . Configure the link:#server-configuration[PostgreSQL server] to support logical replication
-. Create a configuration file for the PostgreSQL connector in JSON.
+. Create a configuration file for the PostgreSQL connector.
 
 When the connector starts, it will grab a consistent snapshot of the databases in your PostgreSQL server and start streaming changes, producing events for every inserted, updated, and deleted row. You can also choose to produce events for a subset of the schemas and tables.
 Optionally ignore, mask, or truncate columns that are sensitive, too large, or not needed.
 
-Here is an example of the configuration for a PostgreSQL connector that monitors a PostgreSQL server at port 5432 on 192.168.99.100, which we logically name `fullfillment`:
+ifndef::cdc-product[]
+
+Following is an example of the configuration for a PostgreSQL connector that monitors a PostgreSQL server at port 5432 on 192.168.99.100, which we logically name `fullfillment`.
+Typically, you configure the {prodname} PostgreSQL connector in a `.json` file using the configuration properties available for the connector.
 
 [source,json]
 ----
@@ -1629,6 +1632,49 @@ Here is an example of the configuration for a PostgreSQL connector that monitors
 <8> The logical name of the PostgreSQL server/cluster, which forms a namespace and is used in all the names of the Kafka topics to which the connector writes, the Kafka Connect schema names, and the namespaces of the corresponding Avro schema when the Avro Connector is used.
 <9> A list of all tables hosted by this server that this connector will monitor. This is optional, and there are other properties for listing the schemas and tables to include or exclude from monitoring.
 
+endif::[]
+
+ifdef::cdc-product[]
+
+Following is an example of the configuration for a PostgreSQL connector that monitors a PostgreSQL server at port 5432 on 192.168.99.100, which we logically name `fullfillment`.
+Typically, you configure the {prodname} PostgreSQL connector in a `.yaml` file using the configuration properties available for the connector.
+
+[source,yaml,options=""nowrap""]
+----
+apiVersion: 
+  kind: PostgresConnector
+  metadata:
+    name: inventory-connector  // <1>
+    labels:  
+  spec:
+    class: io.debezium.connector.postgresql.PostgresConnector
+    tasksMax: 1  // <2>
+    config:  // <3>
+      database.hostname: postgresqldb   // <4>
+      database.port: 5432
+      database.user: debezium
+      database.password: dbz
+      database.dbname: postgres  
+      database.server.name: fullfillment   // <5>
+      database.whitelist: public.inventory   // <6>
+----
+<1> The name of the connector.
+<2> Only one task should operate at any one time.
+Because the PostgreSQL connector reads the PostgreSQL servers `binlog`,
+using a single connector task ensures proper order and event handling.
+The Kafka Connect service uses connectors to start one or more tasks that do the work,
+and it automatically distributes the running tasks across the cluster of Kafka Connect services.
+If any of the services stop or crash,
+those tasks will be redistributed to running services.
+<3> The connectors configuration.
+<4> The database host, which is the name of the container running the PostgreSQL server (`postgresqldb`).
+<5> A unique server name.
+The server name is the logical identifier for the PostgreSQL server or cluster of servers.
+This name will be used as the prefix for all Kafka topics.
+<6> Only changes in the `public.inventory` database will be detected.
+
+endif::[]
+
 See the link:#connector-properties[complete list of connector properties] that can be specified in these configurations.
 
 This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the PostgreSQL database and record events to Kafka topics.",2020-04-28T16:27:43Z,11
"@@ -1086,14 +1086,16 @@ endif::cdc-product[]
 
 To use the connector to produce change events for a particular SQL Server database or cluster:
 
-. Enable the link:#setting-up-sqlserver[CDC on SQL Server] to publish the _CDC_ events in the database
-. Create a configuration file for the SQL Server connector in JSON.
+. Enable the link:#setting-up-sqlserver[CDC on SQL Server] to publish the _CDC_ events in the database.
+. Create a configuration file for the SQL Server connector.
 
 When the connector starts, it will grab a consistent snapshot of the schemas in your SQL Server database and start streaming changes, producing events for every inserted, updated, and deleted row.
 You can also choose to produce events for a subset of the schemas and tables.
 Optionally ignore, mask, or truncate columns that are sensitive, too large, or not needed.
 
-Here is an example of the configuration for a connector instance that monitors a SQL Server server at port 3306 on 192.168.99.100, which we logically name `fullfillment`:
+ifndef::cdc-product[]
+Following is an example of the configuration for a connector instance that monitors a SQL Server server at port 3306 on 192.168.99.100, which we logically name `fullfillment`.
+Typically, you configure the {prodname} SQL Server connector in a `.json` file using the configuration properties available for the connector.
 
 [source,json]
 ----
@@ -1102,7 +1104,7 @@ Here is an example of the configuration for a connector instance that monitors a
   ""config"": {
     ""connector.class"": ""io.debezium.connector.sqlserver.SqlServerConnector"", // <2>
     ""database.hostname"": ""192.168.99.100"", // <3>
-    ""database.port"": ""1433"", // <4>
+    ""database.port"": ""3306"", // <4>
     ""database.user"": ""sa"", // <5>
     ""database.password"": ""Password!"", // <6>
     ""database.dbname"": ""testDB"", // <7>
@@ -1119,12 +1121,53 @@ Here is an example of the configuration for a connector instance that monitors a
 <4> The port number of the SQL Server instance.
 <5> The name of the SQL Server user
 <6> The password for the SQL Server user
-<7> The name of the database to capture changes from
+<7> The name of the database to capture changes from.
+<8> The logical name of the SQL Server instance/cluster, which forms a namespace and is used in all the names of the Kafka topics to which the connector writes, the Kafka Connect schema names, and the namespaces of the corresponding Avro schema when the Avro Connector is used.
+<9> A list of all tables whose changes {prodname} should capture.
+<10> The list of Kafka brokers that this connector will use to write and recover DDL statements to the database history topic.
+<11> The name of the database history topic where the connector will write and recover DDL statements. This topic is for internal use only and should not be used by consumers.
+
+endif::[]
+
+ifdef::cdc-product[]
+Following is an example of the configuration for a connector instance that monitors a SQL Server server at port 3306 on 192.168.99.100, which we logically name `fullfillment`.
+Typically, you configure the {prodname} SQL Server connector in a `.yaml` file using the configuration properties available for the connector.
+
+[source,yaml,options=""nowrap""]
+----
+apiVersion: 
+  kind: SqlServerConnector
+  metadata:
+    name: inventory-connector  // <1>
+    labels:
+  spec:
+    class: io.debezium.connector.sqlserver.SqlServerConnector // <2>
+    config:  
+      database.hostname: 192.168.99.100  // <3>
+      database.port: 3306 //<4>
+      database.user: debezium  //<5>
+      database.password: dbz  //<6>
+      database.dbname: testDB  //<7>
+      database.server.name: fullfullment //<8>
+      database.whitelist: dbo.customers   //<9>
+      database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092  //<10> 
+      database.history.kafka.topic: dbhistory.fullfillment  //<11>
+
+----
+<1> The name of our connector when we register it with a Kafka Connect service.
+<2> The name of this SQL Server connector class.
+<3> The address of the SQL Server instance.
+<4> The port number of the SQL Server instance.
+<5> The name of the SQL Server user
+<6> The password for the SQL Server user
+<7> The name of the database to capture changes from.
 <8> The logical name of the SQL Server instance/cluster, which forms a namespace and is used in all the names of the Kafka topics to which the connector writes, the Kafka Connect schema names, and the namespaces of the corresponding Avro schema when the Avro Connector is used.
-<9> A list of all tables whose changes {prodname} should capture
+<9> A list of all tables whose changes {prodname} should capture.
 <10> The list of Kafka brokers that this connector will use to write and recover DDL statements to the database history topic.
 <11> The name of the database history topic where the connector will write and recover DDL statements. This topic is for internal use only and should not be used by consumers.
 
+endif::[]
+
 See the link:#sqlserver-connector-properties[complete list of connector properties] that can be specified in these configurations.
 
 This configuration can be sent via POST to a running Kafka Connect service, which will then record the configuration and start up the one connector task that will connect to the SQL Server database, read the transaction log, and record events to Kafka topics.",2020-04-28T16:27:43Z,1
"@@ -3,17 +3,16 @@
 
 [id=""configure-the-mysql-connector_{context}""]
 = Configuring the MySQL connector
-// Start the title of a procedure module with a verb, such as Creating or Create. See also _Wording of headings_ in _The IBM Style Guide_.
 
-Typically, you configure the {prodname} MySQL connector in a `JSON` file using the configuration properties available for the connector.
+ifndef::cdc-product[]
+Typically, you configure the {prodname} MySQL connector in a `.json` file using the configuration properties available for the connector.
 
 .Prerequisites
 * You should have completed the xref:install-the-mysql-connector_{context}[installation process] for the connector.
 
-
 .Procedure
 
-. Set the `""name""` of the connector in the JSON file.
+. Set the `""name""` of the connector in the `.json` file.
 . Set the configuration properties that you require for your {prodname} MySQL connector.
 
 TIP: For a complete list of configuration properties, see xref:mysql-connector-configuration-properties_{context}[MySQL connector configuration properties].
@@ -28,8 +27,8 @@ TIP: For a complete list of configuration properties, see xref:mysql-connector-c
     ""connector.class"": ""io.debezium.connector.mysql.MySqlConnector"", <2>
     ""database.hostname"": ""192.168.99.100"", <3>
     ""database.port"": ""3306"", <4>
-    ""database.user"": ""{prodname}-user"", <5>
-    ""database.password"": ""thePassword"", <6>
+    ""database.user"": ""debezium-user"", <5>
+    ""database.password"": ""debezium-user-pw"", <6>
     ""database.server.id"": ""184054"", <7>
     ""database.server.name"": ""fullfillment"", <8>
     ""database.whitelist"": ""inventory"", <9>
@@ -55,3 +54,64 @@ TIP: For a complete list of configuration properties, see xref:mysql-connector-c
 . A list of Kafka brokers that the connector uses to write and recover DDL statements to the database history topic.
 . The name of the database history topic.
 . The flag that specifies if the connector should generate on the schema change topic named `fulfillment` events with DDL changes that can be used by consumers.
+
+endif::[]
+ifdef::cdc-product[]
+
+Typically, you configure the {prodname} MySQL connector in a `.yaml` file using the configuration properties available for the connector.
+
+.Prerequisites
+* You should have completed the xref:install-the-mysql-connector_{context}[installation process] for the connector.
+
+.Procedure
+
+. Set the `""name""` of the connector in the `.yaml` file.
+
+. Set the configuration properties that you require for your {prodname} MySQL connector.
+
+TIP: For a complete list of configuration properties, see xref:mysql-connector-configuration-properties_{context}[MySQL connector configuration properties].
+
+.MySQL connector example configuration
+=========
+[source,yaml,options=""nowrap""]
+----
+  apiVersion: kafka.strimzi.io/v1alpha1
+  kind: KafkaConnector
+  metadata:
+    name: inventory-connector  // <1>
+    labels:
+      strimzi.io/cluster: my-connect-cluster
+  spec:
+    class: io.debezium.connector.mysql.MySqlConnector
+    tasksMax: 1  // <2>
+    config:  // <3>
+      database.hostname: mysql  // <4>
+      database.port: 3306
+      database.user: debezium
+      database.password: dbz
+      database.server.id: 184054  // <5>
+      database.server.name: dbserver1  // <5>
+      database.whitelist: inventory  // <6>
+      database.history.kafka.bootstrap.servers: my-cluster-kafka-bootstrap:9092  // <7>
+      database.history.kafka.topic: schema-changes.inventory  // <7>
+----
+<1> The name of the connector.
+<2> Only one task should operate at any one time.
+Because the MySQL connector reads the MySQL servers `binlog`,
+using a single connector task ensures proper order and event handling.
+The Kafka Connect service uses connectors to start one or more tasks that do the work,
+and it automatically distributes the running tasks across the cluster of Kafka Connect services.
+If any of the services stop or crash,
+those tasks will be redistributed to running services.
+<3> The connectors configuration.
+<4> The database host, which is the name of the container running the MySQL server (`mysql`).
+<5> A unique server ID and name.
+The server name is the logical identifier for the MySQL server or cluster of servers.
+This name will be used as the prefix for all Kafka topics.
+<6> Only changes in the `inventory` database will be detected.
+<7> The connector will store the history of the database schemas in Kafka using this broker (the same broker to which you are sending events) and topic name.
+Upon restart, the connector will recover the schemas of the database that existed at the point in time in the `binlog` when the connector should begin reading.
+----
+=========
+
+endif::[]",2020-04-28T16:27:43Z,12
"@@ -1964,7 +1964,7 @@ Finally, if set to *custom* then the user must also set `snapshot.custom.class`
 endif::cdc-product[]
 |`snapshot.lock.timeout.ms`
 |`10000`
-|Positive integer value that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot. If table locks cannot be acquired in this time interval, the snapshot will fail See link:#snapshots[snapshots]
+|Positive integer value that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot. If table locks cannot be acquired in this time interval, the snapshot will fail. See link:#snapshots[snapshots]
 
 |`snapshot.select.statement.overrides`
 |",2020-03-31T10:03:23Z,11
"@@ -299,6 +299,11 @@ Can be used to avoid snapshot interruptions when starting multiple connectors in
 |Specifies the maximum number of rows that should be read in one go from each table while taking a snapshot.
 The connector will read the table contents in multiple batches of this size.
 
+|`snapshot.lock.timeout.ms`
+|`10000`
+|Positive integer value that specifies the maximum amount of time (in milliseconds) to wait to obtain table locks when performing a snapshot.
+If table locks cannot be acquired in this time interval, the snapshot will fail. See link:#snapshots[snapshots]
+
 |`enable.time.adjuster`
 |
 |MySQL allows user to insert year value as either 2-digit or 4-digit.",2020-03-31T10:03:23Z,13
"@@ -7,14 +7,14 @@
 
 import java.util.concurrent.TimeUnit;
 
-import io.debezium.config.EnumeratedValue;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.common.config.ConfigDef.Importance;
 import org.apache.kafka.common.config.ConfigDef.Type;
 import org.apache.kafka.common.config.ConfigDef.Width;
 
 import io.debezium.config.CommonConnectorConfig;
 import io.debezium.config.Configuration;
+import io.debezium.config.EnumeratedValue;
 import io.debezium.config.Field;
 import io.debezium.config.Field.ValidationOutput;
 
@@ -318,8 +318,13 @@ public static SnapshotMode parse(String value, String defaultValue) {
 
     protected static Field.Set EXPOSED_FIELDS = ALL_FIELDS;
 
+    private final SnapshotMode snapshotMode;
+
     public MongoDbConnectorConfig(Configuration config) {
         super(config, config.getString(LOGICAL_NAME));
+
+        String snapshotModeValue = config.getString(MongoDbConnectorConfig.SNAPSHOT_MODE);
+        this.snapshotMode = SnapshotMode.parse(snapshotModeValue, MongoDbConnectorConfig.SNAPSHOT_MODE.defaultValueAsString());
     }
 
     protected static ConfigDef configDef() {
@@ -367,4 +372,8 @@ private static int validateDatabaseBlacklist(Configuration config, Field field,
         }
         return 0;
     }
+
+    public SnapshotMode getSnapshotMode() {
+        return snapshotMode;
+    }
 }",2019-02-21T16:00:12Z,14
"@@ -10,13 +10,11 @@
 import io.debezium.config.CommonConnectorConfig;
 import io.debezium.config.Configuration;
 import io.debezium.connector.common.CdcSourceTaskContext;
-import io.debezium.connector.mongodb.MongoDbConnectorConfig.SnapshotMode;
 import io.debezium.heartbeat.Heartbeat;
 import io.debezium.schema.TopicSelector;
 
 /**
  * @author Randall Hauch
- *
  */
 public class MongoDbTaskContext extends CdcSourceTaskContext {
 
@@ -26,7 +24,7 @@ public class MongoDbTaskContext extends CdcSourceTaskContext {
     private final boolean emitTombstoneOnDelete;
     private final String serverName;
     private final ConnectionContext connectionContext;
-    private final String snapshotMode;
+    private final MongoDbConnectorConfig connectorConfig;
 
     /**
      * @param config the configuration
@@ -41,7 +39,7 @@ public MongoDbTaskContext(Configuration config) {
         this.emitTombstoneOnDelete = config.getBoolean(CommonConnectorConfig.TOMBSTONES_ON_DELETE);
         this.serverName = config.getString(MongoDbConnectorConfig.LOGICAL_NAME);
         this.connectionContext = new ConnectionContext(config);
-        this.snapshotMode = config.getString(MongoDbConnectorConfig.SNAPSHOT_MODE);
+        this.connectorConfig = new MongoDbConnectorConfig(config);
     }
 
     public TopicSelector<CollectionId> topicSelector() {
@@ -68,16 +66,7 @@ public ConnectionContext getConnectionContext() {
         return connectionContext;
     }
 
-    public boolean isSnapshotAllowed() {
-        return snapshotMode() == SnapshotMode.INITIAL;
-    }
-
-    public boolean isSnapshotNeverAllowed() {
-        return snapshotMode() == SnapshotMode.NEVER;
-    }
-
-    protected SnapshotMode snapshotMode() {
-        String value = this.snapshotMode;
-        return SnapshotMode.parse(value, MongoDbConnectorConfig.SNAPSHOT_MODE.defaultValueAsString());
+    public MongoDbConnectorConfig getConnectorConfig() {
+        return this.connectorConfig;
     }
 }",2019-02-21T16:00:12Z,15
"@@ -38,6 +38,7 @@
 import io.debezium.annotation.ThreadSafe;
 import io.debezium.config.CommonConnectorConfig;
 import io.debezium.config.ConfigurationDefaults;
+import io.debezium.connector.mongodb.MongoDbConnectorConfig.SnapshotMode;
 import io.debezium.connector.mongodb.RecordMakers.RecordsForCollection;
 import io.debezium.function.BlockingConsumer;
 import io.debezium.function.BufferedBlockingConsumer;
@@ -146,8 +147,11 @@ public void run() {
                 if (establishConnectionToPrimary()) {
                     if (isInitialSyncExpected()) {
                         recordCurrentOplogPosition();
-                        if (context.isSnapshotAllowed() && !performInitialSync()) {
-                            return;
+                        if (context.getConnectorConfig().getSnapshotMode() == SnapshotMode.INITIAL) {
+                            boolean snapshotCompleted = performInitialSync();
+                            if (!snapshotCompleted) {
+                                return;
+                            }
                         }
                     }
                     readOplog();",2019-02-21T16:00:12Z,16
"@@ -299,9 +299,6 @@ public void shouldNotReplicateSnapshot() throws InterruptedException {
         // Sleep for 2 seconds ...
         Thread.sleep(2000);
 
-        // Stop the replicator ...
-        replicator.stop();
-
         // ------------------------------------------------------------------------------
         // VERIFY WE FOUND NO NEW EVENTS (SNAPSHOT IS NEVER)
         // ------------------------------------------------------------------------------
@@ -312,15 +309,6 @@ public void shouldNotReplicateSnapshot() throws InterruptedException {
         });
         assertThat(records.isEmpty()).isTrue();
 
-        // Start the replicator again ...
-        records = new LinkedList<>();
-        replicator = new Replicator(context, replicaSet, records::add, (x) ->  {});
-        thread = new Thread(replicator::run);
-        thread.start();
-
-        // Sleep for 2 seconds ...
-        Thread.sleep(2000);
-
         primary.execute(""shouldCreateContactsDatabase"", mongo -> {
             Testing.debug(""Populating the 'dbA.contacts' collection"");
 
@@ -331,12 +319,11 @@ public void shouldNotReplicateSnapshot() throws InterruptedException {
             contacts.insertOne(Document.parse(""{ \""name\"":\""Ygritte\""}""), insertOptions);
             assertThat(db.getCollection(""contacts"").countDocuments()).isEqualTo(2);
 
-            Testing.debug(""Completed document to 'dbA.contacts' collection"");
+            Testing.debug(""Added document to 'dbA.contacts' collection"");
         });
 
-
         // For a minimum number of events or max time ...
-        int numEventsExpected = 1; // both documents
+        int numEventsExpected = 1; // one document inserted during streaming
         long stop = System.currentTimeMillis() + TimeUnit.SECONDS.toMillis(3);
         while (records.size() < numEventsExpected && System.currentTimeMillis() < stop) {
             Thread.sleep(100);
@@ -349,16 +336,17 @@ public void shouldNotReplicateSnapshot() throws InterruptedException {
 
         // Verify each record is valid and that we found the one record we expect ...
         final Set<String> foundNames = new HashSet<>();
-        records.forEach(record -> {
+        for (SourceRecord record : records) {
             VerifyRecord.isValid(record);
             Struct value = (Struct) record.value();
             String after = value.getString(""after"");
             Document afterDoc = Document.parse(after);
             foundNames.add(afterDoc.getString(""name""));
             Operation op = Operation.forCode(value.getString(""op""));
             assertThat(op == Operation.CREATE).isTrue();
-        });
+        }
+
+        assertThat(foundNames).containsOnly(""Ygritte"");
         assertThat(records.size()).isEqualTo(1);
     }
-
 }",2019-02-21T16:00:12Z,17
"@@ -73,6 +73,7 @@ public Object convert(Column column, String value) {
 
         case Types.FLOAT:
         case Types.DOUBLE:
+        case Types.REAL:
             return convertToDouble(value);
         case Types.BIGINT:
             return convertToBigInt(value);",2018-06-15T09:42:24Z,18
"@@ -5,6 +5,7 @@
  */
 package io.debezium.connector.mysql;
 
+import java.lang.reflect.InvocationTargetException;
 import java.util.Collection;
 import java.util.Map;
 import java.util.Set;
@@ -98,18 +99,6 @@ public MySqlSchema(Configuration config, String serverName, Predicate<String> gt
         this.topicSelector = topicSelector;
         this.tableIdCaseInsensitive = tableIdCaseInsensitive;
 
-        String ddlParsingModeStr = config.getString(MySqlConnectorConfig.DDL_PARSER_MODE);
-        DdlParsingMode parsingMode = DdlParsingMode.parse(ddlParsingModeStr, MySqlConnectorConfig.DDL_PARSER_MODE.defaultValueAsString());
-
-        try {
-            this.ddlParser = parsingMode.getParserClass().newInstance();
-            this.ddlChanges = this.ddlParser.getDdlChanges();
-        }
-        catch (InstantiationException | IllegalAccessException e) {
-            // ddl parser constructors are not throwing any exceptions, so this should never happen
-            throw new IllegalArgumentException(""Unable to create new instance for ddl parser class "" + parsingMode.getParserClass().getCanonicalName());
-        }
-
         // Use MySQL-specific converters and schemas for values ...
         String timePrecisionModeStr = config.getString(MySqlConnectorConfig.TIME_PRECISION_MODE);
         TemporalPrecisionMode timePrecisionMode = TemporalPrecisionMode.parse(timePrecisionModeStr);
@@ -122,9 +111,17 @@ public MySqlSchema(Configuration config, String serverName, Predicate<String> gt
         MySqlValueConverters valueConverters = new MySqlValueConverters(decimalMode, timePrecisionMode, bigIntUnsignedMode);
         this.schemaBuilder = new TableSchemaBuilder(valueConverters, schemaNameAdjuster, SourceInfo.SCHEMA);
 
-        this.ddlParser = new MySqlDdlParser(false, valueConverters);
-        this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
-        this.ddlParser.addListener(ddlChanges);
+        String ddlParsingModeStr = config.getString(MySqlConnectorConfig.DDL_PARSER_MODE);
+        DdlParsingMode parsingMode = DdlParsingMode.parse(ddlParsingModeStr, MySqlConnectorConfig.DDL_PARSER_MODE.defaultValueAsString());
+
+        try {
+            this.ddlParser = parsingMode.getParserClass().getConstructor(MySqlValueConverters.class).newInstance(valueConverters);
+            this.ddlChanges = this.ddlParser.getDdlChanges();
+        }
+        catch (InstantiationException | IllegalAccessException | InvocationTargetException | NoSuchMethodException e) {
+            // ddl parser constructors are not throwing any exceptions, so this should never happen
+            throw new IllegalArgumentException(""Unable to create new instance for ddl parser class "" + parsingMode.getParserClass().getCanonicalName());
+        }
 
         // Set up the server name and schema prefix ...
         if (serverName != null) serverName = serverName.trim();",2018-06-15T09:42:24Z,19
"@@ -11,6 +11,7 @@
 import io.debezium.antlr.DataTypeResolver;
 import io.debezium.antlr.DataTypeResolver.DataTypeEntry;
 import io.debezium.connector.mysql.MySqlSystemVariables;
+import io.debezium.connector.mysql.MySqlValueConverters;
 import io.debezium.connector.mysql.antlr.listener.MySqlAntlrDdlParserListener;
 import io.debezium.ddl.parser.mysql.generated.MySqlLexer;
 import io.debezium.ddl.parser.mysql.generated.MySqlParser;
@@ -40,18 +41,24 @@
 public class MySqlAntlrDdlParser extends AntlrDdlParser<MySqlLexer, MySqlParser> {
 
     private final ConcurrentMap<String, String> charsetNameForDatabase = new ConcurrentHashMap<>();
+    private final MySqlValueConverters converters;
 
     public MySqlAntlrDdlParser() {
         this(true);
     }
 
+    public MySqlAntlrDdlParser(MySqlValueConverters converters) {
+        this(true, false, converters);
+    }
+
     public MySqlAntlrDdlParser(boolean throwErrorsFromTreeWalk) {
-        this(throwErrorsFromTreeWalk, false);
+        this(throwErrorsFromTreeWalk, false, null);
     }
 
-    public MySqlAntlrDdlParser(boolean throwErrorsFromTreeWalk, boolean includeViews) {
+    public MySqlAntlrDdlParser(boolean throwErrorsFromTreeWalk, boolean includeViews, MySqlValueConverters converters) {
         super(throwErrorsFromTreeWalk, includeViews);
         systemVariables = new MySqlSystemVariables();
+        this.converters = converters;
     }
 
     @Override
@@ -291,4 +298,8 @@ public static List<String> parseSetAndEnumOptions(String typeExpression) {
         return options;
     }
 
+    public MySqlValueConverters getConverters() {
+        return converters;
+    }
+
 }",2018-06-15T09:42:24Z,20
"@@ -70,7 +70,7 @@ public void enterAlterByAddColumn(MySqlParser.AlterByAddColumnContext ctx) {
         parserCtx.runIfNotNull(() -> {
             String columnName = parserCtx.parseName(ctx.uid(0));
             ColumnEditor columnEditor = Column.editor().name(columnName);
-            columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, columnEditor, parserCtx.dataTypeResolver());
+            columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, columnEditor, parserCtx.dataTypeResolver(), parserCtx.getConverters());
             listeners.add(columnDefinitionListener);
         }, tableEditor);
         super.exitAlterByAddColumn(ctx);
@@ -103,7 +103,7 @@ public void enterAlterByAddColumns(MySqlParser.AlterByAddColumnsContext ctx) {
                 String columnName = parserCtx.parseName(uidContext);
                 columnEditors.add(Column.editor().name(columnName));
             }
-            columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, columnEditors.get(0), parserCtx.dataTypeResolver());
+            columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, columnEditors.get(0), parserCtx.dataTypeResolver(), parserCtx.getConverters());
             listeners.add(columnDefinitionListener);
         }, tableEditor);
         super.enterAlterByAddColumns(ctx);
@@ -143,7 +143,7 @@ public void enterAlterByChangeColumn(MySqlParser.AlterByChangeColumnContext ctx)
             String oldColumnName = parserCtx.parseName(ctx.oldColumn);
             Column existingColumn = tableEditor.columnWithName(oldColumnName);
             if (existingColumn != null) {
-                columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, existingColumn.edit(), parserCtx.dataTypeResolver());
+                columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, existingColumn.edit(), parserCtx.dataTypeResolver(), parserCtx.getConverters());
                 listeners.add(columnDefinitionListener);
             }
             else {
@@ -178,7 +178,7 @@ public void enterAlterByModifyColumn(MySqlParser.AlterByModifyColumnContext ctx)
             String columnName = parserCtx.parseName(ctx.uid(0));
             Column column = tableEditor.columnWithName(columnName);
             if (column != null) {
-                columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, column.edit(), parserCtx.dataTypeResolver());
+                columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, column.edit(), parserCtx.dataTypeResolver(), parserCtx.getConverters());
                 listeners.add(columnDefinitionListener);
             }
             else {",2018-06-15T09:42:24Z,21
"@@ -6,19 +6,27 @@
 
 package io.debezium.connector.mysql.antlr.listener;
 
+import static io.debezium.antlr.AntlrDdlParser.getText;
+
+import java.sql.Types;
+import java.util.List;
+
+import org.apache.kafka.connect.data.Field;
+import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.SchemaBuilder;
+
 import io.debezium.antlr.DataTypeResolver;
+import io.debezium.connector.mysql.MySqlDefaultValuePreConverter;
+import io.debezium.connector.mysql.MySqlValueConverters;
 import io.debezium.ddl.parser.mysql.generated.MySqlParser;
+import io.debezium.ddl.parser.mysql.generated.MySqlParser.DefaultValueContext;
 import io.debezium.ddl.parser.mysql.generated.MySqlParserBaseListener;
 import io.debezium.relational.Column;
 import io.debezium.relational.ColumnEditor;
 import io.debezium.relational.TableEditor;
+import io.debezium.relational.ValueConverter;
 import io.debezium.relational.ddl.DataType;
 
-import java.sql.Types;
-import java.util.List;
-
-import static io.debezium.antlr.AntlrDdlParser.getText;
-
 /**
  * Parser listeners that is parsing column definition part of MySQL statements.
  *
@@ -30,10 +38,14 @@ public class ColumnDefinitionParserListener extends MySqlParserBaseListener {
     private final TableEditor tableEditor;
     private ColumnEditor columnEditor;
 
-    public ColumnDefinitionParserListener(TableEditor tableEditor, ColumnEditor columnEditor, DataTypeResolver dataTypeResolver) {
+    private final MySqlValueConverters converters;
+    private final MySqlDefaultValuePreConverter defaultValuePreConverter = new MySqlDefaultValuePreConverter();
+
+    public ColumnDefinitionParserListener(TableEditor tableEditor, ColumnEditor columnEditor, DataTypeResolver dataTypeResolver, MySqlValueConverters converters) {
         this.tableEditor = tableEditor;
         this.columnEditor = columnEditor;
         this.dataTypeResolver = dataTypeResolver;
+        this.converters = converters;
     }
 
     public void setColumnEditor(ColumnEditor columnEditor) {
@@ -80,6 +92,44 @@ public void enterNullNotnull(MySqlParser.NullNotnullContext ctx) {
         super.enterNullNotnull(ctx);
     }
 
+    @Override
+    public void enterDefaultValue(DefaultValueContext ctx) {
+        String sign = """";
+        if (ctx.NULL_LITERAL() != null) {
+            return;
+        }
+        if (ctx.unaryOperator() != null) {
+            sign = ctx.unaryOperator().getText();
+        }
+        if (ctx.constant() != null) {
+            if (ctx.constant().stringLiteral() != null) {
+                columnEditor.defaultValue(sign + unquote(ctx.constant().stringLiteral().getText()));
+            }
+            else if (ctx.constant().decimalLiteral() != null) {
+                columnEditor.defaultValue(sign + ctx.constant().decimalLiteral().getText());
+            }
+            else if (ctx.constant().BIT_STRING() != null) {
+                columnEditor.defaultValue(unquoteBinary(ctx.constant().BIT_STRING().getText()));
+            }
+            else if (ctx.constant().booleanLiteral() != null) {
+                columnEditor.defaultValue(ctx.constant().booleanLiteral().getText());
+            }
+            else if (ctx.constant().REAL_LITERAL() != null) {
+                columnEditor.defaultValue(ctx.constant().REAL_LITERAL().getText());
+            }
+        }
+        else if (ctx.timeDefinition() != null) {
+            if (ctx.timeDefinition().CURRENT_TIMESTAMP() != null || ctx.timeDefinition().NOW() != null) {
+                columnEditor.defaultValue(""1970-01-01 00:00:00"");
+            }
+            else {
+                columnEditor.defaultValue(ctx.timeDefinition().getText());
+            }
+        }
+        convertDefaultValueToSchemaType(columnEditor);
+        super.enterDefaultValue(ctx);
+    }
+
     @Override
     public void enterAutoIncrementColumnConstraint(MySqlParser.AutoIncrementColumnConstraintContext ctx) {
         columnEditor.autoIncremented(true);
@@ -191,4 +241,35 @@ else if (dataTypeContext instanceof MySqlParser.CollectionDataTypeContext) {
             columnEditor.charsetName(charsetName);
         }
     }
+
+    private void convertDefaultValueToSchemaType(ColumnEditor columnEditor) {
+        final Column column = columnEditor.create();
+        // if converters is not null and the default value is not null, we need to convert default value
+        if (converters != null && columnEditor.defaultValue() != null) {
+            Object defaultValue = columnEditor.defaultValue();
+            final SchemaBuilder schemaBuilder = converters.schemaBuilder(column);
+            if (schemaBuilder == null) {
+                return;
+            }
+            final Schema schema = schemaBuilder.build();
+            //In order to get the valueConverter for this column, we have to create a field;
+            //The index value -1 in the field will never used when converting default value;
+            //So we can set any number here;
+            final Field field = new Field(column.name(), -1, schema);
+            final ValueConverter valueConverter = converters.converter(column, field);
+            if (defaultValue instanceof String) {
+                defaultValue = defaultValuePreConverter.convert(column, (String)defaultValue);
+            }
+            defaultValue = valueConverter.convert(defaultValue);
+            columnEditor.defaultValue(defaultValue);
+        }
+    }
+
+    private String unquote(String stringLiteral) {
+        return stringLiteral.substring(1, stringLiteral.length() - 1);
+    }
+
+    private String unquoteBinary(String stringLiteral) {
+        return stringLiteral.substring(2, stringLiteral.length() - 1);
+    }
 }
\ No newline at end of file",2018-06-15T09:42:24Z,22
"@@ -77,7 +77,7 @@ public void enterColumnDeclaration(MySqlParser.ColumnDeclarationContext ctx) {
             String columnName = parserCtx.parseName(ctx.uid());
             ColumnEditor columnEditor = Column.editor().name(columnName);
             if (columnDefinitionListener == null) {
-                columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, columnEditor, parserCtx.dataTypeResolver());
+                columnDefinitionListener = new ColumnDefinitionParserListener(tableEditor, columnEditor, parserCtx.dataTypeResolver(), parserCtx.getConverters());
                 listeners.add(columnDefinitionListener);
             } else {
                 columnDefinitionListener.setColumnEditor(columnEditor);",2018-06-15T09:42:24Z,23
"@@ -0,0 +1,320 @@
+/*
+ * Copyright Debezium Authors.
+ *
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import java.math.BigDecimal;
+import java.util.function.Function;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import io.debezium.jdbc.JdbcValueConverters;
+import io.debezium.jdbc.TemporalPrecisionMode;
+import io.debezium.relational.Table;
+import io.debezium.relational.TableId;
+import io.debezium.relational.Tables;
+import io.debezium.relational.ddl.AbstractDdlParser;
+
+/**
+ * @author laomei
+ */
+public abstract class AbstractMysqlDefaultValueTest {
+
+    private AbstractDdlParser parser;
+    private Tables tables;
+    private MySqlValueConverters converters;
+    protected Function<MySqlValueConverters, AbstractDdlParser> parserProducer;
+    
+    @Before
+    public void beforeEach() {
+        converters = new MySqlValueConverters(JdbcValueConverters.DecimalMode.DOUBLE,
+                                              TemporalPrecisionMode.CONNECT,
+                                              JdbcValueConverters.BigIntUnsignedMode.LONG);
+        parser = parserProducer.apply(converters);
+        tables = new Tables();
+    }
+
+    @Test
+    public void parseUnsignedTinyintDefaultValue() {
+        String sql = ""CREATE TABLE UNSIGNED_TINYINT_TABLE ("" +
+                ""    A TINYINT UNSIGNED NULL DEFAULT 0,"" +
+                ""    B TINYINT UNSIGNED NULL DEFAULT '10',"" +
+                ""    C TINYINT UNSIGNED NULL,"" +
+                ""    D TINYINT UNSIGNED NOT NULL,"" +
+                ""    E TINYINT UNSIGNED NOT NULL DEFAULT 0,"" +
+                ""    F TINYINT UNSIGNED NOT NULL DEFAULT '0'"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_TINYINT_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo((short) 0);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo((short) 10);
+        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
+        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
+        assertThat(table.columnWithName(""C"").defaultValue()).isNull();
+        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
+        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo((short) 0);
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo((short) 0);
+    }
+
+    @Test
+    public void parseUnsignedSmallintDefaultValue() {
+        String sql = ""CREATE TABLE UNSIGNED_SMALLINT_TABLE (\n"" +
+                ""  A SMALLINT UNSIGNED NULL DEFAULT 0,\n"" +
+                ""  B SMALLINT UNSIGNED NULL DEFAULT '10',\n"" +
+                ""  C SMALLINT UNSIGNED NULL,\n"" +
+                ""  D SMALLINT UNSIGNED NOT NULL,\n"" +
+                ""  E SMALLINT UNSIGNED NOT NULL DEFAULT 0,\n"" +
+                ""  F SMALLINT UNSIGNED NOT NULL DEFAULT '0'\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_SMALLINT_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(0);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(10);
+        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
+        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
+        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
+        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(0);
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0);
+    }
+
+    @Test
+    public void parseUnsignedMediumintDefaultValue() {
+        String sql = ""CREATE TABLE UNSIGNED_MEDIUMINT_TABLE (\n"" +
+                ""  A MEDIUMINT UNSIGNED NULL DEFAULT 0,\n"" +
+                ""  B MEDIUMINT UNSIGNED NULL DEFAULT '10',\n"" +
+                ""  C MEDIUMINT UNSIGNED NULL,\n"" +
+                ""  D MEDIUMINT UNSIGNED NOT NULL,\n"" +
+                ""  E MEDIUMINT UNSIGNED NOT NULL DEFAULT 0,\n"" +
+                ""  F MEDIUMINT UNSIGNED NOT NULL DEFAULT '0'\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_MEDIUMINT_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(0);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(10);
+        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
+        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
+        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
+        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(0);
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0);
+    }
+
+    @Test
+    public void parseUnsignedIntDefaultValue() {
+        String sql = ""CREATE TABLE UNSIGNED_INT_TABLE (\n"" +
+                ""  A INT UNSIGNED NULL DEFAULT 0,\n"" +
+                ""  B INT UNSIGNED NULL DEFAULT '10',\n"" +
+                ""  C INT UNSIGNED NULL,\n"" +
+                ""  D INT UNSIGNED NOT NULL,\n"" +
+                ""  E INT UNSIGNED NOT NULL DEFAULT 0,\n"" +
+                ""  F INT UNSIGNED NOT NULL DEFAULT '0'\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_INT_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(0L);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(10L);
+        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
+        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
+        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
+        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(0L);
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0L);
+    }
+
+    @Test
+    public void parseUnsignedBigIntDefaultValueToLong() {
+        String sql = ""CREATE TABLE UNSIGNED_BIGINT_TABLE (\n"" +
+                ""  A BIGINT UNSIGNED NULL DEFAULT 0,\n"" +
+                ""  B BIGINT UNSIGNED NULL DEFAULT '10',\n"" +
+                ""  C BIGINT UNSIGNED NULL,\n"" +
+                ""  D BIGINT UNSIGNED NOT NULL,\n"" +
+                ""  E BIGINT UNSIGNED NOT NULL DEFAULT 0,\n"" +
+                ""  F BIGINT UNSIGNED NOT NULL DEFAULT '0'\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_BIGINT_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(0L);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(10L);
+        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
+        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
+        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
+        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(0L);
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0L);
+    }
+
+    @Test
+    public void parseUnsignedBigIntDefaultValueToBigDecimal() {
+        final MySqlValueConverters converters = new MySqlValueConverters(JdbcValueConverters.DecimalMode.DOUBLE,
+                TemporalPrecisionMode.CONNECT,
+                JdbcValueConverters.BigIntUnsignedMode.PRECISE);
+        final AbstractDdlParser parser = parserProducer.apply(converters);
+        String sql = ""CREATE TABLE UNSIGNED_BIGINT_TABLE (\n"" +
+                ""  A BIGINT UNSIGNED NULL DEFAULT 0,\n"" +
+                ""  B BIGINT UNSIGNED NULL DEFAULT '10',\n"" +
+                ""  C BIGINT UNSIGNED NULL,\n"" +
+                ""  D BIGINT UNSIGNED NOT NULL,\n"" +
+                ""  E BIGINT UNSIGNED NOT NULL DEFAULT 0,\n"" +
+                ""  F BIGINT UNSIGNED NOT NULL DEFAULT '0'\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_BIGINT_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(BigDecimal.ZERO);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(new BigDecimal(10));
+        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
+        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
+        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
+        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(BigDecimal.ZERO);
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(BigDecimal.ZERO);
+    }
+
+    @Test
+    public void parseStringDefaultValue() {
+        String sql = ""CREATE TABLE UNSIGNED_STRING_TABLE (\n"" +
+                ""  A CHAR NULL DEFAULT 'A',\n"" +
+                ""  B CHAR NULL DEFAULT 'b',\n"" +
+                ""  C VARCHAR(10) NULL DEFAULT 'CC',\n"" +
+                ""  D NCHAR(10) NULL DEFAULT '10',\n"" +
+                ""  E NVARCHAR NULL DEFAULT '0',\n"" +
+                ""  F CHAR DEFAULT NULL,\n"" +
+                ""  G VARCHAR(10) DEFAULT NULL,\n"" +
+                ""  H NCHAR(10) DEFAULT NULL\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_STRING_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(""A"");
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(""b"");
+        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(""CC"");
+        assertThat(table.columnWithName(""D"").defaultValue()).isEqualTo(""10"");
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(""0"");
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(null);
+        assertThat(table.columnWithName(""G"").defaultValue()).isEqualTo(null);
+        assertThat(table.columnWithName(""H"").defaultValue()).isEqualTo(null);
+    }
+
+    @Test
+    public void parseBitDefaultValue() {
+        String sql = ""CREATE TABLE BIT_TABLE (\n"" +
+                ""  A BIT(1) NULL DEFAULT NULL,\n"" +
+                ""  B BIT(1) DEFAULT 0,\n"" +
+                ""  C BIT(1) DEFAULT 1,\n"" +
+                ""  D BIT(1) DEFAULT b'0',\n"" +
+                ""  E BIT(1) DEFAULT b'1',\n"" +
+                ""  F BIT(1) DEFAULT TRUE,\n"" +
+                ""  G BIT(1) DEFAULT FALSE,\n"" +
+                ""  H BIT(10) DEFAULT b'101000010',\n"" +
+                ""  I BIT(10) DEFAULT NULL,\n"" +
+                ""  J BIT(25) DEFAULT b'10110000100001111'\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""BIT_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(null);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(false);
+        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(true);
+        assertThat(table.columnWithName(""D"").defaultValue()).isEqualTo(false);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(true);
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(true);
+        assertThat(table.columnWithName(""G"").defaultValue()).isEqualTo(false);
+        assertThat(table.columnWithName(""H"").defaultValue()).isEqualTo(new byte[] {66, 1});
+        assertThat(table.columnWithName(""I"").defaultValue()).isEqualTo(null);
+        assertThat(table.columnWithName(""J"").defaultValue()).isEqualTo(new byte[] {15, 97, 1, 0});
+    }
+
+    @Test
+    public void parseBooleanDefaultValue() {
+        String sql = ""CREATE TABLE BOOLEAN_TABLE (\n"" +
+                ""  A BOOLEAN NULL DEFAULT 0,\n"" +
+                ""  B BOOLEAN NOT NULL DEFAULT '1',\n"" +
+                ""  C BOOLEAN NOT NULL DEFAULT '9',\n"" +
+                ""  D BOOLEAN NOT NULL DEFAULT TRUE,\n"" +
+                ""  E BOOLEAN DEFAULT NULL\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""BOOLEAN_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(false);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(true);
+        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(true);
+        assertThat(table.columnWithName(""D"").defaultValue()).isEqualTo(true);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(null);
+    }
+
+    @Test
+    public void parseNumberDefaultValue() {
+        String sql = ""CREATE TABLE NUMBER_TABLE (\n"" +
+                ""  A TINYINT NULL DEFAULT 10,\n"" +
+                ""  B SMALLINT NOT NULL DEFAULT '5',\n"" +
+                ""  C INTEGER NOT NULL DEFAULT 0,\n"" +
+                ""  D BIGINT NOT NULL DEFAULT 20,\n"" +
+                ""  E INT NULL DEFAULT NULL,\n"" +
+                ""  F FLOAT NULL DEFAULT 0,\n"" +
+                ""  G DOUBLE NOT NULL DEFAULT 1.0\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""NUMBER_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo((short) 10);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo((short) 5);
+        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(0);
+        assertThat(table.columnWithName(""D"").defaultValue()).isEqualTo(20L);
+        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(null);
+        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0d);
+
+    }
+
+    @Test
+    public void parseRealDefaultValue() {
+        String sql = ""CREATE TABLE REAL_TABLE (\n"" +
+                ""  A REAL NOT NULL DEFAULT 1,\n"" +
+                ""  B REAL NULL DEFAULT NULL \n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""REAL_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(1f);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(null);
+    }
+
+    @Test
+    public void parseNumericAndDecimalToDoubleDefaultValue() {
+        String sql = ""CREATE TABLE NUMERIC_DECIMAL_TABLE (\n"" +
+                ""  A NUMERIC NOT NULL DEFAULT 1.23,\n"" +
+                ""  B DECIMAL NOT NULL DEFAULT 2.321,\n"" +
+                ""  C NUMERIC NULL DEFAULT '12.678'\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""NUMERIC_DECIMAL_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(1.23d);
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(2.321d);
+        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(12.678d);
+    }
+
+    @Test
+    public void parseNumericAndDecimalToDecimalDefaultValue() {
+        final MySqlValueConverters converters = new MySqlValueConverters(JdbcValueConverters.DecimalMode.PRECISE,
+                TemporalPrecisionMode.CONNECT,
+                JdbcValueConverters.BigIntUnsignedMode.LONG);
+        final AbstractDdlParser parser = parserProducer.apply(converters);
+        String sql = ""CREATE TABLE NUMERIC_DECIMAL_TABLE (\n"" +
+                ""  A NUMERIC NOT NULL DEFAULT 1.23,\n"" +
+                ""  B DECIMAL NOT NULL DEFAULT 2.321,\n"" +
+                ""  C NUMERIC NULL DEFAULT '12.678'\n"" +
+                "");"";
+        parser.parse(sql, tables);
+        Table table = tables.forTable(new TableId(null, null, ""NUMERIC_DECIMAL_TABLE""));
+        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(BigDecimal.valueOf(1.23));
+        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(BigDecimal.valueOf(2.321));
+        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(BigDecimal.valueOf(12.678));
+    }
+}",2018-06-15T09:42:24Z,24
"@@ -253,7 +253,7 @@ public MysqlDdlParserWithSimpleTestListener(DdlChanges changesListener) {
         }
 
         public MysqlDdlParserWithSimpleTestListener(DdlChanges changesListener, boolean includeViews) {
-            super(false, includeViews);
+            super(false, includeViews, null);
             this.ddlChanges = changesListener;
         }
     }",2018-06-15T09:42:24Z,25
"@@ -156,14 +156,20 @@ public void shouldParseCreateTableStatementWithCollate() {
     @Test
     @FixFor(""DBZ-646"")
     public void shouldParseTokuDBTable() {
-        String ddl = ""CREATE TABLE foo ( "" + System.lineSeparator()
+        String ddl1 = ""CREATE TABLE foo ( "" + System.lineSeparator()
                 + "" c1 INTEGER NOT NULL, "" + System.lineSeparator()
                 + "" c2 VARCHAR(22) "" + System.lineSeparator()
-                + "") engine=TokuDB `compression`=tokudb_zlib;"";
-        parser.parse(ddl, tables);
-        assertThat(tables.size()).isEqualTo(1);
+                + "") engine=TokuDB compression=tokudb_zlib;"";
+        String ddl2 = ""CREATE TABLE bar ( "" + System.lineSeparator()
+        + "" c1 INTEGER NOT NULL, "" + System.lineSeparator()
+        + "" c2 VARCHAR(22) "" + System.lineSeparator()
+        + "") engine=TokuDB compression='tokudb_zlib';"";
+        parser.parse(ddl1 + ddl2, tables);
+        assertThat(tables.size()).isEqualTo(2);
         listener.assertNext().createTableNamed(""foo"").ddlStartsWith(""CREATE TABLE foo ("");
+        listener.assertNext().createTableNamed(""bar"").ddlStartsWith(""CREATE TABLE bar ("");
         parser.parse(""DROP TABLE foo"", tables);
+        parser.parse(""DROP TABLE bar"", tables);
         assertThat(tables.size()).isEqualTo(0);
     }
 
@@ -490,22 +496,22 @@ public void shouldParseAlterTableStatementAddConstraintUniqueKey() {
         parser.parse(ddl, tables);
         assertThat(tables.size()).isEqualTo(1);
 
-        ddl = ""ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key ('col1');"";
+        ddl = ""ALTER TABLE t ADD CONSTRAINT UNIQUE KEY col_key (col1);"";
         parser.parse(ddl, tables);
 
-        ddl = ""ALTER TABLE t ADD CONSTRAINT UNIQUE KEY ('col1');"";
+        ddl = ""ALTER TABLE t ADD CONSTRAINT UNIQUE KEY (col1);"";
         parser.parse(ddl, tables);
 
-        ddl = ""ALTER TABLE t ADD UNIQUE KEY col_key ('col1');"";
+        ddl = ""ALTER TABLE t ADD UNIQUE KEY col_key (col1);"";
         parser.parse(ddl, tables);
 
-        ddl = ""ALTER TABLE t ADD UNIQUE KEY ('col1');"";
+        ddl = ""ALTER TABLE t ADD UNIQUE KEY (col1);"";
         parser.parse(ddl, tables);
 
-        ddl = ""ALTER TABLE t ADD CONSTRAINT 'xx' UNIQUE KEY col_key ('col1');"";
+        ddl = ""ALTER TABLE t ADD CONSTRAINT xx UNIQUE KEY col_key (col1);"";
         parser.parse(ddl, tables);
 
-        ddl = ""ALTER TABLE t ADD CONSTRAINT 'xx' UNIQUE KEY ('col1');"";
+        ddl = ""ALTER TABLE t ADD CONSTRAINT xy UNIQUE KEY (col1);"";
         parser.parse(ddl, tables);
     }
 
@@ -819,11 +825,11 @@ public void shouldParseAndIgnoreCreateFunction() {
     @FixFor(""DBZ-667"")
     @Test
     public void shouldParseScientificNotationNumber() {
-        String ddl = ""CREATE TABLE t (id INT NOT NULL, myvalue DOUBLE DEFAULT 1E-10, PRIMARY KEY (`id`));""
+        String ddl = ""CREATE TABLE t (id INT NOT NULL, myvalue DOUBLE DEFAULT 1E10, PRIMARY KEY (`id`));""
                 + ""CREATE TABLE t (id INT NOT NULL, myvalue DOUBLE DEFAULT 1.3E-10, PRIMARY KEY (`id`));""
-                + ""CREATE TABLE t (id INT NOT NULL, myvalue DOUBLE DEFAULT 1.3E+10, PRIMARY KEY (`id`));""
+                + ""CREATE TABLE t (id INT NOT NULL, myvalue DOUBLE DEFAULT 1.4E+10, PRIMARY KEY (`id`));""
                 + ""CREATE TABLE t (id INT NOT NULL, myvalue DOUBLE DEFAULT 3E10, PRIMARY KEY (`id`));""
-                + ""CREATE TABLE t (id INT NOT NULL, myvalue DOUBLE DEFAULT 1.3e10, PRIMARY KEY (`id`))"";
+                + ""CREATE TABLE t (id INT NOT NULL, myvalue DOUBLE DEFAULT 1.5e10, PRIMARY KEY (`id`))"";
         parser.parse(ddl, tables);
         assertThat(tables.size()).isEqualTo(1);
     }",2018-06-15T09:42:24Z,26
"@@ -0,0 +1,18 @@
+/*
+ * Copyright Debezium Authors.
+ *
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import io.debezium.connector.mysql.antlr.MySqlAntlrDdlParser;
+
+/**
+ * @author Jiri Pechanec <jpechane@redhat.com>
+ */
+public class MysqlAntlrDefaultValueTest extends AbstractMysqlDefaultValueTest {
+
+    {
+        parserProducer = MySqlAntlrDdlParser::new;
+    }
+}",2018-06-15T09:42:24Z,27
"@@ -5,322 +5,20 @@
  */
 package io.debezium.connector.mysql;
 
-import io.debezium.jdbc.JdbcValueConverters;
-import io.debezium.jdbc.TemporalPrecisionMode;
-import io.debezium.relational.Table;
-import io.debezium.relational.TableId;
-import io.debezium.relational.Tables;
 import io.debezium.relational.ddl.SimpleDdlParserListener;
-import org.junit.Before;
-import org.junit.Test;
-
-import java.math.BigDecimal;
-
-import static org.fest.assertions.Assertions.assertThat;
 
 /**
  * @author laomei
  */
-public class MysqlDefaultValueTest {
+public class MysqlDefaultValueTest extends AbstractMysqlDefaultValueTest {
 
-    private MySqlDdlParser parser;
-    private Tables tables;
     private SimpleDdlParserListener listener;
-    private MySqlValueConverters converters;
-
-    @Before
-    public void beforeEach() {
-        converters = new MySqlValueConverters(JdbcValueConverters.DecimalMode.DOUBLE,
-                                              TemporalPrecisionMode.CONNECT,
-                                              JdbcValueConverters.BigIntUnsignedMode.LONG);
-        parser = new MySqlDdlParser(false, converters);
-        tables = new Tables();
-        listener = new SimpleDdlParserListener();
-        parser.addListener(listener);
-    }
-
-    @Test
-    public void parseUnsignedTinyintDefaultValue() {
-        String sql = ""CREATE TABLE UNSIGNED_TINYINT_TABLE ("" +
-                ""    A TINYINT UNSIGNED NULL DEFAULT 0,"" +
-                ""    B TINYINT UNSIGNED NULL DEFAULT '10',"" +
-                ""    C TINYINT UNSIGNED NULL,"" +
-                ""    D TINYINT UNSIGNED NOT NULL,"" +
-                ""    E TINYINT UNSIGNED NOT NULL DEFAULT 0,"" +
-                ""    F TINYINT UNSIGNED NOT NULL DEFAULT '0'"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_TINYINT_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo((short) 0);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo((short) 10);
-        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
-        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
-        assertThat(table.columnWithName(""C"").defaultValue()).isNull();
-        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
-        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo((short) 0);
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo((short) 0);
-    }
-
-    @Test
-    public void parseUnsignedSmallintDefaultValue() {
-        String sql = ""CREATE TABLE UNSIGNED_SMALLINT_TABLE (\n"" +
-                ""  A SMALLINT UNSIGNED NULL DEFAULT 0,\n"" +
-                ""  B SMALLINT UNSIGNED NULL DEFAULT '10',\n"" +
-                ""  C SMALLINT UNSIGNED NULL,\n"" +
-                ""  D SMALLINT UNSIGNED NOT NULL,\n"" +
-                ""  E SMALLINT UNSIGNED NOT NULL DEFAULT 0,\n"" +
-                ""  F SMALLINT UNSIGNED NOT NULL DEFAULT '0'\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_SMALLINT_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(0);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(10);
-        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
-        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
-        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
-        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(0);
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0);
-    }
-
-    @Test
-    public void parseUnsignedMediumintDefaultValue() {
-        String sql = ""CREATE TABLE UNSIGNED_MEDIUMINT_TABLE (\n"" +
-                ""  A MEDIUMINT UNSIGNED NULL DEFAULT 0,\n"" +
-                ""  B MEDIUMINT UNSIGNED NULL DEFAULT '10',\n"" +
-                ""  C MEDIUMINT UNSIGNED NULL,\n"" +
-                ""  D MEDIUMINT UNSIGNED NOT NULL,\n"" +
-                ""  E MEDIUMINT UNSIGNED NOT NULL DEFAULT 0,\n"" +
-                ""  F MEDIUMINT UNSIGNED NOT NULL DEFAULT '0'\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_MEDIUMINT_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(0);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(10);
-        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
-        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
-        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
-        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(0);
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0);
-    }
-
-    @Test
-    public void parseUnsignedIntDefaultValue() {
-        String sql = ""CREATE TABLE UNSIGNED_INT_TABLE (\n"" +
-                ""  A INT UNSIGNED NULL DEFAULT 0,\n"" +
-                ""  B INT UNSIGNED NULL DEFAULT '10',\n"" +
-                ""  C INT UNSIGNED NULL,\n"" +
-                ""  D INT UNSIGNED NOT NULL,\n"" +
-                ""  E INT UNSIGNED NOT NULL DEFAULT 0,\n"" +
-                ""  F INT UNSIGNED NOT NULL DEFAULT '0'\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_INT_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(0L);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(10L);
-        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
-        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
-        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
-        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(0L);
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0L);
-    }
-
-    @Test
-    public void parseUnsignedBigIntDefaultValueToLong() {
-        String sql = ""CREATE TABLE UNSIGNED_BIGINT_TABLE (\n"" +
-                ""  A BIGINT UNSIGNED NULL DEFAULT 0,\n"" +
-                ""  B BIGINT UNSIGNED NULL DEFAULT '10',\n"" +
-                ""  C BIGINT UNSIGNED NULL,\n"" +
-                ""  D BIGINT UNSIGNED NOT NULL,\n"" +
-                ""  E BIGINT UNSIGNED NOT NULL DEFAULT 0,\n"" +
-                ""  F BIGINT UNSIGNED NOT NULL DEFAULT '0'\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_BIGINT_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(0L);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(10L);
-        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
-        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
-        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
-        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(0L);
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0L);
-    }
-
-    @Test
-    public void parseUnsignedBigIntDefaultValueToBigDecimal() {
-        converters = new MySqlValueConverters(JdbcValueConverters.DecimalMode.DOUBLE,
-                TemporalPrecisionMode.CONNECT,
-                JdbcValueConverters.BigIntUnsignedMode.PRECISE);
-        parser = new MySqlDdlParser(false, converters);
-        String sql = ""CREATE TABLE UNSIGNED_BIGINT_TABLE (\n"" +
-                ""  A BIGINT UNSIGNED NULL DEFAULT 0,\n"" +
-                ""  B BIGINT UNSIGNED NULL DEFAULT '10',\n"" +
-                ""  C BIGINT UNSIGNED NULL,\n"" +
-                ""  D BIGINT UNSIGNED NOT NULL,\n"" +
-                ""  E BIGINT UNSIGNED NOT NULL DEFAULT 0,\n"" +
-                ""  F BIGINT UNSIGNED NOT NULL DEFAULT '0'\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_BIGINT_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(BigDecimal.ZERO);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(new BigDecimal(10));
-        assertThat(table.columnWithName(""C"").isOptional()).isEqualTo(true);
-        assertThat(table.columnWithName(""C"").hasDefaultValue()).isTrue();
-        assertThat(table.columnWithName(""D"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""D"").hasDefaultValue()).isFalse();
-        assertThat(table.columnWithName(""E"").isOptional()).isEqualTo(false);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(BigDecimal.ZERO);
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(BigDecimal.ZERO);
-    }
-
-    @Test
-    public void parseStringDefaultValue() {
-        parser = new MySqlDdlParser(false, converters);
-        String sql = ""CREATE TABLE UNSIGNED_STRING_TABLE (\n"" +
-                ""  A CHAR NULL DEFAULT 'A',\n"" +
-                ""  B CHAR NULL DEFAULT 'b',\n"" +
-                ""  C VARCHAR(10) NULL DEFAULT 'CC',\n"" +
-                ""  D NCHAR(10) NULL DEFAULT '10',\n"" +
-                ""  E NVARCHAR NULL DEFAULT '0',\n"" +
-                ""  F CHAR DEFAULT NULL,\n"" +
-                ""  G VARCHAR(10) DEFAULT NULL,\n"" +
-                ""  H NCHAR(10) DEFAULT NULL\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""UNSIGNED_STRING_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(""A"");
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(""b"");
-        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(""CC"");
-        assertThat(table.columnWithName(""D"").defaultValue()).isEqualTo(""10"");
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(""0"");
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(null);
-        assertThat(table.columnWithName(""G"").defaultValue()).isEqualTo(null);
-        assertThat(table.columnWithName(""H"").defaultValue()).isEqualTo(null);
-    }
-
-    @Test
-    public void parseBitDefaultValue() {
-        parser = new MySqlDdlParser(false, converters);
-        String sql = ""CREATE TABLE BIT_TABLE (\n"" +
-                ""  A BIT(1) NULL DEFAULT NULL,\n"" +
-                ""  B BIT(1) DEFAULT 0,\n"" +
-                ""  C BIT(1) DEFAULT 1,\n"" +
-                ""  D BIT(1) DEFAULT b'0',\n"" +
-                ""  E BIT(1) DEFAULT b'1',\n"" +
-                ""  F BIT(1) DEFAULT TRUE,\n"" +
-                ""  G BIT(1) DEFAULT FALSE,\n"" +
-                ""  H BIT(10) DEFAULT b'101000010',\n"" +
-                ""  I BIT(10) DEFAULT NULL,\n"" +
-                ""  J BIT(25) DEFAULT b'10110000100001111'\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""BIT_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(null);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(false);
-        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(true);
-        assertThat(table.columnWithName(""D"").defaultValue()).isEqualTo(false);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(true);
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(true);
-        assertThat(table.columnWithName(""G"").defaultValue()).isEqualTo(false);
-        assertThat(table.columnWithName(""H"").defaultValue()).isEqualTo(new byte[] {66, 1});
-        assertThat(table.columnWithName(""I"").defaultValue()).isEqualTo(null);
-        assertThat(table.columnWithName(""J"").defaultValue()).isEqualTo(new byte[] {15, 97, 1, 0});
-    }
-
-    @Test
-    public void parseBooleanDefaultValue() {
-        parser = new MySqlDdlParser(false, converters);
-        String sql = ""CREATE TABLE BOOLEAN_TABLE (\n"" +
-                ""  A BOOLEAN NULL DEFAULT 0,\n"" +
-                ""  B BOOLEAN NOT NULL DEFAULT '1',\n"" +
-                ""  C BOOLEAN NOT NULL DEFAULT '9',\n"" +
-                ""  D BOOLEAN NOT NULL DEFAULT TRUE,\n"" +
-                ""  E BOOLEAN DEFAULT NULL\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""BOOLEAN_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(false);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(true);
-        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(true);
-        assertThat(table.columnWithName(""D"").defaultValue()).isEqualTo(true);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(null);
-    }
-
-    @Test
-    public void parseNumberDefaultValue() {
-        parser = new MySqlDdlParser(false, converters);
-        String sql = ""CREATE TABLE NUMBER_TABLE (\n"" +
-                ""  A TINYINT NULL DEFAULT 10,\n"" +
-                ""  B SMALLINT NOT NULL DEFAULT '5',\n"" +
-                ""  C INTEGER NOT NULL DEFAULT 0,\n"" +
-                ""  D BIGINT NOT NULL DEFAULT 20,\n"" +
-                ""  E INT NULL DEFAULT NULL,\n"" +
-                ""  F FLOAT NULL DEFAULT 0,\n"" +
-                ""  G DOUBLE NOT NULL DEFAULT 1.0\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""NUMBER_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo((short) 10);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo((short) 5);
-        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(0);
-        assertThat(table.columnWithName(""D"").defaultValue()).isEqualTo(20L);
-        assertThat(table.columnWithName(""E"").defaultValue()).isEqualTo(null);
-        assertThat(table.columnWithName(""F"").defaultValue()).isEqualTo(0d);
-        assertThat(table.columnWithName(""G"").defaultValue()).isEqualTo(1.0d);
-    }
-
-    @Test
-    public void parseRealDefaultValue() {
-        parser = new MySqlDdlParser(false, converters);
-        String sql = ""CREATE TABLE REAL_TABLE (\n"" +
-                ""  A REAL NOT NULL DEFAULT 1,\n"" +
-                ""  B REAL NULL DEFAULT NULL \n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""REAL_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(1f);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(null);
-    }
-
-    @Test
-    public void parseNumericAndDecimalToDoubleDefaultValue() {
-        parser = new MySqlDdlParser(false, converters);
-        String sql = ""CREATE TABLE NUMERIC_DECIMAL_TABLE (\n"" +
-                ""  A NUMERIC NOT NULL DEFAULT 1.23,\n"" +
-                ""  B DECIMAL NOT NULL DEFAULT 2.321,\n"" +
-                ""  C NUMERIC NULL DEFAULT '12.678'\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""NUMERIC_DECIMAL_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(1.23d);
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(2.321d);
-        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(12.678d);
-    }
-
-    @Test
-    public void parseNumericAndDecimalToDecimalDefaultValue() {
-        converters = new MySqlValueConverters(JdbcValueConverters.DecimalMode.PRECISE,
-                TemporalPrecisionMode.CONNECT,
-                JdbcValueConverters.BigIntUnsignedMode.LONG);
-        parser = new MySqlDdlParser(false, converters);
-        String sql = ""CREATE TABLE NUMERIC_DECIMAL_TABLE (\n"" +
-                ""  A NUMERIC NOT NULL DEFAULT 1.23,\n"" +
-                ""  B DECIMAL NOT NULL DEFAULT 2.321,\n"" +
-                ""  C NUMERIC NULL DEFAULT '12.678'\n"" +
-                "");"";
-        parser.parse(sql, tables);
-        Table table = tables.forTable(new TableId(null, null, ""NUMERIC_DECIMAL_TABLE""));
-        assertThat(table.columnWithName(""A"").defaultValue()).isEqualTo(BigDecimal.valueOf(1.23));
-        assertThat(table.columnWithName(""B"").defaultValue()).isEqualTo(BigDecimal.valueOf(2.321));
-        assertThat(table.columnWithName(""C"").defaultValue()).isEqualTo(BigDecimal.valueOf(12.678));
+    {
+        parserProducer = (converters) -> {
+            MySqlDdlParser parser = new MySqlDdlParser(false, converters);
+            listener = new SimpleDdlParserListener();
+            parser.addListener(listener);
+            return parser;
+        };
     }
 }",2018-06-15T09:42:24Z,28
"@@ -621,15 +621,16 @@ protected Object parseNumericLiteral(Marker start, boolean signed) {
             sb.append(tokens.consumeInteger());
         }
         if (tokens.canConsume('.')) {
+            sb.append('.');
             sb.append(tokens.consumeInteger());
             decimal = true;
         }
-        if (!tokens.canConsume('E')) {
+        if (!tokens.canConsumeAnyOf(""E"", ""e"")) {
             if (decimal) return Double.parseDouble(sb.toString());
             return Integer.parseInt(sb.toString());
         }
         sb.append('E');
-        if (tokens.matches(""+"", ""-"")) {
+        if (tokens.matchesAnyOf(""+"", ""-"")) {
             sb.append(tokens.consumeAnyOf(""+"", ""-""));
         }
         sb.append(tokens.consumeInteger());",2018-06-15T09:42:24Z,29
"@@ -61,6 +61,10 @@ protected static LocalDate toLocalDate(Object obj) {
             // Assume the value is the epoch day number
             return LocalDate.ofEpochDay((Long)obj);
         }
+        if ( obj instanceof Integer) {
+            // Assume the value is the epoch day number
+            return LocalDate.ofEpochDay((Integer)obj);
+        }
         throw new IllegalArgumentException(""Unable to convert to LocalDate from unexpected value '"" + obj + ""' of type "" + obj.getClass().getName());
     }
 ",2018-06-15T09:42:24Z,30
"@@ -65,6 +65,9 @@ public static Schema schema() {
      * @throws IllegalArgumentException if the value is not an instance of the acceptable types
      */
     public static long toEpochMillis(Object value, TemporalAdjuster adjuster) {
+        if (value instanceof Long) {
+            return (Long)value;
+        }
         LocalDateTime dateTime = Conversions.toLocalDateTime(value);
         if (adjuster != null) {
             dateTime = dateTime.with(adjuster);",2018-06-15T09:42:24Z,31
"@@ -78,6 +78,9 @@ public static Schema schema() {
      * @throws IllegalArgumentException if the value is not an instance of the acceptable types
      */
     public static String toIsoString(Object value, ZoneId defaultZone, TemporalAdjuster adjuster) {
+        if (value instanceof String) {
+            return (String)value;
+        }
         if (value instanceof OffsetDateTime) {
             return toIsoString((OffsetDateTime) value, adjuster);
         }",2018-06-15T09:42:24Z,32
"@@ -705,6 +705,7 @@ MYISAM:                              'MYISAM';
 NDB:                                 'NDB';
 NDBCLUSTER:                          'NDBCLUSTER';
 PERFORMANCE_SCHEMA:                  'PERFORMANCE_SCHEMA';
+TOKUDB:                              'TOKUDB';
 
 
 // Transaction Levels
@@ -1162,7 +1163,7 @@ fragment CHARSET_NAME:               ARMSCII8 | ASCII | BIG5 | BINARY | CP1250
                                      | UCS2 | UJIS | UTF16 | UTF16LE | UTF32
                                      | UTF8 | UTF8MB4;
 
-fragment EXPONENT_NUM_PART:          'E' '-'? DEC_DIGIT+;
+fragment EXPONENT_NUM_PART:          'E' [\-+]? DEC_DIGIT+;
 fragment ID_LITERAL:                 [A-Z_$0-9]*?[A-Z_$]+?[A-Z_$0-9]*;
 fragment DQUOTA_STRING:              '""' ( '\\'. | '""""' | ~('""'| '\\') )* '""';
 fragment SQUOTA_STRING:              '\'' ('\\'. | '\'\'' | ~('\'' | '\\'))* '\'';",2018-06-15T09:42:24Z,33
"@@ -409,7 +409,7 @@ tableOption
     | (CHECKSUM | PAGE_CHECKSUM) '='? boolValue=('0' | '1')         #tableOptionChecksum
     | DEFAULT? COLLATE '='? collationName                           #tableOptionCollate
     | COMMENT '='? STRING_LITERAL                                   #tableOptionComment
-    | COMPRESSION '='? STRING_LITERAL                               #tableOptionCompression
+    | COMPRESSION '='? (STRING_LITERAL | ID)                        #tableOptionCompression
     | CONNECTION '='? STRING_LITERAL                                #tableOptionConnection
     | DATA DIRECTORY '='? STRING_LITERAL                            #tableOptionDataDirectory
     | DELAY_KEY_WRITE '='? boolValue=('0' | '1')                    #tableOptionDelay
@@ -1877,6 +1877,7 @@ collationName
 engineName
     : ARCHIVE | BLACKHOLE | CSV | FEDERATED | INNODB | MEMORY 
     | MRG_MYISAM | MYISAM | NDB | NDBCLUSTER | PERFORMANCE_SCHEMA
+    | TOKUDB
     ;
 
 uuidSet",2018-06-15T09:42:24Z,34
"@@ -296,7 +296,7 @@ public void taskStarted() {
             engine.run();
         });
         try {
-            if (!latch.await(10, TimeUnit.SECONDS)) {
+            if (!latch.await(1000, TimeUnit.SECONDS)) {
                 // maybe it takes more time to start up, so just log a warning and continue
                 logger.warn(""The connector did not finish starting its task(s) or complete in the expected amount of time"");
             }",2018-06-15T09:42:24Z,35
"@@ -51,11 +51,11 @@ public Object convert(Column column, String value) {
         }
         switch (column.jdbcType()) {
         case Types.DATE:
-            return convertToLocalDate(value);
+            return convertToLocalDate(column, value);
         case Types.TIMESTAMP:
             return convertToLocalDateTime(column, value);
         case Types.TIMESTAMP_WITH_TIMEZONE:
-            return convertToTimestamp(value);
+            return convertToTimestamp(column, value);
         case Types.TIME:
             return convertToDuration(column, value);
         case Types.BOOLEAN:
@@ -84,38 +84,46 @@ public Object convert(Column column, String value) {
 
     /**
      * Converts a string object for an object type of {@link LocalDate}.
-     * 0000-00-00 will be replaced with 1970-01-01;
+     * If the column definition allows null and default value is 0000-00-00, we need return null;
+     * else 0000-00-00 will be replaced with 1970-01-01;
      *
+     * @param column the column definition describing the {@code data} value; never null
      * @param value the string object to be converted into a {@link LocalDate} type;
      * @return the converted value;
      */
-    private Object convertToLocalDate(String value) {
+    private Object convertToLocalDate(Column column, String value) {
+        if (ALL_ZERO_DATE.equals(value) && column.isOptional()) return null;
         if (ALL_ZERO_DATE.equals(value)) value = EPOCH_DATE;
         return LocalDate.from(DateTimeFormatter.ISO_LOCAL_DATE.parse(value));
     }
 
     /**
      * Converts a string object for an object type of {@link LocalDateTime}.
-     * 0000-00-00 00:00:00 will be replaced with 1970-01-01 00:00:00;
+     * If the column definition allows null and default value is 0000-00-00 00:00:00, we need return null,
+     * else 0000-00-00 00:00:00 will be replaced with 1970-01-01 00:00:00;
      *
      * @param column the column definition describing the {@code data} value; never null
      * @param value the string object to be converted into a {@link LocalDateTime} type;
      * @return the converted value;
      */
     private Object convertToLocalDateTime(Column column, String value) {
+        if (ALL_ZERO_TIMESTAMP.equals(value) && column.isOptional()) return null;
         if (ALL_ZERO_TIMESTAMP.equals(value)) value = EPOCH_TIMESTAMP;
         String timestampFormat = timestampFormat(column.length());
         return LocalDateTime.from(DateTimeFormatter.ofPattern(timestampFormat).parse(value));
     }
 
     /**
      * Converts a string object for an object type of {@link Timestamp}.
-     * 0000-00-00 00:00:00 will be replaced with 1970-01-01 00:00:00;
+     * If the column definition allows null and default value is 0000-00-00 00:00:00, we need return null,
+     * else 0000-00-00 00:00:00 will be replaced with 1970-01-01 00:00:00;
      *
+     * @param column the column definition describing the {@code data} value; never null
      * @param value the string object to be converted into a {@link Timestamp} type;
      * @return the converted value;
      */
-    private Object convertToTimestamp(String value) {
+    private Object convertToTimestamp(Column column, String value) {
+        if (ALL_ZERO_TIMESTAMP.equals(value) && column.isOptional()) return null;
         if (ALL_ZERO_TIMESTAMP.equals(value)) value = EPOCH_TIMESTAMP;
         return Timestamp.valueOf(value).toInstant().atZone(ZoneId.systemDefault());
     }",2018-06-11T08:01:22Z,18
"@@ -9,17 +9,19 @@
 import io.debezium.jdbc.JdbcConfiguration;
 import io.debezium.jdbc.JdbcConnection;
 
+import java.util.Map;
+
 /**
  * A utility for integration test cases to connect the MySQL server running in the Docker container created by this module's
  * build.
- * 
+ *
  * @author Randall Hauch
  */
 public class MySQLConnection extends JdbcConnection {
 
     /**
      * Obtain a connection instance to the named test database.
-     * 
+     *
      * @param databaseName the name of the test database
      * @return the MySQLConnection instance; never null
      */
@@ -33,7 +35,22 @@ public static MySQLConnection forTestDatabase(String databaseName) {
 
     /**
      * Obtain a connection instance to the named test database.
-     * 
+     * @param databaseName the name of the test database
+     * @param urlProperties url properties
+     * @return the MySQLConnection instance; never null
+     */
+    public static MySQLConnection forTestDatabase(String databaseName, Map<String, Object> urlProperties) {
+        JdbcConfiguration.Builder builder = JdbcConfiguration.copy(Configuration.fromSystemProperties(""database.""))
+                                                    .withDatabase(databaseName)
+                                                    .with(""useSSL"", false)
+                                                    .with(""characterEncoding"", ""utf8"");
+        urlProperties.forEach(builder::with);
+        return new MySQLConnection(builder.build());
+    }
+
+    /**
+     * Obtain a connection instance to the named test database.
+     *
      * @param databaseName the name of the test database
      * @param username the username
      * @param password the password
@@ -59,7 +76,7 @@ protected static void addDefaults(Configuration.Builder builder) {
 
     /**
      * Create a new instance with the given configuration and connection factory.
-     * 
+     *
      * @param config the configuration; may not be null
      */
     public MySQLConnection(Configuration config) {
@@ -69,7 +86,7 @@ public MySQLConnection(Configuration config) {
     /**
      * Create a new instance with the given configuration and connection factory, and specify the operations that should be
      * run against each newly-established connection.
-     * 
+     *
      * @param config the configuration; may not be null
      * @param initialOperations the initial operations that should be run on each new connection; may be null
      */",2018-06-11T08:01:22Z,36
"@@ -0,0 +1,103 @@
+/*
+ * Copyright Debezium Authors.
+ *
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import io.debezium.config.Configuration;
+import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.time.Timestamp;
+import io.debezium.time.ZonedTimestamp;
+import io.debezium.util.Testing;
+import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.source.SourceRecord;
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.nio.file.Path;
+import java.time.LocalDateTime;
+import java.time.ZoneId;
+import java.time.ZonedDateTime;
+import java.time.format.DateTimeFormatter;
+import java.util.Collections;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author luobo on 2018/6/8 14:16
+ */
+public class MysqlDefaultValueAllZeroTimeIT extends AbstractConnectorTest {
+
+    private static final Path DB_HISTORY_PATH = Testing.Files.createTestingPath(""file-db-history-connect.txt"").toAbsolutePath();
+    private final UniqueDatabase DATABASE = new UniqueDatabase(""myServer1"", ""default_value_all_zero_time"")
+            .withDbHistoryPath(DB_HISTORY_PATH);
+
+    private Configuration config;
+
+    @Before
+    public void beforeEach() {
+        stopConnector();
+        DATABASE.createAndInitialize(Collections.singletonMap(""sessionVariables"", ""sql_mode=''""));
+        initializeConnectorTestFramework();
+        Testing.Files.delete(DB_HISTORY_PATH);
+    }
+
+    @After
+    public void afterEach() {
+        try {
+            stopConnector();
+        } finally {
+            Testing.Files.delete(DB_HISTORY_PATH);
+        }
+    }
+
+    @Test
+    public void allZeroDateAndTimeTypeTest() throws InterruptedException {
+        config = DATABASE.defaultConfig()
+                .with(MySqlConnectorConfig.SNAPSHOT_MODE, MySqlConnectorConfig.SnapshotMode.INITIAL)
+                .with(MySqlConnectorConfig.TABLE_WHITELIST, DATABASE.qualifiedTableName(""ALL_ZERO_DATE_AND_TIME_TABLE""))
+                .build();
+        start(MySqlConnector.class, config);
+
+        Testing.Print.enable();
+
+        AbstractConnectorTest.SourceRecords records = consumeRecordsByTopic(7);
+        final SourceRecord record = records.recordsForTopic(DATABASE.topicForTable(""ALL_ZERO_DATE_AND_TIME_TABLE"")).get(0);
+
+        Schema schemaA = record.valueSchema().fields().get(1).schema().fields().get(0).schema();
+        Schema schemaB = record.valueSchema().fields().get(1).schema().fields().get(1).schema();
+        Schema schemaC = record.valueSchema().fields().get(1).schema().fields().get(2).schema();
+        Schema schemaD = record.valueSchema().fields().get(1).schema().fields().get(3).schema();
+        Schema schemaE = record.valueSchema().fields().get(1).schema().fields().get(4).schema();
+        Schema schemaF = record.valueSchema().fields().get(1).schema().fields().get(5).schema();
+
+        //column A, 0000-00-00 00:00:00 => 1970-01-01 00:00:00
+        String valueA = ""1970-01-01 00:00:00"";
+        ZonedDateTime a = java.sql.Timestamp.valueOf(valueA).toInstant().atZone(ZoneId.systemDefault());
+        String isoStringA = ZonedTimestamp.toIsoString(a, ZoneId.systemDefault(), MySqlValueConverters::adjustTemporal);
+        assertThat(schemaA.defaultValue()).isEqualTo(isoStringA);
+
+        //column B allows null, default value should be null
+        assertThat(schemaB.isOptional()).isEqualTo(true);
+        assertThat(schemaB.defaultValue()).isEqualTo(null);
+
+        //column C, 0000-00-00 => 1970-01-01
+        assertThat(schemaC.defaultValue()).isEqualTo(0);
+
+        //column D allows null, default value should be null
+        assertThat(schemaD.isOptional()).isEqualTo(true);
+        assertThat(schemaD.defaultValue()).isEqualTo(null);
+
+        //column E, 0000-00-00 00:00:00 => 1970-01-01 00:00:00
+        String valueE = ""1970-01-01 00:00:00"";
+        long toEpochMillisE = Timestamp.toEpochMillis(LocalDateTime.from(DateTimeFormatter.ofPattern(""yyyy-MM-dd HH:mm:ss"").parse(valueE)), MySqlValueConverters::adjustTemporal);
+        assertThat(schemaE.defaultValue()).isEqualTo(toEpochMillisE);
+
+        //column F allows null, default value should be null
+        assertThat(schemaF.isOptional()).isEqualTo(true);
+        assertThat(schemaF.defaultValue()).isEqualTo(null);
+
+    }
+}",2018-06-11T08:01:22Z,37
"@@ -12,7 +12,9 @@
 import java.nio.file.Path;
 import java.nio.file.Paths;
 import java.util.Arrays;
+import java.util.Collections;
 import java.util.List;
+import java.util.Map;
 import java.util.Random;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
@@ -110,11 +112,22 @@ protected String getServerName() {
      * See fnDbz162 procedure in reqression_test.sql for example of usage.
      */
     public void createAndInitialize() {
+        createAndInitialize(Collections.emptyMap());
+    }
+
+    /**
+     * Creates the database and populates it with initialization SQL script. To use multiline
+     * statements for stored procedures definition use delimiter $$ to delimit statements in the procedure.
+     * See fnDbz162 procedure in reqression_test.sql for example of usage.
+     *
+     * @param urlProperties jdbc url properties
+     */
+    public void createAndInitialize(Map<String, Object> urlProperties) {
         final String ddlFile = String.format(""ddl/%s.sql"", templateName);
         final URL ddlTestFile = UniqueDatabase.class.getClassLoader().getResource(ddlFile);
         assertNotNull(""Cannot locate "" + ddlFile, ddlTestFile);
         try {
-            try (MySQLConnection connection = MySQLConnection.forTestDatabase(DEFAULT_DATABASE)) {
+            try (MySQLConnection connection = MySQLConnection.forTestDatabase(DEFAULT_DATABASE, urlProperties)) {
                 final List<String> statements = Arrays.stream(
                         Stream.concat(
                                 Arrays.stream(CREATE_DATABASE_DDL),",2018-06-11T08:01:22Z,38
"@@ -0,0 +1,10 @@
+CREATE TABLE ALL_ZERO_DATE_AND_TIME_TABLE (
+  A TIMESTAMP NOT NULL DEFAULT '0000-00-00 00:00:00',
+  B TIMESTAMP NULL DEFAULT '0000-00-00 00:00:00',
+  C DATE NOT NULL DEFAULT '0000-00-00',
+  D DATE NULL DEFAULT '0000-00-00',
+  E DATETIME NOT NULL DEFAULT '0000-00-00 00:00:00',
+  F DATETIME NULL DEFAULT '0000-00-00 00:00:00'
+);
+INSERT INTO ALL_ZERO_DATE_AND_TIME_TABLE
+VALUES (DEFAULT, DEFAULT, DEFAULT, DEFAULT, DEFAULT, DEFAULT );",2018-06-11T08:01:22Z,39
"@@ -844,6 +844,7 @@ public static final Field MASK_COLUMN(int length) {
                                                                 KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS,
                                                                 KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS,
                                                                 DatabaseHistory.SKIP_UNPARSEABLE_DDL_STATEMENTS,
+                                                                DatabaseHistory.STORE_ONLY_MONITORED_TABLES_DDL,
                                                                 DatabaseHistory.DDL_FILTER);
 
     protected static ConfigDef configDef() {
@@ -853,7 +854,8 @@ protected static ConfigDef configDef() {
         Field.group(config, ""History Storage"", KafkaDatabaseHistory.BOOTSTRAP_SERVERS,
                     KafkaDatabaseHistory.TOPIC, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS,
                     KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS, DATABASE_HISTORY,
-                    DatabaseHistory.SKIP_UNPARSEABLE_DDL_STATEMENTS, DatabaseHistory.DDL_FILTER);
+                    DatabaseHistory.SKIP_UNPARSEABLE_DDL_STATEMENTS, DatabaseHistory.DDL_FILTER,
+                    DatabaseHistory.STORE_ONLY_MONITORED_TABLES_DDL);
         Field.group(config, ""Events"", INCLUDE_SCHEMA_CHANGES, TABLES_IGNORE_BUILTIN, DATABASE_WHITELIST, TABLE_WHITELIST,
                     COLUMN_BLACKLIST, TABLE_BLACKLIST, DATABASE_BLACKLIST,
                     GTID_SOURCE_INCLUDES, GTID_SOURCE_EXCLUDES, GTID_SOURCE_FILTER_DML_EVENTS, BUFFER_SIZE_FOR_BINLOG_READER,",2018-01-24T13:43:57Z,40
"@@ -74,6 +74,8 @@ public class MySqlSchema {
     private Tables tables;
     private final boolean skipUnparseableDDL;
     private final boolean tableIdCaseInsensitive;
+    private final boolean storeOnlyMonitoredTablesDdl;
+
     /**
      * Create a schema component given the supplied {@link MySqlConnectorConfig MySQL connector configuration}.
      *
@@ -136,8 +138,8 @@ protected boolean isPositionAtOrBefore(Document recorded, Document desired) {
         this.dbHistory.configure(dbHistoryConfig, historyComparator); // validates
 
         this.skipUnparseableDDL = dbHistoryConfig.getBoolean(DatabaseHistory.SKIP_UNPARSEABLE_DDL_STATEMENTS);
-
         tableSchemaByTableId = new SchemasByTableId(tableIdCaseInsensitive);
+        this.storeOnlyMonitoredTablesDdl = dbHistoryConfig.getBoolean(DatabaseHistory.STORE_ONLY_MONITORED_TABLES_DDL);
     }
 
     protected HistoryRecordComparator historyComparator() {
@@ -254,13 +256,13 @@ public void loadHistory(SourceInfo startingPoint) {
         dbHistory.recover(startingPoint.partition(), startingPoint.offset(), tables, ddlParser);
         refreshSchemas();
     }
-    
+
     /**
      * Return true if the database history entity exists
      */
     public boolean historyExists() {
         return dbHistory.exists();
-    }    
+    }
 
     /**
      * Discard any currently-cached schemas and rebuild them using the filters.
@@ -293,6 +295,7 @@ protected void refreshSchemas() {
      */
     public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatements,
                             DatabaseStatementStringConsumer statementConsumer) {
+        Set<TableId> changes;
         if (ignoredQueryStatements.contains(ddlStatements)) return false;
         try {
             this.ddlChanges.reset();
@@ -305,45 +308,55 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
                 throw e;
             }
         } finally {
-            if (statementConsumer != null) {
-
-                // We are supposed to _also_ record the schema changes as SourceRecords, but these need to be filtered
-                // by database. Unfortunately, the databaseName on the event might not be the same database as that
-                // being modified by the DDL statements (since the DDL statements can have fully-qualified names).
-                // Therefore, we have to look at each statement to figure out which database it applies and then
-                // record the DDL statements (still in the same order) to those databases.
-
-                if (!ddlChanges.isEmpty() && ddlChanges.applyToMoreDatabasesThan(databaseName)) {
-
-                    // We understood at least some of the DDL statements and can figure out to which database they apply.
-                    // They also apply to more databases than 'databaseName', so we need to apply the DDL statements in
-                    // the same order they were read for each _affected_ database, grouped together if multiple apply
-                    // to the same _affected_ database...
-                    ddlChanges.groupStatementStringsByDatabase((dbName, ddl) -> {
-                        if (filters.databaseFilter().test(dbName) || dbName == null || """".equals(dbName)) {
-                            if (dbName == null) dbName = """";
-                            statementConsumer.consume(dbName, ddlStatements);
-                        }
-                    });
-                } else if (filters.databaseFilter().test(databaseName) || databaseName == null || """".equals(databaseName)) {
-                    if (databaseName == null) databaseName = """";
-                    statementConsumer.consume(databaseName, ddlStatements);
+            changes = tables.drainChanges();
+            // No need to send schema events or store DDL if no table has changed
+            // Note that, unlike with the DB history topic, we don't filter out non-whitelisted tables here
+            // (which writes to the public schema change topic); if required, a second option could be added
+            // for controlling this, too
+            if (!storeOnlyMonitoredTablesDdl || !changes.isEmpty()) {
+                if (statementConsumer != null) {
+
+                    // We are supposed to _also_ record the schema changes as SourceRecords, but these need to be filtered
+                    // by database. Unfortunately, the databaseName on the event might not be the same database as that
+                    // being modified by the DDL statements (since the DDL statements can have fully-qualified names).
+                    // Therefore, we have to look at each statement to figure out which database it applies and then
+                    // record the DDL statements (still in the same order) to those databases.
+
+                    if (!ddlChanges.isEmpty() && ddlChanges.applyToMoreDatabasesThan(databaseName)) {
+
+                        // We understood at least some of the DDL statements and can figure out to which database they apply.
+                        // They also apply to more databases than 'databaseName', so we need to apply the DDL statements in
+                        // the same order they were read for each _affected_ database, grouped together if multiple apply
+                        // to the same _affected_ database...
+                        ddlChanges.groupStatementStringsByDatabase((dbName, ddl) -> {
+                            if (filters.databaseFilter().test(dbName) || dbName == null || """".equals(dbName)) {
+                                if (dbName == null) dbName = """";
+                                statementConsumer.consume(dbName, ddlStatements);
+                            }
+                        });
+                    } else if (filters.databaseFilter().test(databaseName) || databaseName == null || """".equals(databaseName)) {
+                        if (databaseName == null) databaseName = """";
+                        statementConsumer.consume(databaseName, ddlStatements);
+                    }
                 }
-            }
 
-            // Record the DDL statement so that we can later recover them if needed. We do this _after_ writing the
-            // schema change records so that failure recovery (which is based on of the history) won't lose
-            // schema change records.
-            try {
-                dbHistory.record(source.partition(), source.offset(), databaseName, tables, ddlStatements);
-            } catch (Throwable e) {
-                throw new ConnectException(
-                        ""Error recording the DDL statement(s) in the database history "" + dbHistory + "": "" + ddlStatements, e);
+                // Record the DDL statement so that we can later recover them if needed. We do this _after_ writing the
+                // schema change records so that failure recovery (which is based on of the history) won't lose
+                // schema change records.
+                try {
+                    if (!storeOnlyMonitoredTablesDdl || changes.stream().anyMatch(filters().tableFilter()::test)) {
+                        dbHistory.record(source.partition(), source.offset(), databaseName, tables, ddlStatements);
+                    } else {
+                        logger.debug(""Changes for DDL '{}' were filtered and not recorded in database history"", ddlStatements);
+                    }
+                } catch (Throwable e) {
+                    throw new ConnectException(
+                            ""Error recording the DDL statement(s) in the database history "" + dbHistory + "": "" + ddlStatements, e);
+                }
             }
         }
 
         // Figure out what changed ...
-        Set<TableId> changes = tables.drainChanges();
         changes.forEach(tableId -> {
             Table table = tables.forTable(tableId);
             if (table == null) { // removed",2018-01-24T13:43:57Z,19
"@@ -45,6 +45,16 @@ public interface DatabaseHistory {
                     + ""which it cannot parse. If skipping is enabled then Debezium can miss metadata changes."")
             .withDefault(false);
 
+    public static final Field STORE_ONLY_MONITORED_TABLES_DDL = Field.create(CONFIGURATION_FIELD_PREFIX_STRING + ""store.only.monitored.tables.ddl"")
+            .withDisplayName(""Store only DDL that modifies whitelisted/not-blacklisted tables"")
+            .withType(Type.BOOLEAN)
+            .withWidth(Width.SHORT)
+            .withImportance(Importance.LOW)
+            .withDescription(""Controls what DDL will Debezium store in database history.""
+                    + ""By default (false) Debezium will store all incoming DDL statements. If set to true""
+                    + ""then only DDL that manipulates a monitored table will be stored."")
+            .withDefault(false);
+
     public static final Field DDL_FILTER = Field.create(CONFIGURATION_FIELD_PREFIX_STRING + ""ddl.filter"")
                                                 .withDisplayName(""DDL filter"")
                                                 .withType(Type.STRING)",2018-01-24T13:43:57Z,41
"@@ -6,10 +6,12 @@
 
 package io.debezium.connector.postgresql;
 
+import java.math.BigDecimal;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
 import java.util.function.Consumer;
 
+import io.debezium.jdbc.JdbcValueConverters.DecimalMode;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.common.config.ConfigDef.Importance;
 import org.apache.kafka.common.config.ConfigDef.Type;
@@ -86,6 +88,72 @@ public static TemporalPrecisionMode parse(String value, String defaultValue) {
         }
     }
 
+    /**
+     * The set of predefined DecimalHandlingMode options or aliases.
+     */
+    public static enum DecimalHandlingMode implements EnumeratedValue {
+        /**
+         * Represent {@code DECIMAL} and {@code NUMERIC} values as precise {@link BigDecimal} values, which are
+         * represented in change events in a binary form. This is precise but difficult to use.
+         */
+        PRECISE(""precise""),
+
+        /**
+         * Represent {@code DECIMAL} and {@code NUMERIC} values as precise {@code double} values. This may be less precise
+         * but is far easier to use.
+         */
+        DOUBLE(""double"");
+
+        private final String value;
+
+        private DecimalHandlingMode(String value) {
+            this.value = value;
+        }
+
+        @Override
+        public String getValue() {
+            return value;
+        }
+
+        public DecimalMode asDecimalMode() {
+            switch (this) {
+                case DOUBLE:
+                    return DecimalMode.DOUBLE;
+                case PRECISE:
+                default:
+                    return DecimalMode.PRECISE;
+            }
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         *
+         * @param value the configuration property value; may not be null
+         * @return the matching option, or null if no match is found
+         */
+        public static DecimalHandlingMode parse(String value) {
+            if (value == null) return null;
+            value = value.trim();
+            for (DecimalHandlingMode option : DecimalHandlingMode.values()) {
+                if (option.getValue().equalsIgnoreCase(value)) return option;
+            }
+            return null;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         *
+         * @param value the configuration property value; may not be null
+         * @param defaultValue the default value; may be null
+         * @return the matching option, or null if no match is found and the non-null default is invalid
+         */
+        public static DecimalHandlingMode parse(String value, String defaultValue) {
+            DecimalHandlingMode mode = parse(value);
+            if (mode == null && defaultValue != null) mode = parse(defaultValue);
+            return mode;
+        }
+    }
+
     /**
      * The set of predefined SnapshotMode options or aliases.
      */
@@ -552,6 +620,15 @@ public static TopicSelectionStrategy parse(String value) {
                                                                  + ""'connect' always represents time, date, and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, ""
                                                                  + ""which uses millisecond precision regardless of the database columns' precision ."");
 
+    public static final Field DECIMAL_HANDLING_MODE = Field.create(""decimal.handling.mode"")
+                                                        .withDisplayName(""Decimal Handling"")
+                                                        .withEnum(DecimalHandlingMode.class, DecimalHandlingMode.PRECISE)
+                                                        .withWidth(Width.SHORT)
+                                                        .withImportance(Importance.MEDIUM)
+                                                        .withDescription(""Specify how DECIMAL and NUMERIC columns should be represented in change events, including:""
+                                                                + ""'precise' (the default) uses java.math.BigDecimal to represent values, which are encoded in the change events using a binary representation and Kafka Connect's 'org.apache.kafka.connect.data.Decimal' type; ""
+                                                                + ""'double' represents values using Java's 'double', which may not offer the precision but will be far easier to use in consumers."");
+
     public static final Field STATUS_UPDATE_INTERVAL_MS = Field.create(""status.update.interval.ms"")
             .withDisplayName(""Status update interval (ms)"")
             .withType(Type.INT) // Postgres doesn't accept long for this value
@@ -577,14 +654,15 @@ public static TopicSelectionStrategy parse(String value) {
                                                      MAX_QUEUE_SIZE, POLL_INTERVAL_MS, SCHEMA_WHITELIST,
                                                      SCHEMA_BLACKLIST, TABLE_WHITELIST, TABLE_BLACKLIST,
                                                      COLUMN_BLACKLIST, SNAPSHOT_MODE,
-                                                     TIME_PRECISION_MODE,
+                                                     TIME_PRECISION_MODE, DECIMAL_HANDLING_MODE,
                                                      SSL_MODE, SSL_CLIENT_CERT, SSL_CLIENT_KEY_PASSWORD,
                                                      SSL_ROOT_CERT, SSL_CLIENT_KEY, SNAPSHOT_LOCK_TIMEOUT_MS, ROWS_FETCH_SIZE, SSL_SOCKET_FACTORY,
                                                      STATUS_UPDATE_INTERVAL_MS, TCP_KEEPALIVE);
 
     private final Configuration config;
     private final String serverName;
     private final boolean adaptiveTimePrecision;
+    private final DecimalMode decimalHandlingMode;
     private final SnapshotMode snapshotMode;
 
     protected PostgresConnectorConfig(Configuration config) {
@@ -596,6 +674,9 @@ protected PostgresConnectorConfig(Configuration config) {
         this.serverName = serverName;
         TemporalPrecisionMode timePrecisionMode = TemporalPrecisionMode.parse(config.getString(TIME_PRECISION_MODE));
         this.adaptiveTimePrecision = TemporalPrecisionMode.ADAPTIVE == timePrecisionMode;
+        String decimalHandlingModeStr = config.getString(PostgresConnectorConfig.DECIMAL_HANDLING_MODE);
+        DecimalHandlingMode decimalHandlingMode = DecimalHandlingMode.parse(decimalHandlingModeStr);
+        this.decimalHandlingMode = decimalHandlingMode.asDecimalMode();
         this.snapshotMode = SnapshotMode.parse(config.getString(SNAPSHOT_MODE));
     }
 
@@ -643,6 +724,10 @@ protected boolean adaptiveTimePrecision() {
         return adaptiveTimePrecision;
     }
 
+    protected DecimalMode decimalHandlingMode() {
+        return decimalHandlingMode;
+    }
+
     protected Configuration jdbcConfig() {
         return config.subset(DATABASE_CONFIG_PREFIX, true);
     }
@@ -713,7 +798,7 @@ protected static ConfigDef configDef() {
         Field.group(config, ""Events"", SCHEMA_WHITELIST, SCHEMA_BLACKLIST, TABLE_WHITELIST, TABLE_BLACKLIST,
                     COLUMN_BLACKLIST);
         Field.group(config, ""Connector"", TOPIC_SELECTION_STRATEGY, POLL_INTERVAL_MS, MAX_BATCH_SIZE, MAX_QUEUE_SIZE,
-                    SNAPSHOT_MODE, SNAPSHOT_LOCK_TIMEOUT_MS, TIME_PRECISION_MODE, ROWS_FETCH_SIZE);
+                    SNAPSHOT_MODE, SNAPSHOT_LOCK_TIMEOUT_MS, TIME_PRECISION_MODE, DECIMAL_HANDLING_MODE, ROWS_FETCH_SIZE);
         return config;
     }
 ",2017-08-17T07:55:50Z,42
"@@ -61,7 +61,8 @@ protected PostgresSchema(PostgresConnectorConfig config) {
         this.filters = new Filters(config);
         this.tables = new Tables();
 
-        PostgresValueConverter valueConverter = new PostgresValueConverter(config.adaptiveTimePrecision(), ZoneOffset.UTC);
+        PostgresValueConverter valueConverter = new PostgresValueConverter(config.decimalHandlingMode(), config.adaptiveTimePrecision(),
+                ZoneOffset.UTC);
         this.schemaNameValidator = AvroValidator.create(LOGGER)::validate;
         this.schemaBuilder = new TableSchemaBuilder(valueConverter, this.schemaNameValidator);
 ",2017-08-17T07:55:50Z,43
"@@ -54,8 +54,8 @@ public class PostgresValueConverter extends JdbcValueConverters {
      */
     protected static final double DAYS_PER_MONTH_AVG = 365.25 / 12.0d;
 
-    protected PostgresValueConverter(boolean adaptiveTimePrecision, ZoneOffset defaultOffset) {
-        super(DecimalMode.PRECISE, adaptiveTimePrecision, defaultOffset, null);
+    protected PostgresValueConverter(DecimalMode decimalMode, boolean adaptiveTimePrecision, ZoneOffset defaultOffset) {
+        super(decimalMode, adaptiveTimePrecision, defaultOffset, null);
     }
 
     @Override
@@ -167,7 +167,12 @@ public ValueConverter converter(Column column, Field fieldDefn) {
             case PgOid.MONEY:
                 return data -> convertMoney(column, fieldDefn, data);
             case PgOid.NUMERIC:
-                return data -> convertDecimal(column, fieldDefn, data);
+                switch (decimalMode) {
+                case DOUBLE:
+                    return (data) -> convertDouble(column, fieldDefn, data);
+                case PRECISE:
+                    return (data) -> convertDecimal(column, fieldDefn, data);
+                }
             case PgOid.INT2_ARRAY:
             case PgOid.INT4_ARRAY:
             case PgOid.INT8_ARRAY:",2017-08-17T07:55:50Z,44
"@@ -145,6 +145,7 @@ public void shouldValidateConfiguration() throws Exception {
         validateField(validatedConfig, PostgresConnectorConfig.SNAPSHOT_MODE, INITIAL);
         validateField(validatedConfig, PostgresConnectorConfig.SNAPSHOT_LOCK_TIMEOUT_MS, PostgresConnectorConfig.DEFAULT_SNAPSHOT_LOCK_TIMEOUT_MILLIS);
         validateField(validatedConfig, PostgresConnectorConfig.TIME_PRECISION_MODE, PostgresConnectorConfig.TemporalPrecisionMode.ADAPTIVE);
+        validateField(validatedConfig, PostgresConnectorConfig.DECIMAL_HANDLING_MODE, PostgresConnectorConfig.DecimalHandlingMode.PRECISE);
         validateField(validatedConfig, PostgresConnectorConfig.SSL_SOCKET_FACTORY, null);
         validateField(validatedConfig, PostgresConnectorConfig.TCP_KEEPALIVE, null);
    }",2017-08-17T07:55:50Z,45
"@@ -16,6 +16,7 @@
 import java.util.List;
 import java.util.concurrent.TimeUnit;
 
+import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaBuilder;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.junit.After;
@@ -305,6 +306,26 @@ record = consumer.remove();
         VerifyRecord.isValidTombstone(record, PK_FIELD, 2);
     }
 
+    @Test
+    public void shouldReceiveNumericTypeAsDouble() throws Exception {
+        PostgresConnectorConfig config = new PostgresConnectorConfig(TestHelper.defaultConfig()
+                .with(PostgresConnectorConfig.DECIMAL_HANDLING_MODE, PostgresConnectorConfig.DecimalHandlingMode.DOUBLE)
+                .build());
+        PostgresTaskContext context = new PostgresTaskContext(config, new PostgresSchema(config));
+        recordsProducer = new RecordsStreamProducer(context, new SourceInfo(config.serverName()));
+
+        TestHelper.executeDDL(""postgres_create_tables.ddl"");
+
+        consumer = testConsumer(1);
+        recordsProducer.start(consumer);
+
+        List<SchemaAndValueField> schemasAndValuesForNumericType = schemasAndValuesForNumericType();
+        schemasAndValuesForNumericType.set(3, new SchemaAndValueField(""d"", Schema.OPTIONAL_FLOAT64_SCHEMA, 1.1d));
+        schemasAndValuesForNumericType.set(4, new SchemaAndValueField(""n"", Schema.OPTIONAL_FLOAT64_SCHEMA, 22.22d));
+
+        assertInsert(INSERT_NUMERIC_TYPES_STMT, schemasAndValuesForNumericType);
+    }
+
     private void assertInsert(String statement, List<SchemaAndValueField> expectedSchemaAndValuesByColumn) {
         TableId table = tableIdFromInsertStmt(statement);
         String expectedTopicName = table.schema() + ""."" + table.table();",2017-08-17T07:55:50Z,46
"@@ -28,6 +28,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
 
+import com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer;
 import com.github.shyiko.mysql.binlog.event.deserialization.json.JsonBinary;
 import com.mysql.jdbc.CharsetMapping;
 
@@ -44,32 +45,29 @@
 /**
  * MySQL-specific customization of the conversions from JDBC values obtained from the MySQL binlog client library.
  * <p>
- * This class always uses UTC for the default time zone when converting values without timezone information to values
- * that require
+ * This class always uses UTC for the default time zone when converting values without timezone information to values that require
  * timezones. This is because MySQL {@code TIMESTAMP} values are always
  * <a href=""https://dev.mysql.com/doc/refman/5.7/en/datetime.html"">stored in UTC</a> (unlike {@code DATETIME} values) and
  * are replicated in this form. Meanwhile, the MySQL Binlog Client library will {@link AbstractRowsEventDataDeserializer
  * deserialize} these as {@link java.sql.Timestamp} values that have no timezone and, therefore, are presumed to be in UTC.
  * When the column is properly marked with a {@link Types#TIMESTAMP_WITH_TIMEZONE} type, the converters will need to convert
  * that {@link java.sql.Timestamp} value into an {@link OffsetDateTime} using the default time zone, which always is UTC.
- * 
+ *
  * @author Randall Hauch
  * @see com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer
  */
 @Immutable
 public class MySqlValueConverters extends JdbcValueConverters {
 
     /**
-     * A utility method that adjusts <a href=""https://dev.mysql.com/doc/refman/5.7/en/two-digit-years.html"">ambiguous</a>
-     * 2-digit
+     * A utility method that adjusts <a href=""https://dev.mysql.com/doc/refman/5.7/en/two-digit-years.html"">ambiguous</a> 2-digit
      * year values of DATETIME, DATE, and TIMESTAMP types using these MySQL-specific rules:
      * <ul>
      * <li>Year values in the range 00-69 are converted to 2000-2069.</li>
      * <li>Year values in the range 70-99 are converted to 1970-1999.</li>
      * </ul>
      *
      * @param temporal the temporal instance to adjust; may not be null
-     *
      * @return the possibly adjusted temporal instance; never null
      */
     protected static Temporal adjustTemporal(Temporal temporal) {
@@ -85,21 +83,17 @@ protected static Temporal adjustTemporal(Temporal temporal) {
     }
 
     /**
-     * A utility method that adjusts <a href=""https://dev.mysql.com/doc/refman/5.7/en/two-digit-years.html"">ambiguous</a>
-     * 2-digit
+     * A utility method that adjusts <a href=""https://dev.mysql.com/doc/refman/5.7/en/two-digit-years.html"">ambiguous</a> 2-digit
      * year values of YEAR type using these MySQL-specific rules:
      * <ul>
      * <li>Year values in the range 01-69 are converted to 2001-2069.</li>
      * <li>Year values in the range 70-99 are converted to 1970-1999.</li>
      * </ul>
-     * MySQL treats YEAR(4) the same, except that a numeric 00 inserted into YEAR(4) results in 0000 rather than 2000;
-     * to
-     * specify zero for YEAR(4) and have it be interpreted as 2000, specify it as a string '0' or '00'. This should be
-     * handled
+     * MySQL treats YEAR(4) the same, except that a numeric 00 inserted into YEAR(4) results in 0000 rather than 2000; to
+     * specify zero for YEAR(4) and have it be interpreted as 2000, specify it as a string '0' or '00'. This should be handled
      * by MySQL before Debezium sees the value.
      *
      * @param year the year value to adjust; may not be null
-     *
      * @return the possibly adjusted year number; never null
      */
     protected static int adjustYear(int year) {
@@ -112,11 +106,10 @@ protected static int adjustYear(int year) {
     }
 
     /**
-     * Create a new instance that always uses UTC for the default time zone when converting values without timezone
-     * information
+     * Create a new instance that always uses UTC for the default time zone when converting values without timezone information
      * to values that require timezones.
      * <p>
-     * 
+     *
      * @param decimalMode how {@code DECIMAL} and {@code NUMERIC} values should be treated; may be null if
      *            {@link io.debezium.jdbc.JdbcValueConverters.DecimalMode#PRECISE} is to be used
      * @param adaptiveTimePrecision {@code true} if the time, date, and timestamp values should be based upon the precision of the
@@ -128,12 +121,10 @@ public MySqlValueConverters(DecimalMode decimalMode, boolean adaptiveTimePrecisi
     }
 
     /**
-     * Create a new instance, and specify the time zone offset that should be used only when converting values without
-     * timezone
-     * information to values that require timezones. This default offset should not be needed when values are
-     * highly-correlated
+     * Create a new instance, and specify the time zone offset that should be used only when converting values without timezone
+     * information to values that require timezones. This default offset should not be needed when values are highly-correlated
      * with the expected SQL/JDBC types.
-     * 
+     *
      * @param decimalMode how {@code DECIMAL} and {@code NUMERIC} values should be treated; may be null if
      *            {@link io.debezium.jdbc.JdbcValueConverters.DecimalMode#PRECISE} is to be used
      * @param adaptiveTimePrecision {@code true} if the time, date, and timestamp values should be based upon the precision of the
@@ -237,25 +228,25 @@ public ValueConverter converter(Column column, Field fieldDefn) {
 
         // We have to convert bytes encoded in the column's character set ...
         switch (column.jdbcType()) {
-        case Types.CHAR: // variable-length
-        case Types.VARCHAR: // variable-length
-        case Types.LONGVARCHAR: // variable-length
-        case Types.CLOB: // variable-length
-        case Types.NCHAR: // fixed-length
-        case Types.NVARCHAR: // fixed-length
-        case Types.LONGNVARCHAR: // fixed-length
-        case Types.NCLOB: // fixed-length
-        case Types.DATALINK:
-        case Types.SQLXML:
-            Charset charset = charsetFor(column);
-            if (charset != null) {
-                logger.debug(""Using {} charset by default for column: {}"", charset, column);
-                return (data) -> convertString(column, fieldDefn, charset, data);
-            }
-            logger.warn(""Using UTF-8 charset by default for column without charset: {}"", column);
-            return (data) -> convertString(column, fieldDefn, StandardCharsets.UTF_8, data);
-        default:
-            break;
+            case Types.CHAR: // variable-length
+            case Types.VARCHAR: // variable-length
+            case Types.LONGVARCHAR: // variable-length
+            case Types.CLOB: // variable-length
+            case Types.NCHAR: // fixed-length
+            case Types.NVARCHAR: // fixed-length
+            case Types.LONGNVARCHAR: // fixed-length
+            case Types.NCLOB: // fixed-length
+            case Types.DATALINK:
+            case Types.SQLXML:
+                Charset charset = charsetFor(column);
+                if (charset != null) {
+                    logger.debug(""Using {} charset by default for column: {}"", charset, column);
+                    return (data) -> convertString(column, fieldDefn, charset, data);
+                }
+                logger.warn(""Using UTF-8 charset by default for column without charset: {}"", column);
+                return (data) -> convertString(column, fieldDefn, StandardCharsets.UTF_8, data);
+            default:
+                break;
         }
 
         // Otherwise, let the base class handle it ...
@@ -266,7 +257,6 @@ public ValueConverter converter(Column column, Field fieldDefn) {
      * Return the {@link Charset} instance with the MySQL-specific character set name used by the given column.
      *
      * @param column the column in which the character set is used; never null
-     *
      * @return the Java {@link Charset}, or null if there is no mapping
      */
     protected Charset charsetFor(Column column) {
@@ -277,40 +267,32 @@ protected Charset charsetFor(Column column) {
         }
         String encoding = CharsetMapping.getJavaEncodingForMysqlCharset(mySqlCharsetName);
         if (encoding == null) {
-            logger.warn(""Column uses MySQL character set '{}', which has no mapping to a Java character set"",
-                    mySqlCharsetName
-            );
+            logger.warn(""Column uses MySQL character set '{}', which has no mapping to a Java character set"", mySqlCharsetName);
         } else {
             try {
                 return Charset.forName(encoding);
             } catch (IllegalCharsetNameException e) {
-                logger.error(""Unable to load Java charset '{}' for column with MySQL character set '{}'"",
-                        encoding,
-                        mySqlCharsetName
-                );
+                logger.error(""Unable to load Java charset '{}' for column with MySQL character set '{}'"", encoding, mySqlCharsetName);
             }
         }
         return null;
     }
 
     /**
      * Convert the {@link String} {@code byte[]} value to a string value used in a {@link SourceRecord}.
-     * 
+     *
      * @param column the column in which the value appears
      * @param fieldDefn the field definition for the {@link SourceRecord}'s {@link Schema}; never null
-     * @param data      the data; may be null
-     *
+     * @param data the data; may be null
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
-     *
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
     protected Object convertJson(Column column, Field fieldDefn, Object data) {
         if (data == null) {
             data = fieldDefn.schema().defaultValue();
         }
         if (data == null) {
-            if (column.isOptional())
-                return null;
+            if (column.isOptional()) return null;
             return ""{}"";
         }
         if (data instanceof byte[]) {
@@ -320,9 +302,7 @@ protected Object convertJson(Column column, Field fieldDefn, Object data) {
                 String json = JsonBinary.parseAsString((byte[]) data);
                 return json;
             } catch (IOException e) {
-                throw new ConnectException(""Failed to parse and read a JSON value on "" + column + "": "" + e.getMessage(),
-                        e
-                );
+                throw new ConnectException(""Failed to parse and read a JSON value on "" + column + "": "" + e.getMessage(), e);
             }
         }
         if (data instanceof String) {
@@ -334,23 +314,20 @@ protected Object convertJson(Column column, Field fieldDefn, Object data) {
 
     /**
      * Convert the {@link String} or {@code byte[]} value to a string value used in a {@link SourceRecord}.
-     * 
+     *
      * @param column the column in which the value appears
      * @param fieldDefn the field definition for the {@link SourceRecord}'s {@link Schema}; never null
      * @param columnCharset the Java character set in which column byte[] values are encoded; may not be null
-     * @param data          the data; may be null
-     *
+     * @param data the data; may be null
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
-     *
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
     protected Object convertString(Column column, Field fieldDefn, Charset columnCharset, Object data) {
         if (data == null) {
             data = fieldDefn.schema().defaultValue();
         }
         if (data == null) {
-            if (column.isOptional())
-                return null;
+            if (column.isOptional()) return null;
             return """";
         }
         if (data instanceof byte[]) {
@@ -366,13 +343,11 @@ protected Object convertString(Column column, Field fieldDefn, Charset columnCha
     /**
      * Converts a value object for a MySQL {@code YEAR}, which appear in the binlog as an integer though returns from
      * the MySQL JDBC driver as either a short or a {@link java.sql.Date}.
-     * 
+     *
      * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
-     * @param data      the data object to be converted into a year literal integer value; never null
-     *
+     * @param data the data object to be converted into a year literal integer value; never null
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
-     *
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
     @SuppressWarnings(""deprecation"")
@@ -381,8 +356,7 @@ protected Object convertYearToInt(Column column, Field fieldDefn, Object data) {
             data = fieldDefn.schema().defaultValue();
         }
         if (data == null) {
-            if (column.isOptional())
-                return null;
+            if (column.isOptional()) return null;
             return 0;
         }
         if (data instanceof java.time.Year) {
@@ -401,27 +375,23 @@ protected Object convertYearToInt(Column column, Field fieldDefn, Object data) {
     }
 
     /**
-     * Converts a value object for a MySQL {@code ENUM}, which is represented in the binlog events as an integer value
-     * containing
+     * Converts a value object for a MySQL {@code ENUM}, which is represented in the binlog events as an integer value containing
      * the index of the enum option. The MySQL JDBC driver returns a string containing the option,
      * so this method calculates the same.
-     * 
+     *
      * @param options the characters that appear in the same order as defined in the column; may not be null
      * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
-     * @param data      the data object to be converted into an {@code ENUM} literal String value
-     *
+     * @param data the data object to be converted into an {@code ENUM} literal String value
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
-     *
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
     protected Object convertEnumToString(List<String> options, Column column, Field fieldDefn, Object data) {
         if (data == null) {
             data = fieldDefn.schema().defaultValue();
         }
         if (data == null) {
-            if (column.isOptional())
-                return null;
+            if (column.isOptional()) return null;
             return """";
         }
         if (data instanceof String) {
@@ -448,28 +418,23 @@ protected Object convertEnumToString(List<String> options, Column column, Field
     }
 
     /**
-     * Converts a value object for a MySQL {@code SET}, which is represented in the binlog events contain a long number
-     * in which
-     * every bit corresponds to a different option. The MySQL JDBC driver returns a string containing the
-     * comma-separated options,
+     * Converts a value object for a MySQL {@code SET}, which is represented in the binlog events contain a long number in which
+     * every bit corresponds to a different option. The MySQL JDBC driver returns a string containing the comma-separated options,
      * so this method calculates the same.
-     * 
+     *
      * @param options the characters that appear in the same order as defined in the column; may not be null
      * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
-     * @param data      the data object to be converted into an {@code SET} literal String value; never null
-     *
+     * @param data the data object to be converted into an {@code SET} literal String value; never null
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
-     *
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
     protected Object convertSetToString(List<String> options, Column column, Field fieldDefn, Object data) {
         if (data == null) {
             data = fieldDefn.schema().defaultValue();
         }
         if (data == null) {
-            if (column.isOptional())
-                return null;
+            if (column.isOptional()) return null;
             return """";
         }
         if (data instanceof String) {
@@ -484,6 +449,92 @@ protected Object convertSetToString(List<String> options, Column column, Field f
         return handleUnknownData(column, fieldDefn, data);
     }
 
+    /**
+     * Determine if the uppercase form of a column's type exactly matches or begins with the specified prefix.
+     * Note that this logic works when the column's {@link Column#typeName() type} contains the type name followed by parentheses.
+     *
+     * @param upperCaseTypeName the upper case form of the column's {@link Column#typeName() type name}
+     * @param upperCaseMatch the upper case form of the expected type or prefix of the type; may not be null
+     * @return {@code true} if the type matches the specified type, or {@code false} otherwise
+     */
+    protected boolean matches(String upperCaseTypeName, String upperCaseMatch) {
+        if (upperCaseTypeName == null) return false;
+        return upperCaseMatch.equals(upperCaseTypeName) || upperCaseTypeName.startsWith(upperCaseMatch + ""("");
+    }
+
+    protected List<String> extractEnumAndSetOptions(Column column) {
+        return MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
+    }
+
+    protected String extractEnumAndSetOptionsAsString(Column column) {
+        return Strings.join("","", extractEnumAndSetOptions(column));
+    }
+
+    protected String convertSetValue(Column column, long indexes, List<String> options) {
+        StringBuilder sb = new StringBuilder();
+        int index = 0;
+        boolean first = true;
+        int optionLen = options.size();
+        while (indexes != 0L) {
+            if (indexes % 2L != 0) {
+                if (first) {
+                    first = false;
+                } else {
+                    sb.append(',');
+                }
+                if (index < optionLen) {
+                    sb.append(options.get(index));
+                } else {
+                    logger.warn(""Found unexpected index '{}' on column {}"", index, column);
+                }
+            }
+            ++index;
+            indexes = indexes >>> 1;
+        }
+        return sb.toString();
+
+    }
+
+    /**
+     * Convert the a value representing a POINT {@code byte[]} value to a Point value used in a {@link SourceRecord}.
+     *
+     * @param column the column in which the value appears
+     * @param fieldDefn the field definition for the {@link SourceRecord}'s {@link Schema}; never null
+     * @param data the data; may be null
+     * @return the converted value, or null if the conversion could not be made and the column allows nulls
+     * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
+     */
+    protected Object convertPoint(Column column, Field fieldDefn, Object data){
+        if (data == null) {
+            data = fieldDefn.schema().defaultValue();
+        }
+
+        Schema schema = fieldDefn.schema();
+
+        if (data instanceof byte[]) {
+            // The binlog utility sends a byte array for any Geometry type, we will use our own binaryParse to parse the byte to WKB, hence
+            // to the suitable class
+            try {
+                MySqlGeometry mySqlGeometry = MySqlGeometry.fromBytes((byte[]) data);
+                Point point = mySqlGeometry.getPoint();
+                return io.debezium.data.geometry.Point.createValue(schema, point.getX(), point.getY(), mySqlGeometry.getWkb());
+            } catch (WkbException e) {
+                throw new ConnectException(""Failed to parse and read a value of type POINT on "" + column + "": "" + e.getMessage(), e);
+            }
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    @Override
+    protected ByteBuffer convertByteArray(Column column, byte[] data) {
+        // DBZ-254 right-pad fixed-length binary column values with 0x00 (zero byte)
+        if (column.jdbcType() == Types.BINARY && data.length < column.length()) {
+            data = Arrays.copyOf(data, column.length());
+        }
+
+        return super.convertByteArray(column, data);
+    }
+
     /**
      * Convert the a value representing a Unsigned TINYINT value to the correct Unsigned TINYINT representation.
      *
@@ -608,93 +659,4 @@ protected Object convertUnsignedBigint(Column column, Field fieldDefn, Object da
         //We continue with the original converting method (numeric) since we have an unsigned Integer
         return convertNumeric(column, fieldDefn, data);
     }
-
-
-    /**
-     * Determine if the uppercase form of a column's type exactly matches or begins with the specified prefix.
-     * Note that this logic works when the column's {@link Column#typeName() type} contains the type name followed by parentheses.
-     * 
-     * @param upperCaseTypeName the upper case form of the column's {@link Column#typeName() type name}
-     * @param upperCaseMatch    the upper case form of the expected type or prefix of the type; may not be null
-     *
-     * @return {@code true} if the type matches the specified type, or {@code false} otherwise
-     */
-    protected boolean matches(String upperCaseTypeName, String upperCaseMatch) {
-        if (upperCaseTypeName == null)
-            return false;
-        return upperCaseMatch.equals(upperCaseTypeName) || upperCaseTypeName.startsWith(upperCaseMatch + ""("");
-    }
-
-    protected List<String> extractEnumAndSetOptions(Column column) {
-        return MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
-    }
-
-    protected String extractEnumAndSetOptionsAsString(Column column) {
-        return Strings.join("","", extractEnumAndSetOptions(column));
-    }
-
-    protected String convertSetValue(Column column, long indexes, List<String> options) {
-        StringBuilder sb = new StringBuilder();
-        int index = 0;
-        boolean first = true;
-        int optionLen = options.size();
-        while (indexes != 0L) {
-            if (indexes % 2L != 0) {
-                if (first) {
-                    first = false;
-                } else {
-                    sb.append(',');
-                }
-                if (index < optionLen) {
-                    sb.append(options.get(index));
-                } else {
-                    logger.warn(""Found unexpected index '{}' on column {}"", index, column);
-                }
-            }
-            ++index;
-            indexes = indexes >>> 1;
-        }
-        return sb.toString();
-
-    }
-
-    /**
-     * Convert the a value representing a POINT {@code byte[]} value to a Point value used in a {@link SourceRecord}.
-     *
-     * @param column the column in which the value appears
-     * @param fieldDefn the field definition for the {@link SourceRecord}'s {@link Schema}; never null
-     * @param data the data; may be null
-     * @return the converted value, or null if the conversion could not be made and the column allows nulls
-     * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
-     */
-    protected Object convertPoint(Column column, Field fieldDefn, Object data){
-        if (data == null) {
-            data = fieldDefn.schema().defaultValue();
-        }
-
-        Schema schema = fieldDefn.schema();
-
-        if (data instanceof byte[]) {
-            // The binlog utility sends a byte array for any Geometry type, we will use our own binaryParse to parse the byte to WKB, hence
-            // to the suitable class
-            try {
-                MySqlGeometry mySqlGeometry = MySqlGeometry.fromBytes((byte[]) data);
-                Point point = mySqlGeometry.getPoint();
-                return io.debezium.data.geometry.Point.createValue(schema, point.getX(), point.getY(), mySqlGeometry.getWkb());
-            } catch (WkbException e) {
-                throw new ConnectException(""Failed to parse and read a value of type POINT on "" + column + "": "" + e.getMessage(), e);
-            }
-        }
-        return handleUnknownData(column, fieldDefn, data);
-    }
-
-    @Override
-    protected ByteBuffer convertByteArray(Column column, byte[] data) {
-        // DBZ-254 right-pad fixed-length binary column values with 0x00 (zero byte)
-        if (column.jdbcType() == Types.BINARY && data.length < column.length()) {
-            data = Arrays.copyOf(data, column.length());
-        }
-
-        return super.convertByteArray(column, data);
-    }
 }",2017-06-09T11:53:45Z,47
"@@ -71,20 +71,20 @@ public void afterEach() {
     public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()
-                .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
-                .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
-                .with(MySqlConnectorConfig.USER, ""snapper"")
-                .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
-                .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED)
-                .with(MySqlConnectorConfig.SERVER_ID, 18765)
-                .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
-                .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
-                .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
-                .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
-                .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
-                .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                .build();
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED)
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
 
@@ -168,8 +168,8 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 long c3Seconds = c3 / 1000;
                 long c3Millis = c3 % 1000;
                 LocalDateTime c3DateTime = LocalDateTime.ofEpochSecond(c3Seconds,
-                        (int) TimeUnit.MILLISECONDS.toNanos(c3Millis),
-                        ZoneOffset.UTC);
+                                                                       (int) TimeUnit.MILLISECONDS.toNanos(c3Millis),
+                                                                       ZoneOffset.UTC);
                 assertThat(c3DateTime.getYear()).isEqualTo(2014);
                 assertThat(c3DateTime.getMonth()).isEqualTo(Month.SEPTEMBER);
                 assertThat(c3DateTime.getDayOfMonth()).isEqualTo(8);
@@ -278,21 +278,21 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
     public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnectTimesTypes() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()
-                .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
-                .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
-                .with(MySqlConnectorConfig.USER, ""snapper"")
-                .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
-                .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED)
-                .with(MySqlConnectorConfig.SERVER_ID, 18765)
-                .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
-                .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
-                .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
-                .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
-                .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER)
-                .with(MySqlConnectorConfig.TIME_PRECISION_MODE, TemporalPrecisionMode.CONNECT)
-                .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                .build();
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED)
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER)
+                              .with(MySqlConnectorConfig.TIME_PRECISION_MODE, TemporalPrecisionMode.CONNECT)
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
 
@@ -375,8 +375,8 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                 long c3Seconds = c3.getTime() / 1000;
                 long c3Millis = c3.getTime() % 1000;
                 LocalDateTime c3DateTime = LocalDateTime.ofEpochSecond(c3Seconds,
-                        (int) TimeUnit.MILLISECONDS.toNanos(c3Millis),
-                        ZoneOffset.UTC);
+                                                                       (int) TimeUnit.MILLISECONDS.toNanos(c3Millis),
+                                                                       ZoneOffset.UTC);
                 assertThat(c3DateTime.getYear()).isEqualTo(2014);
                 assertThat(c3DateTime.getMonth()).isEqualTo(Month.SEPTEMBER);
                 assertThat(c3DateTime.getDayOfMonth()).isEqualTo(8);
@@ -486,18 +486,18 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
     public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()
-                .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
-                .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
-                .with(MySqlConnectorConfig.USER, ""snapper"")
-                .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
-                .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED)
-                .with(MySqlConnectorConfig.SERVER_ID, 18765)
-                .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
-                .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
-                .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
-                .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                .build();
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED)
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
 
@@ -578,16 +578,16 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                 assertThat(c2Time.getMinute()).isEqualTo(51);
                 assertThat(c2Time.getSecond()).isEqualTo(4);
                 assertThat(c2Time.getNano()).isEqualTo(0); // What!?!? The MySQL Connect/J driver indeed returns 0 for fractional
-                // part
+                                                           // part
                 assertThat(io.debezium.time.Time.toMilliOfDay(c2Time, ADJUSTER)).isEqualTo(c2);
 
                 // '2014-09-08 17:51:04.777'
                 Long c3 = after.getInt64(""c3""); // epoch millis
                 long c3Seconds = c3 / 1000;
                 long c3Millis = c3 % 1000;
                 LocalDateTime c3DateTime = LocalDateTime.ofEpochSecond(c3Seconds,
-                        (int) TimeUnit.MILLISECONDS.toNanos(c3Millis),
-                        ZoneOffset.UTC);
+                                                                       (int) TimeUnit.MILLISECONDS.toNanos(c3Millis),
+                                                                       ZoneOffset.UTC);
                 assertThat(c3DateTime.getYear()).isEqualTo(2014);
                 assertThat(c3DateTime.getMonth()).isEqualTo(Month.SEPTEMBER);
                 assertThat(c3DateTime.getDayOfMonth()).isEqualTo(8);
@@ -669,22 +669,22 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
     public void shouldConsumeAllEventsFromDecimalTableInDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()
-                .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
-                .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
-                .with(MySqlConnectorConfig.USER, ""snapper"")
-                .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
-                .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED)
-                .with(MySqlConnectorConfig.SERVER_ID, 18765)
-                .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
-                .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
-                .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
-                .with(MySqlConnectorConfig.TABLE_WHITELIST, ""regression_test.dbz_147_decimalvalues"")
-                .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
-                .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
-                .with(MySqlConnectorConfig.DECIMAL_HANDLING_MODE, DecimalHandlingMode.DOUBLE)
-                .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                .build();
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED)
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.TABLE_WHITELIST, ""regression_test.dbz_147_decimalvalues"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(MySqlConnectorConfig.DECIMAL_HANDLING_MODE, DecimalHandlingMode.DOUBLE)
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
 
@@ -733,4 +733,4 @@ private void assertTimestamp(String c4) {
         ZoneOffset expectedOffset = defaultZoneId.getRules().getOffset(expectedLocalDateTime);
         assertThat(c4DateTime.getOffset()).isEqualTo(expectedOffset);
     }
-}
\ No newline at end of file
+}",2017-06-09T11:53:45Z,48
"@@ -108,9 +108,9 @@ protected void initializeDataTypes(DataTypeParser dataTypes) {
         dataTypes.register(Types.INTEGER, ""YEAR[(2|4)]"");
         dataTypes.register(Types.BLOB, ""CHAR[(L)] BINARY"");
         dataTypes.register(Types.BLOB, ""VARCHAR(L) BINARY"");
-        dataTypes.register(Types.CHAR, ""CHAR[(L)]"");
+        dataTypes.register(Types.BLOB, ""BINARY[(L)]"");
         dataTypes.register(Types.VARCHAR, ""VARCHAR(L)"");
-        dataTypes.register(Types.CHAR, ""BINARY[(L)]"");
+        dataTypes.register(Types.CHAR, ""CHAR[(L)]"");
         dataTypes.register(Types.VARBINARY, ""VARBINARY(L)"");
         dataTypes.register(Types.BLOB, ""TINYBLOB"");
         dataTypes.register(Types.BLOB, ""BLOB"");",2016-11-08T23:41:01Z,49
"@@ -12,7 +12,6 @@
     <name>Debezium Connector for PostgreSQL</name>
     <packaging>jar</packaging>
     <properties>
-        <version.com.google.protobuf>2.6.1</version.com.google.protobuf>
         <!-- 
           Specify the properties that will be used for setting up the integration tests' Docker container.
           Note that the `dockerhost.ip` property is computed from the IP address of DOCKER_HOST, which will
@@ -23,7 +22,7 @@
         <database.user>postgres</database.user>
         <database.password>postgres</database.password>
         <database.name>postgres</database.name>
-        <docker.image>debezium/postgres:${version.postgres.server}</docker.image>
+        <docker.filter>debezium/postgres:${version.postgres.server}</docker.filter>
         <docker.skip>false</docker.skip>
         <docker.showLogs>true</docker.showLogs>
     </properties>
@@ -37,13 +36,11 @@
         <dependency>
             <groupId>org.postgresql</groupId>
             <artifactId>postgresql</artifactId>
-            <version>9.4.1209.jre7</version>
         </dependency>
         -->
         <dependency>
             <groupId>com.google.protobuf</groupId>
             <artifactId>protobuf-java</artifactId>
-            <version>${version.com.google.protobuf}</version>
         </dependency>
         <dependency>
             <groupId>org.apache.kafka</groupId>
@@ -99,7 +96,6 @@
             <plugin>
                 <groupId>com.github.os72</groupId>
                 <artifactId>protoc-jar-maven-plugin</artifactId>
-                <version>3.0.0.1</version>
                 <executions>
                     <execution>
                         <phase>generate-sources</phase>
@@ -126,7 +122,7 @@
                     <images>
                         <image>
                             <!-- A Docker image using the Postgres Server with the DBZ decoderbufs plugin -->
-                            <name>debezium/postgres:${version.postgres.server}</name>  
+                            <name>${docker.filter}</name>  
                             <alias>database</alias>
                             <run>
                                 <namingStrategy>alias</namingStrategy>",2016-12-27T12:44:32Z,50
"@@ -22,7 +22,6 @@
 
 import io.debezium.config.Configuration;
 import io.debezium.connector.postgresql.connection.PostgresConnection;
-import io.debezium.connector.postgresql.connection.ServerInfo;
 
 /**
  * A Kafka Connect source connector that creates tasks which use Postgresql streaming replication off a logical replication slot
@@ -95,8 +94,9 @@ public Config validate(Map<String, String> connectorConfigs) {
             // Try to connect to the database ...
             try (PostgresConnection connection = new PostgresConnection(config.jdbcConfig())) {
                 try {
-                    ServerInfo serverInfo = connection.serverInfo();
-                    logger.info(serverInfo.toString());
+                    connection.execute(""SELECT version()"");
+                    logger.info(""Successfully tested connection for {} with user '{}'"", connection.connectionString(),
+                                connection.username());
                 } catch (SQLException e) {
                     logger.info(""Failed testing connection for {} with user '{}'"", connection.connectionString(),
                                 connection.username());",2016-12-27T12:44:32Z,51
"@@ -26,7 +26,6 @@
 import io.debezium.connector.postgresql.connection.ReplicationConnection;
 import io.debezium.jdbc.JdbcConfiguration;
 import io.debezium.relational.TableId;
-import io.debezium.relational.history.KafkaDatabaseHistory;
 
 /**
  * The configuration properties for the {@link PostgresConnector}
@@ -560,14 +559,6 @@ public static TopicSelectionStrategy parse(String value) {
                                                      TIME_PRECISION_MODE,
                                                      SSL_MODE, SSL_CLIENT_CERT, SSL_CLIENT_KEY_PASSWORD,
                                                      SSL_ROOT_CERT, SSL_CLIENT_KEY, SNAPSHOT_LOCK_TIMEOUT_MS, ROWS_FETCH_SIZE);
-
-    /**
-     * The set of {@link Field}s that are included in the {@link #configDef() configuration definition}. This includes
-     * all fields defined in this class (though some are always invisible since they are not to be exposed to the user interface)
-     * plus several that are specific to the {@link KafkaDatabaseHistory} class, since history is always stored in Kafka
-     * when run via the user interface.
-     */
-    protected static Field.Set EXPOSED_FIELDS = ALL_FIELDS.with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS);
     
     private final Configuration config;
     private final String serverName;
@@ -642,7 +633,7 @@ protected TopicSelectionStrategy topicSelectionStrategy() {
     }
     
     protected Map<String, ConfigValue> validate() {
-        return config.validate(EXPOSED_FIELDS);
+        return config.validate(ALL_FIELDS);
     }  
     
     protected boolean validateAndRecord(Consumer<String> errorConsumer) {",2016-12-27T12:44:32Z,42
"@@ -6,6 +6,7 @@
 
 package io.debezium.connector.postgresql;
 
+import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
@@ -20,6 +21,7 @@
 import org.slf4j.LoggerFactory;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.postgresql.connection.PostgresConnection;
 import io.debezium.util.Clock;
 import io.debezium.util.LoggingContext;
 import io.debezium.util.Metronome;
@@ -61,10 +63,11 @@ public void start(Map<String, String> props) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }
         
+        
         // create the task context and schema...
         PostgresSchema schema = new PostgresSchema(config);
         this.taskContext = new PostgresTaskContext(config, schema);
-    
+        
         // create the queue in which records will be produced
         this.queue = new LinkedBlockingDeque<>(config.maxQueueSize());
         this.maxBatchSize = config.maxBatchSize();
@@ -73,6 +76,11 @@ public void start(Map<String, String> props) {
         Map<String, Object> existingOffset = context.offsetStorageReader().offset(sourceInfo.partition());
         LoggingContext.PreviousContext previousContext = taskContext.configureLoggingContext(CONTEXT_NAME);
         try {
+            //Print out the server information
+            try (PostgresConnection connection = taskContext.createConnection()) {
+                logger.info(connection.serverInfo().toString());
+            }
+            
             if (existingOffset == null) {
                 logger.info(""No previous offset found"");
                 if (config.snapshotNeverAllowed()) {
@@ -108,6 +116,8 @@ public void start(Map<String, String> props) {
             metronome = Metronome.sleeper(config.pollIntervalMs(), TimeUnit.MILLISECONDS, Clock.SYSTEM);
             producer.start(this::enqueueRecord);
             running.compareAndSet(false, true);
+        }  catch (SQLException e) {
+            throw new ConnectException(e);
         } finally {
             previousContext.restore();
         }",2016-12-27T12:44:32Z,52
"@@ -152,20 +152,13 @@ protected Table tableFor(String fqn) {
     protected String validateSchemaName(String name) {
         return this.schemaNameValidator.apply(name);
     }
-  
-    /**
-     * Get the {@link TableSchema Schema information} for the table with the given identifier, if that table exists and is
-     * included by the {@link #filters() filter}.
-     * <p>
-     * Note that the {@link Schema} will not contain any columns that have been {@link PostgresConnectorConfig#COLUMN_BLACKLIST
-     * filtered out}.
-     * 
-     * @param id the fully-qualified table identifier; may be null
-     * @return the schema information, or null if there is no table with the given identifier, if the identifier is null,
-     *         or if the table has been excluded by the filters
-     */
-    public TableSchema schemaFor(TableId id) {
-        return filters.tableFilter().test(id) ? tableSchemaByTableId.get(id) : null;
+    
+    protected TableSchema schemaFor(TableId id) {
+        return tableSchemaByTableId.get(id);
+    }
+    
+    protected boolean isFilteredOut(TableId id) {
+        return !filters.tableFilter().test(id);        
     }
 
     protected TableSchema schemaFor(String fqn) {",2016-12-27T12:44:32Z,43
"@@ -23,15 +23,14 @@
 @ThreadSafe
 public class PostgresTaskContext {
     
-    private static final String CONNECTOR_TYPE = ""Postgres"";
-    
     private final PostgresConnectorConfig config;
-    private final Clock clock = Clock.system();
+    private final Clock clock;
     private final TopicSelector topicSelector;
     private final PostgresSchema schema;
     
     protected PostgresTaskContext(PostgresConnectorConfig config, PostgresSchema schema) {
         this.config = config;
+        this.clock = Clock.system(); 
         this.topicSelector = initTopicSelector();
         assert schema != null;
         this.schema = schema;
@@ -90,6 +89,6 @@ protected PostgresConnection createConnection() {
      * @throws IllegalArgumentException if {@code contextName} is null
      */
     protected LoggingContext.PreviousContext configureLoggingContext(String contextName) {
-        return LoggingContext.forConnector(CONNECTOR_TYPE, config.serverName(), contextName);
+        return LoggingContext.forConnector(""Postgres"", config.serverName(), contextName);
     }
 }",2016-12-27T12:44:32Z,53
"@@ -138,21 +138,22 @@ private void takeSnapshot(Consumer<SourceRecord> consumer) {
             statements.delete(0, statements.length());
     
             //next refresh the schema which will load all the tables taking the filters into account 
-            schema().refresh(connection, false);
+            PostgresSchema schema = schema();
+            schema.refresh(connection, false);
     
             logger.info(""Step 2: locking each of the database tables, waiting a maximum of '{}' seconds for each lock"",
                         lockTimeoutMillis / 1000d);
             statements.append(""SET lock_timeout = "").append(lockTimeoutMillis).append("";"").append(lineSeparator);
             // we're locking in SHARE UPDATE EXCLUSIVE MODE to avoid concurrent schema changes while we're taking the snapshot
             // this does not prevent writes to the table, but prevents changes to the table's schema....
-            schema().tables().forEach(tableId -> statements.append(""LOCK TABLE "")
-                                                .append(tableId.toString())
-                                                .append("" IN SHARE UPDATE EXCLUSIVE MODE;"")
-                                                .append(lineSeparator));
+            schema.tables().forEach(tableId -> statements.append(""LOCK TABLE "")
+                                                         .append(tableId.toString())
+                                                         .append("" IN SHARE UPDATE EXCLUSIVE MODE;"")
+                                                         .append(lineSeparator));
             connection.executeWithoutCommitting(statements.toString());
     
             //now that we have the locks, refresh the schema
-            schema().refresh(connection, false);
+            schema.refresh(connection, false);
     
             // get the current position in the log, from which we'll continue streaming once the snapshot it finished
             // If rows are being inserted while we're doing the snapshot, the xlog pos should increase and so when 
@@ -166,11 +167,14 @@ private void takeSnapshot(Consumer<SourceRecord> consumer) {
             sourceInfo.update(xlogStart, clock().currentTimeInMicros(), txId);
     
             logger.info(""Step 3: reading and exporting the contents of each table"");
-            AtomicInteger counter = new AtomicInteger(0);
             AtomicInteger rowsCounter = new AtomicInteger(0);
-            schema().tables().forEach(tableId -> {
+            schema.tables().forEach(tableId -> {
+                if (schema.isFilteredOut(tableId)) {
+                    logger.info(""\t table '{}' is filtered out, ignoring"", tableId);
+                    return;
+                }
                 long exportStart = clock().currentTimeInMillis();
-                logger.info(""Step 3.{}: exporting data from table '{}'"", counter.incrementAndGet(), tableId);
+                logger.info(""\t exporting data from table '{}'"", tableId);
                 try {
                     connection.query(""SELECT * FROM "" + tableId, 
                                      this::readTableStatement, 
@@ -189,8 +193,8 @@ private void takeSnapshot(Consumer<SourceRecord> consumer) {
     
             // process and send the last record after marking it as such
             logger.info(""Step 5: sending the last snapshot record"");
+            SourceRecord currentRecord = this.currentRecord.get();
             if (currentRecord != null) {  
-                SourceRecord currentRecord = this.currentRecord.get();
                 sourceInfo.markLastSnapshotRecord();
                 this.currentRecord.set(new SourceRecord(currentRecord.sourcePartition(), sourceInfo.offset(),
                                                         currentRecord.topic(), currentRecord.kafkaPartition(),
@@ -252,15 +256,14 @@ private Object valueForColumn(ResultSet rs, int colIdx, ResultSetMetaData metaDa
                 default:
                     return rs.getObject(colIdx);
             }
-        
         } catch (SQLException e) {
             // not a known type
             return rs.getObject(colIdx);
         }
     }
     
     protected void generateReadRecord(TableId tableId, Object[] rowData) {
-        if (rowData == null || rowData.length == 0) {
+        if (rowData.length == 0) {
             return;
         }
         TableSchema tableSchema = schema().schemaFor(tableId);",2016-12-27T12:44:32Z,54
"@@ -33,7 +33,6 @@
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.relational.TableSchema;
-import io.debezium.relational.Tables;
 import io.debezium.util.LoggingContext;
 
 /**
@@ -62,13 +61,10 @@ public RecordsStreamProducer(PostgresTaskContext taskContext,
         super(taskContext, sourceInfo);
         this.executorService = Executors.newSingleThreadExecutor(runnable -> new Thread(runnable, CONTEXT_NAME + ""-thread""));
         this.replicationStream = new AtomicReference<>();
-        LoggingContext.PreviousContext previousContext = taskContext.configureLoggingContext(CONTEXT_NAME);
         try {
             this.replicationConnection = taskContext.createReplicationConnection();
         } catch (SQLException e) {
             throw new ConnectException(e);
-        } finally {
-            previousContext.restore();
         }
     }
     
@@ -126,7 +122,7 @@ protected synchronized void commit()  {
         try {
             ReplicationStream replicationStream = this.replicationStream.get();
             if (replicationStream != null) {
-                // tell the server the point up until we've processed data, so it can be free to recycle WAL segments
+                // tell the server the point up to which we've processed data, so it can be free to recycle WAL segments
                 logger.debug(""flushing offsets to server..."");
                 replicationStream.flushLSN();
             } else {
@@ -207,7 +203,7 @@ private void process(PgProto.RowMessage message, Long lsn, Consumer<SourceRecord
                 break;
             }
             default: {
-                throw new IllegalArgumentException(""unknown message operation: "" + operation);
+               logger.warn(""unknown message operation: "" + operation);
             }
         }
     }
@@ -387,26 +383,25 @@ private boolean schemaChanged(List<PgProto.DatumMessage> messageList, Table tabl
     }
     
     private TableSchema tableSchemaFor(TableId tableId) throws SQLException {
-        TableSchema tableSchema = schema().schemaFor(tableId);
+        PostgresSchema schema = schema();
+        if (schema.isFilteredOut(tableId)) {
+            logger.debug(""table '{}' is filtered out, ignoring"", tableId);
+            return null;
+        }
+        TableSchema tableSchema = schema.schemaFor(tableId);
         if (tableSchema != null) {
             return tableSchema;
         }
-        Tables.TableNameFilter tableNameFilter = schema().filters().tableNameFilter();
-        if (tableNameFilter.matches(tableId.catalog(), tableId.schema(), tableId.table())) {
-            // we don't have a schema registered for this table, even though the filters would allow it...
-            // which means that is a newly created table; so refresh our schema to get the definition for this table
-            schema().refresh(taskContext.createConnection(), tableId);
-            tableSchema = schema().schemaFor(tableId);
-            if (tableSchema == null) {
-                logger.warn(""cannot load schema for table '{}'"", tableId);
-                return null;
-            } else {
-                logger.debug(""refreshed DB schema to include table '{}'"", tableId);
-                return tableSchema;
-            }
-        } else {
-            logger.debug(""ignoring message for table '{}' because it has been filtered out"", tableId);
+        // we don't have a schema registered for this table, even though the filters would allow it...
+        // which means that is a newly created table; so refresh our schema to get the definition for this table
+        schema.refresh(taskContext.createConnection(), tableId);
+        tableSchema = schema.schemaFor(tableId);
+        if (tableSchema == null) {
+            logger.warn(""cannot load schema for table '{}'"", tableId);
             return null;
+        } else {
+            logger.debug(""refreshed DB schema to include table '{}'"", tableId);
+            return tableSchema;
         }
     }
     ",2016-12-27T12:44:32Z,46
"@@ -204,7 +204,7 @@ public ServerInfo serverInfo() throws SQLException {
         if (username != null) {
             query(""SELECT oid, rolname, rolsuper, rolinherit, rolcreaterole, rolcreatedb, rolcanlogin, rolreplication FROM pg_roles "" +
                   ""WHERE pg_has_role('"" + username +""', oid, 'member')"", rs -> {
-                if (rs.next()) {
+                while (rs.next()) {
                     String roleInfo = ""superuser: "" + rs.getBoolean(3) + "", replication: "" + rs.getBoolean(8) + 
                                       "", inherit: "" + rs.getBoolean(4) + "", create role: "" + rs.getBoolean(5) +
                                       "", create db: "" + rs.getBoolean(6) + "", can log in: "" + rs.getBoolean(7);
@@ -226,7 +226,7 @@ private static void validateServerVersion(Statement statement) throws SQLExcepti
         int majorVersion = metaData.getDatabaseMajorVersion();
         int minorVersion = metaData.getDriverMinorVersion();
         if (majorVersion < 9 || (majorVersion == 9 && minorVersion < 4)) {
-            throw new IllegalStateException(""Cannot connect to a version of Postgres lower than 9.4"");
+            throw new SQLException(""Cannot connect to a version of Postgres lower than 9.4"");
         }
     }
     ",2016-12-27T12:44:32Z,55
"@@ -90,7 +90,7 @@ public String toString() {
                                             .map(entry -> ""\trole '"" + entry.getKey() + ""' ["" + entry.getValue() + ""]"")
                                             .collect(Collectors.joining(lineSeparator));
     
-        return ""User '"" + username + ""' connected to database '"" + database + ""' on "" + server + "" with:"" + lineSeparator + roles;
+        return ""user '"" + username + ""' connected to database '"" + database + ""' on "" + server + "" with roles:"" + lineSeparator + roles;
     }
     
     /**",2016-12-27T12:44:32Z,56
"@@ -12,17 +12,22 @@
 import static io.debezium.connector.postgresql.PostgresConnectorConfig.SnapshotMode.NEVER;
 import static io.debezium.connector.postgresql.TestHelper.PK_FIELD;
 import static org.fest.assertions.Assertions.assertThat;
+import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
 import java.sql.SQLException;
 import java.util.List;
+import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Predicate;
 import java.util.stream.IntStream;
 import org.apache.kafka.common.config.Config;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.ConnectException;
+import org.apache.kafka.connect.errors.DataException;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.junit.Before;
 import org.junit.BeforeClass;
@@ -32,10 +37,12 @@
 import io.debezium.config.Configuration;
 import io.debezium.config.Field;
 import io.debezium.connector.postgresql.connection.ReplicationConnection;
+import io.debezium.data.Envelope;
 import io.debezium.data.VerifyRecord;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.embedded.EmbeddedEngine;
-import io.debezium.jdbc.JdbcConnectionException;
+import io.debezium.junit.SkipLongRunning;
+import io.debezium.util.Strings;
 
 /**
  * Integration test for {@link PostgresConnector} using an {@link io.debezium.embedded.EmbeddedEngine} 
@@ -87,6 +94,14 @@ public void shouldNotStartWithInvalidConfiguration() throws Exception {
         assertConnectorNotRunning();            
     }
     
+    @Test
+    public void shouldValidateMinimalConfiguration() throws Exception {
+        Configuration config = TestHelper.defaultConfig().build();
+        Config validateConfig = new PostgresConnector().validate(config.asMap());
+        validateConfig.configValues().forEach(configValue -> assertTrue(""Unexpected error for: "" + configValue.name(), 
+                                                                        configValue.errorMessages().isEmpty()));
+    }
+    
     @Test
     public void shouldValidateConfiguration() throws Exception {
         // use an empty configuration which should be invalid because of the lack of DB connection details
@@ -137,10 +152,12 @@ public void shouldSupportSSLParameters() throws Exception {
         Configuration config = TestHelper.defaultConfig().with(PostgresConnectorConfig.SSL_MODE,  
                                                                PostgresConnectorConfig.SecureConnectionMode.REQUIRED).build();
         start(PostgresConnector.class, config, (success, msg, error) -> {
+            // we expect the task to fail at startup when we're printing the server info
             assertThat(success).isFalse();
-            assertThat(error).isInstanceOf(JdbcConnectionException.class);
-            JdbcConnectionException jdbcException = (JdbcConnectionException)error;
-            assertThat(PSQLState.CONNECTION_UNABLE_TO_CONNECT).isEqualTo(new PSQLState(jdbcException.getSqlState()));
+            assertThat(error).isInstanceOf(ConnectException.class);
+            Throwable cause = error.getCause();
+            assertThat(cause).isInstanceOf(SQLException.class);
+            assertThat(PSQLState.CONNECTION_UNABLE_TO_CONNECT).isEqualTo(new PSQLState(((SQLException)cause).getSQLState()));
         });
         assertConnectorNotRunning();
     }
@@ -247,7 +264,6 @@ public void shouldResumeSnapshotIfFailingMidstream() throws Exception {
         CountDownLatch latch = new CountDownLatch(1);
         String setupStmt = SETUP_TABLES_STMT + INSERT_STMT;
         TestHelper.execute(setupStmt);
-        // use 1 ms polling interval to ensure we get the exception during the snapshot process
         Configuration.Builder configBuilder = TestHelper.defaultConfig()
                                                         .with(PostgresConnectorConfig.SNAPSHOT_MODE, INITIAL.getValue())
                                                         .with(PostgresConnectorConfig.DROP_SLOT_ON_STOP, Boolean.FALSE);
@@ -284,6 +300,136 @@ public void shouldResumeSnapshotIfFailingMidstream() throws Exception {
         assertRecordsAfterInsert(2, 3, 3);
     }
     
+    @Test
+    public void shouldTakeBlacklistFiltersIntoAccount() throws Exception {
+        String setupStmt = SETUP_TABLES_STMT +
+                           ""CREATE TABLE s1.b (pk SERIAL, aa integer, bb integer, PRIMARY KEY(pk));"" +
+                           ""ALTER TABLE s1.a ADD COLUMN bb integer;"" +
+                           ""INSERT INTO s1.a (aa, bb) VALUES (2, 2);"" +
+                           ""INSERT INTO s1.a (aa, bb) VALUES (3, 3);"" +
+                           ""INSERT INTO s1.b (aa, bb) VALUES (4, 4);"" +
+                           ""INSERT INTO s2.a (aa) VALUES (5);"";
+        TestHelper.execute(setupStmt);
+        Configuration.Builder configBuilder = TestHelper.defaultConfig()
+                                                        .with(PostgresConnectorConfig.SNAPSHOT_MODE, INITIAL.getValue())
+                                                        .with(PostgresConnectorConfig.DROP_SLOT_ON_STOP, Boolean.TRUE)
+                                                        .with(PostgresConnectorConfig.SCHEMA_BLACKLIST, ""s2"")
+                                                        .with(PostgresConnectorConfig.TABLE_BLACKLIST, "".+b"")
+                                                        .with(PostgresConnectorConfig.COLUMN_BLACKLIST, "".+bb"");
+    
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+    
+        //check the records from the snapshot take the filters into account
+        SourceRecords actualRecords = consumeRecordsByTopic(4); //3 records in s1.a and 1 in s1.b
+        
+        assertThat(actualRecords.recordsForTopic(""s2.a"")).isNullOrEmpty();
+        assertThat(actualRecords.recordsForTopic(""s1.b"")).isNullOrEmpty();
+        List<SourceRecord> recordsForS1a = actualRecords.recordsForTopic(""s1.a"");
+        assertThat(recordsForS1a.size()).isEqualTo(3);
+        AtomicInteger pkValue = new AtomicInteger(1);
+        recordsForS1a.forEach(record -> {
+            VerifyRecord.isValidRead(record, PK_FIELD, pkValue.getAndIncrement());
+            assertFieldAbsent(record, ""bb"");
+        });
+        
+        
+        // insert some more records and verify the filtering behavior
+        String insertStmt =  ""INSERT INTO s1.b (aa, bb) VALUES (6, 6);"" +
+                             ""INSERT INTO s2.a (aa) VALUES (7);"";
+        TestHelper.execute(insertStmt);
+        assertNoRecordsToConsume();
+    }
+    
+    private void assertFieldAbsent(SourceRecord record, String fieldName) {
+        Struct value = (Struct) ((Struct) record.value()).get(Envelope.FieldName.AFTER);
+        try {
+            value.get(fieldName);
+            fail(""field should not be present"");
+        } catch (DataException e) {
+            //expected
+        }
+    }
+    
+    @Test
+    @SkipLongRunning(""performance"")
+    public void testStreamingPerformance() throws Exception {
+        TestHelper.dropAllSchemas();
+        TestHelper.executeDDL(""postgres_create_tables.ddl"");        
+        Configuration.Builder configBuilder = TestHelper.defaultConfig()
+                                                        .with(PostgresConnectorConfig.SNAPSHOT_MODE, NEVER.getValue())
+                                                        .with(PostgresConnectorConfig.DROP_SLOT_ON_STOP, Boolean.TRUE);
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+        final long recordsCount = 1000000;
+        final int batchSize = 1000;
+        
+        batchInsertRecords(recordsCount, batchSize);
+        CompletableFuture.runAsync(() -> consumeRecords(recordsCount))
+                         .exceptionally(throwable -> {
+                             throw new RuntimeException(throwable);
+                         }).get();
+    }
+    
+    private void consumeRecords(long recordsCount) {
+        int totalConsumedRecords = 0;
+        long start = System.currentTimeMillis();
+        while (totalConsumedRecords < recordsCount) {
+            int consumed = super.consumeAvailableRecords(record -> {});
+            if (consumed > 0) {
+                totalConsumedRecords += consumed;
+                System.out.println(""consumed "" + totalConsumedRecords + "" records"");
+            }
+        }
+        System.out.println(""total duration to ingest '"" + recordsCount + ""' records: "" + 
+                           Strings.duration(System.currentTimeMillis() - start));
+    }
+    
+    @Test
+    @SkipLongRunning(""performance"")
+    public void testSnapshotPerformance() throws Exception {
+        TestHelper.dropAllSchemas();
+        TestHelper.executeDDL(""postgres_create_tables.ddl"");
+        Configuration.Builder configBuilder = TestHelper.defaultConfig()
+                                                        .with(PostgresConnectorConfig.SNAPSHOT_MODE, INITIAL_ONLY.getValue())
+                                                        .with(PostgresConnectorConfig.DROP_SLOT_ON_STOP, Boolean.TRUE);
+        final long recordsCount = 1000000;
+        final int batchSize = 1000;
+    
+        batchInsertRecords(recordsCount, batchSize).get();
+
+        // start the connector only after we've finished inserting all the records
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+        
+        CompletableFuture.runAsync(() -> consumeRecords(recordsCount))
+                         .exceptionally(throwable -> {
+                             throw new RuntimeException(throwable);
+                         }).get();
+    }
+    
+    private CompletableFuture<Void> batchInsertRecords(long recordsCount, int batchSize) {
+        String insertStmt = ""INSERT INTO text_table(j, jb, x, u) "" +
+                            ""VALUES ('{\""bar\"": \""baz\""}'::json, '{\""bar\"": \""baz\""}'::jsonb, "" +
+                            ""'<foo>bar</foo><foo>bar</foo>'::xml, 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11'::UUID);"";
+        return CompletableFuture.runAsync(() -> {
+            StringBuilder stmtBuilder = new StringBuilder();
+            for (int i = 0; i < recordsCount; i++) {
+                stmtBuilder.append(insertStmt).append(System.lineSeparator());
+                if (i > 0  && i % batchSize == 0) {
+                    System.out.println(""inserting batch ["" + (i - batchSize) + "","" + i + ""]"");
+                    TestHelper.execute(stmtBuilder.toString());
+                    stmtBuilder.delete(0, stmtBuilder.length());
+                }
+            }
+            System.out.println(""inserting batch ["" + (recordsCount - batchSize) + "","" + recordsCount + ""]"");
+            TestHelper.execute(stmtBuilder.toString());
+            stmtBuilder.delete(0, stmtBuilder.length());
+        }).exceptionally(throwable -> {
+            throw new RuntimeException(throwable);
+        });
+    }
+    
     private Predicate<SourceRecord> stopOnPKPredicate(int pkValue) {
         return record -> {
             Struct key = (Struct) record.key();",2016-12-27T12:44:32Z,45
"@@ -120,7 +120,7 @@ protected static Configuration.Builder defaultConfig() {
     
     protected static void executeDDL(String ddlFile) throws Exception {
         URL ddlTestFile = TestHelper.class.getClassLoader().getResource(ddlFile);
-        assertNotNull(""Cannot locate postgres_create_tables.ddl"", ddlTestFile);
+        assertNotNull(""Cannot locate "" + ddlFile, ddlTestFile);
         String statements = Files.readAllLines(Paths.get(ddlTestFile.toURI()))
                                  .stream()
                                  .collect(Collectors.joining(System.lineSeparator()));",2016-12-27T12:44:32Z,57
"@@ -91,8 +91,9 @@ public void shouldDropReplicationSlot() throws Exception {
         }
         // create a new replication slot via a replication connection
         try (ReplicationConnection connection = TestHelper.createForReplication(""test"", false)) {
-            //nothing
-        };
+            assertTrue(connection.isConnected());
+        }
+        // drop the slot from the previous connection
         try (PostgresConnection connection = TestHelper.create()) {
             // try to drop the previous slot
             assertTrue(connection.dropReplicationSlot(""test""));",2016-12-27T12:44:32Z,58
"@@ -6,7 +6,6 @@
 package io.debezium.junit;
 
 import java.lang.annotation.Annotation;
-
 import org.junit.rules.TestRule;
 import org.junit.runner.Description;
 import org.junit.runners.model.Statement;
@@ -25,8 +24,8 @@ public Statement apply( Statement base,
                             Description description ) {
         SkipLongRunning skipLongRunningAnnotation = hasAnnotation(description, SkipLongRunning.class);
         if (skipLongRunningAnnotation != null) {
-            boolean skipLongRunning = Boolean.valueOf(System.getProperty(SkipLongRunning.SKIP_LONG_RUNNING_PROPERTY));
-            if (skipLongRunning) {
+            String skipLongRunning = System.getProperty(SkipLongRunning.SKIP_LONG_RUNNING_PROPERTY);
+            if (skipLongRunning == null || Boolean.valueOf(skipLongRunning)) {
                 return emptyStatement(skipLongRunningAnnotation.value(), description);
             }
         }",2016-12-27T12:44:32Z,59
"@@ -5,6 +5,7 @@
  */
 package io.debezium.embedded;
 
+import static org.fest.assertions.Assertions.assertThat;
 import static org.junit.Assert.fail;
 
 import java.math.BigDecimal;
@@ -26,7 +27,6 @@
 import java.util.concurrent.TimeUnit;
 import java.util.function.Consumer;
 import java.util.function.Predicate;
-
 import org.apache.kafka.common.config.Config;
 import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.connect.data.Field;
@@ -46,18 +46,19 @@
 import org.fest.assertions.Delta;
 import org.junit.After;
 import org.junit.Before;
+import org.junit.Rule;
+import org.junit.rules.TestRule;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.fest.assertions.Assertions.assertThat;
-
 import io.debezium.config.Configuration;
 import io.debezium.data.SchemaUtil;
 import io.debezium.data.VerifyRecord;
 import io.debezium.embedded.EmbeddedEngine.CompletionCallback;
 import io.debezium.embedded.EmbeddedEngine.ConnectorCallback;
 import io.debezium.embedded.EmbeddedEngine.EmbeddedConfig;
 import io.debezium.function.BooleanConsumer;
+import io.debezium.junit.SkipTestRule;
 import io.debezium.relational.history.HistoryRecord;
 import io.debezium.util.LoggingContext;
 import io.debezium.util.Testing;
@@ -74,7 +75,10 @@
  * @author Randall Hauch
  */
 public abstract class AbstractConnectorTest implements Testing {
-
+    
+    @Rule
+    public TestRule skipTestRule = new SkipTestRule();
+    
     protected static final Path OFFSET_STORE_PATH = Testing.Files.createTestingPath(""file-connector-offsets.txt"").toAbsolutePath();
 
     private ExecutorService executor;",2016-12-27T12:44:32Z,35
"@@ -67,6 +67,9 @@
         <version.mysql.binlog>0.5.1</version.mysql.binlog>
         <version.mongo.server>3.2.6</version.mongo.server>
         <version.mongo.driver>3.2.2</version.mongo.driver>
+        
+        <!-- Connectors -->
+        <version.com.google.protobuf>2.6.1</version.com.google.protobuf>
 
         <!-- Testing -->
         <version.junit>4.12</version.junit>
@@ -81,6 +84,7 @@
         <version.google.formatter.plugin>0.3.1</version.google.formatter.plugin>
         <version.docker.maven.plugin>0.18.1</version.docker.maven.plugin>
         <version.staging.plugin>1.6.3</version.staging.plugin>
+        <version.protoc.maven.plugin>3.0.0.1</version.protoc.maven.plugin>
 
         <!-- Dockerfiles -->
         <docker.maintainer>Debezium community</docker.maintainer>
@@ -197,12 +201,23 @@
                 <scope>test</scope>
             </dependency>
 
-            <!-- PostgreSQL JDBC driver -->
+            <!-- PostgreSQL connector -->
+            <!--
+                //TODO author=Horia Chiorean date=21/11/2016 description=ignored ATM until the streaming replication support
+                is incoroporated
+             
             <dependency>
                 <groupId>org.postgresql</groupId>
                 <artifactId>postgresql</artifactId>
                 <version>${version.postgresql.driver}</version>
-            </dependency>
+            </dependency>-->
+           
+            <!--Make sure this version is compatible with the Protbuf-C version used on the server -->
+            <dependency>
+                <groupId>com.google.protobuf</groupId>
+                <artifactId>protobuf-java</artifactId>
+                <version>${version.com.google.protobuf}</version>
+            </dependency> 
 
             <!-- MySQL JDBC Driver and Binlog reader -->
             <dependency>
@@ -400,6 +415,11 @@
                     <artifactId>docker-maven-plugin</artifactId>
                     <version>${version.docker.maven.plugin}</version>
                 </plugin>
+                <plugin>
+                    <groupId>com.github.os72</groupId>
+                    <artifactId>protoc-jar-maven-plugin</artifactId>
+                    <version>${version.protoc.maven.plugin}</version>
+                </plugin>
             </plugins>
         </pluginManagement>
         <plugins>",2016-12-27T12:44:32Z,60
"@@ -12,7 +12,6 @@
     <name>Debezium Connector for PostgreSQL</name>
     <packaging>jar</packaging>
     <properties>
-        <version.com.google.protobuf>2.6.1</version.com.google.protobuf>
         <!-- 
           Specify the properties that will be used for setting up the integration tests' Docker container.
           Note that the `dockerhost.ip` property is computed from the IP address of DOCKER_HOST, which will
@@ -23,7 +22,7 @@
         <database.user>postgres</database.user>
         <database.password>postgres</database.password>
         <database.name>postgres</database.name>
-        <docker.image>debezium/postgres:${version.postgres.server}</docker.image>
+        <docker.filter>debezium/postgres:${version.postgres.server}</docker.filter>
         <docker.skip>false</docker.skip>
         <docker.showLogs>true</docker.showLogs>
     </properties>
@@ -37,13 +36,11 @@
         <dependency>
             <groupId>org.postgresql</groupId>
             <artifactId>postgresql</artifactId>
-            <version>9.4.1209.jre7</version>
         </dependency>
         -->
         <dependency>
             <groupId>com.google.protobuf</groupId>
             <artifactId>protobuf-java</artifactId>
-            <version>${version.com.google.protobuf}</version>
         </dependency>
         <dependency>
             <groupId>org.apache.kafka</groupId>
@@ -99,7 +96,6 @@
             <plugin>
                 <groupId>com.github.os72</groupId>
                 <artifactId>protoc-jar-maven-plugin</artifactId>
-                <version>3.0.0.1</version>
                 <executions>
                     <execution>
                         <phase>generate-sources</phase>
@@ -126,7 +122,7 @@
                     <images>
                         <image>
                             <!-- A Docker image using the Postgres Server with the DBZ decoderbufs plugin -->
-                            <name>debezium/postgres:${version.postgres.server}</name>  
+                            <name>${docker.filter}</name>  
                             <alias>database</alias>
                             <run>
                                 <namingStrategy>alias</namingStrategy>",2016-12-27T12:44:32Z,50
"@@ -22,7 +22,6 @@
 
 import io.debezium.config.Configuration;
 import io.debezium.connector.postgresql.connection.PostgresConnection;
-import io.debezium.connector.postgresql.connection.ServerInfo;
 
 /**
  * A Kafka Connect source connector that creates tasks which use Postgresql streaming replication off a logical replication slot
@@ -95,8 +94,9 @@ public Config validate(Map<String, String> connectorConfigs) {
             // Try to connect to the database ...
             try (PostgresConnection connection = new PostgresConnection(config.jdbcConfig())) {
                 try {
-                    ServerInfo serverInfo = connection.serverInfo();
-                    logger.info(serverInfo.toString());
+                    connection.execute(""SELECT version()"");
+                    logger.info(""Successfully tested connection for {} with user '{}'"", connection.connectionString(),
+                                connection.username());
                 } catch (SQLException e) {
                     logger.info(""Failed testing connection for {} with user '{}'"", connection.connectionString(),
                                 connection.username());",2016-12-27T12:44:32Z,51
"@@ -26,7 +26,6 @@
 import io.debezium.connector.postgresql.connection.ReplicationConnection;
 import io.debezium.jdbc.JdbcConfiguration;
 import io.debezium.relational.TableId;
-import io.debezium.relational.history.KafkaDatabaseHistory;
 
 /**
  * The configuration properties for the {@link PostgresConnector}
@@ -560,14 +559,6 @@ public static TopicSelectionStrategy parse(String value) {
                                                      TIME_PRECISION_MODE,
                                                      SSL_MODE, SSL_CLIENT_CERT, SSL_CLIENT_KEY_PASSWORD,
                                                      SSL_ROOT_CERT, SSL_CLIENT_KEY, SNAPSHOT_LOCK_TIMEOUT_MS, ROWS_FETCH_SIZE);
-
-    /**
-     * The set of {@link Field}s that are included in the {@link #configDef() configuration definition}. This includes
-     * all fields defined in this class (though some are always invisible since they are not to be exposed to the user interface)
-     * plus several that are specific to the {@link KafkaDatabaseHistory} class, since history is always stored in Kafka
-     * when run via the user interface.
-     */
-    protected static Field.Set EXPOSED_FIELDS = ALL_FIELDS.with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS);
     
     private final Configuration config;
     private final String serverName;
@@ -642,7 +633,7 @@ protected TopicSelectionStrategy topicSelectionStrategy() {
     }
     
     protected Map<String, ConfigValue> validate() {
-        return config.validate(EXPOSED_FIELDS);
+        return config.validate(ALL_FIELDS);
     }  
     
     protected boolean validateAndRecord(Consumer<String> errorConsumer) {",2016-12-27T12:44:32Z,42
"@@ -6,6 +6,7 @@
 
 package io.debezium.connector.postgresql;
 
+import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
@@ -20,6 +21,7 @@
 import org.slf4j.LoggerFactory;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.postgresql.connection.PostgresConnection;
 import io.debezium.util.Clock;
 import io.debezium.util.LoggingContext;
 import io.debezium.util.Metronome;
@@ -61,10 +63,11 @@ public void start(Map<String, String> props) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }
         
+        
         // create the task context and schema...
         PostgresSchema schema = new PostgresSchema(config);
         this.taskContext = new PostgresTaskContext(config, schema);
-    
+        
         // create the queue in which records will be produced
         this.queue = new LinkedBlockingDeque<>(config.maxQueueSize());
         this.maxBatchSize = config.maxBatchSize();
@@ -73,6 +76,11 @@ public void start(Map<String, String> props) {
         Map<String, Object> existingOffset = context.offsetStorageReader().offset(sourceInfo.partition());
         LoggingContext.PreviousContext previousContext = taskContext.configureLoggingContext(CONTEXT_NAME);
         try {
+            //Print out the server information
+            try (PostgresConnection connection = taskContext.createConnection()) {
+                logger.info(connection.serverInfo().toString());
+            }
+            
             if (existingOffset == null) {
                 logger.info(""No previous offset found"");
                 if (config.snapshotNeverAllowed()) {
@@ -108,6 +116,8 @@ public void start(Map<String, String> props) {
             metronome = Metronome.sleeper(config.pollIntervalMs(), TimeUnit.MILLISECONDS, Clock.SYSTEM);
             producer.start(this::enqueueRecord);
             running.compareAndSet(false, true);
+        }  catch (SQLException e) {
+            throw new ConnectException(e);
         } finally {
             previousContext.restore();
         }",2016-12-27T12:44:32Z,52
"@@ -152,20 +152,13 @@ protected Table tableFor(String fqn) {
     protected String validateSchemaName(String name) {
         return this.schemaNameValidator.apply(name);
     }
-  
-    /**
-     * Get the {@link TableSchema Schema information} for the table with the given identifier, if that table exists and is
-     * included by the {@link #filters() filter}.
-     * <p>
-     * Note that the {@link Schema} will not contain any columns that have been {@link PostgresConnectorConfig#COLUMN_BLACKLIST
-     * filtered out}.
-     * 
-     * @param id the fully-qualified table identifier; may be null
-     * @return the schema information, or null if there is no table with the given identifier, if the identifier is null,
-     *         or if the table has been excluded by the filters
-     */
-    public TableSchema schemaFor(TableId id) {
-        return filters.tableFilter().test(id) ? tableSchemaByTableId.get(id) : null;
+    
+    protected TableSchema schemaFor(TableId id) {
+        return tableSchemaByTableId.get(id);
+    }
+    
+    protected boolean isFilteredOut(TableId id) {
+        return !filters.tableFilter().test(id);        
     }
 
     protected TableSchema schemaFor(String fqn) {",2016-12-27T12:44:32Z,43
"@@ -23,15 +23,14 @@
 @ThreadSafe
 public class PostgresTaskContext {
     
-    private static final String CONNECTOR_TYPE = ""Postgres"";
-    
     private final PostgresConnectorConfig config;
-    private final Clock clock = Clock.system();
+    private final Clock clock;
     private final TopicSelector topicSelector;
     private final PostgresSchema schema;
     
     protected PostgresTaskContext(PostgresConnectorConfig config, PostgresSchema schema) {
         this.config = config;
+        this.clock = Clock.system(); 
         this.topicSelector = initTopicSelector();
         assert schema != null;
         this.schema = schema;
@@ -90,6 +89,6 @@ protected PostgresConnection createConnection() {
      * @throws IllegalArgumentException if {@code contextName} is null
      */
     protected LoggingContext.PreviousContext configureLoggingContext(String contextName) {
-        return LoggingContext.forConnector(CONNECTOR_TYPE, config.serverName(), contextName);
+        return LoggingContext.forConnector(""Postgres"", config.serverName(), contextName);
     }
 }",2016-12-27T12:44:32Z,53
"@@ -138,21 +138,22 @@ private void takeSnapshot(Consumer<SourceRecord> consumer) {
             statements.delete(0, statements.length());
     
             //next refresh the schema which will load all the tables taking the filters into account 
-            schema().refresh(connection, false);
+            PostgresSchema schema = schema();
+            schema.refresh(connection, false);
     
             logger.info(""Step 2: locking each of the database tables, waiting a maximum of '{}' seconds for each lock"",
                         lockTimeoutMillis / 1000d);
             statements.append(""SET lock_timeout = "").append(lockTimeoutMillis).append("";"").append(lineSeparator);
             // we're locking in SHARE UPDATE EXCLUSIVE MODE to avoid concurrent schema changes while we're taking the snapshot
             // this does not prevent writes to the table, but prevents changes to the table's schema....
-            schema().tables().forEach(tableId -> statements.append(""LOCK TABLE "")
-                                                .append(tableId.toString())
-                                                .append("" IN SHARE UPDATE EXCLUSIVE MODE;"")
-                                                .append(lineSeparator));
+            schema.tables().forEach(tableId -> statements.append(""LOCK TABLE "")
+                                                         .append(tableId.toString())
+                                                         .append("" IN SHARE UPDATE EXCLUSIVE MODE;"")
+                                                         .append(lineSeparator));
             connection.executeWithoutCommitting(statements.toString());
     
             //now that we have the locks, refresh the schema
-            schema().refresh(connection, false);
+            schema.refresh(connection, false);
     
             // get the current position in the log, from which we'll continue streaming once the snapshot it finished
             // If rows are being inserted while we're doing the snapshot, the xlog pos should increase and so when 
@@ -166,11 +167,14 @@ private void takeSnapshot(Consumer<SourceRecord> consumer) {
             sourceInfo.update(xlogStart, clock().currentTimeInMicros(), txId);
     
             logger.info(""Step 3: reading and exporting the contents of each table"");
-            AtomicInteger counter = new AtomicInteger(0);
             AtomicInteger rowsCounter = new AtomicInteger(0);
-            schema().tables().forEach(tableId -> {
+            schema.tables().forEach(tableId -> {
+                if (schema.isFilteredOut(tableId)) {
+                    logger.info(""\t table '{}' is filtered out, ignoring"", tableId);
+                    return;
+                }
                 long exportStart = clock().currentTimeInMillis();
-                logger.info(""Step 3.{}: exporting data from table '{}'"", counter.incrementAndGet(), tableId);
+                logger.info(""\t exporting data from table '{}'"", tableId);
                 try {
                     connection.query(""SELECT * FROM "" + tableId, 
                                      this::readTableStatement, 
@@ -189,8 +193,8 @@ private void takeSnapshot(Consumer<SourceRecord> consumer) {
     
             // process and send the last record after marking it as such
             logger.info(""Step 5: sending the last snapshot record"");
+            SourceRecord currentRecord = this.currentRecord.get();
             if (currentRecord != null) {  
-                SourceRecord currentRecord = this.currentRecord.get();
                 sourceInfo.markLastSnapshotRecord();
                 this.currentRecord.set(new SourceRecord(currentRecord.sourcePartition(), sourceInfo.offset(),
                                                         currentRecord.topic(), currentRecord.kafkaPartition(),
@@ -252,15 +256,14 @@ private Object valueForColumn(ResultSet rs, int colIdx, ResultSetMetaData metaDa
                 default:
                     return rs.getObject(colIdx);
             }
-        
         } catch (SQLException e) {
             // not a known type
             return rs.getObject(colIdx);
         }
     }
     
     protected void generateReadRecord(TableId tableId, Object[] rowData) {
-        if (rowData == null || rowData.length == 0) {
+        if (rowData.length == 0) {
             return;
         }
         TableSchema tableSchema = schema().schemaFor(tableId);",2016-12-27T12:44:32Z,54
"@@ -33,7 +33,6 @@
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.relational.TableSchema;
-import io.debezium.relational.Tables;
 import io.debezium.util.LoggingContext;
 
 /**
@@ -62,13 +61,10 @@ public RecordsStreamProducer(PostgresTaskContext taskContext,
         super(taskContext, sourceInfo);
         this.executorService = Executors.newSingleThreadExecutor(runnable -> new Thread(runnable, CONTEXT_NAME + ""-thread""));
         this.replicationStream = new AtomicReference<>();
-        LoggingContext.PreviousContext previousContext = taskContext.configureLoggingContext(CONTEXT_NAME);
         try {
             this.replicationConnection = taskContext.createReplicationConnection();
         } catch (SQLException e) {
             throw new ConnectException(e);
-        } finally {
-            previousContext.restore();
         }
     }
     
@@ -126,7 +122,7 @@ protected synchronized void commit()  {
         try {
             ReplicationStream replicationStream = this.replicationStream.get();
             if (replicationStream != null) {
-                // tell the server the point up until we've processed data, so it can be free to recycle WAL segments
+                // tell the server the point up to which we've processed data, so it can be free to recycle WAL segments
                 logger.debug(""flushing offsets to server..."");
                 replicationStream.flushLSN();
             } else {
@@ -207,7 +203,7 @@ private void process(PgProto.RowMessage message, Long lsn, Consumer<SourceRecord
                 break;
             }
             default: {
-                throw new IllegalArgumentException(""unknown message operation: "" + operation);
+               logger.warn(""unknown message operation: "" + operation);
             }
         }
     }
@@ -387,26 +383,25 @@ private boolean schemaChanged(List<PgProto.DatumMessage> messageList, Table tabl
     }
     
     private TableSchema tableSchemaFor(TableId tableId) throws SQLException {
-        TableSchema tableSchema = schema().schemaFor(tableId);
+        PostgresSchema schema = schema();
+        if (schema.isFilteredOut(tableId)) {
+            logger.debug(""table '{}' is filtered out, ignoring"", tableId);
+            return null;
+        }
+        TableSchema tableSchema = schema.schemaFor(tableId);
         if (tableSchema != null) {
             return tableSchema;
         }
-        Tables.TableNameFilter tableNameFilter = schema().filters().tableNameFilter();
-        if (tableNameFilter.matches(tableId.catalog(), tableId.schema(), tableId.table())) {
-            // we don't have a schema registered for this table, even though the filters would allow it...
-            // which means that is a newly created table; so refresh our schema to get the definition for this table
-            schema().refresh(taskContext.createConnection(), tableId);
-            tableSchema = schema().schemaFor(tableId);
-            if (tableSchema == null) {
-                logger.warn(""cannot load schema for table '{}'"", tableId);
-                return null;
-            } else {
-                logger.debug(""refreshed DB schema to include table '{}'"", tableId);
-                return tableSchema;
-            }
-        } else {
-            logger.debug(""ignoring message for table '{}' because it has been filtered out"", tableId);
+        // we don't have a schema registered for this table, even though the filters would allow it...
+        // which means that is a newly created table; so refresh our schema to get the definition for this table
+        schema.refresh(taskContext.createConnection(), tableId);
+        tableSchema = schema.schemaFor(tableId);
+        if (tableSchema == null) {
+            logger.warn(""cannot load schema for table '{}'"", tableId);
             return null;
+        } else {
+            logger.debug(""refreshed DB schema to include table '{}'"", tableId);
+            return tableSchema;
         }
     }
     ",2016-12-27T12:44:32Z,46
"@@ -204,7 +204,7 @@ public ServerInfo serverInfo() throws SQLException {
         if (username != null) {
             query(""SELECT oid, rolname, rolsuper, rolinherit, rolcreaterole, rolcreatedb, rolcanlogin, rolreplication FROM pg_roles "" +
                   ""WHERE pg_has_role('"" + username +""', oid, 'member')"", rs -> {
-                if (rs.next()) {
+                while (rs.next()) {
                     String roleInfo = ""superuser: "" + rs.getBoolean(3) + "", replication: "" + rs.getBoolean(8) + 
                                       "", inherit: "" + rs.getBoolean(4) + "", create role: "" + rs.getBoolean(5) +
                                       "", create db: "" + rs.getBoolean(6) + "", can log in: "" + rs.getBoolean(7);
@@ -226,7 +226,7 @@ private static void validateServerVersion(Statement statement) throws SQLExcepti
         int majorVersion = metaData.getDatabaseMajorVersion();
         int minorVersion = metaData.getDriverMinorVersion();
         if (majorVersion < 9 || (majorVersion == 9 && minorVersion < 4)) {
-            throw new IllegalStateException(""Cannot connect to a version of Postgres lower than 9.4"");
+            throw new SQLException(""Cannot connect to a version of Postgres lower than 9.4"");
         }
     }
     ",2016-12-27T12:44:32Z,55
"@@ -90,7 +90,7 @@ public String toString() {
                                             .map(entry -> ""\trole '"" + entry.getKey() + ""' ["" + entry.getValue() + ""]"")
                                             .collect(Collectors.joining(lineSeparator));
     
-        return ""User '"" + username + ""' connected to database '"" + database + ""' on "" + server + "" with:"" + lineSeparator + roles;
+        return ""user '"" + username + ""' connected to database '"" + database + ""' on "" + server + "" with roles:"" + lineSeparator + roles;
     }
     
     /**",2016-12-27T12:44:32Z,56
"@@ -12,17 +12,22 @@
 import static io.debezium.connector.postgresql.PostgresConnectorConfig.SnapshotMode.NEVER;
 import static io.debezium.connector.postgresql.TestHelper.PK_FIELD;
 import static org.fest.assertions.Assertions.assertThat;
+import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
 
 import java.sql.SQLException;
 import java.util.List;
+import java.util.concurrent.CompletableFuture;
 import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.Predicate;
 import java.util.stream.IntStream;
 import org.apache.kafka.common.config.Config;
 import org.apache.kafka.common.config.ConfigDef;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.ConnectException;
+import org.apache.kafka.connect.errors.DataException;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.junit.Before;
 import org.junit.BeforeClass;
@@ -32,10 +37,12 @@
 import io.debezium.config.Configuration;
 import io.debezium.config.Field;
 import io.debezium.connector.postgresql.connection.ReplicationConnection;
+import io.debezium.data.Envelope;
 import io.debezium.data.VerifyRecord;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.embedded.EmbeddedEngine;
-import io.debezium.jdbc.JdbcConnectionException;
+import io.debezium.junit.SkipLongRunning;
+import io.debezium.util.Strings;
 
 /**
  * Integration test for {@link PostgresConnector} using an {@link io.debezium.embedded.EmbeddedEngine} 
@@ -87,6 +94,14 @@ public void shouldNotStartWithInvalidConfiguration() throws Exception {
         assertConnectorNotRunning();            
     }
     
+    @Test
+    public void shouldValidateMinimalConfiguration() throws Exception {
+        Configuration config = TestHelper.defaultConfig().build();
+        Config validateConfig = new PostgresConnector().validate(config.asMap());
+        validateConfig.configValues().forEach(configValue -> assertTrue(""Unexpected error for: "" + configValue.name(), 
+                                                                        configValue.errorMessages().isEmpty()));
+    }
+    
     @Test
     public void shouldValidateConfiguration() throws Exception {
         // use an empty configuration which should be invalid because of the lack of DB connection details
@@ -137,10 +152,12 @@ public void shouldSupportSSLParameters() throws Exception {
         Configuration config = TestHelper.defaultConfig().with(PostgresConnectorConfig.SSL_MODE,  
                                                                PostgresConnectorConfig.SecureConnectionMode.REQUIRED).build();
         start(PostgresConnector.class, config, (success, msg, error) -> {
+            // we expect the task to fail at startup when we're printing the server info
             assertThat(success).isFalse();
-            assertThat(error).isInstanceOf(JdbcConnectionException.class);
-            JdbcConnectionException jdbcException = (JdbcConnectionException)error;
-            assertThat(PSQLState.CONNECTION_UNABLE_TO_CONNECT).isEqualTo(new PSQLState(jdbcException.getSqlState()));
+            assertThat(error).isInstanceOf(ConnectException.class);
+            Throwable cause = error.getCause();
+            assertThat(cause).isInstanceOf(SQLException.class);
+            assertThat(PSQLState.CONNECTION_UNABLE_TO_CONNECT).isEqualTo(new PSQLState(((SQLException)cause).getSQLState()));
         });
         assertConnectorNotRunning();
     }
@@ -247,7 +264,6 @@ public void shouldResumeSnapshotIfFailingMidstream() throws Exception {
         CountDownLatch latch = new CountDownLatch(1);
         String setupStmt = SETUP_TABLES_STMT + INSERT_STMT;
         TestHelper.execute(setupStmt);
-        // use 1 ms polling interval to ensure we get the exception during the snapshot process
         Configuration.Builder configBuilder = TestHelper.defaultConfig()
                                                         .with(PostgresConnectorConfig.SNAPSHOT_MODE, INITIAL.getValue())
                                                         .with(PostgresConnectorConfig.DROP_SLOT_ON_STOP, Boolean.FALSE);
@@ -284,6 +300,136 @@ public void shouldResumeSnapshotIfFailingMidstream() throws Exception {
         assertRecordsAfterInsert(2, 3, 3);
     }
     
+    @Test
+    public void shouldTakeBlacklistFiltersIntoAccount() throws Exception {
+        String setupStmt = SETUP_TABLES_STMT +
+                           ""CREATE TABLE s1.b (pk SERIAL, aa integer, bb integer, PRIMARY KEY(pk));"" +
+                           ""ALTER TABLE s1.a ADD COLUMN bb integer;"" +
+                           ""INSERT INTO s1.a (aa, bb) VALUES (2, 2);"" +
+                           ""INSERT INTO s1.a (aa, bb) VALUES (3, 3);"" +
+                           ""INSERT INTO s1.b (aa, bb) VALUES (4, 4);"" +
+                           ""INSERT INTO s2.a (aa) VALUES (5);"";
+        TestHelper.execute(setupStmt);
+        Configuration.Builder configBuilder = TestHelper.defaultConfig()
+                                                        .with(PostgresConnectorConfig.SNAPSHOT_MODE, INITIAL.getValue())
+                                                        .with(PostgresConnectorConfig.DROP_SLOT_ON_STOP, Boolean.TRUE)
+                                                        .with(PostgresConnectorConfig.SCHEMA_BLACKLIST, ""s2"")
+                                                        .with(PostgresConnectorConfig.TABLE_BLACKLIST, "".+b"")
+                                                        .with(PostgresConnectorConfig.COLUMN_BLACKLIST, "".+bb"");
+    
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+    
+        //check the records from the snapshot take the filters into account
+        SourceRecords actualRecords = consumeRecordsByTopic(4); //3 records in s1.a and 1 in s1.b
+        
+        assertThat(actualRecords.recordsForTopic(""s2.a"")).isNullOrEmpty();
+        assertThat(actualRecords.recordsForTopic(""s1.b"")).isNullOrEmpty();
+        List<SourceRecord> recordsForS1a = actualRecords.recordsForTopic(""s1.a"");
+        assertThat(recordsForS1a.size()).isEqualTo(3);
+        AtomicInteger pkValue = new AtomicInteger(1);
+        recordsForS1a.forEach(record -> {
+            VerifyRecord.isValidRead(record, PK_FIELD, pkValue.getAndIncrement());
+            assertFieldAbsent(record, ""bb"");
+        });
+        
+        
+        // insert some more records and verify the filtering behavior
+        String insertStmt =  ""INSERT INTO s1.b (aa, bb) VALUES (6, 6);"" +
+                             ""INSERT INTO s2.a (aa) VALUES (7);"";
+        TestHelper.execute(insertStmt);
+        assertNoRecordsToConsume();
+    }
+    
+    private void assertFieldAbsent(SourceRecord record, String fieldName) {
+        Struct value = (Struct) ((Struct) record.value()).get(Envelope.FieldName.AFTER);
+        try {
+            value.get(fieldName);
+            fail(""field should not be present"");
+        } catch (DataException e) {
+            //expected
+        }
+    }
+    
+    @Test
+    @SkipLongRunning(""performance"")
+    public void testStreamingPerformance() throws Exception {
+        TestHelper.dropAllSchemas();
+        TestHelper.executeDDL(""postgres_create_tables.ddl"");        
+        Configuration.Builder configBuilder = TestHelper.defaultConfig()
+                                                        .with(PostgresConnectorConfig.SNAPSHOT_MODE, NEVER.getValue())
+                                                        .with(PostgresConnectorConfig.DROP_SLOT_ON_STOP, Boolean.TRUE);
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+        final long recordsCount = 1000000;
+        final int batchSize = 1000;
+        
+        batchInsertRecords(recordsCount, batchSize);
+        CompletableFuture.runAsync(() -> consumeRecords(recordsCount))
+                         .exceptionally(throwable -> {
+                             throw new RuntimeException(throwable);
+                         }).get();
+    }
+    
+    private void consumeRecords(long recordsCount) {
+        int totalConsumedRecords = 0;
+        long start = System.currentTimeMillis();
+        while (totalConsumedRecords < recordsCount) {
+            int consumed = super.consumeAvailableRecords(record -> {});
+            if (consumed > 0) {
+                totalConsumedRecords += consumed;
+                System.out.println(""consumed "" + totalConsumedRecords + "" records"");
+            }
+        }
+        System.out.println(""total duration to ingest '"" + recordsCount + ""' records: "" + 
+                           Strings.duration(System.currentTimeMillis() - start));
+    }
+    
+    @Test
+    @SkipLongRunning(""performance"")
+    public void testSnapshotPerformance() throws Exception {
+        TestHelper.dropAllSchemas();
+        TestHelper.executeDDL(""postgres_create_tables.ddl"");
+        Configuration.Builder configBuilder = TestHelper.defaultConfig()
+                                                        .with(PostgresConnectorConfig.SNAPSHOT_MODE, INITIAL_ONLY.getValue())
+                                                        .with(PostgresConnectorConfig.DROP_SLOT_ON_STOP, Boolean.TRUE);
+        final long recordsCount = 1000000;
+        final int batchSize = 1000;
+    
+        batchInsertRecords(recordsCount, batchSize).get();
+
+        // start the connector only after we've finished inserting all the records
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+        
+        CompletableFuture.runAsync(() -> consumeRecords(recordsCount))
+                         .exceptionally(throwable -> {
+                             throw new RuntimeException(throwable);
+                         }).get();
+    }
+    
+    private CompletableFuture<Void> batchInsertRecords(long recordsCount, int batchSize) {
+        String insertStmt = ""INSERT INTO text_table(j, jb, x, u) "" +
+                            ""VALUES ('{\""bar\"": \""baz\""}'::json, '{\""bar\"": \""baz\""}'::jsonb, "" +
+                            ""'<foo>bar</foo><foo>bar</foo>'::xml, 'a0eebc99-9c0b-4ef8-bb6d-6bb9bd380a11'::UUID);"";
+        return CompletableFuture.runAsync(() -> {
+            StringBuilder stmtBuilder = new StringBuilder();
+            for (int i = 0; i < recordsCount; i++) {
+                stmtBuilder.append(insertStmt).append(System.lineSeparator());
+                if (i > 0  && i % batchSize == 0) {
+                    System.out.println(""inserting batch ["" + (i - batchSize) + "","" + i + ""]"");
+                    TestHelper.execute(stmtBuilder.toString());
+                    stmtBuilder.delete(0, stmtBuilder.length());
+                }
+            }
+            System.out.println(""inserting batch ["" + (recordsCount - batchSize) + "","" + recordsCount + ""]"");
+            TestHelper.execute(stmtBuilder.toString());
+            stmtBuilder.delete(0, stmtBuilder.length());
+        }).exceptionally(throwable -> {
+            throw new RuntimeException(throwable);
+        });
+    }
+    
     private Predicate<SourceRecord> stopOnPKPredicate(int pkValue) {
         return record -> {
             Struct key = (Struct) record.key();",2016-12-27T12:44:32Z,45
"@@ -120,7 +120,7 @@ protected static Configuration.Builder defaultConfig() {
     
     protected static void executeDDL(String ddlFile) throws Exception {
         URL ddlTestFile = TestHelper.class.getClassLoader().getResource(ddlFile);
-        assertNotNull(""Cannot locate postgres_create_tables.ddl"", ddlTestFile);
+        assertNotNull(""Cannot locate "" + ddlFile, ddlTestFile);
         String statements = Files.readAllLines(Paths.get(ddlTestFile.toURI()))
                                  .stream()
                                  .collect(Collectors.joining(System.lineSeparator()));",2016-12-27T12:44:32Z,57
"@@ -91,8 +91,9 @@ public void shouldDropReplicationSlot() throws Exception {
         }
         // create a new replication slot via a replication connection
         try (ReplicationConnection connection = TestHelper.createForReplication(""test"", false)) {
-            //nothing
-        };
+            assertTrue(connection.isConnected());
+        }
+        // drop the slot from the previous connection
         try (PostgresConnection connection = TestHelper.create()) {
             // try to drop the previous slot
             assertTrue(connection.dropReplicationSlot(""test""));",2016-12-27T12:44:32Z,58
"@@ -6,7 +6,6 @@
 package io.debezium.junit;
 
 import java.lang.annotation.Annotation;
-
 import org.junit.rules.TestRule;
 import org.junit.runner.Description;
 import org.junit.runners.model.Statement;
@@ -25,8 +24,8 @@ public Statement apply( Statement base,
                             Description description ) {
         SkipLongRunning skipLongRunningAnnotation = hasAnnotation(description, SkipLongRunning.class);
         if (skipLongRunningAnnotation != null) {
-            boolean skipLongRunning = Boolean.valueOf(System.getProperty(SkipLongRunning.SKIP_LONG_RUNNING_PROPERTY));
-            if (skipLongRunning) {
+            String skipLongRunning = System.getProperty(SkipLongRunning.SKIP_LONG_RUNNING_PROPERTY);
+            if (skipLongRunning == null || Boolean.valueOf(skipLongRunning)) {
                 return emptyStatement(skipLongRunningAnnotation.value(), description);
             }
         }",2016-12-27T12:44:32Z,59
"@@ -5,6 +5,7 @@
  */
 package io.debezium.embedded;
 
+import static org.fest.assertions.Assertions.assertThat;
 import static org.junit.Assert.fail;
 
 import java.math.BigDecimal;
@@ -26,7 +27,6 @@
 import java.util.concurrent.TimeUnit;
 import java.util.function.Consumer;
 import java.util.function.Predicate;
-
 import org.apache.kafka.common.config.Config;
 import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.connect.data.Field;
@@ -46,18 +46,19 @@
 import org.fest.assertions.Delta;
 import org.junit.After;
 import org.junit.Before;
+import org.junit.Rule;
+import org.junit.rules.TestRule;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import static org.fest.assertions.Assertions.assertThat;
-
 import io.debezium.config.Configuration;
 import io.debezium.data.SchemaUtil;
 import io.debezium.data.VerifyRecord;
 import io.debezium.embedded.EmbeddedEngine.CompletionCallback;
 import io.debezium.embedded.EmbeddedEngine.ConnectorCallback;
 import io.debezium.embedded.EmbeddedEngine.EmbeddedConfig;
 import io.debezium.function.BooleanConsumer;
+import io.debezium.junit.SkipTestRule;
 import io.debezium.relational.history.HistoryRecord;
 import io.debezium.util.LoggingContext;
 import io.debezium.util.Testing;
@@ -74,7 +75,10 @@
  * @author Randall Hauch
  */
 public abstract class AbstractConnectorTest implements Testing {
-
+    
+    @Rule
+    public TestRule skipTestRule = new SkipTestRule();
+    
     protected static final Path OFFSET_STORE_PATH = Testing.Files.createTestingPath(""file-connector-offsets.txt"").toAbsolutePath();
 
     private ExecutorService executor;",2016-12-27T12:44:32Z,35
"@@ -67,6 +67,9 @@
         <version.mysql.binlog>0.5.1</version.mysql.binlog>
         <version.mongo.server>3.2.6</version.mongo.server>
         <version.mongo.driver>3.2.2</version.mongo.driver>
+        
+        <!-- Connectors -->
+        <version.com.google.protobuf>2.6.1</version.com.google.protobuf>
 
         <!-- Testing -->
         <version.junit>4.12</version.junit>
@@ -81,6 +84,7 @@
         <version.google.formatter.plugin>0.3.1</version.google.formatter.plugin>
         <version.docker.maven.plugin>0.18.1</version.docker.maven.plugin>
         <version.staging.plugin>1.6.3</version.staging.plugin>
+        <version.protoc.maven.plugin>3.0.0.1</version.protoc.maven.plugin>
 
         <!-- Dockerfiles -->
         <docker.maintainer>Debezium community</docker.maintainer>
@@ -197,12 +201,23 @@
                 <scope>test</scope>
             </dependency>
 
-            <!-- PostgreSQL JDBC driver -->
+            <!-- PostgreSQL connector -->
+            <!--
+                //TODO author=Horia Chiorean date=21/11/2016 description=ignored ATM until the streaming replication support
+                is incoroporated
+             
             <dependency>
                 <groupId>org.postgresql</groupId>
                 <artifactId>postgresql</artifactId>
                 <version>${version.postgresql.driver}</version>
-            </dependency>
+            </dependency>-->
+           
+            <!--Make sure this version is compatible with the Protbuf-C version used on the server -->
+            <dependency>
+                <groupId>com.google.protobuf</groupId>
+                <artifactId>protobuf-java</artifactId>
+                <version>${version.com.google.protobuf}</version>
+            </dependency> 
 
             <!-- MySQL JDBC Driver and Binlog reader -->
             <dependency>
@@ -400,6 +415,11 @@
                     <artifactId>docker-maven-plugin</artifactId>
                     <version>${version.docker.maven.plugin}</version>
                 </plugin>
+                <plugin>
+                    <groupId>com.github.os72</groupId>
+                    <artifactId>protoc-jar-maven-plugin</artifactId>
+                    <version>${version.protoc.maven.plugin}</version>
+                </plugin>
             </plugins>
         </pluginManagement>
         <plugins>",2016-12-27T12:44:32Z,60
"@@ -11,6 +11,7 @@
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
@@ -146,6 +147,33 @@ public String knownGtidSet() {
         return result != null ? result : """";
     }
 
+    /**
+     * Determine if the current user has the named privilege. Note that if the user has the ""ALL"" privilege this method
+     * returns {@code true}.
+     * 
+     * @param grantName the name of the MySQL privilege; may not be null
+     * @return {@code true} if the user has the named privilege, or {@code false} otherwise
+     */
+    public boolean userHasPrivileges(String grantName) {
+        AtomicBoolean result = new AtomicBoolean(false);
+        try {
+            jdbc.query(""SHOW GRANTS FOR CURRENT_USER"", rs -> {
+                if (rs.next()) {
+                    String grants = rs.getString(1);
+                    logger.debug(grants);
+                    if (grants == null) return;
+                    grants = grants.toUpperCase();
+                    if (grants.contains(""ALL"") || grants.contains(grantName.toUpperCase())) {
+                        result.set(true);
+                    }
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at privileges for current user: "", e);
+        }
+        return result.get();
+    }
+
     protected String connectionString() {
         return jdbc.connectionString(MYSQL_CONNECTION_URL);
     }",2017-02-06T19:56:55Z,61
"@@ -20,6 +20,7 @@
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
 
+import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
 
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
@@ -139,6 +140,7 @@ protected void execute() {
         logServerInformation(mysql);
         boolean isLocked = false;
         boolean isTxnStarted = false;
+        boolean tableLocks = false;
         try {
             metrics.startSnapshot();
 
@@ -160,13 +162,13 @@ protected void execute() {
             mysql.setAutoCommit(false);
             sql.set(""SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"");
             mysql.execute(sql.get());
-            metrics.globalLockAcquired();
 
             // Generate the DDL statements that set the charset-related system variables ...
             Map<String, String> systemVariables = context.readMySqlCharsetSystemVariables(sql);
             String setSystemVariablesStatement = context.setStatementFor(systemVariables);
             AtomicBoolean interrupted = new AtomicBoolean(false);
             long lockAcquired = 0L;
+            int step = 1;
 
             try {
                 // ------
@@ -180,59 +182,40 @@ protected void execute() {
                 mysql.execute(sql.get());
                 isTxnStarted = true;
 
-                // ------
-                // STEP 2
-                // ------
+                // ------------------------------------
+                // LOCK TABLES and READ BINLOG POSITION
+                // ------------------------------------
                 // Obtain read lock on all tables. This statement closes all open tables and locks all tables
                 // for all databases with a global read lock, and it prevents ALL updates while we have this lock.
                 // It also ensures that everything we do while we have this lock will be consistent.
                 if (!isRunning()) return;
-                lockAcquired = clock.currentTimeInMillis();
-                logger.info(""Step 2: flush and obtain global read lock (preventing writes to database)"");
-                sql.set(""FLUSH TABLES WITH READ LOCK"");
-                mysql.execute(sql.get());
-                isLocked = true;
+                try {
+                    logger.info(""Step 2: flush and obtain global read lock to prevent writes to database"");
+                    sql.set(""FLUSH TABLES WITH READ LOCK"");
+                    mysql.execute(sql.get());
+                    lockAcquired = clock.currentTimeInMillis();
+                    metrics.globalLockAcquired();
+                    isLocked = true;
+                } catch (SQLException e) {
+                    logger.info(""Step 2: unable to flush and acquire global read lock, will use table read locks after reading table names"");
+                    // Continue anyway, since RDS (among others) don't allow setting a global lock
+                    assert !isLocked;
+                }
 
-                // ------
-                // STEP 3
-                // ------
-                // Obtain the binlog position and update the SourceInfo in the context. This means that all source records
-                // generated
-                // as part of the snapshot will contain the binlog position of the snapshot.
                 if (!isRunning()) return;
-                logger.info(""Step 3: read binlog position of MySQL master"");
-                String showMasterStmt = ""SHOW MASTER STATUS"";
-                sql.set(showMasterStmt);
-                mysql.query(sql.get(), rs -> {
-                    if (rs.next()) {
-                        String binlogFilename = rs.getString(1);
-                        long binlogPosition = rs.getLong(2);
-                        source.setBinlogStartPoint(binlogFilename, binlogPosition);
-                        if (rs.getMetaData().getColumnCount() > 4) {
-                            // This column exists only in MySQL 5.6.5 or later ...
-                            String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
-                            source.setCompletedGtidSet(gtidSet);
-                            logger.info(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
-                                        gtidSet);
-                        } else {
-                            logger.info(""\t using binlog '{}' at position '{}'"", binlogFilename, binlogPosition);
-                        }
-                        source.startSnapshot();
-                    } else {
-                        throw new IllegalStateException(""Cannot read the binlog filename and position via '"" + showMasterStmt
-                                + ""'. Make sure your server is correctly configured"");
-                    }
-                });
-
-                // From this point forward, all source records produced by this connector will have an offset that includes a
-                // ""snapshot"" field (with value of ""true"").
+                step = 3;
+                if (isLocked) {
+                    // Obtain the binlog position and update the SourceInfo in the context. This means that all source records
+                    // generated as part of the snapshot will contain the binlog position of the snapshot.
+                    readBinlogPosition(step++, source, mysql, sql);
+                }
 
-                // ------
-                // STEP 4
-                // ------
+                // -------------------
+                // READ DATABASE NAMES
+                // -------------------
                 // Get the list of databases ...
                 if (!isRunning()) return;
-                logger.info(""Step 4: read list of available databases"");
+                logger.info(""Step {}: read list of available databases"", step++);
                 final List<String> databaseNames = new ArrayList<>();
                 sql.set(""SHOW DATABASES"");
                 mysql.query(sql.get(), rs -> {
@@ -242,14 +225,14 @@ protected void execute() {
                 });
                 logger.info(""\t list of available databases is: {}"", databaseNames);
 
-                // ------
-                // STEP 5
-                // ------
+                // ----------------
+                // READ TABLE NAMES
+                // ----------------
                 // Get the list of table IDs for each database. We can't use a prepared statement with MySQL, so we have to
                 // build the SQL statement each time. Although in other cases this might lead to SQL injection, in our case
                 // we are reading the database names from the database and not taking them from the user ...
                 if (!isRunning()) return;
-                logger.info(""Step 5: read list of available tables in each database"");
+                logger.info(""Step {}: read list of available tables in each database"", step++);
                 List<TableId> tableIds = new ArrayList<>();
                 final Map<String, List<TableId>> tableIdsByDbName = new HashMap<>();
                 final Set<String> readableDatabaseNames = new HashSet<>();
@@ -278,12 +261,42 @@ protected void execute() {
                 }
                 logger.info(""\t snapshot continuing with databases: {}"", readableDatabaseNames);
 
+                if (!isLocked) {
+                    // ------------------------------------
+                    // LOCK TABLES and READ BINLOG POSITION
+                    // ------------------------------------
+                    // We were not able to acquire the global read lock, so instead we have to obtain a read lock on each table.
+                    // This requires different privileges than normal, and also means we can't unlock the tables without
+                    // implicitly committing our transaction ...
+                    if (!context.userHasPrivileges(""LOCK TABLES"")) {
+                        // We don't have the right privileges
+                        throw new ConnectException(""User does not have the 'LOCK TABLES' privilege required to obtain a ""
+                                + ""consistent snapshot by preventing concurrent writes to tables."");
+                    }
+                    // We have the required privileges, so try to lock all of the tables we're interested in ...
+                    logger.info(""Step {}: flush and obtain read lock for {} tables (preventing writes)"", step++, tableIds.size());
+                    for (TableId tableId : tableIds) {
+                        sql.set(""FLUSH TABLES "" + quote(tableId) + "" WITH READ LOCK"");
+                        mysql.execute(sql.get());
+                    }
+                    lockAcquired = clock.currentTimeInMillis();
+                    metrics.globalLockAcquired();
+                    isLocked = true;
+                    tableLocks = true;
+
+                    // Our tables are locked, so read the binlog position ...
+                    readBinlogPosition(step++, source, mysql, sql);
+                }
+
+                // From this point forward, all source records produced by this connector will have an offset that includes a
+                // ""snapshot"" field (with value of ""true"").
+
                 // ------
                 // STEP 6
                 // ------
                 // Transform the current schema so that it reflects the *current* state of the MySQL server's contents.
                 // First, get the DROP TABLE and CREATE TABLE statement (with keys and constraint definitions) for our tables ...
-                logger.info(""Step 6: generating DROP and CREATE statements to reflect current database schemas:"");
+                logger.info(""Step {}: generating DROP and CREATE statements to reflect current database schemas:"", step++);
                 schema.applyDdl(source, null, setSystemVariablesStatement, this::enqueueSchemaChanges);
 
                 // Add DROP TABLE statements for all tables that we knew about AND those tables found in the databases ...
@@ -327,17 +340,28 @@ protected void execute() {
                 // STEP 7
                 // ------
                 if (minimalBlocking && isLocked) {
-                    // We are doing minimal blocking, then we should release the read lock now. All subsequent SELECT
-                    // should still use the MVCC snapshot obtained when we started our transaction (since we started it
-                    // ""...with consistent snapshot""). So, since we're only doing very simple SELECT without WHERE predicates,
-                    // we can release the lock now ...
-                    logger.info(""Step 7: releasing global read lock to enable MySQL writes"");
-                    sql.set(""UNLOCK TABLES"");
-                    mysql.execute(sql.get());
-                    isLocked = false;
-                    long lockReleased = clock.currentTimeInMillis();
-                    metrics.globalLockReleased();
-                    logger.info(""Step 7: blocked writes to MySQL for a total of {}"", Strings.duration(lockReleased - lockAcquired));
+                    if (tableLocks) {
+                        // We could not acquire a global read lock and instead had to obtain individual table-level read locks
+                        // using 'FLUSH TABLE <tableName> WITH READ LOCK'. However, if we were to do this, the 'UNLOCK TABLES'
+                        // would implicitly commit our active transaction, and this would break our consistent snapshot logic.
+                        // Therefore, we cannot unlock the tables here!
+                        // https://dev.mysql.com/doc/refman/5.7/en/flush.html
+                        logger.info(""Step {}: tables were locked explicitly, but to get a consistent snapshot we cannot ""
+                                + ""release the locks until we've read all tables."", step++);
+                    } else {
+                        // We are doing minimal blocking via a global read lock, so we should release the global read lock now.
+                        // All subsequent SELECT should still use the MVCC snapshot obtained when we started our transaction
+                        // (since we started it ""...with consistent snapshot""). So, since we're only doing very simple SELECT
+                        // without WHERE predicates, we can release the lock now ...
+                        logger.info(""Step {}: releasing global read lock to enable MySQL writes"", step);
+                        sql.set(""UNLOCK TABLES"");
+                        mysql.execute(sql.get());
+                        isLocked = false;
+                        long lockReleased = clock.currentTimeInMillis();
+                        metrics.globalLockReleased();
+                        logger.info(""Step {}: blocked writes to MySQL for a total of {}"", step++,
+                                    Strings.duration(lockReleased - lockAcquired));
+                    }
                 }
 
                 // ------
@@ -350,7 +374,7 @@ protected void execute() {
                     BufferedBlockingConsumer<SourceRecord> bufferedRecordQueue = BufferedBlockingConsumer.bufferLast(super::enqueueRecord);
 
                     // Dump all of the tables and generate source records ...
-                    logger.info(""Step 8: scanning contents of {} tables"", tableIds.size());
+                    logger.info(""Step {}: scanning contents of {} tables while still in transaction"", step, tableIds.size());
                     metrics.setTableCount(tableIds.size());
 
                     long startScan = clock.currentTimeInMillis();
@@ -395,9 +419,10 @@ protected void execute() {
 
                             // Scan the rows in the table ...
                             long start = clock.currentTimeInMillis();
-                            logger.info(""Step 8: - scanning table '{}' ({} of {} tables)"", tableId, ++counter, tableIds.size());
+                            logger.info(""Step {}: - scanning table '{}' ({} of {} tables)"", step, tableId, ++counter, tableIds.size());
                             sql.set(""SELECT * FROM "" + quote(tableId));
                             try {
+                                int stepNum = step;
                                 mysql.query(sql.get(), statementFactory, rs -> {
                                     long rowNum = 0;
                                     try {
@@ -418,21 +443,21 @@ protected void execute() {
                                             }
                                             if (rowNum % 10_000 == 0) {
                                                 long stop = clock.currentTimeInMillis();
-                                                logger.info(""Step 8: - {} of {} rows scanned from table '{}' after {}"",
-                                                            rowNum, rowCountStr, tableId, Strings.duration(stop - start));
+                                                logger.info(""Step {}: - {} of {} rows scanned from table '{}' after {}"",
+                                                            stepNum, rowNum, rowCountStr, tableId, Strings.duration(stop - start));
                                             }
                                         }
 
                                         totalRowCount.addAndGet(rowNum);
                                         if (isRunning()) {
                                             long stop = clock.currentTimeInMillis();
-                                            logger.info(""Step 8: - Completed scanning a total of {} rows from table '{}' after {}"",
-                                                        rowNum, tableId, Strings.duration(stop - start));
+                                            logger.info(""Step {}: - Completed scanning a total of {} rows from table '{}' after {}"",
+                                                        stepNum, rowNum, tableId, Strings.duration(stop - start));
                                         }
                                     } catch (InterruptedException e) {
                                         Thread.interrupted();
                                         // We were not able to finish all rows in all tables ...
-                                        logger.info(""Step 8: Stopping the snapshot due to thread interruption"");
+                                        logger.info(""Step {}: Stopping the snapshot due to thread interruption"", stepNum);
                                         interrupted.set(true);
                                     }
                                 });
@@ -454,41 +479,27 @@ protected void execute() {
                     long stop = clock.currentTimeInMillis();
                     try {
                         bufferedRecordQueue.flush(this::replaceOffset);
-                        logger.info(""Step 8: scanned {} rows in {} tables in {}"",
-                                    totalRowCount, tableIds.size(), Strings.duration(stop - startScan));
+                        logger.info(""Step {}: scanned {} rows in {} tables in {}"",
+                                    step, totalRowCount, tableIds.size(), Strings.duration(stop - startScan));
                     } catch (InterruptedException e) {
                         Thread.interrupted();
                         // We were not able to finish all rows in all tables ...
-                        logger.info(""Step 8: aborting the snapshot after {} rows in {} of {} tables {}"",
-                                    totalRowCount, completedCounter, tableIds.size(), Strings.duration(stop - startScan));
+                        logger.info(""Step {}: aborting the snapshot after {} rows in {} of {} tables {}"",
+                                    step, totalRowCount, completedCounter, tableIds.size(), Strings.duration(stop - startScan));
                         interrupted.set(true);
                     }
                 } else {
                     // source.markLastSnapshot(); Think we will not be needing this here it is used to mark last row entry?
-                    logger.info(""Step 8: encountered only schema based snapshot, skipping data snapshot"");
+                    logger.info(""Step {}: encountered only schema based snapshot, skipping data snapshot"", step);
                 }
+                step++;
             } finally {
                 // No matter what, we always want to do these steps if necessary ...
 
                 // ------
                 // STEP 9
                 // ------
-                // Release the read lock if we have not yet done so. Locks are not released when committing/rolling back ...
-                int step = 9;
-                if (isLocked) {
-                    logger.info(""Step {}: releasing global read lock to enable MySQL writes"", step++);
-                    sql.set(""UNLOCK TABLES"");
-                    mysql.execute(sql.get());
-                    isLocked = false;
-                    long lockReleased = clock.currentTimeInMillis();
-                    metrics.globalLockReleased();
-                    logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased - lockAcquired));
-                }
-
-                // -------
-                // STEP 10
-                // -------
-                // Either commit or roll back the transaction ...
+                // Either commit or roll back the transaction, BEFORE releasing the locks ...
                 if (isTxnStarted) {
                     if (interrupted.get() || !isRunning()) {
                         // We were interrupted or were stopped while reading the tables,
@@ -504,7 +515,29 @@ protected void execute() {
                     sql.set(""COMMIT"");
                     mysql.execute(sql.get());
                     metrics.completeSnapshot();
-                } else {}
+                }
+                
+                // -------
+                // STEP 10
+                // -------
+                // Release the read lock(s) if we have not yet done so. Locks are not released when committing/rolling back ...
+                if (isLocked) {
+                    if (tableLocks) {
+                        logger.info(""Step {}: releasing table read locks to enable MySQL writes"", step++);
+                    } else {
+                        logger.info(""Step {}: releasing global read lock to enable MySQL writes"", step++);
+                    }
+                    sql.set(""UNLOCK TABLES"");
+                    mysql.execute(sql.get());
+                    isLocked = false;
+                    long lockReleased = clock.currentTimeInMillis();
+                    metrics.globalLockReleased();
+                    if (tableLocks) {
+                        logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased - lockAcquired));
+                    } else {
+                        logger.info(""Writes to MySQL tables prevented for a total of {}"", Strings.duration(lockReleased - lockAcquired));
+                    }
+                }
             }
 
             if (!isRunning()) {
@@ -536,6 +569,32 @@ protected void execute() {
         }
     }
 
+    protected void readBinlogPosition(int step, SourceInfo source, JdbcConnection mysql, AtomicReference<String> sql) throws SQLException {
+        logger.info(""Step {}: read binlog position of MySQL master"", step);
+        String showMasterStmt = ""SHOW MASTER STATUS"";
+        sql.set(showMasterStmt);
+        mysql.query(sql.get(), rs -> {
+            if (rs.next()) {
+                String binlogFilename = rs.getString(1);
+                long binlogPosition = rs.getLong(2);
+                source.setBinlogStartPoint(binlogFilename, binlogPosition);
+                if (rs.getMetaData().getColumnCount() > 4) {
+                    // This column exists only in MySQL 5.6.5 or later ...
+                    String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                    source.setCompletedGtidSet(gtidSet);
+                    logger.info(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
+                                gtidSet);
+                } else {
+                    logger.info(""\t using binlog '{}' at position '{}'"", binlogFilename, binlogPosition);
+                }
+                source.startSnapshot();
+            } else {
+                throw new IllegalStateException(""Cannot read the binlog filename and position via '"" + showMasterStmt
+                        + ""'. Make sure your server is correctly configured"");
+            }
+        });
+    }
+
     protected String quote(String dbOrTableName) {
         return ""`"" + dbOrTableName + ""`"";
     }",2017-02-06T19:56:55Z,62
"@@ -138,16 +138,7 @@
                                 </log>
                                 <wait>
                                     <time>30000</time> <!-- 30 seconds max -->
-                                    <!--
-                                        The current debezium/postgres docker image restarts the server once when it initializes 
-                                        permissions, so we can't really wait for a log. Instead, we'll wait for the PG port
-                                        to become available
-                                    -->
-                                    <tcp>
-                                        <ports>
-                                            <port>5432</port>
-                                        </ports>
-                                    </tcp>
+                                    <log>PostgreSQL init process complete</log>
                                 </wait>
                             </run>
                         </image>",2017-01-13T22:44:30Z,50
"@@ -81,7 +81,7 @@
         <version.war.plugin>2.5</version.war.plugin>
         <version.codehaus.helper.plugin>1.8</version.codehaus.helper.plugin>
         <version.google.formatter.plugin>0.3.1</version.google.formatter.plugin>
-        <version.docker.maven.plugin>0.18.1</version.docker.maven.plugin>
+        <version.docker.maven.plugin>0.19.0</version.docker.maven.plugin>
         <version.staging.plugin>1.6.3</version.staging.plugin>
         <version.protoc.maven.plugin>3.0.0.1</version.protoc.maven.plugin>
 ",2017-01-13T22:44:30Z,60
"@@ -306,8 +306,13 @@ protected Object convertEnumToString(List<String> options, Column column, Field
 
             if (options != null) {
                 // The binlog will contain an int with the 1-based index of the option in the enum value ...
-                int index = ((Integer) data).intValue() - 1; // 'options' is 0-based
-                if (index < options.size()) {
+                int value = ((Integer)data).intValue();
+                if (value == 0) {
+                    // an invalid value was specified, which corresponds to the empty string '' and an index of 0
+                    return """";
+                }
+                int index = value - 1; // 'options' is 0-based
+                if (index < options.size() && index >= 0) {
                     return options.get(index);
                 }
             }",2017-03-16T18:55:24Z,47
"@@ -606,7 +606,7 @@ protected void parseCreateDefinition(Marker start, TableEditor table) {
             }
             parseIndexColumnNames(start);
             parseIndexOptions(start);
-        } else if (!quoted && tokens.canConsume(""FULLTEXT"", ""SPATIAL"")) {
+        } else if (!quoted && tokens.canConsumeAnyOf(""FULLTEXT"", ""SPATIAL"")) {
             tokens.canConsumeAnyOf(""INDEX"", ""KEY"");
             if (!tokens.matches('(')) {
                 tokens.consume(); // name of unique index ...",2017-02-10T21:49:20Z,49
"@@ -773,6 +773,33 @@ public void shouldParseButIgnoreCreateTriggerWithDefiner() {
         listener.forEach(this::printEvent);
     }
     
+    @FixFor(""DBZ-193"")
+    @Test
+    public void shouldParseFulltextKeyInCreateTable() {
+        parser.parse(readFile(""ddl/mysql-dbz-193.ddl""), tables);
+        Testing.print(tables);
+        assertThat(tables.size()).isEqualTo(1); // 1 table
+        assertThat(listener.total()).isEqualTo(1);
+        listener.forEach(this::printEvent);
+
+        Table t = tables.forTable(new TableId(null, null, ""roles""));
+        assertThat(t).isNotNull();
+        assertThat(t.columnNames()).containsExactly(""id"", ""name"", ""context"", ""organization_id"", ""client_id"", ""scope_action_ids"");
+        assertThat(t.primaryKeyColumnNames()).containsExactly(""id"");
+        assertColumn(t, ""id"", ""VARCHAR"", Types.VARCHAR, 32, -1, false, false, false);
+        assertColumn(t, ""name"", ""VARCHAR"", Types.VARCHAR, 100, -1, false, false, false);
+        assertColumn(t, ""context"", ""VARCHAR"", Types.VARCHAR, 20, -1, false, false, false);
+        assertColumn(t, ""organization_id"", ""INT"", Types.INTEGER, 11, -1, true, false, false);
+        assertColumn(t, ""client_id"", ""VARCHAR"", Types.VARCHAR, 32, -1, false, false, false);
+        assertColumn(t, ""scope_action_ids"", ""TEXT"", Types.VARCHAR, -1, -1, false, false, false);
+        assertThat(t.columnWithName(""id"").position()).isEqualTo(1);
+        assertThat(t.columnWithName(""name"").position()).isEqualTo(2);
+        assertThat(t.columnWithName(""context"").position()).isEqualTo(3);
+        assertThat(t.columnWithName(""organization_id"").position()).isEqualTo(4);
+        assertThat(t.columnWithName(""client_id"").position()).isEqualTo(5);
+        assertThat(t.columnWithName(""scope_action_ids"").position()).isEqualTo(6);
+    }
+    
     @Test
     public void shouldParseTicketMonsterLiquibaseStatements() {
         parser.parse(readLines(1, ""ddl/mysql-ticketmonster-liquibase.ddl""), tables);",2017-02-10T21:49:20Z,26
"@@ -0,0 +1,10 @@
+CREATE TABLE `roles` (
+`id` varchar(32) NOT NULL,
+`name` varchar(100) NOT NULL,
+`context` varchar(20) NOT NULL,
+`organization_id` int(11) DEFAULT NULL,
+`client_id` varchar(32) NOT NULL,
+`scope_action_ids` text NOT NULL,
+PRIMARY KEY (`id`),
+FULLTEXT KEY `scope_action_ids_idx` (`scope_action_ids`)
+) ENGINE=InnoDB DEFAULT CHARSET=utf8;",2017-02-10T21:49:20Z,63
"@@ -63,6 +63,7 @@ public class MySqlDdlParser extends DdlParser {
     private final MySqlSystemVariables systemVariables = new MySqlSystemVariables();
     private final ConcurrentMap<String, String> charsetNameForDatabase = new ConcurrentHashMap<>();
 
+    public static final char ENUM_AND_SET_DELIMINATOR = ',';
     /**
      * Create a new DDL parser for MySQL that does not include view definitions.
      */
@@ -655,21 +656,21 @@ protected Column parseCreateColumn(Marker start, TableEditor table, String colum
      * @param typeExpression the data type expression
      * @return the string containing the character options allowed by the {@code ENUM} or {@code SET}; never null
      */
-    public static String parseSetAndEnumOptions(String typeExpression) {
+    public static List<String> parseSetAndEnumOptions(String typeExpression) {
         Matcher matcher = ENUM_AND_SET_LITERALS.matcher(typeExpression);
+        List<String> options = new ArrayList<>();
         if (matcher.matches()) {
             String literals = matcher.group(2);
             Matcher optionMatcher = ENUM_AND_SET_OPTIONS.matcher(literals);
             StringBuilder sb = new StringBuilder();
             while (optionMatcher.find()) {
                 String option = optionMatcher.group(1);
                 if (option.length() > 0) {
-                    sb.append(option.charAt(0));
+                    options.add(option);
                 }
             }
-            return sb.toString();
         }
-        return """";
+        return options;
     }
 
     protected void parseColumnDefinition(Marker start, String columnName, TokenStream tokens, TableEditor table, ColumnEditor column,
@@ -692,8 +693,9 @@ protected void parseColumnDefinition(Marker start, String columnName, TokenStrea
         if (""ENUM"".equals(dataType.name())) {
             column.length(1);
         } else if (""SET"".equals(dataType.name())) {
-            String options = parseSetAndEnumOptions(dataType.expression());
-            column.length(Math.max(0, options.length() * 2 - 1)); // number of options + number of commas
+            List<String> options = parseSetAndEnumOptions(dataType.expression());
+            //After DBZ-132, it will always be comma seperated
+            column.length(Math.max(0, options.size() * 2 - 1)); // number of options + number of commas
         } else {
             if (dataType.length() > -1) column.length((int) dataType.length());
             if (dataType.scale() > -1) column.scale(dataType.scale());",2016-10-11T19:11:08Z,49
"@@ -12,6 +12,7 @@
 import java.sql.Types;
 import java.time.OffsetDateTime;
 import java.time.ZoneOffset;
+import java.util.List;
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
@@ -85,12 +86,32 @@ public SchemaBuilder schemaBuilder(Column column) {
             return Year.builder();
         }
         if (matches(typeName, ""ENUM"")) {
-            String commaSeparatedOptions = extractEnumAndSetOptions(column, true);
-            return io.debezium.data.Enum.builder(commaSeparatedOptions);
+            List<String> options = extractEnumAndSetOptions(column);
+            StringBuilder commaSeparatedOptions = new StringBuilder();
+            boolean first = true;
+            for (String value:options) {
+                if (first) {
+                    first = false;
+                } else {
+                    commaSeparatedOptions.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+                }
+                commaSeparatedOptions.append(value);
+            }
+            return io.debezium.data.Enum.builder(commaSeparatedOptions.toString());
         }
         if (matches(typeName, ""SET"")) {
-            String commaSeparatedOptions = extractEnumAndSetOptions(column, true);
-            return io.debezium.data.EnumSet.builder(commaSeparatedOptions);
+            List<String> options = extractEnumAndSetOptions(column);
+            StringBuilder commaSeparatedOptions = new StringBuilder();
+            boolean first = true;
+            for (String value:options) {
+                if (first) {
+                    first = false;
+                } else {
+                    commaSeparatedOptions.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+                }
+                commaSeparatedOptions.append(value);
+            }
+            return io.debezium.data.EnumSet.builder(commaSeparatedOptions.toString());
         }
         // Otherwise, let the base class handle it ...
         return super.schemaBuilder(column);
@@ -105,12 +126,12 @@ public ValueConverter converter(Column column, Field fieldDefn) {
         }
         if (matches(typeName, ""ENUM"")) {
             // Build up the character array based upon the column's type ...
-            String options = extractEnumAndSetOptions(column, false);
+            List<String> options = extractEnumAndSetOptions(column);
             return (data) -> convertEnumToString(options, column, fieldDefn, data);
         }
         if (matches(typeName, ""SET"")) {
             // Build up the character array based upon the column's type ...
-            String options = extractEnumAndSetOptions(column, false);
+            List<String> options = extractEnumAndSetOptions(column);
             return (data) -> convertSetToString(options, column, fieldDefn, data);
         }
         
@@ -240,7 +261,7 @@ protected Object convertYearToInt(Column column, Field fieldDefn, Object data) {
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
-    protected Object convertEnumToString(String options, Column column, Field fieldDefn, Object data) {
+    protected Object convertEnumToString(List<String> options, Column column, Field fieldDefn, Object data) {
         if (data == null) {
             data = fieldDefn.schema().defaultValue();
         }
@@ -253,10 +274,13 @@ protected Object convertEnumToString(String options, Column column, Field fieldD
             return data;
         }
         if (data instanceof Integer) {
-            // The binlog will contain an int with the 1-based index of the option in the enum value ...
-            int index = ((Integer) data).intValue() - 1; // 'options' is 0-based
-            if (index < options.length()) {
-                return options.substring(index, index + 1);
+
+            if (options != null) {
+                // The binlog will contain an int with the 1-based index of the option in the enum value ...
+                int index = ((Integer) data).intValue() - 1; // 'options' is 0-based
+                if (index < options.size()) {
+                    return options.get(index);
+                }
             }
             return null;
         }
@@ -275,7 +299,7 @@ protected Object convertEnumToString(String options, Column column, Field fieldD
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
-    protected Object convertSetToString(String options, Column column, Field fieldDefn, Object data) {
+    protected Object convertSetToString(List<String> options, Column column, Field fieldDefn, Object data) {
         if (data == null) {
             data = fieldDefn.schema().defaultValue();
         }
@@ -308,32 +332,25 @@ protected boolean matches(String upperCaseTypeName, String upperCaseMatch) {
         return upperCaseMatch.equals(upperCaseTypeName) || upperCaseTypeName.startsWith(upperCaseMatch + ""("");
     }
 
-    protected String extractEnumAndSetOptions(Column column, boolean commaSeparated) {
-        String options = MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
-        if (!commaSeparated) return options;
-        StringBuilder sb = new StringBuilder();
-        boolean first = true;
-        for (int i = 0; i != options.length(); ++i) {
-            if (first)
-                first = false;
-            else
-                sb.append(',');
-            sb.append(options.charAt(i));
-        }
-        return sb.toString();
+    protected List<String> extractEnumAndSetOptions(Column column) {
+        List<String> options = MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
+        return options;
     }
 
-    protected String convertSetValue(long indexes, String options) {
+    protected String convertSetValue(long indexes, List<String> options) {
         StringBuilder sb = new StringBuilder();
         int index = 0;
         boolean first = true;
+        int optionLen = options.size();
         while (indexes != 0L) {
             if (indexes % 2L != 0) {
                 if (first)
                     first = false;
                 else
-                    sb.append(',');
-                sb.append(options.substring(index, index + 1));
+                    sb.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+                if (index < optionLen) {
+                    sb.append(options.get(index));
+                }
             }
             ++index;
             indexes = indexes >>> 1;",2016-10-11T19:11:08Z,47
"@@ -10,6 +10,7 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.sql.Types;
+import java.util.List;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.junit.Before;
@@ -638,27 +639,37 @@ public void shouldParseTicketMonsterLiquibaseStatements() {
 
     @Test
     public void shouldParseEnumOptions() {
-        assertParseEnumAndSetOptions(""ENUM('a','b','c')"", ""abc"");
+        assertParseEnumAndSetOptions(""ENUM('a','b','c')"", ""a,b,c"");
         assertParseEnumAndSetOptions(""ENUM('a')"", ""a"");
         assertParseEnumAndSetOptions(""ENUM()"", """");
-        assertParseEnumAndSetOptions(""ENUM ('a','b','c') CHARACTER SET"", ""abc"");
+        assertParseEnumAndSetOptions(""ENUM ('a','b','c') CHARACTER SET"", ""a,b,c"");
         assertParseEnumAndSetOptions(""ENUM ('a') CHARACTER SET"", ""a"");
         assertParseEnumAndSetOptions(""ENUM () CHARACTER SET"", """");
     }
 
     @Test
     public void shouldParseSetOptions() {
-        assertParseEnumAndSetOptions(""SET('a','b','c')"", ""abc"");
+        assertParseEnumAndSetOptions(""SET('a','b','c')"", ""a,b,c"");
         assertParseEnumAndSetOptions(""SET('a')"", ""a"");
         assertParseEnumAndSetOptions(""SET()"", """");
-        assertParseEnumAndSetOptions(""SET ('a','b','c') CHARACTER SET"", ""abc"");
+        assertParseEnumAndSetOptions(""SET ('a','b','c') CHARACTER SET"", ""a,b,c"");
         assertParseEnumAndSetOptions(""SET ('a') CHARACTER SET"", ""a"");
         assertParseEnumAndSetOptions(""SET () CHARACTER SET"", """");
     }
 
     protected void assertParseEnumAndSetOptions(String typeExpression, String optionString) {
-        String options = MySqlDdlParser.parseSetAndEnumOptions(typeExpression);
-        assertThat(options).isEqualTo(optionString);
+        List<String> options = MySqlDdlParser.parseSetAndEnumOptions(typeExpression);
+        StringBuilder sb = new StringBuilder();
+        boolean first = true;
+        for (String value:options) {
+            if (first) {
+                first = false;
+            } else {
+                sb.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+            }
+            sb.append(value);
+        }
+        assertThat(optionString).isEqualTo(sb.toString());
     }
 
     protected void assertVariable(String name, String expectedValue) {",2016-10-11T19:11:08Z,26
"@@ -252,21 +252,31 @@ protected void execute() {
                 logger.info(""Step 5: read list of available tables in each database"");
                 List<TableId> tableIds = new ArrayList<>();
                 final Map<String, List<TableId>> tableIdsByDbName = new HashMap<>();
+                final Set<String> readableDatabaseNames = new HashSet<>();
                 for (String dbName : databaseNames) {
-                    sql.set(""SHOW TABLES IN "" + dbName);
-                    mysql.query(sql.get(), rs -> {
-                        while (rs.next() && isRunning()) {
-                            TableId id = new TableId(dbName, null, rs.getString(1));
-                            if (filters.tableFilter().test(id)) {
-                                tableIds.add(id);
-                                tableIdsByDbName.computeIfAbsent(dbName, k -> new ArrayList<>()).add(id);
-                                logger.info(""\t including '{}'"", id);
-                            } else {
-                                logger.info(""\t '{}' is filtered out, discarding"", id);
+                    try {
+                        // MySQL sometimes considers some local files as databases (see DBZ-164),
+                        // so we will simply try each one and ignore the problematic ones ...
+                        sql.set(""SHOW TABLES IN "" + quote(dbName));
+                        mysql.query(sql.get(), rs -> {
+                            while (rs.next() && isRunning()) {
+                                TableId id = new TableId(dbName, null, rs.getString(1));
+                                if (filters.tableFilter().test(id)) {
+                                    tableIds.add(id);
+                                    tableIdsByDbName.computeIfAbsent(dbName, k -> new ArrayList<>()).add(id);
+                                    logger.info(""\t including '{}'"", id);
+                                } else {
+                                    logger.info(""\t '{}' is filtered out, discarding"", id);
+                                }
                             }
-                        }
-                    });
+                        });
+                        readableDatabaseNames.add(dbName);
+                    } catch (SQLException e) {
+                        // We were unable to execute the query or process the results, so skip this ...
+                        logger.warn(""\t skipping database '{}' due to error reading tables: {}"", dbName, e.getMessage());
+                    }
                 }
+                logger.info(""\t snapshot continuing with databases: {}"", readableDatabaseNames);
 
                 // ------
                 // STEP 6
@@ -282,28 +292,28 @@ protected void execute() {
                 allTableIds.stream()
                            .filter(id -> isRunning()) // ignore all subsequent tables if this reader is stopped
                            .forEach(tableId -> schema.applyDdl(source, tableId.schema(),
-                                                               ""DROP TABLE IF EXISTS "" + tableId,
+                                                               ""DROP TABLE IF EXISTS "" + quote(tableId),
                                                                this::enqueueSchemaChanges));
 
                 // Add a DROP DATABASE statement for each database that we no longer know about ...
                 schema.tables().tableIds().stream().map(TableId::catalog)
-                      .filter(Predicates.not(databaseNames::contains))
+                      .filter(Predicates.not(readableDatabaseNames::contains))
                       .filter(id -> isRunning()) // ignore all subsequent tables if this reader is stopped
                       .forEach(missingDbName -> schema.applyDdl(source, missingDbName,
-                                                                ""DROP DATABASE IF EXISTS "" + missingDbName,
+                                                                ""DROP DATABASE IF EXISTS "" + quote(missingDbName),
                                                                 this::enqueueSchemaChanges));
 
                 // Now process all of our tables for each database ...
                 for (Map.Entry<String, List<TableId>> entry : tableIdsByDbName.entrySet()) {
                     if (!isRunning()) break;
                     String dbName = entry.getKey();
                     // First drop, create, and then use the named database ...
-                    schema.applyDdl(source, dbName, ""DROP DATABASE IF EXISTS "" + dbName, this::enqueueSchemaChanges);
-                    schema.applyDdl(source, dbName, ""CREATE DATABASE "" + dbName, this::enqueueSchemaChanges);
-                    schema.applyDdl(source, dbName, ""USE "" + dbName, this::enqueueSchemaChanges);
+                    schema.applyDdl(source, dbName, ""DROP DATABASE IF EXISTS "" + quote(dbName), this::enqueueSchemaChanges);
+                    schema.applyDdl(source, dbName, ""CREATE DATABASE "" + quote(dbName), this::enqueueSchemaChanges);
+                    schema.applyDdl(source, dbName, ""USE "" + quote(dbName), this::enqueueSchemaChanges);
                     for (TableId tableId : entry.getValue()) {
                         if (!isRunning()) break;
-                        sql.set(""SHOW CREATE TABLE "" + tableId);
+                        sql.set(""SHOW CREATE TABLE "" + quote(tableId));
                         mysql.query(sql.get(), rs -> {
                             if (rs.next()) {
                                 schema.applyDdl(source, dbName, rs.getString(2), this::enqueueSchemaChanges);
@@ -357,31 +367,36 @@ protected void execute() {
                         RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, bufferedRecordQueue);
                         if (recordMaker != null) {
 
+                            // Switch to the table's database ...
+                            sql.set(""USE "" + quote(tableId.catalog()) + "";"");
+                            mysql.execute(sql.get());
+
                             AtomicLong numRows = new AtomicLong(-1);
                             AtomicReference<String> rowCountStr = new AtomicReference<>(""<unknown>"");
                             StatementFactory statementFactory = this::createStatementWithLargeResultSet;
                             if (largeTableCount > 0) {
-                                // Switch to the table's database ...
-                                sql.set(""USE "" + tableId.catalog() + "";"");
-                                mysql.execute(sql.get());
-
-                                // Choose how we create statements based on the # of rows.
-                                // This is approximate and less accurate then COUNT(*),
-                                // but far more efficient for large InnoDB tables.
-                                sql.set(""SHOW TABLE STATUS LIKE '"" + tableId.table() + ""';"");
-                                mysql.query(sql.get(), rs -> {
-                                    if (rs.next()) numRows.set(rs.getLong(5));
-                                });
-                                if (numRows.get() <= largeTableCount) {
-                                    statementFactory = this::createStatement;
+                                try {
+                                    // Choose how we create statements based on the # of rows.
+                                    // This is approximate and less accurate then COUNT(*),
+                                    // but far more efficient for large InnoDB tables.
+                                    sql.set(""SHOW TABLE STATUS LIKE '"" + tableId.table() + ""';"");
+                                    mysql.query(sql.get(), rs -> {
+                                        if (rs.next()) numRows.set(rs.getLong(5));
+                                    });
+                                    if (numRows.get() <= largeTableCount) {
+                                        statementFactory = this::createStatement;
+                                    }
+                                    rowCountStr.set(numRows.toString());
+                                } catch (SQLException e) {
+                                    // Log it, but otherwise just use large result set by default ...
+                                    logger.debug(""Error while getting number of rows in table {}: {}"", tableId, e.getMessage(), e);
                                 }
-                                rowCountStr.set(numRows.toString());
                             }
 
                             // Scan the rows in the table ...
                             long start = clock.currentTimeInMillis();
                             logger.info(""Step 8: - scanning table '{}' ({} of {} tables)"", tableId, ++counter, tableIds.size());
-                            sql.set(""SELECT * FROM "" + tableId);
+                            sql.set(""SELECT * FROM "" + quote(tableId));
                             try {
                                 mysql.query(sql.get(), statementFactory, rs -> {
                                     long rowNum = 0;
@@ -521,6 +536,14 @@ protected void execute() {
         }
     }
 
+    protected String quote(String dbOrTableName) {
+        return ""`"" + dbOrTableName + ""`"";
+    }
+
+    protected String quote(TableId id) {
+        return quote(id.catalog()) + ""."" + quote(id.table());
+    }
+
     /**
      * Create a JDBC statement that can be used for large result sets.
      * <p>",2016-12-21T04:03:46Z,62
"@@ -412,10 +412,14 @@ protected void parseCreateDefinitionList(Marker start, TableEditor table) {
     }
 
     protected void parseCreateDefinition(Marker start, TableEditor table) {
+        // If the first token is a quoted identifier, then we know it is a column name ...
+        boolean quoted = isNextTokenQuotedIdentifier();
+
         // Try to parse the constraints first ...
-        if (tokens.canConsume(""CHECK"")) {
+        if (!quoted && tokens.canConsume(""CHECK"")) {
             consumeExpression(start);
-        } else if (tokens.canConsume(""CONSTRAINT"", TokenStream.ANY_VALUE, ""PRIMARY"", ""KEY"") || tokens.canConsume(""PRIMARY"", ""KEY"")) {
+        } else if (!quoted && tokens.canConsume(""CONSTRAINT"", TokenStream.ANY_VALUE, ""PRIMARY"", ""KEY"")
+                || tokens.canConsume(""PRIMARY"", ""KEY"")) {
             if (tokens.canConsume(""USING"")) {
                 parseIndexType(start);
             }
@@ -432,7 +436,7 @@ protected void parseCreateDefinition(Marker start, TableEditor table) {
                     table.addColumn(c.edit().optional(false).create());
                 }
             });
-        } else if (tokens.canConsume(""CONSTRAINT"", TokenStream.ANY_VALUE, ""UNIQUE"") || tokens.canConsume(""UNIQUE"")) {
+        } else if (!quoted && tokens.canConsume(""CONSTRAINT"", TokenStream.ANY_VALUE, ""UNIQUE"") || tokens.canConsume(""UNIQUE"")) {
             tokens.canConsumeAnyOf(""KEY"", ""INDEX"");
             if (!tokens.matches('(')) {
                 if (!tokens.matches(""USING"")) {
@@ -447,15 +451,16 @@ protected void parseCreateDefinition(Marker start, TableEditor table) {
                 table.setPrimaryKeyNames(uniqueKeyColumnNames); // this may eventually get overwritten by a real PK
             }
             parseIndexOptions(start);
-        } else if (tokens.canConsume(""CONSTRAINT"", TokenStream.ANY_VALUE, ""FOREIGN"", ""KEY"") || tokens.canConsume(""FOREIGN"", ""KEY"")) {
+        } else if (!quoted && tokens.canConsume(""CONSTRAINT"", TokenStream.ANY_VALUE, ""FOREIGN"", ""KEY"")
+                || tokens.canConsume(""FOREIGN"", ""KEY"")) {
             if (!tokens.matches('(')) {
                 tokens.consume(); // name of foreign key
             }
             parseIndexColumnNames(start);
             if (tokens.matches(""REFERENCES"")) {
                 parseReferenceDefinition(start);
             }
-        } else if (tokens.canConsumeAnyOf(""INDEX"", ""KEY"")) {
+        } else if (!quoted && tokens.canConsumeAnyOf(""INDEX"", ""KEY"")) {
             if (!tokens.matches('(')) {
                 if (!tokens.matches(""USING"")) {
                     tokens.consume(); // name of unique index ...
@@ -466,15 +471,15 @@ protected void parseCreateDefinition(Marker start, TableEditor table) {
             }
             parseIndexColumnNames(start);
             parseIndexOptions(start);
-        } else if (tokens.canConsume(""FULLTEXT"", ""SPATIAL"")) {
+        } else if (!quoted && tokens.canConsume(""FULLTEXT"", ""SPATIAL"")) {
             tokens.canConsumeAnyOf(""INDEX"", ""KEY"");
             if (!tokens.matches('(')) {
                 tokens.consume(); // name of unique index ...
             }
             parseIndexColumnNames(start);
             parseIndexOptions(start);
         } else {
-            tokens.canConsume(""COLUMN""); // optional in ALTER TABLE but never CREATE TABLE
+            tokens.canConsume(""COLUMN""); // optional
 
             // Obtain the column editor ...
             String columnName = tokens.consume();
@@ -1151,21 +1156,26 @@ protected void sequentially(Consumer<Marker>... functions) {
 
     protected void parseDefaultClause(Marker start) {
         tokens.consume(""DEFAULT"");
-        if (tokens.canConsume(""CURRENT_TIMESTAMP"")) {
-            if (tokens.canConsume('(')) {
-                tokens.consumeInteger();
-                tokens.consume(')');
-            }
-            tokens.canConsume(""ON"", ""UPDATE"", ""CURRENT_TIMESTAMP"");
-            if (tokens.canConsume('(')) {
-                tokens.consumeInteger();
-                tokens.consume(')');
-            }
-        } else if (tokens.canConsume(""NULL"")) {
-            // do nothing ...
-        } else {
+        if (isNextTokenQuotedIdentifier()) {
+            // We know that it is a quoted literal ...
             parseLiteral(start);
-            // do nothing ...
+        } else {
+            if (tokens.canConsume(""CURRENT_TIMESTAMP"")) {
+                if (tokens.canConsume('(')) {
+                    tokens.consumeInteger();
+                    tokens.consume(')');
+                }
+                tokens.canConsume(""ON"", ""UPDATE"", ""CURRENT_TIMESTAMP"");
+                if (tokens.canConsume('(')) {
+                    tokens.consumeInteger();
+                    tokens.consume(')');
+                }
+            } else if (tokens.canConsume(""NULL"")) {
+                // do nothing ...
+            } else {
+                parseLiteral(start);
+                // do nothing ...
+            }
         }
     }
 }",2016-08-23T22:03:53Z,49
"@@ -331,6 +331,14 @@ public void shouldParseCreateStatements() {
         assertThat(listener.total()).isEqualTo(144);
     }
 
+    @Test
+    public void shouldParseStatementForDbz106() {
+        parser.parse(readFile(""ddl/mysql-dbz-106.ddl""), tables);
+        Testing.print(tables);
+        assertThat(tables.size()).isEqualTo(1);
+        assertThat(listener.total()).isEqualTo(1);
+    }
+
     @Test
     public void shouldParseTestStatements() {
         parser.parse(readFile(""ddl/mysql-test-statements.ddl""), tables);",2016-08-23T22:03:53Z,26
"@@ -0,0 +1,18 @@
+CREATE TABLE `DBZ106` (
+ `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT,
+ `YYYY` bigint(20) unsigned NOT NULL,
+ `key` varchar(64) NOT NULL DEFAULT '',
+ `value` text,
+ `FFFFF` tinyint(4) DEFAULT '0',
+ `DDDD` bigint(20) DEFAULT '0',
+ `TTTT` binary(16) DEFAULT NULL,
+ `BBB` text,
+ PRIMARY KEY (`id`),
+ KEY `TYTUT` (`risk_info_id`),
+ KEY `BGFH` (`key`),
+ KEY `HGJG` (`risk_info_id`,`create_time`),
+ KEY `EEEEE` (`is_encrypted`),
+ KEY `GGGGG` (`risk_info_id`,`context_uuid`),
+ KEY `HGHGHG` (`context_uuid`),
+ CONSTRAINT `GHGHGHGHG` FOREIGN KEY (`risk_info_id`) REFERENCES `HGHGHG` (`id`)
+) ENGINE=InnoDB AUTO_INCREMENT=10851 DEFAULT CHARSET=utf8;",2016-08-23T22:03:53Z,64
"@@ -137,7 +137,17 @@ protected void initializeStatementStarts(TokenSet statementStartTokens) {
     public final String terminator() {
         return terminator;
     }
-
+    
+    /**
+     * Determine if the next token is a single- or double-quoted string.
+     * 
+     * @return {@code true} if the next token is a {@link DdlTokenizer#SINGLE_QUOTED_STRING single-quoted string} or
+     * {@link DdlTokenizer#DOUBLE_QUOTED_STRING double-quoted string}, or {@code false} otherwise
+     */
+    protected boolean isNextTokenQuotedIdentifier() {
+        return tokens.matchesAnyOf(DdlTokenizer.SINGLE_QUOTED_STRING,DdlTokenizer.DOUBLE_QUOTED_STRING);
+    }
+    
     protected int determineTokenType(int type, String token) {
         if (statementStarts.contains(token)) type |= DdlTokenizer.STATEMENT_KEY;
         if (keywords.contains(token)) type |= DdlTokenizer.KEYWORD;",2016-08-23T22:03:53Z,65
"@@ -2,6 +2,38 @@
 
 All notable changes are documented in this file. Release numbers follow [Semantic Versioning](http://semver.org)
 
+## 0.4.0
+
+February 7, 2017 - [Detailed release notes](https://issues.jboss.org/projects/DBZ/versions/12330743)
+
+### New features since 0.3.6
+
+* New PostgreSQL connector. [DBZ-3](https://issues.jboss.org/projects/DBZ/issues/DBZ-3)
+* Preliminary support for [Amazon RDS](https://aws.amazon.com/rds/mysql/) and [Amazon Aurora (MySQL compatibility)](https://aws.amazon.com/rds/aurora/). [DBZ-140](https://issues.jboss.org/projects/DBZ/issues/DBZ-140)
+
+### Breaking changes since 0.3.6
+
+None
+
+### Fixes and changes since 0.3.6
+
+This release includes the following fixes, changes, or improvements since the link:release-0-3-6[0.3.6] release:
+
+* Update Kafka dependencies to 0.10.1.1. [DBZ-173](https://issues.jboss.org/projects/DBZ/issues/DBZ-173)
+* Update MySQL binary log client library to 0.9.0. [DBZ-186](https://issues.jboss.org/projects/DBZ/issues/DBZ-186)
+* MySQL should apply GTID filters to database history. [DBZ-185](https://issues.jboss.org/projects/DBZ/issues/DBZ-185)
+* Add names of database and table to the MySQL event metadata. [DBZ-184](https://issues.jboss.org/projects/DBZ/issues/DBZ-184)
+* Add the MySQL thread ID to the MySQL event metadata. [DBZ-113](https://issues.jboss.org/projects/DBZ/issues/DBZ-113)
+* Corrects MySQL connector to properly handle timezone information for `TIMESTAMP`. [DBZ-183](https://issues.jboss.org/projects/DBZ/issues/DBZ-183)
+* Correct MySQL DDL parser to handle `CREATE TRIGGER` command with `DEFINER` clauses. [DBZ-176](https://issues.jboss.org/projects/DBZ/issues/DBZ-176)
+* Update MongoDB Java driver and MongoDB server versions. [DBZ-187](https://issues.jboss.org/projects/DBZ/issues/DBZ-187)
+* MongoDB connector should restart incomplete initial sync. [DBZ-182](https://issues.jboss.org/projects/DBZ/issues/DBZ-182)
+* MySQL and PostgreSQL connectors should load JDBC driver independently of DriverManager. [DBZ-177](https://issues.jboss.org/projects/DBZ/issues/DBZ-177)
+* Upgrade MySQL binlog client library to support new binlog events added with MySQL 5.7. [DBZ-174](https://issues.jboss.org/projects/DBZ/issues/DBZ-174)
+* EmbeddedEngine should log all errors. [DBZ-178](https://issues.jboss.org/projects/DBZ/issues/DBZ-178)
+* PostgreSQL containers' generated Protobuf source moved to separate directory. [DBZ-179](https://issues.jboss.org/projects/DBZ/issues/DBZ-179)
+
+
 ## 0.3.6
 
 December 21, 2016 - [Detailed release notes](https://issues.jboss.org/projects/DBZ/versions/12332775)",2017-02-07T19:35:30Z,66
"@@ -70,6 +70,8 @@ public class BinlogReader extends AbstractReader {
     private final ElapsedTimeStrategy pollOutputDelay;
     private long recordCounter = 0L;
     private long previousOutputMillis = 0L;
+    private long initialEventsToSkip = 0L;
+    private boolean skipEvent = false;
     private final AtomicLong totalRecordCounter = new AtomicLong();
     private volatile Map<String, ?> lastOffset = null;
     private com.github.shyiko.mysql.binlog.GtidSet gtidSet;
@@ -164,15 +166,21 @@ protected void doStart() {
             logger.info(""Registering binlog reader with GTID set: {}"", filteredGtidSet);
             String filteredGtidSetStr = filteredGtidSet.toString();
             client.setGtidSet(filteredGtidSetStr);
-            source.setGtidSet(filteredGtidSetStr);
+            source.setCompletedGtidSet(filteredGtidSetStr);
             gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(filteredGtidSetStr);
         } else {
             client.setBinlogFilename(source.binlogFilename());
-            client.setBinlogPosition(source.nextBinlogPosition());
+            client.setBinlogPosition(source.binlogPosition());
         }
 
+        // We may be restarting in the middle of a transaction, so see how far into the transaction we have already processed...
+        initialEventsToSkip = source.eventsToSkipUponRestart();
+
         // Set the starting row number, which is the next row number to be read ...
-        startingRowNumber = source.nextEventRowNumber();
+        startingRowNumber = source.rowsToSkipUponRestart();
+
+        // Only when we reach the first BEGIN event will we start to skip events ...
+        skipEvent = false;
 
         // Initial our poll output delay logic ...
         pollOutputDelay.hasElapsed();
@@ -287,8 +295,15 @@ protected void handleEvent(Event event) {
             // Forward the event to the handler ...
             eventHandlers.getOrDefault(eventType, this::ignoreEvent).accept(event);
 
-            // And after that event has been processed, always set the starting row number to 0 ...
-            startingRowNumber = 0;
+            // Capture that we've completed another event ...
+            source.completeEvent();
+
+            if (skipEvent) {
+                // We're in the mode of skipping events and we just skipped this one, so decrement our skip count ...
+                --initialEventsToSkip;
+                skipEvent = initialEventsToSkip > 0;
+            }
+
         } catch (RuntimeException e) {
             // There was an error in the event handler, so propagate the failure to Kafka Connect ...
             failed(e, ""Error processing binlog event"");
@@ -375,8 +390,7 @@ protected void handleGtidEvent(Event event) {
         GtidEventData gtidEvent = unwrapData(event);
         String gtid = gtidEvent.getGtid();
         gtidSet.add(gtid);
-        source.setGtid(gtid);
-        source.setGtidSet(gtidSet.toString()); // rather than use the client's GTID set
+        source.startGtid(gtid, gtidSet.toString()); // rather than use the client's GTID set
     }
 
     /**
@@ -387,14 +401,23 @@ protected void handleGtidEvent(Event event) {
      */
     protected void handleQueryEvent(Event event) {
         QueryEventData command = unwrapData(event);
-        logger.debug(""Received update table command: {}"", event);
+        logger.debug(""Received query command: {}"", event);
         String sql = command.getSql().trim();
         if (sql.equalsIgnoreCase(""BEGIN"")) {
-            // ignore these altogether ...
+            // We are starting a new transaction ...
+            source.startNextTransaction();
+            if (initialEventsToSkip != 0) {
+                logger.debug(""Restarting partially-processed transaction; change events will not be created for the first {} events plus {} more rows in the next event"",
+                             initialEventsToSkip, startingRowNumber);
+                // We are restarting, so we need to skip the events in this transaction that we processed previously...
+                skipEvent = true;
+            }
             return;
         }
         if (sql.equalsIgnoreCase(""COMMIT"")) {
-            // ignore these altogether ...
+            // We are completing the transaction ...
+            source.commitTransaction();
+            skipEvent = false;
             return;
         }
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
@@ -438,6 +461,11 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
@@ -447,13 +475,26 @@ protected void handleInsert(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                count += recordMaker.create(rows.get(row), ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    count += recordMaker.create(rows.get(row), ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} insert record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} insert record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed insert event: {}"", event);
             }
-            logger.debug(""Recorded {} insert records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping insert row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     /**
@@ -463,6 +504,11 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
@@ -473,16 +519,29 @@ protected void handleUpdate(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
-                Serializable[] before = changes.getKey();
-                Serializable[] after = changes.getValue();
-                count += recordMaker.update(before, after, ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
+                    Serializable[] before = changes.getKey();
+                    Serializable[] after = changes.getValue();
+                    count += recordMaker.update(before, after, ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} update record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} update record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed update event: {}"", event);
             }
-            logger.debug(""Recorded {} update records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping update row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     /**
@@ -492,6 +551,11 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
@@ -501,13 +565,26 @@ protected void handleDelete(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                count += recordMaker.delete(rows.get(row), ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    count += recordMaker.delete(rows.get(row), ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} delete record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} delete record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed delete event: {}"", event);
             }
-            logger.debug(""Recorded {} delete records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping delete row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     protected SSLMode sslModeFor(SecureConnectionMode mode) {",2016-11-09T14:11:41Z,67
"@@ -139,7 +139,7 @@ public synchronized void start(Map<String, String> props) {
             if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
                 // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
                 // previously used but the MySQL server has them enabled ...
-                source.setGtidSet("""");
+                source.setCompletedGtidSet("""");
             }
 
             // Check whether the row-level binlog is enabled ...",2016-11-09T14:11:41Z,68
"@@ -265,11 +265,11 @@ public ObjectName metricName(String contextName) throws MalformedObjectNameExcep
      *         none were filtered
      */
     public GtidSet filterGtidSet(GtidSet availableServerGtidSet) {
-        logger.info(""Attempting to generate a filtered GTID set"");
         String gtidStr = source.gtidSet();
         if (gtidStr == null) {
             return null;
         }
+        logger.info(""Attempting to generate a filtered GTID set"");
         logger.info(""GTID set from previous recorded offset: {}"", gtidStr);
         GtidSet filteredGtidSet = new GtidSet(gtidStr);
         Predicate<String> gtidSourceFilter = gtidSourceFilter();",2016-11-09T14:11:41Z,69
"@@ -222,7 +222,7 @@ protected void execute() {
                     if (rs.getMetaData().getColumnCount() > 4) {
                         // This column exists only in MySQL 5.6.5 or later ...
                         String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
-                        source.setGtidSet(gtidSet);
+                        source.setCompletedGtidSet(gtidSet);
                         logger.info(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
                                     gtidSet);
                     } else {",2016-11-09T14:11:41Z,62
"@@ -32,10 +32,13 @@
  * </pre>
  * 
  * <p>
- * The {@link #offset() source offset} information describes how much of the database's binary log the source the change detector
- * has already processed, and it includes the {@link #binlogFilename() binlog filename}, the {@link #nextBinlogPosition() next
- * position} in the binlog to start reading, and the {@link #nextEventRowNumber() next event row number}. Here's a JSON-like
- * representation of an example:
+ * The {@link #offset() source offset} information is included in each event and captures where the connector should restart
+ * if this event's offset is the last one recorded. The offset includes the {@link #binlogFilename() binlog filename},
+ * the {@link #binlogPosition() position of the first event} in the binlog, the
+ * {@link #eventsToSkipUponRestart() number of events to skip}, and the
+ * {@link #rowsToSkipUponRestart() number of rows to also skip}.
+ * <p>
+ * Here's a JSON-like representation of an example:
  * 
  * <pre>
  * {
@@ -44,22 +47,26 @@
  *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
  *     ""file"": ""mysql-bin.000003"",
  *     ""pos"" = 990,
+ *     ""event"" = 0,
  *     ""row"": 0,
  *     ""snapshot"": true
  * }
  * </pre>
- * 
+ * <p>
  * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
  * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
  * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
  * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
- * 
- * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
- * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), the
- * {@link #lastBinlogPosition() position} of the event (and {@link #lastEventRowNumber() row number} within the event) inside
- * the {@link #binlogFilename() binlog file}. When GTIDs are enabled, it also includes the GTID of the transaction in which the
- * event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced when the connector is in the
- * middle of a snapshot. Here's a JSON-like representation of the source for an event that corresponds to the above partition and
+ * <p>
+ * Each change event envelope also includes the {@link #struct() source} struct that contains MySQL information about that
+ * particular event, including a mixture the fields from the {@link #partition() partition} (which is renamed in the source to
+ * make more sense), the binlog filename and position where the event can be found, and when GTIDs are enabled the GTID of the
+ * transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced
+ * when the connector is in the middle of a snapshot. Note that this information is likely different than the offset information,
+ * since the connector may need to restart from either just after the most recently completed transaction or the beginning
+ * of the most recently started transaction (whichever appears later in the binlog).
+ * <p>
+ * Here's a JSON-like representation of the source for an event that corresponds to the above partition and
  * offset:
  * 
  * <pre>
@@ -88,9 +95,10 @@ final class SourceInfo {
     public static final String SERVER_PARTITION_KEY = ""server"";
     public static final String GTID_SET_KEY = ""gtids"";
     public static final String GTID_KEY = ""gtid"";
+    public static final String EVENTS_TO_SKIP_OFFSET_KEY = ""event"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
-    public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
+    public static final String BINLOG_ROW_IN_EVENT_OFFSET_KEY = ""row"";
     public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
@@ -105,17 +113,22 @@ final class SourceInfo {
                                                      .field(GTID_KEY, Schema.OPTIONAL_STRING_SCHEMA)
                                                      .field(BINLOG_FILENAME_OFFSET_KEY, Schema.STRING_SCHEMA)
                                                      .field(BINLOG_POSITION_OFFSET_KEY, Schema.INT64_SCHEMA)
-                                                     .field(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, Schema.INT32_SCHEMA)
+                                                     .field(BINLOG_ROW_IN_EVENT_OFFSET_KEY, Schema.INT32_SCHEMA)
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private String gtidSet;
-    private String binlogGtid;
-    private String binlogFilename;
-    private long lastBinlogPosition = 0;
-    private int lastEventRowNumber = 0;
-    private long nextBinlogPosition = 4;
-    private int nextEventRowNumber = 0;
+    private String currentGtidSet;
+    private String currentGtid;
+    private String currentBinlogFilename;
+    private long currentBinlogPosition = 0L;
+    private int currentRowNumber = 0;
+    private long currentEventLengthInBytes = 0;
+    private String restartGtidSet;
+    private String restartBinlogFilename;
+    private long restartBinlogPosition = 0L;
+    private long restartEventsToSkip = 0;
+    private int restartRowsToSkip = 0;
+    private boolean inTransaction = false;
     private String serverName;
     private long serverId = 0;
     private long binlogTimestampSeconds = 0;
@@ -149,50 +162,91 @@ public Map<String, String> partition() {
         return sourcePartition;
     }
 
+    /**
+     * Set the position in the MySQL binlog where we will start reading.
+     * 
+     * @param binlogFilename the name of the binary log file; may not be null
+     * @param positionOfFirstEvent the position in the binary log file to begin processing
+     */
+    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
+        if (binlogFilename != null) {
+            this.currentBinlogFilename = binlogFilename;
+            this.restartBinlogFilename = binlogFilename;
+        }
+        assert positionOfFirstEvent >= 0;
+        this.currentBinlogPosition = positionOfFirstEvent;
+        this.restartBinlogPosition = positionOfFirstEvent;
+        this.currentRowNumber = 0;
+        this.restartRowsToSkip = 0;
+    }
+
+    /**
+     * Set the position within the MySQL binary log file of the <em>current event</em>.
+     * 
+     * @param positionOfCurrentEvent the position within the binary log file of the current event
+     * @param eventSizeInBytes the size in bytes of this event
+     */
+    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
+        this.currentBinlogPosition = positionOfCurrentEvent;
+        this.currentEventLengthInBytes = eventSizeInBytes;
+        if (!inTransaction) {
+            this.restartBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
+        }
+        // Don't set anything else, since the row numbers are set in the offset(int,int) method called at least once
+        // for each processed event
+    }
+
     /**
      * Get the Kafka Connect detail about the source ""offset"", which describes the position within the source where we last
      * have last read.
      * 
      * @return a copy of the current offset; never null
      */
     public Map<String, ?> offset() {
-        return offsetUsingPosition(nextBinlogPosition);
+        return offsetUsingPosition(this.restartRowsToSkip);
     }
 
     /**
-     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
-     * describes the position within the source where we have last read.
+     * Given the row number within a binlog event and the total number of rows in that event, compute and return the
+     * Kafka Connect offset that is be included in the produced change event describing the row.
      * <p>
      * This method should always be called before {@link #struct()}.
      * 
-     * @param eventRowNumber the 0-based row number within the event being processed
+     * @param eventRowNumber the 0-based row number within the event for which the offset is to be produced
      * @param totalNumberOfRows the total number of rows within the event being processed
      * @return a copy of the current offset; never null
+     * @see #struct()
      */
     public Map<String, ?> offsetForRow(int eventRowNumber, int totalNumberOfRows) {
         if (eventRowNumber < (totalNumberOfRows - 1)) {
             // This is not the last row, so our offset should record the next row to be used ...
-            this.lastEventRowNumber = eventRowNumber;
-            this.nextEventRowNumber = eventRowNumber + 1;
+            this.currentRowNumber = eventRowNumber;
+            this.restartRowsToSkip = this.currentRowNumber + 1;
             // so write out the offset with the position of this event
-            return offsetUsingPosition(lastBinlogPosition);
+            return offsetUsingPosition(this.restartRowsToSkip);
         }
         // This is the last row, so write out the offset that has the position of the next event ...
-        this.lastEventRowNumber = this.nextEventRowNumber;
-        this.nextEventRowNumber = 0;
-        return offsetUsingPosition(nextBinlogPosition);
+        this.currentRowNumber = eventRowNumber;
+        this.restartRowsToSkip = 0;
+        return offsetUsingPosition(totalNumberOfRows);
     }
 
-    private Map<String, ?> offsetUsingPosition(long binlogPosition) {
+    private Map<String, ?> offsetUsingPosition(long rowsToSkip) {
         Map<String, Object> map = new HashMap<>();
         if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
-        if (gtidSet != null) {
-            map.put(GTID_SET_KEY, gtidSet);
+        if (restartGtidSet != null) {
+            // Put the previously-completed GTID set in the offset along with the event number ...
+            map.put(GTID_SET_KEY, restartGtidSet);
+        }
+        map.put(BINLOG_FILENAME_OFFSET_KEY, restartBinlogFilename);
+        map.put(BINLOG_POSITION_OFFSET_KEY, restartBinlogPosition);
+        if (restartEventsToSkip != 0) {
+            map.put(EVENTS_TO_SKIP_OFFSET_KEY, restartEventsToSkip);
         }
-        map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, nextEventRowNumber);
+        if (rowsToSkip != 0) {
+            map.put(BINLOG_ROW_IN_EVENT_OFFSET_KEY, rowsToSkip);
+        }
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             map.put(SNAPSHOT_KEY, true);
         }
@@ -223,13 +277,13 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        // Don't put the GTID Set into the struct; only the current GTID is fine ...
-        if (binlogGtid != null) {
-            result.put(GTID_KEY, binlogGtid);
+        if (currentGtid != null) {
+            // Don't put the GTID Set into the struct; only the current GTID is fine ...
+            result.put(GTID_KEY, currentGtid);
         }
-        result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        result.put(BINLOG_POSITION_OFFSET_KEY, lastBinlogPosition);
-        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, lastEventRowNumber);
+        result.put(BINLOG_FILENAME_OFFSET_KEY, currentBinlogFilename);
+        result.put(BINLOG_POSITION_OFFSET_KEY, currentBinlogPosition);
+        result.put(BINLOG_ROW_IN_EVENT_OFFSET_KEY, currentRowNumber);
         result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (lastSnapshot) {
             result.put(SNAPSHOT_KEY, true);
@@ -246,54 +300,73 @@ public boolean isSnapshotInEffect() {
         return nextSnapshot;
     }
 
+    public void startNextTransaction() {
+        // If we have to restart, then we'll start with this BEGIN transaction
+        this.restartRowsToSkip = 0;
+        this.restartEventsToSkip = 0;
+        this.restartBinlogFilename = this.currentBinlogFilename;
+        this.restartBinlogPosition = this.currentBinlogPosition;
+        this.inTransaction = true;
+    }
+
     /**
-     * Set the latest GTID from the MySQL binary log file.
-     * 
-     * @param gtid the string representation of a specific GTID; may not be null
+     * Capture that we're starting a new event.
      */
-    public void setGtid(String gtid) {
-        this.binlogGtid = gtid;
+    public void completeEvent() {
+        ++restartEventsToSkip;
     }
 
     /**
-     * Set the set of GTIDs known to the MySQL server.
+     * Get the number of events after the last transaction BEGIN that we've already processed.
      * 
-     * @param gtidSet the string representation of GTID set; may not be null
+     * @return the number of events in the transaction that have been processed completely
+     * @see #completeEvent()
+     * @see #startNextTransaction()
      */
-    public void setGtidSet(String gtidSet) {
-        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
-            this.gtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """"); // remove all of the newline chars if they exist
-        }
+    public long eventsToSkipUponRestart() {
+        return restartEventsToSkip;
+    }
+
+    public void commitTransaction() {
+        this.restartGtidSet = this.currentGtidSet;
+        this.restartBinlogFilename = this.currentBinlogFilename;
+        this.restartBinlogPosition = this.currentBinlogPosition + this.currentEventLengthInBytes;
+        this.restartRowsToSkip = 0;
+        this.restartEventsToSkip = 0;
+        this.inTransaction = false;
     }
 
     /**
-     * Set the name of the MySQL binary log file.
+     * Record that a new GTID transaction has been started and has been included in the set of GTIDs known to the MySQL server.
      * 
-     * @param binlogFilename the name of the binary log file; may not be null
-     * @param positionOfFirstEvent the position in the binary log file to begin processing
+     * @param gtid the string representation of a specific GTID that has been begun; may not be null
+     * @param gtidSet the string representation of GTID set that includes the newly begun GTID; may not be null
      */
-    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
-        if (binlogFilename != null) {
-            this.binlogFilename = binlogFilename;
+    public void startGtid(String gtid, String gtidSet) {
+        this.currentGtid = gtid;
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            // Remove all the newline chars that exist in the GTID set string ...
+            String trimmedGtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """");
+            // Set the GTID set that we'll use if restarting BEFORE successful completion of the events in this GTID ...
+            this.restartGtidSet = this.currentGtidSet != null ? this.currentGtidSet : trimmedGtidSet;
+            // Record the GTID set that includes the current transaction ...
+            this.currentGtidSet = trimmedGtidSet;
         }
-        assert positionOfFirstEvent >= 0;
-        this.nextBinlogPosition = positionOfFirstEvent;
-        this.lastBinlogPosition = this.nextBinlogPosition;
-        this.nextEventRowNumber = 0;
-        this.lastEventRowNumber = 0;
     }
 
     /**
-     * Set the position within the MySQL binary log file of the <em>current event</em>.
+     * Set the GTID set that captures all of the GTID transactions that have been completely processed.
      * 
-     * @param positionOfCurrentEvent the position within the binary log file of the current event
-     * @param eventSizeInBytes the size in bytes of this event
+     * @param gtidSet the string representation of the GTID set; may not be null, but may be an empty string if no GTIDs
+     *            have been previously processed
      */
-    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
-        this.lastBinlogPosition = positionOfCurrentEvent;
-        this.nextBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
-        // Don't set anything else, since the row numbers are set in the offset(int,int) method called at least once
-        // for each processed event
+    public void setCompletedGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            // Remove all the newline chars that exist in the GTID set string ...
+            String trimmedGtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """");
+            this.currentGtidSet = trimmedGtidSet;
+            this.restartGtidSet = trimmedGtidSet;
+        }
     }
 
     /**
@@ -350,23 +423,23 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
-            binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
+            setCompletedGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
+            restartEventsToSkip = longOffsetValue(sourceOffset, EVENTS_TO_SKIP_OFFSET_KEY);
+            String binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
             }
-            nextBinlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
-            nextEventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
-            lastBinlogPosition = nextBinlogPosition;
-            lastEventRowNumber = nextEventRowNumber;
+            long binlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
+            setBinlogStartPoint(binlogFilename, binlogPosition);
+            this.restartRowsToSkip = (int) longOffsetValue(sourceOffset, BINLOG_ROW_IN_EVENT_OFFSET_KEY);
             nextSnapshot = booleanOffsetValue(sourceOffset, SNAPSHOT_KEY);
             lastSnapshot = nextSnapshot;
         }
     }
 
     private long longOffsetValue(Map<String, ?> values, String key) {
         Object obj = values.get(key);
-        if (obj == null) return 0;
+        if (obj == null) return 0L;
         if (obj instanceof Number) return ((Number) obj).longValue();
         try {
             return Long.parseLong(obj.toString());
@@ -388,56 +461,44 @@ private boolean booleanOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.gtidSet != null ? this.gtidSet : null;
+        return this.currentGtidSet != null ? this.currentGtidSet : null;
     }
 
     /**
-     * Get the name of the MySQL binary log file that has been processed.
+     * Get the name of the MySQL binary log file that has last been processed.
      * 
      * @return the name of the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
     public String binlogFilename() {
-        return binlogFilename;
+        return restartBinlogFilename;
     }
 
     /**
      * Get the position within the MySQL binary log file of the next event to be processed.
      * 
      * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
-    public long nextBinlogPosition() {
-        return nextBinlogPosition;
+    public long binlogPosition() {
+        return restartBinlogPosition;
     }
 
     /**
      * Get the position within the MySQL binary log file of the most recently processed event.
      * 
      * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
-    public long lastBinlogPosition() {
-        return lastBinlogPosition;
+    protected long restartBinlogPosition() {
+        return restartBinlogPosition;
     }
 
     /**
-     * Get the next row within the event at the {@link #nextBinlogPosition() position} within the {@link #binlogFilename() binary
-     * log file}
-     * .
+     * Get the number of rows beyond the {@link #eventsToSkipUponRestart() last completely processed event} to be skipped
+     * upon restart.
      * 
-     * @return the 0-based row number
+     * @return the number of rows to be skipped
      */
-    public int nextEventRowNumber() {
-        return nextEventRowNumber;
-    }
-
-    /**
-     * Get the previous row within the event at the {@link #lastBinlogPosition() position} within the {@link #binlogFilename()
-     * binary log file}
-     * .
-     * 
-     * @return the 0-based row number
-     */
-    public int lastEventRowNumber() {
-        return lastEventRowNumber;
+    public int rowsToSkipUponRestart() {
+        return restartRowsToSkip;
     }
 
     /**
@@ -452,22 +513,26 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (gtidSet != null) {
+        if (currentGtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(gtidSet);
-            sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
-            sb.append("", pos="").append(nextBinlogPosition());
-            sb.append("", row="").append(nextEventRowNumber());
+            sb.append(currentGtidSet);
+            sb.append("" and binlog file '"").append(restartBinlogFilename).append(""'"");
+            sb.append("", pos="").append(restartBinlogPosition);
+            sb.append("", skipping "").append(restartEventsToSkip);
+            sb.append("" events plus "").append(restartRowsToSkip);
+            sb.append("" rows"");
         } else {
-            if (binlogFilename == null) {
+            if (restartBinlogFilename == null) {
                 sb.append(""<latest>"");
             } else {
-                if ("""".equals(binlogFilename)) {
+                if ("""".equals(restartBinlogFilename)) {
                     sb.append(""earliest binlog file and position"");
                 } else {
-                    sb.append(""binlog file '"").append(binlogFilename).append(""'"");
-                    sb.append("", pos="").append(nextBinlogPosition());
-                    sb.append("", row="").append(nextEventRowNumber());
+                    sb.append(""binlog file '"").append(restartBinlogFilename).append(""'"");
+                    sb.append("", pos="").append(restartBinlogPosition);
+                    sb.append("", skipping "").append(restartEventsToSkip);
+                    sb.append("" events plus "").append(restartRowsToSkip);
+                    sb.append("" rows"");
                 }
             }
         }
@@ -505,7 +570,14 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
-                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired ...
+                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired GTID.
+                    // Now we need to compare how many events in that transaction we've already completed ...
+                    int recordedEventCount = recorded.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+                    int desiredEventCount = desired.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+                    int diff = recordedEventCount - desiredEventCount;
+                    if (diff > 0) return false;
+
+                    // Otherwise the recorded is definitely before or at the desired ...
                     return true;
                 }
                 // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
@@ -543,16 +615,25 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
         if (diff > 0) return false;
+        if (diff < 0) return true;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
         if (diff > 0) return false;
+        if (diff < 0) return true;
+
+        // The positions are the same, so compare the completed events in the transaction ...
+        int recordedEventCount = recorded.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+        int desiredEventCount = desired.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+        diff = recordedEventCount - desiredEventCount;
+        if (diff > 0) return false;
+        if (diff < 0) return true;
 
-        // The positions are the same, so compare the row number ...
-        int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
-        int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        // The completed events are the same, so compare the row number ...
+        int recordedRow = recorded.getInteger(BINLOG_ROW_IN_EVENT_OFFSET_KEY, -1);
+        int desiredRow = desired.getInteger(BINLOG_ROW_IN_EVENT_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
         if (diff > 0) return false;
 ",2016-11-09T14:11:41Z,70
"@@ -8,8 +8,10 @@
 import static org.junit.Assert.fail;
 
 import java.nio.file.Path;
+import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.util.List;
+import java.util.Map;
 import java.util.concurrent.TimeUnit;
 
 import org.apache.kafka.common.config.Config;
@@ -28,6 +30,7 @@
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
 import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.embedded.EmbeddedEngine.CompletionResult;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.relational.history.KafkaDatabaseHistory;
@@ -274,11 +277,12 @@ public void shouldValidateAcceptableConfiguration() {
     public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLException, InterruptedException {
         String masterPort = System.getProperty(""database.port"");
         String replicaPort = System.getProperty(""database.replica.port"");
-        if ( !masterPort.equals(replicaPort)) {
+        boolean replicaIsMaster = masterPort.equals(replicaPort);
+        if (!replicaIsMaster) {
             // Give time for the replica to catch up to the master ...
             Thread.sleep(5000L);
         }
-        
+
         // Use the DB configuration to define the connector's configuration to use the ""replica""
         // which may be the same as the ""master"" ...
         config = Configuration.create()
@@ -352,7 +356,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
             }
         }
 
-        //Testing.Print.enable();
+        // Testing.Print.enable();
 
         // Restart the connector and read the insert record ...
         Testing.print(""*** Restarting connector after inserts were made"");
@@ -388,7 +392,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         inserts = records.recordsForTopic(""myServer.connector_test.products"");
         assertInsert(inserts.get(0), ""id"", 1001);
 
-        Testing.print(""*** Done with simple insert"");
+        // Testing.print(""*** Done with simple insert"");
 
         // ---------------------------------------------------------------------------------------------------------------
         // Changing the primary key of a row should result in 3 events: INSERT, DELETE, and TOMBSTONE
@@ -433,13 +437,15 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
 
         Testing.print(""*** Done with simple update"");
 
+        //Testing.Print.enable();
+
         // ---------------------------------------------------------------------------------------------------------------
         // Change our schema with a fully-qualified name; we should still see this event
         // ---------------------------------------------------------------------------------------------------------------
         // Add a column with default to the 'products' table and explicitly update one record ...
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
-                connection.execute(""ALTER TABLE connector_test.products ADD COLUMN volume FLOAT NOT NULL, ADD COLUMN alias VARCHAR(30) NOT NULL AFTER description"");
+                connection.execute(""ALTER TABLE connector_test.products ADD COLUMN volume FLOAT, ADD COLUMN alias VARCHAR(30) NULL AFTER description"");
                 connection.execute(""UPDATE products SET volume=13.5 WHERE id=2001"");
                 connection.query(""SELECT * FROM products"", rs -> {
                     if (Testing.Print.isEnabled()) connection.print(rs);
@@ -508,6 +514,173 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // Stop the connector ...
         // ---------------------------------------------------------------------------------------------------------------
         stopConnector();
+
+        // ---------------------------------------------------------------------------------------------------------------
+        // Restart the connector to read only part of a transaction ...
+        // ---------------------------------------------------------------------------------------------------------------
+        Testing.print(""*** Restarting connector"");
+        CompletionResult completion = new CompletionResult();
+        start(MySqlConnector.class, config, completion, (record) -> {
+            // We want to stop before processing record 3003 ...
+            Struct key = (Struct) record.key();
+            Number id = (Number) key.get(""id"");
+            if (id.intValue() == 3003) {
+                return true;
+            }
+            return false;
+        });
+
+        BinlogPosition positionBeforeInserts = new BinlogPosition();
+        BinlogPosition positionAfterInserts = new BinlogPosition();
+        BinlogPosition positionAfterUpdate = new BinlogPosition();
+        try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
+            try (JdbcConnection connection = db.connect()) {
+                connection.query(""SHOW MASTER STATUS"", positionBeforeInserts::readFromDatabase);
+                connection.execute(""INSERT INTO products(id,name,description,weight,volume,alias) VALUES ""
+                        + ""(3001,'ashley','super robot',34.56,0.00,'ashbot'), ""
+                        + ""(3002,'arthur','motorcycle',87.65,0.00,'arcycle'), ""
+                        + ""(3003,'oak','tree',987.65,0.00,'oak');"");
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
+                connection.query(""SHOW MASTER STATUS"", positionAfterInserts::readFromDatabase);
+                // Change something else that is unrelated ...
+                connection.execute(""UPDATE products_on_hand SET quantity=40 WHERE product_id=109"");
+                connection.query(""SELECT * FROM products_on_hand"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
+                connection.query(""SHOW MASTER STATUS"", positionAfterUpdate::readFromDatabase);
+            }
+        }
+
+        //Testing.Print.enable();
+
+        // And consume the one insert ...
+        records = consumeRecordsByTopic(2);
+        assertThat(records.recordsForTopic(""myServer.connector_test.products"").size()).isEqualTo(2);
+        assertThat(records.topics().size()).isEqualTo(1);
+        inserts = records.recordsForTopic(""myServer.connector_test.products"");
+        assertInsert(inserts.get(0), ""id"", 3001);
+        assertInsert(inserts.get(1), ""id"", 3002);
+
+        // Verify that the connector has stopped ...
+        completion.await(10, TimeUnit.SECONDS);
+        assertThat(completion.hasCompleted()).isTrue();
+        assertThat(completion.hasError()).isTrue();
+        assertThat(completion.success()).isFalse();
+        assertNoRecordsToConsume();
+        assertConnectorNotRunning();
+
+        // ---------------------------------------------------------------------------------------------------------------
+        // Stop the connector ...
+        // ---------------------------------------------------------------------------------------------------------------
+        stopConnector();
+
+        // Read the last committed offsets, and verify the binlog coordinates ...
+        SourceInfo persistedOffsetSource = new SourceInfo();
+        persistedOffsetSource.setServerName(config.getString(MySqlConnectorConfig.SERVER_NAME));
+        Map<String, ?> lastCommittedOffset = readLastCommittedOffset(config, persistedOffsetSource.partition());
+        persistedOffsetSource.setOffset(lastCommittedOffset);
+        Testing.print(""Position before inserts: "" + positionBeforeInserts);
+        Testing.print(""Position after inserts:  "" + positionAfterInserts);
+        Testing.print(""Offset: "" + lastCommittedOffset);
+        Testing.print(""Position after update:  "" + positionAfterUpdate);
+        if (replicaIsMaster) {
+            // Same binlog filename ...
+            assertThat(persistedOffsetSource.binlogFilename()).isEqualTo(positionBeforeInserts.binlogFilename());
+            assertThat(persistedOffsetSource.binlogFilename()).isEqualTo(positionAfterInserts.binlogFilename());
+            // Binlog position in offset should be more than before the inserts, but less than the position after the inserts ...
+            assertThat(persistedOffsetSource.binlogPosition()).isGreaterThan(positionBeforeInserts.binlogPosition());
+            assertThat(persistedOffsetSource.binlogPosition()).isLessThan(positionAfterInserts.binlogPosition());
+        } else {
+            // the replica is not the same server as the master, so it will have a different binlog filename and position ...
+        }
+        // Event number is 2 ...
+        assertThat(persistedOffsetSource.eventsToSkipUponRestart()).isEqualTo(2);
+        // GTID set should match the before-inserts GTID set ...
+        // assertThat(persistedOffsetSource.gtidSet()).isEqualTo(positionBeforeInserts.gtidSet());
+
+        Testing.print(""*** Restarting connector, and should begin with inserting 3003 (not 109!)"");
+        start(MySqlConnector.class, config);
+
+        // And consume the insert for 3003 ...
+        records = consumeRecordsByTopic(1);
+        assertThat(records.topics().size()).isEqualTo(1);
+        inserts = records.recordsForTopic(""myServer.connector_test.products"");
+        if (inserts == null) {
+            updates = records.recordsForTopic(""myServer.connector_test.products_on_hand"");
+            if (updates != null) {
+                fail(""Restarted connector and missed the insert of product id=3003!"");
+            }
+        }
+        // Read the first record produced since we've restarted
+        SourceRecord prod3003 = inserts.get(0);
+        assertInsert(prod3003, ""id"", 3003);
+        
+        // Check that the offset has the correct/expected values ...
+        assertOffset(prod3003,""file"",lastCommittedOffset.get(""file""));
+        assertOffset(prod3003,""pos"",lastCommittedOffset.get(""pos""));
+        assertOffset(prod3003,""row"",3);
+        assertOffset(prod3003,""event"",lastCommittedOffset.get(""event""));
+
+        // Check that the record has all of the column values ...
+        assertValueField(prod3003,""after/id"",3003);
+        assertValueField(prod3003,""after/name"",""oak"");
+        assertValueField(prod3003,""after/description"",""tree"");
+        assertValueField(prod3003,""after/weight"",987.65d);
+        assertValueField(prod3003,""after/volume"",0.0d);
+        assertValueField(prod3003,""after/alias"",""oak"");
+        
+
+        // And make sure we consume that one extra update ...
+        records = consumeRecordsByTopic(1);
+        assertThat(records.topics().size()).isEqualTo(1);
+        updates = records.recordsForTopic(""myServer.connector_test.products_on_hand"");
+        assertThat(updates.size()).isEqualTo(1);
+        assertUpdate(updates.get(0), ""product_id"", 109);
+        updates.forEach(this::validate);
+
+        // Start the connector again, and we should see the next two
+        Testing.print(""*** Done with simple insert"");
+
+    }
+
+    protected static class BinlogPosition {
+        private String binlogFilename;
+        private long binlogPosition;
+        private String gtidSet;
+
+        public void readFromDatabase(ResultSet rs) throws SQLException {
+            if (rs.next()) {
+                binlogFilename = rs.getString(1);
+                binlogPosition = rs.getLong(2);
+                if (rs.getMetaData().getColumnCount() > 4) {
+                    // This column exists only in MySQL 5.6.5 or later ...
+                    gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                }
+            }
+        }
+
+        public String binlogFilename() {
+            return binlogFilename;
+        }
+
+        public long binlogPosition() {
+            return binlogPosition;
+        }
+
+        public String gtidSet() {
+            return gtidSet;
+        }
+
+        public boolean hasGtids() {
+            return gtidSet != null;
+        }
+
+        @Override
+        public String toString() {
+            return ""file="" + binlogFilename + "", pos="" + binlogPosition + "", gtids="" + (gtidSet != null ? gtidSet : """");
+        }
     }
 
     @Test",2016-11-09T14:11:41Z,71
"@@ -215,7 +215,7 @@ public void shouldFilterAndMergeGtidSet() throws Exception {
                                .build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setGtidSet(gtidStr);
+        context.source().setCompletedGtidSet(gtidStr);
 
         GtidSet mergedGtidSet = context.filterGtidSet(new GtidSet(availableServerGtidStr));
         assertThat(mergedGtidSet).isNotNull();",2016-11-09T14:11:41Z,72
"@@ -31,31 +31,34 @@ public class SourceInfoTest {
     private static final String SERVER_NAME = ""my-server""; // can technically be any string
 
     private SourceInfo source;
+    private boolean inTxn = false;
+    private long positionOfBeginEvent = 0L;
+    private int eventNumberInTxn = 0;
 
     @Before
     public void beforeEach() {
         source = new SourceInfo();
+        inTxn = false;
+        positionOfBeginEvent = 0L;
+        eventNumberInTxn = 0;
     }
 
     @Test
     public void shouldStartSourceInfoFromZeroBinlogCoordinates() {
         source.setBinlogStartPoint(FILENAME, 0);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.eventsToSkipUponRestart()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
     @Test
     public void shouldStartSourceInfoFromNonZeroBinlogCoordinates() {
         source.setBinlogStartPoint(FILENAME, 100);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -68,10 +71,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinates() {
         sourceWith(offset(0, 0));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -80,10 +81,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinates() {
         sourceWith(offset(100, 0));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -92,10 +91,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndNonZero
         sourceWith(offset(0, 5));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -104,10 +101,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndNonZ
         sourceWith(offset(100, 5));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -116,10 +111,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndSnapsho
         sourceWith(offset(0, 0, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -128,10 +121,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndSnap
         sourceWith(offset(100, 0, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -140,10 +131,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndNonZero
         sourceWith(offset(0, 5, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -152,10 +141,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndNonZ
         sourceWith(offset(100, 5, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -164,10 +151,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 0, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -176,10 +161,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 5, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -188,10 +171,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 0, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -200,10 +181,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 5, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -212,10 +191,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 0, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -224,10 +201,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 5, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -236,10 +211,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 0, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -248,10 +221,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 5, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -262,19 +233,89 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
     @Test
     public void shouldAdvanceSourceInfoFromNonZeroPositionAndRowZeroForEventsWithOneRow() {
         sourceWith(offset(100, 0));
+
+        // Try a transactions with just one event ...
+        handleTransactionBegin(150, 2);
         handleNextEvent(200, 10, withRowCount(1));
+        handleTransactionCommit(210, 2);
+
+        handleTransactionBegin(210, 2);
         handleNextEvent(220, 10, withRowCount(1));
+        handleTransactionCommit(230, 3);
+
+        handleTransactionBegin(240, 2);
         handleNextEvent(250, 50, withRowCount(1));
+        handleTransactionCommit(300, 4);
+
+        // Try a transactions with multiple events ...
+        handleTransactionBegin(340, 2);
+        handleNextEvent(350, 20, withRowCount(1));
+        handleNextEvent(370, 30, withRowCount(1));
+        handleNextEvent(400, 40, withRowCount(1));
+        handleTransactionCommit(440, 4);
+
+        handleTransactionBegin(500, 2);
+        handleNextEvent(510, 20, withRowCount(1));
+        handleNextEvent(540, 15, withRowCount(1));
+        handleNextEvent(560, 10, withRowCount(1));
+        handleTransactionCommit(580, 4);
+
+        // Try another single event transaction ...
+        handleTransactionBegin(600, 2);
+        handleNextEvent(610, 50, withRowCount(1));
+        handleTransactionCommit(660, 4);
+
+        // Try event outside of a transaction ...
+        handleNextEvent(670, 10, withRowCount(1));
+
+        // Try another single event transaction ...
+        handleTransactionBegin(700, 2);
+        handleNextEvent(710, 50, withRowCount(1));
+        handleTransactionCommit(760, 4);
     }
 
     @Test
     public void shouldAdvanceSourceInfoFromNonZeroPositionAndRowZeroForEventsWithMultipleRow() {
         sourceWith(offset(100, 0));
+
+        // Try a transactions with just one event ...
+        handleTransactionBegin(150, 2);
         handleNextEvent(200, 10, withRowCount(3));
+        handleTransactionCommit(210, 2);
+
+        handleTransactionBegin(210, 2);
         handleNextEvent(220, 10, withRowCount(4));
-        handleNextEvent(250, 50, withRowCount(6));
-        handleNextEvent(300, 20, withRowCount(1));
-        handleNextEvent(350, 20, withRowCount(3));
+        handleTransactionCommit(230, 3);
+
+        handleTransactionBegin(240, 2);
+        handleNextEvent(250, 50, withRowCount(5));
+        handleTransactionCommit(300, 4);
+
+        // Try a transactions with multiple events ...
+        handleTransactionBegin(340, 2);
+        handleNextEvent(350, 20, withRowCount(6));
+        handleNextEvent(370, 30, withRowCount(1));
+        handleNextEvent(400, 40, withRowCount(3));
+        handleTransactionCommit(440, 4);
+
+        handleTransactionBegin(500, 2);
+        handleNextEvent(510, 20, withRowCount(8));
+        handleNextEvent(540, 15, withRowCount(9));
+        handleNextEvent(560, 10, withRowCount(1));
+        handleTransactionCommit(580, 4);
+
+        // Try another single event transaction ...
+        handleTransactionBegin(600, 2);
+        handleNextEvent(610, 50, withRowCount(1));
+        handleTransactionCommit(660, 4);
+
+        // Try event outside of a transaction ...
+        handleNextEvent(670, 10, withRowCount(5));
+
+        // Try another single event transaction ...
+        handleTransactionBegin(700, 2);
+        handleNextEvent(710, 50, withRowCount(3));
+        handleTransactionCommit(760, 4);
     }
 
     // -------------------------------------------------------------------------------------
@@ -285,33 +326,78 @@ protected int withRowCount(int rowCount) {
         return rowCount;
     }
 
+    protected void handleTransactionBegin(long positionOfEvent, int eventSize) {
+        source.setEventPosition(positionOfEvent, eventSize);
+        positionOfBeginEvent = positionOfEvent;
+        source.startNextTransaction();
+        inTxn = true;
+
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
+    }
+
+    protected void handleTransactionCommit(long positionOfEvent, int eventSize) {
+        source.setEventPosition(positionOfEvent, eventSize);
+        source.commitTransaction();
+        eventNumberInTxn = 0;
+        inTxn = false;
+
+        // Verify the offset ...
+        Map<String, ?> offset = source.offset();
+
+        // The offset position should be the position of the next event
+        long position = (Long) offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY);
+        assertThat(position).isEqualTo(positionOfEvent + eventSize);
+        Long rowsToSkip = (Long) offset.get(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY);
+        if (rowsToSkip == null) rowsToSkip = 0L;
+        assertThat(rowsToSkip).isEqualTo(0);
+        assertThat(offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY)).isNull();
+        if (source.gtidSet() != null) {
+            assertThat(offset.get(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
+        }
+    }
+
     protected void handleNextEvent(long positionOfEvent, long eventSize, int rowCount) {
+        if (inTxn) ++eventNumberInTxn;
         source.setEventPosition(positionOfEvent, eventSize);
-        for (int i = 0; i != rowCount; ++i) {
+        for (int row = 0; row != rowCount; ++row) {
             // Get the offset for this row (always first!) ...
-            Map<String, ?> offset = source.offsetForRow(i, rowCount);
-            if ((i + 1) < rowCount) {
-                // This is not the last row, so the next binlog position should be for next row in this event ...
-                assertThat(offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent);
-                assertThat(offset.get(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(i+1);
-            } else {
-                // This is the last row, so the next binlog position should be for first row in next event ...
-                assertThat(offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent + eventSize);
-                assertThat(offset.get(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(0);
-            }
+            Map<String, ?> offset = source.offsetForRow(row, rowCount);
             assertThat(offset.get(SourceInfo.BINLOG_FILENAME_OFFSET_KEY)).isEqualTo(FILENAME);
-            if ( source.gtidSet() != null ) {
+            if (source.gtidSet() != null) {
                 assertThat(offset.get(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
             }
+            long position = (Long) offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY);
+            if (inTxn) {
+                // regardless of the row count, the position is always the txn begin position ...
+                assertThat(position).isEqualTo(positionOfBeginEvent);
+                // and the number of the last completed event (the previous one) ...
+                Long eventsToSkip = (Long) offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY);
+                if (eventsToSkip == null) eventsToSkip = 0L;
+                assertThat(eventsToSkip).isEqualTo(eventNumberInTxn - 1);
+            } else {
+                // Matches the next event ...
+                assertThat(position).isEqualTo(positionOfEvent + eventSize);
+                assertThat(offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY)).isNull();
+            }
+            Long rowsToSkip = (Long) offset.get(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY);
+            if (rowsToSkip == null) rowsToSkip = 0L;
+            if( (row+1) == rowCount) {
+                // This is the last row, so the next binlog position should be the number of rows in the event ...
+                assertThat(rowsToSkip).isEqualTo(rowCount);
+            } else {
+                // This is not the last row, so the next binlog position should be the row number ...
+                assertThat(rowsToSkip).isEqualTo(row+1);
+            }
             // Get the source struct for this row (always second), which should always reflect this row in this event ...
             Struct recordSource = source.struct();
             assertThat(recordSource.getInt64(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent);
-            assertThat(recordSource.getInt32(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(i);
+            assertThat(recordSource.getInt32(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY)).isEqualTo(row);
             assertThat(recordSource.getString(SourceInfo.BINLOG_FILENAME_OFFSET_KEY)).isEqualTo(FILENAME);
-            if ( source.gtidSet() != null ) {
+            if (source.gtidSet() != null) {
                 assertThat(recordSource.getString(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
             }
         }
+        source.completeEvent();
     }
 
     protected Map<String, String> offset(long position, int row) {
@@ -326,7 +412,7 @@ protected Map<String, String> offset(String gtidSet, long position, int row, boo
         Map<String, String> offset = new HashMap<>();
         offset.put(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, FILENAME);
         offset.put(SourceInfo.BINLOG_POSITION_OFFSET_KEY, Long.toString(position));
-        offset.put(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, Integer.toString(row));
+        offset.put(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, Integer.toString(row));
         if (gtidSet != null) offset.put(SourceInfo.GTID_SET_KEY, gtidSet);
         if (snapshot) offset.put(SourceInfo.SNAPSHOT_KEY, Boolean.TRUE.toString());
         return offset;
@@ -393,12 +479,40 @@ public void shouldOrderPositionsWithSameServerButLowerUpperLimitAsBeforePosition
 
     @Test
     public void shouldOrderPositionWithoutGtidAsBeforePositionWithGtid() {
-        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0).isBefore(positionWithGtids(""IdA:1-5""));
+        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0, 0).isBefore(positionWithGtids(""IdA:1-5""));
     }
 
     @Test
     public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
-        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
+        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0, 0));
+    }
+
+    @Test
+    public void shouldComparePositionsWithoutGtids() {
+        // Same position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isAt(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAt(positionWithoutGtids(""fn.01"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.03"", 1, 0, 1).isAt(positionWithoutGtids(""fn.03"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isAt(positionWithoutGtids(""fn.01"", 1, 1, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAt(positionWithoutGtids(""fn.01"", 1, 1, 1));
+        assertPositionWithoutGtids(""fn.03"", 1, 1, 1).isAt(positionWithoutGtids(""fn.03"", 1, 1, 1));
+
+        // Before position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isBefore(positionWithoutGtids(""fn.01"", 1, 0, 2));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 1, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 2, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isBefore(positionWithoutGtids(""fn.01"", 1, 2, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+
+        // After position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAfter(positionWithoutGtids(""fn.01"", 0, 0, 99));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 0, 0, 99));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 1, 0));
     }
 
     @FixFor(""DBZ-107"")
@@ -410,21 +524,21 @@ public void shouldRemoveNewlinesFromGtidSet() {
         String gtidCleaned = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,"" +
                 ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3149,"" +
                 ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:1-39"";
-        source.setGtidSet(gtidExecuted);
+        source.setCompletedGtidSet(gtidExecuted);
         assertThat(source.gtidSet()).isEqualTo(gtidCleaned);
     }
 
     @FixFor(""DBZ-107"")
     @Test
     public void shouldNotSetBlankGtidSet() {
-        source.setGtidSet("""");
+        source.setCompletedGtidSet("""");
         assertThat(source.gtidSet()).isNull();
     }
 
     @FixFor(""DBZ-107"")
     @Test
     public void shouldNotSetNullGtidSet() {
-        source.setGtidSet(null);
+        source.setCompletedGtidSet(null);
         assertThat(source.gtidSet()).isNull();
     }
 
@@ -439,20 +553,22 @@ protected Document positionWithGtids(String gtids, boolean snapshot) {
         return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
-    protected Document positionWithoutGtids(String filename, int position, int row) {
-        return positionWithoutGtids(filename, position, row, false);
+    protected Document positionWithoutGtids(String filename, int position, int event, int row) {
+        return positionWithoutGtids(filename, position, event, row, false);
     }
 
-    protected Document positionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+    protected Document positionWithoutGtids(String filename, int position, int event, int row, boolean snapshot) {
         if (snapshot) {
             return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
                                    SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
-                                   SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row,
+                                   SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, row,
+                                   SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY, event,
                                    SourceInfo.SNAPSHOT_KEY, true);
         }
         return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
                                SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
-                               SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row);
+                               SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, row,
+                               SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY, event);
     }
 
     protected PositionAssert assertThatDocument(Document position) {
@@ -467,12 +583,12 @@ protected PositionAssert assertPositionWithGtids(String gtids, boolean snapshot)
         return assertThatDocument(positionWithGtids(gtids, snapshot));
     }
 
-    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row) {
-        return assertPositionWithoutGtids(filename, position, row, false);
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int event, int row) {
+        return assertPositionWithoutGtids(filename, position, event, row, false);
     }
 
-    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row, boolean snapshot) {
-        return assertThatDocument(positionWithoutGtids(filename, position, row, snapshot));
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int event, int row, boolean snapshot) {
+        return assertThatDocument(positionWithoutGtids(filename, position, event, row, snapshot));
     }
 
     protected static class PositionAssert extends GenericAssert<PositionAssert, Document> {",2016-11-09T14:11:41Z,70
"@@ -11,4 +11,5 @@ log4j.rootLogger=INFO, stdout
 log4j.logger.io.debezium=INFO
 log4j.logger.io.debezium.embedded.EmbeddedEngine$EmbeddedConfig=WARN
 #log4j.logger.io.debezium.connector.mysql.BinlogReader=DEBUG
-#log4j.logger.io.debezium.connector.mysql.SnapshotReader=DEBUG
\ No newline at end of file
+#log4j.logger.io.debezium.connector.mysql.SnapshotReader=DEBUG
+#log4j.logger.io.debezium.relational.history=DEBUG",2016-11-09T14:11:41Z,73
"@@ -92,6 +92,19 @@ static Document create(CharSequence fieldName1, Object value1, CharSequence fiel
         return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4);
     }
 
+    static Document create(CharSequence fieldName1, Object value1, CharSequence fieldName2, Object value2, CharSequence fieldName3,
+                           Object value3, CharSequence fieldName4, Object value4, CharSequence fieldName5, Object value5) {
+        return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4)
+                                  .set(fieldName5, value5);
+    }
+
+    static Document create(CharSequence fieldName1, Object value1, CharSequence fieldName2, Object value2, CharSequence fieldName3,
+                           Object value3, CharSequence fieldName4, Object value4, CharSequence fieldName5, Object value5,
+                           CharSequence fieldName6, Object value6) {
+        return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4)
+                                  .set(fieldName5, value5).set(fieldName6, value6);
+    }
+
     /**
      * Return the number of name-value fields in this object.
      * 
@@ -159,7 +172,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
             parent = find(parentPath, (missingPath, missingIndex) -> {
                 invalid.accept(missingPath); // invoke the invalid handler
                 return Optional.empty();
-            } , invalid);
+            }, invalid);
         } else {
             // Create any missing intermediaries using the segment after the missing segment to determine which
             // type of intermediate value to add ...
@@ -170,7 +183,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
                 } else {
                     return Optional.of(Value.create(Document.create()));
                 }
-            } , invalid);
+            }, invalid);
         }
         if (!parent.isPresent()) return Optional.empty();
         String lastSegment = path.lastSegment().get();
@@ -202,8 +215,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
      *         valid
      */
     default Optional<Value> find(Path path) {
-        return find(path, (missingPath, missingIndex) -> Optional.empty(), (invalidPath) -> {
-        });
+        return find(path, (missingPath, missingIndex) -> Optional.empty(), (invalidPath) -> {});
     }
 
     /**
@@ -719,7 +731,7 @@ default Value remove(Optional<? extends CharSequence> name) {
      * @return This document, to allow for chaining methods
      */
     Document removeAll();
-    
+
     /**
      * Sets on this object all name/value pairs from the supplied object. If the supplied object is null, this method does
      * nothing.",2016-11-09T14:11:41Z,74
"@@ -1169,8 +1169,9 @@ protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
      */
     protected Object handleUnknownData(Column column, Field fieldDefn, Object data) {
         if (column.isOptional() || fieldDefn.schema().isOptional()) {
+            Class<?> dataClass = data.getClass();
             logger.warn(""Unexpected value for JDBC type {} and column {}: class={}"", column.jdbcType(), column,
-                        data.getClass()); // don't include value in case its sensitive
+                        dataClass.isArray() ? dataClass.getSimpleName() : dataClass.getName()); // don't include value in case its sensitive
             return null;
         }
         throw new IllegalArgumentException(""Unexpected value for JDBC type "" + column.jdbcType() + "" and column "" + column +",2016-11-09T14:11:41Z,75
"@@ -46,14 +46,18 @@ public final void record(Map<String, ?> source, Map<String, ?> position, String
 
     @Override
     public final void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser) {
+        logger.debug(""Recovering DDL history for source partition {} and offset {}"",source,position);
         HistoryRecord stopPoint = new HistoryRecord(source, position, null, null);
         recoverRecords(schema,ddlParser,recovered->{
             if (comparator.isAtOrBefore(recovered,stopPoint)) {
                 String ddl = recovered.ddl();
                 if (ddl != null) {
                     ddlParser.setCurrentSchema(recovered.databaseName()); // may be null
                     ddlParser.parse(ddl, schema);
+                    logger.debug(""Applying: {}"", ddl);
                 }
+            } else {
+                logger.debug(""Skipping: {}"", recovered.ddl());
             }
         });
     }",2016-11-09T14:11:41Z,76
"@@ -281,7 +281,7 @@ public static void debug(SourceRecord record) {
      * @param record the record to validate; may not be null
      */
     public static void isValid(SourceRecord record) {
-        print(record);
+        //print(record);
 
         JsonNode keyJson = null;
         JsonNode valueJson = null;",2016-11-09T14:11:41Z,77
"@@ -8,6 +8,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
+import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Executor;
 import java.util.concurrent.ExecutorService;
@@ -35,7 +36,6 @@
 import org.apache.kafka.connect.storage.OffsetStorageReader;
 import org.apache.kafka.connect.storage.OffsetStorageReaderImpl;
 import org.apache.kafka.connect.storage.OffsetStorageWriter;
-import org.apache.kafka.connect.storage.StringConverter;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -134,7 +134,7 @@ public final class EmbeddedEngine implements Runnable {
 
     protected static final Field INTERNAL_KEY_CONVERTER_CLASS = Field.create(""internal.key.converter"")
                                                                      .withDescription(""The Converter class that should be used to serialize and deserialize key data for offsets."")
-                                                                     .withDefault(StringConverter.class.getName());
+                                                                     .withDefault(JsonConverter.class.getName());
 
     protected static final Field INTERNAL_VALUE_CONVERTER_CLASS = Field.create(""internal.value.converter"")
                                                                        .withDescription(""The Converter class that should be used to serialize and deserialize value data for offsets."")
@@ -159,14 +159,108 @@ public static interface CompletionCallback {
         /**
          * Handle the completion of the embedded connector engine.
          * 
-         * @param success true if the connector completed normally, or {@code false} if the connector produced an error that
-         *            prevented startup or premature termination.
+         * @param success {@code true} if the connector completed normally, or {@code false} if the connector produced an error
+         *            that prevented startup or premature termination.
          * @param message the completion message; never null
          * @param error the error, or null if there was no exception
          */
         void handle(boolean success, String message, Throwable error);
     }
 
+    /**
+     * A callback function to be notified when the connector completes.
+     */
+    public static class CompletionResult implements CompletionCallback {
+        private final CountDownLatch completed = new CountDownLatch(1);
+        private boolean success;
+        private String message;
+        private Throwable error;
+
+        @Override
+        public void handle(boolean success, String message, Throwable error) {
+            this.success = success;
+            this.message = message;
+            this.error = error;
+            this.completed.countDown();
+        }
+
+        /**
+         * Causes the current thread to wait until the {@link #handle(boolean, String, Throwable) completion occurs}
+         * or until the thread is {@linkplain Thread#interrupt interrupted}.
+         * <p>
+         * This method returns immediately if the connector has completed already.
+         * 
+         * @throws InterruptedException if the current thread is interrupted while waiting
+         */
+        public void await() throws InterruptedException {
+            this.completed.await();
+        }
+
+        /**
+         * Causes the current thread to wait until the {@link #handle(boolean, String, Throwable) completion occurs},
+         * unless the thread is {@linkplain Thread#interrupt interrupted}, or the specified waiting time elapses.
+         * <p>
+         * This method returns immediately if the connector has completed already.
+         * 
+         * @param timeout the maximum time to wait
+         * @param unit the time unit of the {@code timeout} argument
+         * @return {@code true} if the completion was received, or {@code false} if the waiting time elapsed before the completion
+         *         was received.
+         * @throws InterruptedException if the current thread is interrupted while waiting
+         */
+        public boolean await(long timeout, TimeUnit unit) throws InterruptedException {
+            return this.completed.await(timeout, unit);
+        }
+
+        /**
+         * Determine if the connector has completed.
+         * 
+         * @return {@code true} if the connector has completed, or {@code false} if the connector is still running and this
+         *         callback has not yet been {@link #handle(boolean, String, Throwable) notified}
+         */
+        public boolean hasCompleted() {
+            return completed.getCount() == 0;
+        }
+
+        /**
+         * Get whether the connector completed normally.
+         * 
+         * @return {@code true} if the connector completed normally, or {@code false} if the connector produced an error that
+         *         prevented startup or premature termination (or the connector has not yet {@link #hasCompleted() completed})
+         */
+        public boolean success() {
+            return success;
+        }
+
+        /**
+         * Get the completion message.
+         * 
+         * @return the completion message, or null if the connector has not yet {@link #hasCompleted() completed}
+         */
+        public String message() {
+            return message;
+        }
+
+        /**
+         * Get the completion error, if there is one.
+         * 
+         * @return the completion error, or null if there is no error or connector has not yet {@link #hasCompleted() completed}
+         */
+        public Throwable error() {
+            return error;
+        }
+
+        /**
+         * Determine if there is a completion error.
+         * 
+         * @return {@code true} if there is a {@link #error completion error}, or {@code false} if there is no error or
+         *         the connector has not yet {@link #hasCompleted() completed}
+         */
+        public boolean hasError() {
+            return error != null;
+        }
+    }
+
     /**
      * A builder to set up and create {@link EmbeddedEngine} instances.
      */
@@ -295,7 +389,7 @@ public EmbeddedEngine build() {
     private long timeSinceLastCommitMillis = 0;
 
     private EmbeddedEngine(Configuration config, ClassLoader classLoader, Clock clock, Consumer<SourceRecord> consumer,
-            CompletionCallback completionCallback) {
+                           CompletionCallback completionCallback) {
         this.config = config;
         this.consumer = consumer;
         this.classLoader = classLoader;
@@ -308,7 +402,7 @@ private EmbeddedEngine(Configuration config, ClassLoader classLoader, Clock cloc
         assert this.classLoader != null;
         assert this.clock != null;
         keyConverter = config.getInstance(INTERNAL_KEY_CONVERTER_CLASS, Converter.class, () -> this.classLoader);
-        keyConverter.configure(config.subset(INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), false);
+        keyConverter.configure(config.subset(INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), true);
         valueConverter = config.getInstance(INTERNAL_VALUE_CONVERTER_CLASS, Converter.class, () -> this.classLoader);
         Configuration valueConverterConfig = config;
         if (valueConverter instanceof JsonConverter) {
@@ -456,8 +550,9 @@ public void raiseError(Exception e) {
                     }
 
                     recordsSinceLastCommit = 0;
+                    Throwable handlerError = null;
                     timeSinceLastCommitMillis = clock.currentTimeInMillis();
-                    while (runningThread.get() != null) {
+                    while (runningThread.get() != null && handlerError == null) {
                         try {
                             logger.debug(""Embedded engine is polling task for records"");
                             List<SourceRecord> changeRecords = task.poll(); // blocks until there are values ...
@@ -469,17 +564,16 @@ public void raiseError(Exception e) {
                                     try {
                                         consumer.accept(record);
                                     } catch (Throwable t) {
-                                        logger.error(""Error in the application's handler method, but continuing anyway"", t);
+                                        handlerError = t;
+                                        break;
                                     }
-                                }
 
-                                // Only then do we write out the last partition to offset storage ...
-                                SourceRecord lastRecord = changeRecords.get(changeRecords.size() - 1);
-                                lastRecord.sourceOffset();
-                                offsetWriter.offset(lastRecord.sourcePartition(), lastRecord.sourceOffset());
+                                    // Record the offset for this record's partition
+                                    offsetWriter.offset(record.sourcePartition(), record.sourceOffset());
+                                    recordsSinceLastCommit += 1;
+                                }
 
                                 // Flush the offsets to storage if necessary ...
-                                recordsSinceLastCommit += changeRecords.size();
                                 maybeFlush(offsetWriter, offsetCommitPolicy, commitTimeoutMs);
                             } else {
                                 logger.debug(""Received no records from the task"");
@@ -501,7 +595,14 @@ public void raiseError(Exception e) {
                     } finally {
                         // Always commit offsets that were captured from the source records we actually processed ...
                         commitOffsets(offsetWriter, commitTimeoutMs);
-                        succeed(""Connector '"" + connectorClassName + ""' completed normally."");
+                        if (handlerError != null) {
+                            // There was an error in the handler ...
+                            fail(""Stopping connector after error in the application's handler method: "" + handlerError.getMessage(),
+                                 handlerError);
+                        } else {
+                            // We stopped normally ...
+                            succeed(""Connector '"" + connectorClassName + ""' completed normally."");
+                        }
                     }
                 } catch (Throwable t) {
                     fail(""Error while trying to run connector class '"" + connectorClassName + ""'"", t);",2016-11-09T14:11:41Z,78
"@@ -7,8 +7,11 @@
 
 import static org.junit.Assert.fail;
 
+import java.math.BigDecimal;
 import java.nio.file.Path;
 import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.LinkedList;
@@ -22,18 +25,25 @@
 import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 import java.util.function.Consumer;
+import java.util.function.Predicate;
 
 import org.apache.kafka.common.config.Config;
 import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.json.JsonConverter;
 import org.apache.kafka.connect.json.JsonDeserializer;
+import org.apache.kafka.connect.runtime.WorkerConfig;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;
 import org.apache.kafka.connect.source.SourceConnector;
 import org.apache.kafka.connect.source.SourceRecord;
+import org.apache.kafka.connect.storage.Converter;
+import org.apache.kafka.connect.storage.FileOffsetBackingStore;
+import org.apache.kafka.connect.storage.OffsetStorageReaderImpl;
+import org.fest.assertions.Delta;
 import org.junit.After;
 import org.junit.Before;
 import org.slf4j.Logger;
@@ -42,8 +52,10 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.data.SchemaUtil;
 import io.debezium.data.VerifyRecord;
 import io.debezium.embedded.EmbeddedEngine.CompletionCallback;
+import io.debezium.embedded.EmbeddedEngine.EmbeddedConfig;
 import io.debezium.function.BooleanConsumer;
 import io.debezium.relational.history.HistoryRecord;
 import io.debezium.util.LoggingContext;
@@ -162,20 +174,44 @@ protected int getMaximumEnqueuedRecordCount() {
     }
 
     /**
-     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
-     * logged.
+     * Create a {@link CompletionCallback} that logs when the engine fails to start the connector or when the connector
+     * stops running after completing successfully or due to an error
      * 
-     * @param connectorClass the connector class; may not be null
-     * @param connectorConfig the configuration for the connector; may not be null
+     * @return the logging {@link CompletionCallback}
      */
-    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig) {
-        start(connectorClass, connectorConfig, (success, msg, error) -> {
+    protected CompletionCallback loggingCompletion() {
+        return (success, msg, error) -> {
             if (success) {
                 logger.info(msg);
             } else {
                 logger.error(msg, error);
             }
-        });
+        };
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
+     * logged.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig) {
+        start(connectorClass, connectorConfig, loggingCompletion(), null);
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
+     * logged. The connector will stop immediately when the supplied predicate returns true.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     * @param isStopRecord the function that will be called to determine if the connector should be stopped before processing
+     *            this record; may be null if not needed
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         Predicate<SourceRecord> isStopRecord) {
+        start(connectorClass, connectorConfig, loggingCompletion(), isStopRecord);
     }
 
     /**
@@ -186,7 +222,23 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
      * @param callback the function that will be called when the engine fails to start the connector or when the connector
      *            stops running after completing successfully or due to an error; may be null
      */
-    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig, CompletionCallback callback) {
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         CompletionCallback callback) {
+        start(connectorClass, connectorConfig, callback, null);
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     * @param isStopRecord the function that will be called to determine if the connector should be stopped before processing
+     *            this record; may be null if not needed
+     * @param callback the function that will be called when the engine fails to start the connector or when the connector
+     *            stops running after completing successfully or due to an error; may be null
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         CompletionCallback callback, Predicate<SourceRecord> isStopRecord) {
         Configuration config = Configuration.copy(connectorConfig)
                                             .with(EmbeddedEngine.ENGINE_NAME, ""testing-connector"")
                                             .with(EmbeddedEngine.CONNECTOR_CLASS, connectorClass.getName())
@@ -202,11 +254,14 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
             }
             Testing.debug(""Stopped connector"");
         };
-
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
                                .notifying((record) -> {
+                                   if (isStopRecord != null && isStopRecord.test(record)) {
+                                       logger.error(""Stopping connector after record as requested"");
+                                       throw new ConnectException(""Stopping connector after record as requested"");
+                                   }
                                    try {
                                        consumedLines.put(record);
                                    } catch (InterruptedException e) {
@@ -306,7 +361,6 @@ protected SourceRecords consumeRecordsByTopic(int numRecords) throws Interrupted
         consumeRecords(numRecords, records::add);
         return records;
     }
-    
 
     protected class SourceRecords {
         private final List<SourceRecord> records = new ArrayList<>();
@@ -467,6 +521,55 @@ protected void assertTombstone(SourceRecord record, String pkField, int pk) {
     protected void assertTombstone(SourceRecord record) {
         VerifyRecord.isValidTombstone(record);
     }
+    
+    protected void assertOffset(SourceRecord record, Map<String,?> expectedOffset) {
+        Map<String,?> offset = record.sourceOffset();
+        assertThat(offset).isEqualTo(expectedOffset);
+    }
+    
+    protected void assertOffset(SourceRecord record, String offsetField, Object expectedValue) {
+        Map<String,?> offset = record.sourceOffset();
+        Object value = offset.get(offsetField);
+        assertSameValue(value,expectedValue);
+    }
+    
+    protected void assertValueField(SourceRecord record, String fieldPath, Object expectedValue) {
+        Object value = record.value();
+        String[] fieldNames = fieldPath.split(""/"");
+        String pathSoFar = null;
+        for (int i=0; i!=fieldNames.length; ++i) {
+            String fieldName = fieldNames[i];
+            if (value instanceof Struct) {
+                value = ((Struct)value).get(fieldName);
+            } else {
+                // We expected the value to be a struct ...
+                String path = pathSoFar == null ? ""record value"" : (""'"" + pathSoFar + ""'"");
+                String msg = ""Expected the "" + path + "" to be a Struct but was "" + value.getClass().getSimpleName() + "" in record: "" + SchemaUtil.asString(record);
+                fail(msg);
+            }
+            pathSoFar = pathSoFar == null ? fieldName : pathSoFar + ""/"" + fieldName;
+        }
+        assertSameValue(value,expectedValue);
+    }
+    
+    private void assertSameValue(Object actual, Object expected) {
+        if(expected instanceof Double || expected instanceof Float || expected instanceof BigDecimal) {
+            // Value should be within 1%
+            double expectedNumericValue = ((Number)expected).doubleValue();
+            double actualNumericValue = ((Number)actual).doubleValue();
+            assertThat(actualNumericValue).isEqualTo(expectedNumericValue, Delta.delta(0.01d*expectedNumericValue));
+        } else if (expected instanceof Integer || expected instanceof Long || expected instanceof Short) {
+            long expectedNumericValue = ((Number)expected).longValue();
+            long actualNumericValue = ((Number)actual).longValue();
+            assertThat(actualNumericValue).isEqualTo(expectedNumericValue);
+        } else if (expected instanceof Boolean) {
+            boolean expectedValue = ((Boolean)expected).booleanValue();
+            boolean actualValue = ((Boolean)expected).booleanValue();
+            assertThat(actualValue).isEqualTo(expectedValue);
+        } else {
+            assertThat(actual).isEqualTo(expected);
+        }
+    }
 
     /**
      * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
@@ -512,7 +615,8 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
         assertThat(value.errorMessages().size()).isEqualTo(numErrors);
     }
 
-    protected void assertConfigurationErrors(Config config, io.debezium.config.Field field, int minErrorsInclusive, int maxErrorsInclusive) {
+    protected void assertConfigurationErrors(Config config, io.debezium.config.Field field, int minErrorsInclusive,
+                                             int maxErrorsInclusive) {
         ConfigValue value = configValue(config, field.name());
         assertThat(value.errorMessages().size()).isGreaterThanOrEqualTo(minErrorsInclusive);
         assertThat(value.errorMessages().size()).isLessThanOrEqualTo(maxErrorsInclusive);
@@ -526,8 +630,8 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
     protected void assertNoConfigurationErrors(Config config, io.debezium.config.Field... fields) {
         for (io.debezium.config.Field field : fields) {
             ConfigValue value = configValue(config, field.name());
-            if ( value != null ) {
-                if ( !value.errorMessages().isEmpty() ) {
+            if (value != null) {
+                if (!value.errorMessages().isEmpty()) {
                     fail(""Error messages on field '"" + field.name() + ""': "" + value.errorMessages());
                 }
             }
@@ -538,4 +642,59 @@ protected ConfigValue configValue(Config config, String fieldName) {
         return config.configValues().stream().filter(value -> value.name().equals(fieldName)).findFirst().orElse(null);
     }
 
+    /**
+     * Utility to read the last committed offset for the specified partition.
+     * 
+     * @param config the configuration of the engine used to persist the offsets
+     * @param partition the partition
+     * @return the map of partitions to offsets; never null but possibly empty
+     */
+    protected <T> Map<String, Object> readLastCommittedOffset(Configuration config, Map<String, T> partition) {
+        return readLastCommittedOffsets(config, Arrays.asList(partition)).get(partition);
+    }
+
+    /**
+     * Utility to read the last committed offsets for the specified partitions.
+     * 
+     * @param config the configuration of the engine used to persist the offsets
+     * @param partitions the partitions
+     * @return the map of partitions to offsets; never null but possibly empty
+     */
+    protected <T> Map<Map<String, T>, Map<String, Object>> readLastCommittedOffsets(Configuration config,
+                                                                                    Collection<Map<String, T>> partitions) {
+        config = config.edit().with(EmbeddedEngine.ENGINE_NAME, ""testing-connector"")
+                       .with(StandaloneConfig.OFFSET_STORAGE_FILE_FILENAME_CONFIG, OFFSET_STORE_PATH)
+                       .with(EmbeddedEngine.OFFSET_FLUSH_INTERVAL_MS, 0)
+                       .build();
+
+        final String engineName = config.getString(EmbeddedEngine.ENGINE_NAME);
+        Converter keyConverter = config.getInstance(EmbeddedEngine.INTERNAL_KEY_CONVERTER_CLASS, Converter.class);
+        keyConverter.configure(config.subset(EmbeddedEngine.INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), true);
+        Converter valueConverter = config.getInstance(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS, Converter.class);
+        Configuration valueConverterConfig = config;
+        if (valueConverter instanceof JsonConverter) {
+            // Make sure that the JSON converter is configured to NOT enable schemas ...
+            valueConverterConfig = config.edit().with(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS + "".schemas.enable"", false).build();
+        }
+        valueConverter.configure(valueConverterConfig.subset(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS.name() + ""."", true).asMap(),
+                                 false);
+
+        // Create the worker config, adding extra fields that are required for validation of a worker config
+        // but that are not used within the embedded engine (since the source records are never serialized) ...
+        Map<String, String> embeddedConfig = config.asMap(EmbeddedEngine.ALL_FIELDS);
+        embeddedConfig.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, JsonConverter.class.getName());
+        embeddedConfig.put(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, JsonConverter.class.getName());
+        WorkerConfig workerConfig = new EmbeddedConfig(embeddedConfig);
+
+        FileOffsetBackingStore offsetStore = new FileOffsetBackingStore();
+        offsetStore.configure(workerConfig);
+        offsetStore.start();
+        try {
+            OffsetStorageReaderImpl offsetReader = new OffsetStorageReaderImpl(offsetStore, engineName, keyConverter, valueConverter);
+            return offsetReader.offsets(partitions);
+        } finally {
+            offsetStore.stop();
+        }
+    }
+
 }",2016-11-09T14:11:41Z,35
"@@ -232,7 +232,7 @@ protected boolean isBinlogAvailable() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking for binary logs: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking for binary logs: "", e);
         }
 
         // And compare with the one we're supposed to use ...
@@ -257,7 +257,7 @@ protected boolean isGtidModeEnabled() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
         }
 
         return !""OFF"".equalsIgnoreCase(mode.get());
@@ -277,7 +277,7 @@ protected String knownGtidSet() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
         }
 
         return gtidSetStr.get();",2016-07-29T05:57:47Z,68
"@@ -5,21 +5,22 @@
  */
 package io.debezium.connector.mysql;
 
+import static org.fest.assertions.Assertions.assertThat;
+
 import java.nio.file.Path;
 import java.sql.SQLException;
 import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.time.LocalTime;
 import java.time.Month;
 import java.time.ZoneId;
-
+import java.time.ZonedDateTime;
+import java.time.temporal.ChronoField;
+import java.time.temporal.ChronoUnit;
 import org.apache.kafka.connect.data.Struct;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
-
-import static org.fest.assertions.Assertions.assertThat;
-
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
@@ -110,12 +111,23 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 assertThat(c2.getTime() % 1000).isEqualTo(780);
                 assertThat(c3.getTime() % 1000).isEqualTo(780);
                 assertThat(c4.getTime() % 1000).isEqualTo(780);
-                assertThat(c1.getTime()).isEqualTo(1410134400000L);
-                assertThat(c2.getTime()).isEqualTo(64264780L);
-                assertThat(c3.getTime()).isEqualTo(1410198664780L);
-                assertThat(c4.getTime()).isEqualTo(1410198664780L);
-                // None of these Dates have timezone information, so to convert to locals we have to use our local timezone ...
                 ZoneId utc = ZoneId.of(""UTC"");
+                ZoneId defaultTZ = ZoneId.systemDefault();
+                LocalDate expectedDate = LocalDate.of(2014, 9, 8);
+                // the time is stored as 17:51:04.777 but rounded up to 780 due to the column configs
+                LocalTime expectedTime = LocalTime.of(17, 51, 4).plus(780, ChronoUnit.MILLIS);
+                // c1 '2014-09-08' is stored as a MySQL DATE (without any time) in the local TZ and then converted to 
+                // a truncated UTC by the connector, so we must assert against the same thing....
+                ZonedDateTime expectedC1UTC = ZonedDateTime.of(expectedDate, LocalTime.of(0, 0), defaultTZ)
+                                                           .withZoneSameInstant(utc)
+                                                           .truncatedTo(ChronoUnit.DAYS);
+                assertThat(c1.getTime()).isEqualTo(expectedC1UTC.toInstant().toEpochMilli());
+                ZonedDateTime expectedC2UTC = ZonedDateTime.of(LocalDate.ofEpochDay(0), expectedTime, utc);                         
+                assertThat(c2.getTime()).isEqualTo(expectedC2UTC.toInstant().toEpochMilli());
+                ZonedDateTime expectedC3UTC = ZonedDateTime.of(expectedDate, expectedTime, utc);
+                assertThat(c3.getTime()).isEqualTo(expectedC3UTC.toInstant().toEpochMilli());
+                assertThat(c4.getTime()).isEqualTo(expectedC3UTC.toInstant().toEpochMilli());
+                // None of these Dates have timezone information, so to convert to locals we have to use our local timezone ...
                 LocalDate localC1 = c1.toInstant().atZone(utc).toLocalDate();
                 LocalTime localC2 = c2.toInstant().atZone(utc).toLocalTime();
                 LocalDateTime localC3 = c3.toInstant().atZone(utc).toLocalDateTime();
@@ -124,7 +136,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 final int expectedNanos = 780 * 1000 * 1000;
                 assertThat(localC1.getYear()).isEqualTo(2014);
                 assertThat(localC1.getMonth()).isEqualTo(Month.SEPTEMBER);
-                assertThat(localC1.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC1.getDayOfMonth()).isEqualTo(expectedC1UTC.get(ChronoField.DAY_OF_MONTH));
                 assertThat(localC2.getHour()).isEqualTo(17);
                 assertThat(localC2.getMinute()).isEqualTo(51);
                 assertThat(localC2.getSecond()).isEqualTo(4);",2016-07-29T05:57:47Z,48
"@@ -298,21 +298,11 @@ protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordC
      * @throws InterruptedException if the thread was interrupted while waiting for a record to be returned
      */
     protected SourceRecords consumeRecordsByTopic(int numRecords) throws InterruptedException {
-        return consumeRecordsByTopic(numRecords, new SourceRecords());
-    }
-
-    /**
-     * Try to consume and capture exactly the specified number of records from the connector.
-     * 
-     * @param numRecords the number of records that should be consumed
-     * @param records the collector into which all consumed messages should be placed
-     * @return the actual number of records that were consumed
-     * @throws InterruptedException if the thread was interrupted while waiting for a record to be returned
-     */
-    protected SourceRecords consumeRecordsByTopic(int numRecords, SourceRecords records) throws InterruptedException {
+        SourceRecords records = new SourceRecords();
         consumeRecords(numRecords, records::add);
         return records;
     }
+    
 
     protected class SourceRecords {
         private final List<SourceRecord> records = new ArrayList<>();",2016-07-29T05:57:47Z,35
"@@ -217,9 +217,9 @@ protected void parseSetVariable(Marker start, AtomicReference<MySqlSystemVariabl
                         charsetNameForDatabase.put(currentDatabaseName, value);
                     }
                 }
-                
+
                 // Signal that the variable was set ...
-                signalEvent(new SetVariableEvent(variableName,value,statement(start)));
+                signalEvent(new SetVariableEvent(variableName, value, statement(start)));
             }
         }
     }
@@ -233,19 +233,19 @@ protected String parseVariableName() {
     }
 
     protected String parseVariableValue() {
-        if ( tokens.canConsumeAnyOf("","","";"")) {
+        if (tokens.canConsumeAnyOf("","", "";"")) {
             // The variable is blank ...
             return """";
         }
         Marker start = tokens.mark();
         tokens.consumeUntilEndOrOneOf("","", "";"");
         String value = tokens.getContentFrom(start);
-        if ( value.startsWith(""'"") && value.endsWith(""'"")) {
+        if (value.startsWith(""'"") && value.endsWith(""'"")) {
             // Remove the single quotes around the value ...
-            if ( value.length() <= 2 ) {
+            if (value.length() <= 2) {
                 value = """";
             } else {
-                value = value.substring(1, value.length()-2);
+                value = value.substring(1, value.length() - 2);
             }
         }
         return value;
@@ -264,7 +264,7 @@ protected void parseCreate(Marker marker) {
         } else if (tokens.matchesAnyOf(""EVENT"")) {
             parseCreateUnknown(marker);
         } else if (tokens.matchesAnyOf(""FUNCTION"", ""PROCEDURE"")) {
-            parseCreateUnknown(marker);
+            parseCreateProcedure(marker);
         } else if (tokens.matchesAnyOf(""UNIQUE"", ""FULLTEXT"", ""SPATIAL"", ""INDEX"")) {
             parseCreateIndex(marker);
         } else if (tokens.matchesAnyOf(""SERVER"")) {
@@ -276,6 +276,7 @@ protected void parseCreate(Marker marker) {
         } else {
             // It could be several possible things (including more elaborate forms of those matches tried above),
             sequentially(this::parseCreateView,
+                         this::parseCreateProcedure,
                          this::parseCreateUnknown);
         }
     }
@@ -697,7 +698,7 @@ protected void parseColumnDefinition(Marker start, String columnName, TokenStrea
             column.length(1);
         } else if (""SET"".equals(dataType.name())) {
             List<String> options = parseSetAndEnumOptions(dataType.expression());
-            //After DBZ-132, it will always be comma seperated
+            // After DBZ-132, it will always be comma seperated
             column.length(Math.max(0, options.size() * 2 - 1)); // number of options + number of commas
         } else {
             if (dataType.length() > -1) column.length((int) dataType.length());
@@ -857,10 +858,7 @@ protected void parseCreateView(Marker start) {
             tokens.consume('=');
             tokens.consumeAnyOf(""UNDEFINED"", ""MERGE"", ""TEMPTABLE"");
         }
-        if (tokens.canConsume(""DEFINER"")) {
-            tokens.consume('=');
-            tokens.consume(); // user or CURRENT_USER
-        }
+        parseDefiner(tokens.mark());
         if (tokens.canConsume(""SQL"", ""SECURITY"")) {
             tokens.consumeAnyOf(""DEFINER"", ""INVOKER"");
         }
@@ -972,6 +970,28 @@ protected void parseCreateIndex(Marker start) {
         debugParsed(start);
     }
 
+    protected void parseDefiner(Marker start) {
+        if (tokens.canConsume(""DEFINER"")) {
+            tokens.consume('=');
+            tokens.consume(); // user or CURRENT_USER
+            if (tokens.canConsume(""@"")) {
+                tokens.consume(); // host
+            } else {
+                String next = tokens.peek();
+                if (next.startsWith(""@"")) { // e.g., @`localhost`
+                    tokens.consume();
+                }
+            }
+        }
+    }
+
+    protected void parseCreateProcedure(Marker start) {
+        parseDefiner(tokens.mark());
+        tokens.consume(""FUNCTION"");
+        tokens.consume(); // name
+        consumeRemainingStatement(start);
+    }
+
     protected void parseCreateUnknown(Marker start) {
         consumeRemainingStatement(start);
     }
@@ -1324,14 +1344,14 @@ protected void sequentially(Consumer<Marker>... functions) {
                 tokens.rewind(marker);
             }
         }
-        parsingFailed(marker.position(), errors, ""Unable to parse statement"");
+        parsingFailed(marker.position(), errors, ""One or more errors trying to parse statement"");
     }
 
     /**
      * Parse and consume the {@code DEFAULT} clause. Currently, this method does not capture the default in any way,
      * since that will likely require parsing the default clause into a useful value (e.g., dealing with hexadecimals,
      * bit-set literals, date-time literals, etc.).
-
+     * 
      * @param start the marker at the beginning of the clause
      */
     protected void parseDefaultClause(Marker start) {",2016-12-06T23:34:52Z,49
"@@ -283,6 +283,23 @@ CREATE TABLE dbz_147_decimalvalues (
 INSERT INTO dbz_147_decimalvalues (pk_column, decimal_value)
 VALUES(default, 12345.67);
 
+-- DBZ-162 handle function declarations with newline characters
+DELIMITER $$
+CREATE FUNCTION fnDbz162( p_creditLimit DOUBLE ) RETURNS VARCHAR(10)
+    DETERMINISTIC
+BEGIN
+ DECLARE lvl VARCHAR(10);
+ IF p_creditLimit > 50000 THEN
+   SET lvl = 'PLATINUM';
+ ELSEIF (p_creditLimit <= 50000 AND p_creditLimit >= 10000) THEN
+   SET lvl = 'GOLD';
+ ELSEIF p_creditLimit < 10000 THEN
+   SET lvl = 'SILVER';
+ END IF;
+ RETURN (lvl);
+END;
+$$
+DELIMITER ;
 
 -- ----------------------------------------------------------------------------------------------------------------
 -- DATABASE:  json_test",2016-12-06T23:34:52Z,79
"@@ -18,6 +18,7 @@
 
 import static org.fest.assertions.Assertions.assertThat;
 
+import io.debezium.doc.FixFor;
 import io.debezium.relational.Column;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
@@ -465,6 +466,35 @@ public void shouldParseCreateTableWithEnumAndSetColumns() {
         assertThat(t.columnWithName(""c2"").position()).isEqualTo(2);
     }
 
+    @Test
+    public void shouldParseDefiner() {
+        String function = ""FUNCTION fnA`( a int, b int ) RETURNS tinyint(1) begin anything end;"";
+        String ddl = ""CREATE DEFINER='mysqluser'@'%' "" + function;
+        parser.parse(ddl, tables);
+        assertThat(tables.size()).isEqualTo(0); // no tables
+        assertThat(listener.total()).isEqualTo(0);
+
+        ddl = ""CREATE DEFINER='mysqluser'@'something' "" + function;
+        parser.parse(ddl, tables);
+        assertThat(tables.size()).isEqualTo(0); // no tables
+        assertThat(listener.total()).isEqualTo(0);
+
+        ddl = ""CREATE DEFINER=`mysqluser`@`something` "" + function;
+        parser.parse(ddl, tables);
+        assertThat(tables.size()).isEqualTo(0); // no tables
+        assertThat(listener.total()).isEqualTo(0);
+
+        ddl = ""CREATE DEFINER=CURRENT_USER "" + function;
+        parser.parse(ddl, tables);
+        assertThat(tables.size()).isEqualTo(0); // no tables
+        assertThat(listener.total()).isEqualTo(0);
+
+        ddl = ""CREATE DEFINER=CURRENT_USER() "" + function;
+        parser.parse(ddl, tables);
+        assertThat(tables.size()).isEqualTo(0); // no tables
+        assertThat(listener.total()).isEqualTo(0);
+    }
+
     @Test
     public void shouldParseGrantStatement() {
         String ddl = ""GRANT ALL PRIVILEGES ON `mysql`.* TO 'mysqluser'@'%'"";
@@ -630,6 +660,38 @@ public void shouldParseMySql57InitializationStatements() {
         listener.forEach(this::printEvent);
     }
 
+    @FixFor(""DBZ-162"")
+    @Test
+    public void shouldParseAndIgnoreCreateFunction() {
+        parser.parse(readFile(""ddl/mysql-dbz-162.ddl""), tables);
+        Testing.print(tables);
+        assertThat(tables.size()).isEqualTo(1); // 1 table
+        assertThat(listener.total()).isEqualTo(2); // 1 create, 1 alter
+        listener.forEach(this::printEvent);
+    }
+
+    @FixFor(""DBZ-162"")
+    @Test
+    public void shouldParseAlterTableWithNewlineFeeds() {
+        String ddl = ""CREATE TABLE `test` (id INT(11) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT);"";
+        parser.parse(ddl, tables);
+        assertThat(tables.size()).isEqualTo(1);
+        Table t = tables.forTable(new TableId(null, null, ""test""));
+        assertThat(t).isNotNull();
+        assertThat(t.columnNames()).containsExactly(""id"");
+        assertThat(t.primaryKeyColumnNames()).containsExactly(""id"");
+        assertColumn(t, ""id"", ""INT UNSIGNED"", Types.INTEGER, 11, -1, false, true, true);
+
+        ddl = ""ALTER TABLE `test` CHANGE `id` `collection_id` INT(11)\n UNSIGNED\n NOT NULL\n AUTO_INCREMENT;"";
+        parser.parse(ddl, tables);
+        assertThat(tables.size()).isEqualTo(1);
+        t = tables.forTable(new TableId(null, null, ""test""));
+        assertThat(t).isNotNull();
+        assertThat(t.columnNames()).containsExactly(""collection_id"");
+        assertThat(t.primaryKeyColumnNames()).containsExactly(""collection_id"");
+        assertColumn(t, ""collection_id"", ""INT UNSIGNED"", Types.INTEGER, 11, -1, false, true, true);
+    }
+    
     @Test
     public void shouldParseTicketMonsterLiquibaseStatements() {
         parser.parse(readLines(1, ""ddl/mysql-ticketmonster-liquibase.ddl""), tables);
@@ -640,7 +702,7 @@ public void shouldParseTicketMonsterLiquibaseStatements() {
 
     @Test
     public void shouldParseEnumOptions() {
-        assertParseEnumAndSetOptions(""ENUM('a','b','c')"",""a,b,c"");
+        assertParseEnumAndSetOptions(""ENUM('a','b','c')"", ""a,b,c"");
         assertParseEnumAndSetOptions(""ENUM('a','multi','multi with () paren', 'other')"", ""a,multi,multi with () paren,other"");
         assertParseEnumAndSetOptions(""ENUM('a')"", ""a"");
         assertParseEnumAndSetOptions(""ENUM()"", """");
@@ -660,6 +722,7 @@ public void shouldParseSetOptions() {
         assertParseEnumAndSetOptions(""SET () CHARACTER SET"", """");
     }
 
+    @FixFor(""DBZ-160"")
     @Test
     public void shouldParseCreateTableWithEnumDefault() {
         String ddl = ""CREATE TABLE t ( c1 ENUM('a','b','c') NOT NULL DEFAULT 'b', c2 ENUM('a', 'b', 'c') NOT NULL DEFAULT 'a');"";
@@ -673,6 +736,7 @@ public void shouldParseCreateTableWithEnumDefault() {
         assertColumn(t, ""c2"", ""ENUM"", Types.CHAR, 1, -1, false, false, false);
     }
 
+    @FixFor(""DBZ-160"")
     @Test
     public void shouldParseCreateTableWithBitDefault() {
         String ddl = ""CREATE TABLE t ( c1 Bit(2) NOT NULL DEFAULT b'1', c2 Bit(2) NOT NULL);"";",2016-12-06T23:34:52Z,26
"@@ -0,0 +1,31 @@
+CREATE DEFINER=`trunk2`@`%` FUNCTION `fnContainsMeasure`(
+ physId int,
+ measureId int
+ ) RETURNS tinyint(1)
+    DETERMINISTIC
+begin
+ select
+ s.Result
+ from (
+ SELECT exists(
+ SELECT
+ ph.PhysicianId is not null as Result
+ FROM Physician ph
+ JOIN ReportingOptions ro ON ph.ReportingOptionsId = ro.Id and ph.PhysicianId = physId
+ JOIN ReportingOptionsIndividualMeasure roim ON ro.Id = roim.ReportingOptionsId
+ WHERE
+ roim.IndividualMeasureId = measureId
+ ) as Result
+ ) s
+ limit 1
+ into @result;
+ return @result;
+end;
+
+CREATE TABLE `test` (id INT(11) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT);
+
+ALTER TABLE `test` CHANGE `id` `collection_id` INT(11)
+UNSIGNED
+NOT NULL
+AUTO_INCREMENT;
+",2016-12-06T23:34:52Z,80
"@@ -582,10 +582,15 @@ protected void consumeRemainingStatement(Marker start) {
             }
             if (tokens.canConsume(""BEGIN"")) {
                 tokens.consumeThrough(""END"");
+                while (tokens.canConsume(""IF"")) {
+                    // We just read through an 'END IF', but need to read until the next 'END'
+                    tokens.consumeThrough(""END"");
+                }
             } else if (tokens.matches(DdlTokenizer.STATEMENT_TERMINATOR)) {
                 tokens.consume();
                 break;
             }
+            if (!tokens.hasNext()) return;
             tokens.consume();
         }
     }",2016-12-06T23:34:52Z,65
"@@ -80,8 +80,8 @@ protected Tables recover(long pos, int entry) {
     @Test
     public void shouldRecordChangesAndRecoverToVariousPoints() {
         record(01, 0, ""CREATE TABLE foo ( first VARCHAR(22) NOT NULL );"", all, t3, t2, t1, t0);
-        record(23, 1, ""CREATE TABLE person ( name VARCHAR(22) NOT NULL );"", all, t3, t2, t1);
-        record(30, 2, ""CREATE TABLE address ( street VARCHAR(22) NOT NULL );"", all, t3, t2);
+        record(23, 1, ""CREATE TABLE\\nperson ( name VARCHAR(22) NOT NULL );"", all, t3, t2, t1);
+        record(30, 2, ""CREATE TABLE address\\n( street VARCHAR(22) NOT NULL );"", all, t3, t2);
         record(32, 3, ""ALTER TABLE address ADD city VARCHAR(22) NOT NULL;"", all, t3);
 
         // Testing.Print.enable();",2016-12-06T23:34:52Z,76
"@@ -0,0 +1,53 @@
+<?xml version=""1.0""?>
+<project xmlns=""http://maven.apache.org/POM/4.0.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd"">
+    <parent>
+        <groupId>io.debezium</groupId>
+        <artifactId>debezium-parent</artifactId>
+        <version>0.1-SNAPSHOT</version>
+        <relativePath>../pom.xml</relativePath>
+    </parent>
+    <modelVersion>4.0.0</modelVersion>
+    <artifactId>debezium-assembly-descriptors</artifactId>
+    <name>Debezium Assembly Descriptors</name>
+    <build>
+        <plugins>
+            <plugin>
+                <!-- We don't want to run this -->
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-source-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <id>attach-sources</id>
+                        <phase>non-existant</phase>
+                    </execution>
+                    <execution>
+                        <id>attach-test-sources</id>
+                        <phase>non-existant</phase>
+                    </execution>
+                </executions>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-surefire-plugin</artifactId>
+                <configuration>
+                    <skip>true</skip>
+                </configuration>
+            </plugin>
+            <plugin>
+                <groupId>org.apache.maven.plugins</groupId>
+                <artifactId>maven-jar-plugin</artifactId>
+                <executions>
+                    <execution>
+                        <!-- We don't want to create a test JAR -->
+                        <id>test-jar</id>
+                        <phase>non-existant</phase>
+                        <goals>
+                            <goal>test-jar</goal>
+                        </goals>
+                    </execution>
+                </executions>
+            </plugin>
+        </plugins>
+    </build>
+</project>
\ No newline at end of file",2016-02-23T19:23:36Z,81
"@@ -0,0 +1,47 @@
+<assembly xmlns=""http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2"" 
+  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
+  xsi:schemaLocation=""http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.2 http://maven.apache.org/xsd/assembly-1.1.2.xsd"">
+  <id>plugin</id>
+  <formats>
+    <format>tar.gz</format>
+    <format>zip</format>
+  </formats>
+  <includeBaseDirectory>false</includeBaseDirectory>
+  <dependencySets>
+      <dependencySet>
+          <outputDirectory>${project.artifactId}</outputDirectory>
+          <unpack>false</unpack>
+          <scope>runtime</scope>
+          <useProjectArtifact>false</useProjectArtifact>
+          <excludes>
+              <exclude>${project.groupId}:${project.artifactId}:*</exclude>
+              <!-- Exclude all Kafka APIs and their dependencies, since they will be available in the runtime -->
+              <exclude>org.apache.kafka:*</exclude>
+              <exclude>org.xerial.snappy:*</exclude>
+              <exclude>net.jpountz.lz4:*</exclude>
+          </excludes>
+      </dependencySet>
+      <dependencySet>
+          <outputDirectory>${project.artifactId}</outputDirectory>
+          <unpack>false</unpack>
+          <includes>
+              <include>${project.groupId}:${project.artifactId}:*</include>
+          </includes>
+      </dependencySet>
+  </dependencySets>
+  <fileSets>
+    <fileSet>
+      <!-- Get the files from the top-level directory, which should be above the connectors -->
+      <directory>${project.basedir}/..</directory>
+      <outputDirectory>${project.artifactId}</outputDirectory>
+      <includes>
+        <include>README*</include>
+        <include>CHANGELOG*</include>
+        <include>CONTRIBUTE*</include>
+        <include>COPYRIGHT*</include>
+        <include>LICENSE*</include>
+      </includes>
+      <useDefaultExcludes>true</useDefaultExcludes>
+    </fileSet>
+  </fileSets>
+</assembly>",2016-02-23T19:23:36Z,82
"@@ -246,6 +246,36 @@
     Define several profiles for working with different Docker images (or no Docker whatsoever)
     -->
     <profiles>
+        <profile>
+            <id>assembly</id>
+            <activation>
+                <activeByDefault>false</activeByDefault>
+            </activation>
+            <build>
+                <plugins>
+                    <plugin>
+                        <artifactId>maven-assembly-plugin</artifactId>
+                        <executions>
+                            <execution>
+                                <id>default</id>
+                                <phase>package</phase>
+                                <goals>
+                                    <goal>single</goal>
+                                </goals>
+                                <configuration>
+                                    <finalName>${project.artifactId}-${project.version}</finalName>
+                                    <attach>true</attach>  <!-- we want attach & deploy these to Maven -->
+                                    <descriptorRefs>
+                                        <descriptorRef>connector-distribution</descriptorRef>
+                                    </descriptorRefs>
+                                </configuration>
+                            </execution>
+                        </executions>
+                    </plugin>
+                </plugins>
+            </build>
+        </profile>
+
       <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
             Do not perform any Docker-related functionality
             To use, specify ""-DskipITs"" on the Maven command line.",2016-02-23T19:23:36Z,83
"@@ -92,6 +92,7 @@
     </properties>
     <modules>
         <module>support/checkstyle</module>
+        <module>debezium-assembly-descriptors</module>
         <module>debezium-core</module>
         <module>debezium-embedded</module>
         <module>debezium-connector-mysql</module>
@@ -348,6 +349,24 @@
                     <artifactId>docker-maven-plugin</artifactId>
                     <version>${version.docker.maven.plugin}</version>
                 </plugin>
+                <plugin>
+                    <artifactId>maven-assembly-plugin</artifactId>
+                    <dependencies>
+                        <dependency>
+                            <groupId>io.debezium</groupId>
+                            <artifactId>debezium-assembly-descriptors</artifactId>
+                            <version>${project.version}</version>
+                        </dependency>
+                    </dependencies>
+                    <executions>
+                        <execution>
+                            <phase>package</phase>
+                            <goals>
+                                <goal>single</goal>
+                            </goals>
+                        </execution>
+                    </executions>
+                </plugin>
             </plugins>
         </pluginManagement>
         <plugins>",2016-02-23T19:23:36Z,60
"@@ -8,6 +8,7 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Deque;
+import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.concurrent.BlockingQueue;
@@ -19,6 +20,7 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
 import java.util.function.BooleanSupplier;
+import java.util.function.Consumer;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -52,6 +54,7 @@ public final class MongoDbConnectorTask extends SourceTask {
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final AtomicBoolean running = new AtomicBoolean(false);
     private final Deque<Replicator> replicators = new ConcurrentLinkedDeque<>();
+    private final RecordBatchSummarizer recordSummarizer = new RecordBatchSummarizer();
 
     // These are all effectively constants between start(...) and stop(...)
     private volatile TaskRecordQueue queue;
@@ -79,7 +82,6 @@ public void start(Map<String, String> props) {
         if (context == null) {
             throw new ConnectException(""Unexpected null context"");
         }
-        
 
         // Read the configuration and set up the replication context ...
         final Configuration config = Configuration.from(props);
@@ -107,7 +109,7 @@ public void start(Map<String, String> props) {
             final ReplicaSets replicaSets = ReplicaSets.parse(hosts);
 
             // Set up the task record queue ...
-            this.queue = new TaskRecordQueue(config, replicaSets.replicaSetCount(), running::get);
+            this.queue = new TaskRecordQueue(config, replicaSets.replicaSetCount(), running::get, recordSummarizer);
 
             // Get the offsets for each of replica set partition ...
             SourceInfo source = replicationContext.source();
@@ -191,14 +193,17 @@ protected static class TaskRecordQueue {
         private final Metronome metronome;
         private final BlockingQueue<SourceRecord> records;
         private final BooleanSupplier isRunning;
+        private final Consumer<List<SourceRecord>> batchConsumer;
 
-        protected TaskRecordQueue(Configuration config, int numThreads, BooleanSupplier isRunning) {
+        protected TaskRecordQueue(Configuration config, int numThreads, BooleanSupplier isRunning,
+                Consumer<List<SourceRecord>> batchConsumer) {
             final int maxQueueSize = config.getInteger(MongoDbConnectorConfig.MAX_QUEUE_SIZE);
             final long pollIntervalMs = config.getLong(MongoDbConnectorConfig.POLL_INTERVAL_MS);
             maxBatchSize = config.getInteger(MongoDbConnectorConfig.MAX_BATCH_SIZE);
             metronome = Metronome.parker(pollIntervalMs, TimeUnit.MILLISECONDS, Clock.SYSTEM);
             records = new LinkedBlockingDeque<>(maxQueueSize);
             this.isRunning = isRunning;
+            this.batchConsumer = batchConsumer != null ? batchConsumer : (records) -> {};
         }
 
         public List<SourceRecord> poll() throws InterruptedException {
@@ -207,6 +212,7 @@ public List<SourceRecord> poll() throws InterruptedException {
                 // No events to process, so sleep for a bit ...
                 metronome.pause();
             }
+            this.batchConsumer.accept(batch);
             return batch;
         }
 
@@ -222,4 +228,50 @@ public void enqueue(SourceRecord record) throws InterruptedException {
             }
         }
     }
+
+    protected final class RecordBatchSummarizer implements Consumer<List<SourceRecord>> {
+        private final Map<String, ReplicaSetSummary> summaryByReplicaSet = new HashMap<>();
+
+        @Override
+        public void accept(List<SourceRecord> records) {
+            if (records.isEmpty()) return;
+            if (!logger.isInfoEnabled()) return;
+            summaryByReplicaSet.clear();
+            records.forEach(record -> {
+                String replicaSetName = SourceInfo.replicaSetNameForPartition(record.sourcePartition());
+                if (replicaSetName != null) {
+                    summaryByReplicaSet.computeIfAbsent(replicaSetName, rsName -> new ReplicaSetSummary()).add(record);
+                }
+            });
+            if (!summaryByReplicaSet.isEmpty()) {
+                PreviousContext prevContext = replContext.configureLoggingContext(""task"");
+                try {
+                    summaryByReplicaSet.forEach((rsName, summary) -> {
+                        logger.info(""{} records sent for replica set '{}', last offset: {}"",
+                                    summary.recordCount(), rsName, summary.lastOffset());
+                    });
+                } finally {
+                    prevContext.restore();
+                }
+            }
+        }
+    }
+
+    protected static final class ReplicaSetSummary {
+        private int numRecords = 0;
+        private Map<String, ?> lastOffset;
+
+        public void add(SourceRecord record) {
+            ++numRecords;
+            lastOffset = record.sourceOffset();
+        }
+
+        public int recordCount() {
+            return numRecords;
+        }
+
+        public Map<String, ?> lastOffset() {
+            return lastOffset;
+        }
+    }
 }",2016-09-06T16:32:04Z,84
"@@ -131,6 +131,17 @@ public Long getOperationId() {
         }
     }
 
+    /**
+     * Get the replica set name for the given partition.
+     * 
+     * @param partition the partition map
+     * @return the replica set name (when the partition is valid), or {@code null} if the partition is null or has no replica
+     *         set name entry
+     */
+    public static String replicaSetNameForPartition(Map<String, ?> partition) {
+        return partition != null ? (String) partition.get(REPLICA_SET_NAME) : null;
+    }
+
     private final String serverName;
 
     public SourceInfo(String serverName) {",2016-09-06T16:32:04Z,85
"@@ -98,7 +98,14 @@ protected void initializeDataTypes(DataTypeParser dataTypes) {
         dataTypes.register(Types.BLOB, ""VARCHAR(L) BINARY"");
         dataTypes.register(Types.BLOB, ""BINARY[(L)]"");
         dataTypes.register(Types.VARCHAR, ""VARCHAR(L)"");
+        dataTypes.register(Types.NVARCHAR, ""NVARCHAR(L)"");
+        dataTypes.register(Types.NVARCHAR, ""NATIONAL VARCHAR(L)"");
+        dataTypes.register(Types.NVARCHAR, ""NCHAR VARCHAR(L)"");
+        dataTypes.register(Types.NVARCHAR, ""NATIONAL CHARACTER VARYING(L)"");
+        dataTypes.register(Types.NVARCHAR, ""NATIONAL CHAR VARYING(L)"");
         dataTypes.register(Types.CHAR, ""CHAR[(L)]"");
+        dataTypes.register(Types.NCHAR, ""NCHAR[(L)]"");
+        dataTypes.register(Types.NCHAR, ""NATIONAL CHARACTER(L)"");
         dataTypes.register(Types.VARBINARY, ""VARBINARY(L)"");
         dataTypes.register(Types.BLOB, ""TINYBLOB"");
         dataTypes.register(Types.BLOB, ""BLOB"");
@@ -705,6 +712,11 @@ protected void parseColumnDefinition(Marker start, String columnName, TokenStrea
             if (dataType.scale() > -1) column.scale(dataType.scale());
         }
 
+        if(Types.NCHAR == dataType.jdbcType() || Types.NVARCHAR == dataType.jdbcType()) {
+            // NCHAR and NVARCHAR columns always uses utf8 as charset
+            column.charsetName(""utf8"");
+        }
+
         if (tokens.canConsume(""CHARSET"") || tokens.canConsume(""CHARACTER"", ""SET"")) {
             String charsetName = tokens.consume();
             if (!""DEFAULT"".equalsIgnoreCase(charsetName)) {",2016-12-07T06:38:30Z,49
"@@ -750,6 +750,25 @@ public void shouldParseCreateTableWithBitDefault() {
         assertColumn(t, ""c2"", ""BIT"", Types.BIT, 2, -1, false, false, false);
     }
 
+    @Test
+    public void shouldParseStatementForDbz142() {
+        parser.parse(readFile(""ddl/mysql-dbz-142.ddl""), tables);
+        Testing.print(tables);
+        assertThat(tables.size()).isEqualTo(2);
+        assertThat(listener.total()).isEqualTo(2);
+
+        Table t = tables.forTable(new TableId(null, null, ""nvarchars""));
+        assertColumn(t, ""c1"", ""NVARCHAR"", Types.NVARCHAR, 255, ""utf8"", true);
+        assertColumn(t, ""c2"", ""NATIONAL VARCHAR"", Types.NVARCHAR, 255, ""utf8"", true);
+        assertColumn(t, ""c3"", ""NCHAR VARCHAR"", Types.NVARCHAR, 255, ""utf8"", true);
+        assertColumn(t, ""c4"", ""NATIONAL CHARACTER VARYING"", Types.NVARCHAR, 255, ""utf8"", true);
+        assertColumn(t, ""c5"", ""NATIONAL CHAR VARYING"", Types.NVARCHAR, 255, ""utf8"", true);
+
+        Table t2 = tables.forTable(new TableId(null, null, ""nchars""));
+        assertColumn(t2, ""c1"", ""NATIONAL CHARACTER"", Types.NCHAR, 10, ""utf8"", true);
+        assertColumn(t2, ""c2"", ""NCHAR"", Types.NCHAR, 10, ""utf8"", true);
+    }
+
     protected void assertParseEnumAndSetOptions(String typeExpression, String optionString) {
         List<String> options = MySqlDdlParser.parseSetAndEnumOptions(typeExpression);
         String commaSeperatedOptions = Strings.join("","", options);",2016-12-07T06:38:30Z,26
"@@ -0,0 +1,2 @@
+CREATE TABLE nvarchars ( c1 NVARCHAR(255), c2 NATIONAL VARCHAR(255), c3 NCHAR VARCHAR(255), c4 NATIONAL CHARACTER VARYING(255), c5 NATIONAL CHAR VARYING(255) );
+CREATE TABLE nchars ( c1 NATIONAL CHARACTER(10), c2 NCHAR(10) );",2016-12-07T06:38:30Z,86
"@@ -70,6 +70,8 @@ public class BinlogReader extends AbstractReader {
     private final ElapsedTimeStrategy pollOutputDelay;
     private long recordCounter = 0L;
     private long previousOutputMillis = 0L;
+    private long initialEventsToSkip = 0L;
+    private boolean skipEvent = false;
     private final AtomicLong totalRecordCounter = new AtomicLong();
     private volatile Map<String, ?> lastOffset = null;
     private com.github.shyiko.mysql.binlog.GtidSet gtidSet;
@@ -164,15 +166,21 @@ protected void doStart() {
             logger.info(""Registering binlog reader with GTID set: {}"", filteredGtidSet);
             String filteredGtidSetStr = filteredGtidSet.toString();
             client.setGtidSet(filteredGtidSetStr);
-            source.setGtidSet(filteredGtidSetStr);
+            source.setCompletedGtidSet(filteredGtidSetStr);
             gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(filteredGtidSetStr);
         } else {
             client.setBinlogFilename(source.binlogFilename());
-            client.setBinlogPosition(source.nextBinlogPosition());
+            client.setBinlogPosition(source.binlogPosition());
         }
 
+        // We may be restarting in the middle of a transaction, so see how far into the transaction we have already processed...
+        initialEventsToSkip = source.eventsToSkipUponRestart();
+
         // Set the starting row number, which is the next row number to be read ...
-        startingRowNumber = source.nextEventRowNumber();
+        startingRowNumber = source.rowsToSkipUponRestart();
+
+        // Only when we reach the first BEGIN event will we start to skip events ...
+        skipEvent = false;
 
         // Initial our poll output delay logic ...
         pollOutputDelay.hasElapsed();
@@ -287,8 +295,15 @@ protected void handleEvent(Event event) {
             // Forward the event to the handler ...
             eventHandlers.getOrDefault(eventType, this::ignoreEvent).accept(event);
 
-            // And after that event has been processed, always set the starting row number to 0 ...
-            startingRowNumber = 0;
+            // Capture that we've completed another event ...
+            source.completeEvent();
+
+            if (skipEvent) {
+                // We're in the mode of skipping events and we just skipped this one, so decrement our skip count ...
+                --initialEventsToSkip;
+                skipEvent = initialEventsToSkip > 0;
+            }
+
         } catch (RuntimeException e) {
             // There was an error in the event handler, so propagate the failure to Kafka Connect ...
             failed(e, ""Error processing binlog event"");
@@ -375,8 +390,7 @@ protected void handleGtidEvent(Event event) {
         GtidEventData gtidEvent = unwrapData(event);
         String gtid = gtidEvent.getGtid();
         gtidSet.add(gtid);
-        source.setGtid(gtid);
-        source.setGtidSet(gtidSet.toString()); // rather than use the client's GTID set
+        source.startGtid(gtid, gtidSet.toString()); // rather than use the client's GTID set
     }
 
     /**
@@ -387,14 +401,23 @@ protected void handleGtidEvent(Event event) {
      */
     protected void handleQueryEvent(Event event) {
         QueryEventData command = unwrapData(event);
-        logger.debug(""Received update table command: {}"", event);
+        logger.debug(""Received query command: {}"", event);
         String sql = command.getSql().trim();
         if (sql.equalsIgnoreCase(""BEGIN"")) {
-            // ignore these altogether ...
+            // We are starting a new transaction ...
+            source.startNextTransaction();
+            if (initialEventsToSkip != 0) {
+                logger.debug(""Restarting partially-processed transaction; change events will not be created for the first {} events plus {} more rows in the next event"",
+                             initialEventsToSkip, startingRowNumber);
+                // We are restarting, so we need to skip the events in this transaction that we processed previously...
+                skipEvent = true;
+            }
             return;
         }
         if (sql.equalsIgnoreCase(""COMMIT"")) {
-            // ignore these altogether ...
+            // We are completing the transaction ...
+            source.commitTransaction();
+            skipEvent = false;
             return;
         }
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
@@ -438,6 +461,11 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
@@ -447,13 +475,26 @@ protected void handleInsert(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                count += recordMaker.create(rows.get(row), ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    count += recordMaker.create(rows.get(row), ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} insert record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} insert record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed insert event: {}"", event);
             }
-            logger.debug(""Recorded {} insert records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping insert row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     /**
@@ -463,6 +504,11 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
@@ -473,16 +519,29 @@ protected void handleUpdate(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
-                Serializable[] before = changes.getKey();
-                Serializable[] after = changes.getValue();
-                count += recordMaker.update(before, after, ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
+                    Serializable[] before = changes.getKey();
+                    Serializable[] after = changes.getValue();
+                    count += recordMaker.update(before, after, ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} update record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} update record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed update event: {}"", event);
             }
-            logger.debug(""Recorded {} update records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping update row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     /**
@@ -492,6 +551,11 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
@@ -501,13 +565,26 @@ protected void handleDelete(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                count += recordMaker.delete(rows.get(row), ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    count += recordMaker.delete(rows.get(row), ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} delete record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} delete record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed delete event: {}"", event);
             }
-            logger.debug(""Recorded {} delete records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping delete row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     protected SSLMode sslModeFor(SecureConnectionMode mode) {",2016-11-09T14:11:41Z,67
"@@ -139,7 +139,7 @@ public synchronized void start(Map<String, String> props) {
             if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
                 // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
                 // previously used but the MySQL server has them enabled ...
-                source.setGtidSet("""");
+                source.setCompletedGtidSet("""");
             }
 
             // Check whether the row-level binlog is enabled ...",2016-11-09T14:11:41Z,68
"@@ -265,11 +265,11 @@ public ObjectName metricName(String contextName) throws MalformedObjectNameExcep
      *         none were filtered
      */
     public GtidSet filterGtidSet(GtidSet availableServerGtidSet) {
-        logger.info(""Attempting to generate a filtered GTID set"");
         String gtidStr = source.gtidSet();
         if (gtidStr == null) {
             return null;
         }
+        logger.info(""Attempting to generate a filtered GTID set"");
         logger.info(""GTID set from previous recorded offset: {}"", gtidStr);
         GtidSet filteredGtidSet = new GtidSet(gtidStr);
         Predicate<String> gtidSourceFilter = gtidSourceFilter();",2016-11-09T14:11:41Z,69
"@@ -222,7 +222,7 @@ protected void execute() {
                     if (rs.getMetaData().getColumnCount() > 4) {
                         // This column exists only in MySQL 5.6.5 or later ...
                         String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
-                        source.setGtidSet(gtidSet);
+                        source.setCompletedGtidSet(gtidSet);
                         logger.info(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
                                     gtidSet);
                     } else {",2016-11-09T14:11:41Z,62
"@@ -32,10 +32,13 @@
  * </pre>
  * 
  * <p>
- * The {@link #offset() source offset} information describes how much of the database's binary log the source the change detector
- * has already processed, and it includes the {@link #binlogFilename() binlog filename}, the {@link #nextBinlogPosition() next
- * position} in the binlog to start reading, and the {@link #nextEventRowNumber() next event row number}. Here's a JSON-like
- * representation of an example:
+ * The {@link #offset() source offset} information is included in each event and captures where the connector should restart
+ * if this event's offset is the last one recorded. The offset includes the {@link #binlogFilename() binlog filename},
+ * the {@link #binlogPosition() position of the first event} in the binlog, the
+ * {@link #eventsToSkipUponRestart() number of events to skip}, and the
+ * {@link #rowsToSkipUponRestart() number of rows to also skip}.
+ * <p>
+ * Here's a JSON-like representation of an example:
  * 
  * <pre>
  * {
@@ -44,22 +47,26 @@
  *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
  *     ""file"": ""mysql-bin.000003"",
  *     ""pos"" = 990,
+ *     ""event"" = 0,
  *     ""row"": 0,
  *     ""snapshot"": true
  * }
  * </pre>
- * 
+ * <p>
  * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
  * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
  * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
  * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
- * 
- * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
- * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), the
- * {@link #lastBinlogPosition() position} of the event (and {@link #lastEventRowNumber() row number} within the event) inside
- * the {@link #binlogFilename() binlog file}. When GTIDs are enabled, it also includes the GTID of the transaction in which the
- * event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced when the connector is in the
- * middle of a snapshot. Here's a JSON-like representation of the source for an event that corresponds to the above partition and
+ * <p>
+ * Each change event envelope also includes the {@link #struct() source} struct that contains MySQL information about that
+ * particular event, including a mixture the fields from the {@link #partition() partition} (which is renamed in the source to
+ * make more sense), the binlog filename and position where the event can be found, and when GTIDs are enabled the GTID of the
+ * transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced
+ * when the connector is in the middle of a snapshot. Note that this information is likely different than the offset information,
+ * since the connector may need to restart from either just after the most recently completed transaction or the beginning
+ * of the most recently started transaction (whichever appears later in the binlog).
+ * <p>
+ * Here's a JSON-like representation of the source for an event that corresponds to the above partition and
  * offset:
  * 
  * <pre>
@@ -88,9 +95,10 @@ final class SourceInfo {
     public static final String SERVER_PARTITION_KEY = ""server"";
     public static final String GTID_SET_KEY = ""gtids"";
     public static final String GTID_KEY = ""gtid"";
+    public static final String EVENTS_TO_SKIP_OFFSET_KEY = ""event"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
-    public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
+    public static final String BINLOG_ROW_IN_EVENT_OFFSET_KEY = ""row"";
     public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
@@ -105,17 +113,22 @@ final class SourceInfo {
                                                      .field(GTID_KEY, Schema.OPTIONAL_STRING_SCHEMA)
                                                      .field(BINLOG_FILENAME_OFFSET_KEY, Schema.STRING_SCHEMA)
                                                      .field(BINLOG_POSITION_OFFSET_KEY, Schema.INT64_SCHEMA)
-                                                     .field(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, Schema.INT32_SCHEMA)
+                                                     .field(BINLOG_ROW_IN_EVENT_OFFSET_KEY, Schema.INT32_SCHEMA)
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private String gtidSet;
-    private String binlogGtid;
-    private String binlogFilename;
-    private long lastBinlogPosition = 0;
-    private int lastEventRowNumber = 0;
-    private long nextBinlogPosition = 4;
-    private int nextEventRowNumber = 0;
+    private String currentGtidSet;
+    private String currentGtid;
+    private String currentBinlogFilename;
+    private long currentBinlogPosition = 0L;
+    private int currentRowNumber = 0;
+    private long currentEventLengthInBytes = 0;
+    private String restartGtidSet;
+    private String restartBinlogFilename;
+    private long restartBinlogPosition = 0L;
+    private long restartEventsToSkip = 0;
+    private int restartRowsToSkip = 0;
+    private boolean inTransaction = false;
     private String serverName;
     private long serverId = 0;
     private long binlogTimestampSeconds = 0;
@@ -149,50 +162,91 @@ public Map<String, String> partition() {
         return sourcePartition;
     }
 
+    /**
+     * Set the position in the MySQL binlog where we will start reading.
+     * 
+     * @param binlogFilename the name of the binary log file; may not be null
+     * @param positionOfFirstEvent the position in the binary log file to begin processing
+     */
+    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
+        if (binlogFilename != null) {
+            this.currentBinlogFilename = binlogFilename;
+            this.restartBinlogFilename = binlogFilename;
+        }
+        assert positionOfFirstEvent >= 0;
+        this.currentBinlogPosition = positionOfFirstEvent;
+        this.restartBinlogPosition = positionOfFirstEvent;
+        this.currentRowNumber = 0;
+        this.restartRowsToSkip = 0;
+    }
+
+    /**
+     * Set the position within the MySQL binary log file of the <em>current event</em>.
+     * 
+     * @param positionOfCurrentEvent the position within the binary log file of the current event
+     * @param eventSizeInBytes the size in bytes of this event
+     */
+    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
+        this.currentBinlogPosition = positionOfCurrentEvent;
+        this.currentEventLengthInBytes = eventSizeInBytes;
+        if (!inTransaction) {
+            this.restartBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
+        }
+        // Don't set anything else, since the row numbers are set in the offset(int,int) method called at least once
+        // for each processed event
+    }
+
     /**
      * Get the Kafka Connect detail about the source ""offset"", which describes the position within the source where we last
      * have last read.
      * 
      * @return a copy of the current offset; never null
      */
     public Map<String, ?> offset() {
-        return offsetUsingPosition(nextBinlogPosition);
+        return offsetUsingPosition(this.restartRowsToSkip);
     }
 
     /**
-     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
-     * describes the position within the source where we have last read.
+     * Given the row number within a binlog event and the total number of rows in that event, compute and return the
+     * Kafka Connect offset that is be included in the produced change event describing the row.
      * <p>
      * This method should always be called before {@link #struct()}.
      * 
-     * @param eventRowNumber the 0-based row number within the event being processed
+     * @param eventRowNumber the 0-based row number within the event for which the offset is to be produced
      * @param totalNumberOfRows the total number of rows within the event being processed
      * @return a copy of the current offset; never null
+     * @see #struct()
      */
     public Map<String, ?> offsetForRow(int eventRowNumber, int totalNumberOfRows) {
         if (eventRowNumber < (totalNumberOfRows - 1)) {
             // This is not the last row, so our offset should record the next row to be used ...
-            this.lastEventRowNumber = eventRowNumber;
-            this.nextEventRowNumber = eventRowNumber + 1;
+            this.currentRowNumber = eventRowNumber;
+            this.restartRowsToSkip = this.currentRowNumber + 1;
             // so write out the offset with the position of this event
-            return offsetUsingPosition(lastBinlogPosition);
+            return offsetUsingPosition(this.restartRowsToSkip);
         }
         // This is the last row, so write out the offset that has the position of the next event ...
-        this.lastEventRowNumber = this.nextEventRowNumber;
-        this.nextEventRowNumber = 0;
-        return offsetUsingPosition(nextBinlogPosition);
+        this.currentRowNumber = eventRowNumber;
+        this.restartRowsToSkip = 0;
+        return offsetUsingPosition(totalNumberOfRows);
     }
 
-    private Map<String, ?> offsetUsingPosition(long binlogPosition) {
+    private Map<String, ?> offsetUsingPosition(long rowsToSkip) {
         Map<String, Object> map = new HashMap<>();
         if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
-        if (gtidSet != null) {
-            map.put(GTID_SET_KEY, gtidSet);
+        if (restartGtidSet != null) {
+            // Put the previously-completed GTID set in the offset along with the event number ...
+            map.put(GTID_SET_KEY, restartGtidSet);
+        }
+        map.put(BINLOG_FILENAME_OFFSET_KEY, restartBinlogFilename);
+        map.put(BINLOG_POSITION_OFFSET_KEY, restartBinlogPosition);
+        if (restartEventsToSkip != 0) {
+            map.put(EVENTS_TO_SKIP_OFFSET_KEY, restartEventsToSkip);
         }
-        map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, nextEventRowNumber);
+        if (rowsToSkip != 0) {
+            map.put(BINLOG_ROW_IN_EVENT_OFFSET_KEY, rowsToSkip);
+        }
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             map.put(SNAPSHOT_KEY, true);
         }
@@ -223,13 +277,13 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        // Don't put the GTID Set into the struct; only the current GTID is fine ...
-        if (binlogGtid != null) {
-            result.put(GTID_KEY, binlogGtid);
+        if (currentGtid != null) {
+            // Don't put the GTID Set into the struct; only the current GTID is fine ...
+            result.put(GTID_KEY, currentGtid);
         }
-        result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        result.put(BINLOG_POSITION_OFFSET_KEY, lastBinlogPosition);
-        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, lastEventRowNumber);
+        result.put(BINLOG_FILENAME_OFFSET_KEY, currentBinlogFilename);
+        result.put(BINLOG_POSITION_OFFSET_KEY, currentBinlogPosition);
+        result.put(BINLOG_ROW_IN_EVENT_OFFSET_KEY, currentRowNumber);
         result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (lastSnapshot) {
             result.put(SNAPSHOT_KEY, true);
@@ -246,54 +300,73 @@ public boolean isSnapshotInEffect() {
         return nextSnapshot;
     }
 
+    public void startNextTransaction() {
+        // If we have to restart, then we'll start with this BEGIN transaction
+        this.restartRowsToSkip = 0;
+        this.restartEventsToSkip = 0;
+        this.restartBinlogFilename = this.currentBinlogFilename;
+        this.restartBinlogPosition = this.currentBinlogPosition;
+        this.inTransaction = true;
+    }
+
     /**
-     * Set the latest GTID from the MySQL binary log file.
-     * 
-     * @param gtid the string representation of a specific GTID; may not be null
+     * Capture that we're starting a new event.
      */
-    public void setGtid(String gtid) {
-        this.binlogGtid = gtid;
+    public void completeEvent() {
+        ++restartEventsToSkip;
     }
 
     /**
-     * Set the set of GTIDs known to the MySQL server.
+     * Get the number of events after the last transaction BEGIN that we've already processed.
      * 
-     * @param gtidSet the string representation of GTID set; may not be null
+     * @return the number of events in the transaction that have been processed completely
+     * @see #completeEvent()
+     * @see #startNextTransaction()
      */
-    public void setGtidSet(String gtidSet) {
-        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
-            this.gtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """"); // remove all of the newline chars if they exist
-        }
+    public long eventsToSkipUponRestart() {
+        return restartEventsToSkip;
+    }
+
+    public void commitTransaction() {
+        this.restartGtidSet = this.currentGtidSet;
+        this.restartBinlogFilename = this.currentBinlogFilename;
+        this.restartBinlogPosition = this.currentBinlogPosition + this.currentEventLengthInBytes;
+        this.restartRowsToSkip = 0;
+        this.restartEventsToSkip = 0;
+        this.inTransaction = false;
     }
 
     /**
-     * Set the name of the MySQL binary log file.
+     * Record that a new GTID transaction has been started and has been included in the set of GTIDs known to the MySQL server.
      * 
-     * @param binlogFilename the name of the binary log file; may not be null
-     * @param positionOfFirstEvent the position in the binary log file to begin processing
+     * @param gtid the string representation of a specific GTID that has been begun; may not be null
+     * @param gtidSet the string representation of GTID set that includes the newly begun GTID; may not be null
      */
-    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
-        if (binlogFilename != null) {
-            this.binlogFilename = binlogFilename;
+    public void startGtid(String gtid, String gtidSet) {
+        this.currentGtid = gtid;
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            // Remove all the newline chars that exist in the GTID set string ...
+            String trimmedGtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """");
+            // Set the GTID set that we'll use if restarting BEFORE successful completion of the events in this GTID ...
+            this.restartGtidSet = this.currentGtidSet != null ? this.currentGtidSet : trimmedGtidSet;
+            // Record the GTID set that includes the current transaction ...
+            this.currentGtidSet = trimmedGtidSet;
         }
-        assert positionOfFirstEvent >= 0;
-        this.nextBinlogPosition = positionOfFirstEvent;
-        this.lastBinlogPosition = this.nextBinlogPosition;
-        this.nextEventRowNumber = 0;
-        this.lastEventRowNumber = 0;
     }
 
     /**
-     * Set the position within the MySQL binary log file of the <em>current event</em>.
+     * Set the GTID set that captures all of the GTID transactions that have been completely processed.
      * 
-     * @param positionOfCurrentEvent the position within the binary log file of the current event
-     * @param eventSizeInBytes the size in bytes of this event
+     * @param gtidSet the string representation of the GTID set; may not be null, but may be an empty string if no GTIDs
+     *            have been previously processed
      */
-    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
-        this.lastBinlogPosition = positionOfCurrentEvent;
-        this.nextBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
-        // Don't set anything else, since the row numbers are set in the offset(int,int) method called at least once
-        // for each processed event
+    public void setCompletedGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            // Remove all the newline chars that exist in the GTID set string ...
+            String trimmedGtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """");
+            this.currentGtidSet = trimmedGtidSet;
+            this.restartGtidSet = trimmedGtidSet;
+        }
     }
 
     /**
@@ -350,23 +423,23 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
-            binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
+            setCompletedGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
+            restartEventsToSkip = longOffsetValue(sourceOffset, EVENTS_TO_SKIP_OFFSET_KEY);
+            String binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
             }
-            nextBinlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
-            nextEventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
-            lastBinlogPosition = nextBinlogPosition;
-            lastEventRowNumber = nextEventRowNumber;
+            long binlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
+            setBinlogStartPoint(binlogFilename, binlogPosition);
+            this.restartRowsToSkip = (int) longOffsetValue(sourceOffset, BINLOG_ROW_IN_EVENT_OFFSET_KEY);
             nextSnapshot = booleanOffsetValue(sourceOffset, SNAPSHOT_KEY);
             lastSnapshot = nextSnapshot;
         }
     }
 
     private long longOffsetValue(Map<String, ?> values, String key) {
         Object obj = values.get(key);
-        if (obj == null) return 0;
+        if (obj == null) return 0L;
         if (obj instanceof Number) return ((Number) obj).longValue();
         try {
             return Long.parseLong(obj.toString());
@@ -388,56 +461,44 @@ private boolean booleanOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.gtidSet != null ? this.gtidSet : null;
+        return this.currentGtidSet != null ? this.currentGtidSet : null;
     }
 
     /**
-     * Get the name of the MySQL binary log file that has been processed.
+     * Get the name of the MySQL binary log file that has last been processed.
      * 
      * @return the name of the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
     public String binlogFilename() {
-        return binlogFilename;
+        return restartBinlogFilename;
     }
 
     /**
      * Get the position within the MySQL binary log file of the next event to be processed.
      * 
      * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
-    public long nextBinlogPosition() {
-        return nextBinlogPosition;
+    public long binlogPosition() {
+        return restartBinlogPosition;
     }
 
     /**
      * Get the position within the MySQL binary log file of the most recently processed event.
      * 
      * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
-    public long lastBinlogPosition() {
-        return lastBinlogPosition;
+    protected long restartBinlogPosition() {
+        return restartBinlogPosition;
     }
 
     /**
-     * Get the next row within the event at the {@link #nextBinlogPosition() position} within the {@link #binlogFilename() binary
-     * log file}
-     * .
+     * Get the number of rows beyond the {@link #eventsToSkipUponRestart() last completely processed event} to be skipped
+     * upon restart.
      * 
-     * @return the 0-based row number
+     * @return the number of rows to be skipped
      */
-    public int nextEventRowNumber() {
-        return nextEventRowNumber;
-    }
-
-    /**
-     * Get the previous row within the event at the {@link #lastBinlogPosition() position} within the {@link #binlogFilename()
-     * binary log file}
-     * .
-     * 
-     * @return the 0-based row number
-     */
-    public int lastEventRowNumber() {
-        return lastEventRowNumber;
+    public int rowsToSkipUponRestart() {
+        return restartRowsToSkip;
     }
 
     /**
@@ -452,22 +513,26 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (gtidSet != null) {
+        if (currentGtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(gtidSet);
-            sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
-            sb.append("", pos="").append(nextBinlogPosition());
-            sb.append("", row="").append(nextEventRowNumber());
+            sb.append(currentGtidSet);
+            sb.append("" and binlog file '"").append(restartBinlogFilename).append(""'"");
+            sb.append("", pos="").append(restartBinlogPosition);
+            sb.append("", skipping "").append(restartEventsToSkip);
+            sb.append("" events plus "").append(restartRowsToSkip);
+            sb.append("" rows"");
         } else {
-            if (binlogFilename == null) {
+            if (restartBinlogFilename == null) {
                 sb.append(""<latest>"");
             } else {
-                if ("""".equals(binlogFilename)) {
+                if ("""".equals(restartBinlogFilename)) {
                     sb.append(""earliest binlog file and position"");
                 } else {
-                    sb.append(""binlog file '"").append(binlogFilename).append(""'"");
-                    sb.append("", pos="").append(nextBinlogPosition());
-                    sb.append("", row="").append(nextEventRowNumber());
+                    sb.append(""binlog file '"").append(restartBinlogFilename).append(""'"");
+                    sb.append("", pos="").append(restartBinlogPosition);
+                    sb.append("", skipping "").append(restartEventsToSkip);
+                    sb.append("" events plus "").append(restartRowsToSkip);
+                    sb.append("" rows"");
                 }
             }
         }
@@ -505,7 +570,14 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
-                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired ...
+                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired GTID.
+                    // Now we need to compare how many events in that transaction we've already completed ...
+                    int recordedEventCount = recorded.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+                    int desiredEventCount = desired.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+                    int diff = recordedEventCount - desiredEventCount;
+                    if (diff > 0) return false;
+
+                    // Otherwise the recorded is definitely before or at the desired ...
                     return true;
                 }
                 // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
@@ -543,16 +615,25 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
         if (diff > 0) return false;
+        if (diff < 0) return true;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
         if (diff > 0) return false;
+        if (diff < 0) return true;
+
+        // The positions are the same, so compare the completed events in the transaction ...
+        int recordedEventCount = recorded.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+        int desiredEventCount = desired.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+        diff = recordedEventCount - desiredEventCount;
+        if (diff > 0) return false;
+        if (diff < 0) return true;
 
-        // The positions are the same, so compare the row number ...
-        int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
-        int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        // The completed events are the same, so compare the row number ...
+        int recordedRow = recorded.getInteger(BINLOG_ROW_IN_EVENT_OFFSET_KEY, -1);
+        int desiredRow = desired.getInteger(BINLOG_ROW_IN_EVENT_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
         if (diff > 0) return false;
 ",2016-11-09T14:11:41Z,70
"@@ -8,8 +8,10 @@
 import static org.junit.Assert.fail;
 
 import java.nio.file.Path;
+import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.util.List;
+import java.util.Map;
 import java.util.concurrent.TimeUnit;
 
 import org.apache.kafka.common.config.Config;
@@ -28,6 +30,7 @@
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
 import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.embedded.EmbeddedEngine.CompletionResult;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.relational.history.KafkaDatabaseHistory;
@@ -274,11 +277,12 @@ public void shouldValidateAcceptableConfiguration() {
     public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLException, InterruptedException {
         String masterPort = System.getProperty(""database.port"");
         String replicaPort = System.getProperty(""database.replica.port"");
-        if ( !masterPort.equals(replicaPort)) {
+        boolean replicaIsMaster = masterPort.equals(replicaPort);
+        if (!replicaIsMaster) {
             // Give time for the replica to catch up to the master ...
             Thread.sleep(5000L);
         }
-        
+
         // Use the DB configuration to define the connector's configuration to use the ""replica""
         // which may be the same as the ""master"" ...
         config = Configuration.create()
@@ -352,7 +356,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
             }
         }
 
-        //Testing.Print.enable();
+        // Testing.Print.enable();
 
         // Restart the connector and read the insert record ...
         Testing.print(""*** Restarting connector after inserts were made"");
@@ -388,7 +392,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         inserts = records.recordsForTopic(""myServer.connector_test.products"");
         assertInsert(inserts.get(0), ""id"", 1001);
 
-        Testing.print(""*** Done with simple insert"");
+        // Testing.print(""*** Done with simple insert"");
 
         // ---------------------------------------------------------------------------------------------------------------
         // Changing the primary key of a row should result in 3 events: INSERT, DELETE, and TOMBSTONE
@@ -433,13 +437,15 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
 
         Testing.print(""*** Done with simple update"");
 
+        //Testing.Print.enable();
+
         // ---------------------------------------------------------------------------------------------------------------
         // Change our schema with a fully-qualified name; we should still see this event
         // ---------------------------------------------------------------------------------------------------------------
         // Add a column with default to the 'products' table and explicitly update one record ...
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
-                connection.execute(""ALTER TABLE connector_test.products ADD COLUMN volume FLOAT NOT NULL, ADD COLUMN alias VARCHAR(30) NOT NULL AFTER description"");
+                connection.execute(""ALTER TABLE connector_test.products ADD COLUMN volume FLOAT, ADD COLUMN alias VARCHAR(30) NULL AFTER description"");
                 connection.execute(""UPDATE products SET volume=13.5 WHERE id=2001"");
                 connection.query(""SELECT * FROM products"", rs -> {
                     if (Testing.Print.isEnabled()) connection.print(rs);
@@ -508,6 +514,173 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // Stop the connector ...
         // ---------------------------------------------------------------------------------------------------------------
         stopConnector();
+
+        // ---------------------------------------------------------------------------------------------------------------
+        // Restart the connector to read only part of a transaction ...
+        // ---------------------------------------------------------------------------------------------------------------
+        Testing.print(""*** Restarting connector"");
+        CompletionResult completion = new CompletionResult();
+        start(MySqlConnector.class, config, completion, (record) -> {
+            // We want to stop before processing record 3003 ...
+            Struct key = (Struct) record.key();
+            Number id = (Number) key.get(""id"");
+            if (id.intValue() == 3003) {
+                return true;
+            }
+            return false;
+        });
+
+        BinlogPosition positionBeforeInserts = new BinlogPosition();
+        BinlogPosition positionAfterInserts = new BinlogPosition();
+        BinlogPosition positionAfterUpdate = new BinlogPosition();
+        try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
+            try (JdbcConnection connection = db.connect()) {
+                connection.query(""SHOW MASTER STATUS"", positionBeforeInserts::readFromDatabase);
+                connection.execute(""INSERT INTO products(id,name,description,weight,volume,alias) VALUES ""
+                        + ""(3001,'ashley','super robot',34.56,0.00,'ashbot'), ""
+                        + ""(3002,'arthur','motorcycle',87.65,0.00,'arcycle'), ""
+                        + ""(3003,'oak','tree',987.65,0.00,'oak');"");
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
+                connection.query(""SHOW MASTER STATUS"", positionAfterInserts::readFromDatabase);
+                // Change something else that is unrelated ...
+                connection.execute(""UPDATE products_on_hand SET quantity=40 WHERE product_id=109"");
+                connection.query(""SELECT * FROM products_on_hand"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
+                connection.query(""SHOW MASTER STATUS"", positionAfterUpdate::readFromDatabase);
+            }
+        }
+
+        //Testing.Print.enable();
+
+        // And consume the one insert ...
+        records = consumeRecordsByTopic(2);
+        assertThat(records.recordsForTopic(""myServer.connector_test.products"").size()).isEqualTo(2);
+        assertThat(records.topics().size()).isEqualTo(1);
+        inserts = records.recordsForTopic(""myServer.connector_test.products"");
+        assertInsert(inserts.get(0), ""id"", 3001);
+        assertInsert(inserts.get(1), ""id"", 3002);
+
+        // Verify that the connector has stopped ...
+        completion.await(10, TimeUnit.SECONDS);
+        assertThat(completion.hasCompleted()).isTrue();
+        assertThat(completion.hasError()).isTrue();
+        assertThat(completion.success()).isFalse();
+        assertNoRecordsToConsume();
+        assertConnectorNotRunning();
+
+        // ---------------------------------------------------------------------------------------------------------------
+        // Stop the connector ...
+        // ---------------------------------------------------------------------------------------------------------------
+        stopConnector();
+
+        // Read the last committed offsets, and verify the binlog coordinates ...
+        SourceInfo persistedOffsetSource = new SourceInfo();
+        persistedOffsetSource.setServerName(config.getString(MySqlConnectorConfig.SERVER_NAME));
+        Map<String, ?> lastCommittedOffset = readLastCommittedOffset(config, persistedOffsetSource.partition());
+        persistedOffsetSource.setOffset(lastCommittedOffset);
+        Testing.print(""Position before inserts: "" + positionBeforeInserts);
+        Testing.print(""Position after inserts:  "" + positionAfterInserts);
+        Testing.print(""Offset: "" + lastCommittedOffset);
+        Testing.print(""Position after update:  "" + positionAfterUpdate);
+        if (replicaIsMaster) {
+            // Same binlog filename ...
+            assertThat(persistedOffsetSource.binlogFilename()).isEqualTo(positionBeforeInserts.binlogFilename());
+            assertThat(persistedOffsetSource.binlogFilename()).isEqualTo(positionAfterInserts.binlogFilename());
+            // Binlog position in offset should be more than before the inserts, but less than the position after the inserts ...
+            assertThat(persistedOffsetSource.binlogPosition()).isGreaterThan(positionBeforeInserts.binlogPosition());
+            assertThat(persistedOffsetSource.binlogPosition()).isLessThan(positionAfterInserts.binlogPosition());
+        } else {
+            // the replica is not the same server as the master, so it will have a different binlog filename and position ...
+        }
+        // Event number is 2 ...
+        assertThat(persistedOffsetSource.eventsToSkipUponRestart()).isEqualTo(2);
+        // GTID set should match the before-inserts GTID set ...
+        // assertThat(persistedOffsetSource.gtidSet()).isEqualTo(positionBeforeInserts.gtidSet());
+
+        Testing.print(""*** Restarting connector, and should begin with inserting 3003 (not 109!)"");
+        start(MySqlConnector.class, config);
+
+        // And consume the insert for 3003 ...
+        records = consumeRecordsByTopic(1);
+        assertThat(records.topics().size()).isEqualTo(1);
+        inserts = records.recordsForTopic(""myServer.connector_test.products"");
+        if (inserts == null) {
+            updates = records.recordsForTopic(""myServer.connector_test.products_on_hand"");
+            if (updates != null) {
+                fail(""Restarted connector and missed the insert of product id=3003!"");
+            }
+        }
+        // Read the first record produced since we've restarted
+        SourceRecord prod3003 = inserts.get(0);
+        assertInsert(prod3003, ""id"", 3003);
+        
+        // Check that the offset has the correct/expected values ...
+        assertOffset(prod3003,""file"",lastCommittedOffset.get(""file""));
+        assertOffset(prod3003,""pos"",lastCommittedOffset.get(""pos""));
+        assertOffset(prod3003,""row"",3);
+        assertOffset(prod3003,""event"",lastCommittedOffset.get(""event""));
+
+        // Check that the record has all of the column values ...
+        assertValueField(prod3003,""after/id"",3003);
+        assertValueField(prod3003,""after/name"",""oak"");
+        assertValueField(prod3003,""after/description"",""tree"");
+        assertValueField(prod3003,""after/weight"",987.65d);
+        assertValueField(prod3003,""after/volume"",0.0d);
+        assertValueField(prod3003,""after/alias"",""oak"");
+        
+
+        // And make sure we consume that one extra update ...
+        records = consumeRecordsByTopic(1);
+        assertThat(records.topics().size()).isEqualTo(1);
+        updates = records.recordsForTopic(""myServer.connector_test.products_on_hand"");
+        assertThat(updates.size()).isEqualTo(1);
+        assertUpdate(updates.get(0), ""product_id"", 109);
+        updates.forEach(this::validate);
+
+        // Start the connector again, and we should see the next two
+        Testing.print(""*** Done with simple insert"");
+
+    }
+
+    protected static class BinlogPosition {
+        private String binlogFilename;
+        private long binlogPosition;
+        private String gtidSet;
+
+        public void readFromDatabase(ResultSet rs) throws SQLException {
+            if (rs.next()) {
+                binlogFilename = rs.getString(1);
+                binlogPosition = rs.getLong(2);
+                if (rs.getMetaData().getColumnCount() > 4) {
+                    // This column exists only in MySQL 5.6.5 or later ...
+                    gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                }
+            }
+        }
+
+        public String binlogFilename() {
+            return binlogFilename;
+        }
+
+        public long binlogPosition() {
+            return binlogPosition;
+        }
+
+        public String gtidSet() {
+            return gtidSet;
+        }
+
+        public boolean hasGtids() {
+            return gtidSet != null;
+        }
+
+        @Override
+        public String toString() {
+            return ""file="" + binlogFilename + "", pos="" + binlogPosition + "", gtids="" + (gtidSet != null ? gtidSet : """");
+        }
     }
 
     @Test",2016-11-09T14:11:41Z,71
"@@ -215,7 +215,7 @@ public void shouldFilterAndMergeGtidSet() throws Exception {
                                .build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setGtidSet(gtidStr);
+        context.source().setCompletedGtidSet(gtidStr);
 
         GtidSet mergedGtidSet = context.filterGtidSet(new GtidSet(availableServerGtidStr));
         assertThat(mergedGtidSet).isNotNull();",2016-11-09T14:11:41Z,72
"@@ -31,31 +31,34 @@ public class SourceInfoTest {
     private static final String SERVER_NAME = ""my-server""; // can technically be any string
 
     private SourceInfo source;
+    private boolean inTxn = false;
+    private long positionOfBeginEvent = 0L;
+    private int eventNumberInTxn = 0;
 
     @Before
     public void beforeEach() {
         source = new SourceInfo();
+        inTxn = false;
+        positionOfBeginEvent = 0L;
+        eventNumberInTxn = 0;
     }
 
     @Test
     public void shouldStartSourceInfoFromZeroBinlogCoordinates() {
         source.setBinlogStartPoint(FILENAME, 0);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.eventsToSkipUponRestart()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
     @Test
     public void shouldStartSourceInfoFromNonZeroBinlogCoordinates() {
         source.setBinlogStartPoint(FILENAME, 100);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -68,10 +71,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinates() {
         sourceWith(offset(0, 0));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -80,10 +81,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinates() {
         sourceWith(offset(100, 0));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -92,10 +91,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndNonZero
         sourceWith(offset(0, 5));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -104,10 +101,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndNonZ
         sourceWith(offset(100, 5));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -116,10 +111,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndSnapsho
         sourceWith(offset(0, 0, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -128,10 +121,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndSnap
         sourceWith(offset(100, 0, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -140,10 +131,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndNonZero
         sourceWith(offset(0, 5, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -152,10 +141,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndNonZ
         sourceWith(offset(100, 5, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -164,10 +151,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 0, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -176,10 +161,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 5, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -188,10 +171,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 0, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -200,10 +181,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 5, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -212,10 +191,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 0, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -224,10 +201,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 5, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -236,10 +211,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 0, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -248,10 +221,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 5, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -262,19 +233,89 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
     @Test
     public void shouldAdvanceSourceInfoFromNonZeroPositionAndRowZeroForEventsWithOneRow() {
         sourceWith(offset(100, 0));
+
+        // Try a transactions with just one event ...
+        handleTransactionBegin(150, 2);
         handleNextEvent(200, 10, withRowCount(1));
+        handleTransactionCommit(210, 2);
+
+        handleTransactionBegin(210, 2);
         handleNextEvent(220, 10, withRowCount(1));
+        handleTransactionCommit(230, 3);
+
+        handleTransactionBegin(240, 2);
         handleNextEvent(250, 50, withRowCount(1));
+        handleTransactionCommit(300, 4);
+
+        // Try a transactions with multiple events ...
+        handleTransactionBegin(340, 2);
+        handleNextEvent(350, 20, withRowCount(1));
+        handleNextEvent(370, 30, withRowCount(1));
+        handleNextEvent(400, 40, withRowCount(1));
+        handleTransactionCommit(440, 4);
+
+        handleTransactionBegin(500, 2);
+        handleNextEvent(510, 20, withRowCount(1));
+        handleNextEvent(540, 15, withRowCount(1));
+        handleNextEvent(560, 10, withRowCount(1));
+        handleTransactionCommit(580, 4);
+
+        // Try another single event transaction ...
+        handleTransactionBegin(600, 2);
+        handleNextEvent(610, 50, withRowCount(1));
+        handleTransactionCommit(660, 4);
+
+        // Try event outside of a transaction ...
+        handleNextEvent(670, 10, withRowCount(1));
+
+        // Try another single event transaction ...
+        handleTransactionBegin(700, 2);
+        handleNextEvent(710, 50, withRowCount(1));
+        handleTransactionCommit(760, 4);
     }
 
     @Test
     public void shouldAdvanceSourceInfoFromNonZeroPositionAndRowZeroForEventsWithMultipleRow() {
         sourceWith(offset(100, 0));
+
+        // Try a transactions with just one event ...
+        handleTransactionBegin(150, 2);
         handleNextEvent(200, 10, withRowCount(3));
+        handleTransactionCommit(210, 2);
+
+        handleTransactionBegin(210, 2);
         handleNextEvent(220, 10, withRowCount(4));
-        handleNextEvent(250, 50, withRowCount(6));
-        handleNextEvent(300, 20, withRowCount(1));
-        handleNextEvent(350, 20, withRowCount(3));
+        handleTransactionCommit(230, 3);
+
+        handleTransactionBegin(240, 2);
+        handleNextEvent(250, 50, withRowCount(5));
+        handleTransactionCommit(300, 4);
+
+        // Try a transactions with multiple events ...
+        handleTransactionBegin(340, 2);
+        handleNextEvent(350, 20, withRowCount(6));
+        handleNextEvent(370, 30, withRowCount(1));
+        handleNextEvent(400, 40, withRowCount(3));
+        handleTransactionCommit(440, 4);
+
+        handleTransactionBegin(500, 2);
+        handleNextEvent(510, 20, withRowCount(8));
+        handleNextEvent(540, 15, withRowCount(9));
+        handleNextEvent(560, 10, withRowCount(1));
+        handleTransactionCommit(580, 4);
+
+        // Try another single event transaction ...
+        handleTransactionBegin(600, 2);
+        handleNextEvent(610, 50, withRowCount(1));
+        handleTransactionCommit(660, 4);
+
+        // Try event outside of a transaction ...
+        handleNextEvent(670, 10, withRowCount(5));
+
+        // Try another single event transaction ...
+        handleTransactionBegin(700, 2);
+        handleNextEvent(710, 50, withRowCount(3));
+        handleTransactionCommit(760, 4);
     }
 
     // -------------------------------------------------------------------------------------
@@ -285,33 +326,78 @@ protected int withRowCount(int rowCount) {
         return rowCount;
     }
 
+    protected void handleTransactionBegin(long positionOfEvent, int eventSize) {
+        source.setEventPosition(positionOfEvent, eventSize);
+        positionOfBeginEvent = positionOfEvent;
+        source.startNextTransaction();
+        inTxn = true;
+
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
+    }
+
+    protected void handleTransactionCommit(long positionOfEvent, int eventSize) {
+        source.setEventPosition(positionOfEvent, eventSize);
+        source.commitTransaction();
+        eventNumberInTxn = 0;
+        inTxn = false;
+
+        // Verify the offset ...
+        Map<String, ?> offset = source.offset();
+
+        // The offset position should be the position of the next event
+        long position = (Long) offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY);
+        assertThat(position).isEqualTo(positionOfEvent + eventSize);
+        Long rowsToSkip = (Long) offset.get(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY);
+        if (rowsToSkip == null) rowsToSkip = 0L;
+        assertThat(rowsToSkip).isEqualTo(0);
+        assertThat(offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY)).isNull();
+        if (source.gtidSet() != null) {
+            assertThat(offset.get(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
+        }
+    }
+
     protected void handleNextEvent(long positionOfEvent, long eventSize, int rowCount) {
+        if (inTxn) ++eventNumberInTxn;
         source.setEventPosition(positionOfEvent, eventSize);
-        for (int i = 0; i != rowCount; ++i) {
+        for (int row = 0; row != rowCount; ++row) {
             // Get the offset for this row (always first!) ...
-            Map<String, ?> offset = source.offsetForRow(i, rowCount);
-            if ((i + 1) < rowCount) {
-                // This is not the last row, so the next binlog position should be for next row in this event ...
-                assertThat(offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent);
-                assertThat(offset.get(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(i+1);
-            } else {
-                // This is the last row, so the next binlog position should be for first row in next event ...
-                assertThat(offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent + eventSize);
-                assertThat(offset.get(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(0);
-            }
+            Map<String, ?> offset = source.offsetForRow(row, rowCount);
             assertThat(offset.get(SourceInfo.BINLOG_FILENAME_OFFSET_KEY)).isEqualTo(FILENAME);
-            if ( source.gtidSet() != null ) {
+            if (source.gtidSet() != null) {
                 assertThat(offset.get(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
             }
+            long position = (Long) offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY);
+            if (inTxn) {
+                // regardless of the row count, the position is always the txn begin position ...
+                assertThat(position).isEqualTo(positionOfBeginEvent);
+                // and the number of the last completed event (the previous one) ...
+                Long eventsToSkip = (Long) offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY);
+                if (eventsToSkip == null) eventsToSkip = 0L;
+                assertThat(eventsToSkip).isEqualTo(eventNumberInTxn - 1);
+            } else {
+                // Matches the next event ...
+                assertThat(position).isEqualTo(positionOfEvent + eventSize);
+                assertThat(offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY)).isNull();
+            }
+            Long rowsToSkip = (Long) offset.get(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY);
+            if (rowsToSkip == null) rowsToSkip = 0L;
+            if( (row+1) == rowCount) {
+                // This is the last row, so the next binlog position should be the number of rows in the event ...
+                assertThat(rowsToSkip).isEqualTo(rowCount);
+            } else {
+                // This is not the last row, so the next binlog position should be the row number ...
+                assertThat(rowsToSkip).isEqualTo(row+1);
+            }
             // Get the source struct for this row (always second), which should always reflect this row in this event ...
             Struct recordSource = source.struct();
             assertThat(recordSource.getInt64(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent);
-            assertThat(recordSource.getInt32(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(i);
+            assertThat(recordSource.getInt32(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY)).isEqualTo(row);
             assertThat(recordSource.getString(SourceInfo.BINLOG_FILENAME_OFFSET_KEY)).isEqualTo(FILENAME);
-            if ( source.gtidSet() != null ) {
+            if (source.gtidSet() != null) {
                 assertThat(recordSource.getString(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
             }
         }
+        source.completeEvent();
     }
 
     protected Map<String, String> offset(long position, int row) {
@@ -326,7 +412,7 @@ protected Map<String, String> offset(String gtidSet, long position, int row, boo
         Map<String, String> offset = new HashMap<>();
         offset.put(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, FILENAME);
         offset.put(SourceInfo.BINLOG_POSITION_OFFSET_KEY, Long.toString(position));
-        offset.put(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, Integer.toString(row));
+        offset.put(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, Integer.toString(row));
         if (gtidSet != null) offset.put(SourceInfo.GTID_SET_KEY, gtidSet);
         if (snapshot) offset.put(SourceInfo.SNAPSHOT_KEY, Boolean.TRUE.toString());
         return offset;
@@ -393,12 +479,40 @@ public void shouldOrderPositionsWithSameServerButLowerUpperLimitAsBeforePosition
 
     @Test
     public void shouldOrderPositionWithoutGtidAsBeforePositionWithGtid() {
-        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0).isBefore(positionWithGtids(""IdA:1-5""));
+        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0, 0).isBefore(positionWithGtids(""IdA:1-5""));
     }
 
     @Test
     public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
-        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
+        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0, 0));
+    }
+
+    @Test
+    public void shouldComparePositionsWithoutGtids() {
+        // Same position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isAt(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAt(positionWithoutGtids(""fn.01"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.03"", 1, 0, 1).isAt(positionWithoutGtids(""fn.03"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isAt(positionWithoutGtids(""fn.01"", 1, 1, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAt(positionWithoutGtids(""fn.01"", 1, 1, 1));
+        assertPositionWithoutGtids(""fn.03"", 1, 1, 1).isAt(positionWithoutGtids(""fn.03"", 1, 1, 1));
+
+        // Before position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isBefore(positionWithoutGtids(""fn.01"", 1, 0, 2));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 1, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 2, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isBefore(positionWithoutGtids(""fn.01"", 1, 2, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+
+        // After position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAfter(positionWithoutGtids(""fn.01"", 0, 0, 99));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 0, 0, 99));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 1, 0));
     }
 
     @FixFor(""DBZ-107"")
@@ -410,21 +524,21 @@ public void shouldRemoveNewlinesFromGtidSet() {
         String gtidCleaned = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,"" +
                 ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3149,"" +
                 ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:1-39"";
-        source.setGtidSet(gtidExecuted);
+        source.setCompletedGtidSet(gtidExecuted);
         assertThat(source.gtidSet()).isEqualTo(gtidCleaned);
     }
 
     @FixFor(""DBZ-107"")
     @Test
     public void shouldNotSetBlankGtidSet() {
-        source.setGtidSet("""");
+        source.setCompletedGtidSet("""");
         assertThat(source.gtidSet()).isNull();
     }
 
     @FixFor(""DBZ-107"")
     @Test
     public void shouldNotSetNullGtidSet() {
-        source.setGtidSet(null);
+        source.setCompletedGtidSet(null);
         assertThat(source.gtidSet()).isNull();
     }
 
@@ -439,20 +553,22 @@ protected Document positionWithGtids(String gtids, boolean snapshot) {
         return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
-    protected Document positionWithoutGtids(String filename, int position, int row) {
-        return positionWithoutGtids(filename, position, row, false);
+    protected Document positionWithoutGtids(String filename, int position, int event, int row) {
+        return positionWithoutGtids(filename, position, event, row, false);
     }
 
-    protected Document positionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+    protected Document positionWithoutGtids(String filename, int position, int event, int row, boolean snapshot) {
         if (snapshot) {
             return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
                                    SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
-                                   SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row,
+                                   SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, row,
+                                   SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY, event,
                                    SourceInfo.SNAPSHOT_KEY, true);
         }
         return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
                                SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
-                               SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row);
+                               SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, row,
+                               SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY, event);
     }
 
     protected PositionAssert assertThatDocument(Document position) {
@@ -467,12 +583,12 @@ protected PositionAssert assertPositionWithGtids(String gtids, boolean snapshot)
         return assertThatDocument(positionWithGtids(gtids, snapshot));
     }
 
-    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row) {
-        return assertPositionWithoutGtids(filename, position, row, false);
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int event, int row) {
+        return assertPositionWithoutGtids(filename, position, event, row, false);
     }
 
-    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row, boolean snapshot) {
-        return assertThatDocument(positionWithoutGtids(filename, position, row, snapshot));
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int event, int row, boolean snapshot) {
+        return assertThatDocument(positionWithoutGtids(filename, position, event, row, snapshot));
     }
 
     protected static class PositionAssert extends GenericAssert<PositionAssert, Document> {",2016-11-09T14:11:41Z,70
"@@ -11,4 +11,5 @@ log4j.rootLogger=INFO, stdout
 log4j.logger.io.debezium=INFO
 log4j.logger.io.debezium.embedded.EmbeddedEngine$EmbeddedConfig=WARN
 #log4j.logger.io.debezium.connector.mysql.BinlogReader=DEBUG
-#log4j.logger.io.debezium.connector.mysql.SnapshotReader=DEBUG
\ No newline at end of file
+#log4j.logger.io.debezium.connector.mysql.SnapshotReader=DEBUG
+#log4j.logger.io.debezium.relational.history=DEBUG",2016-11-09T14:11:41Z,73
"@@ -92,6 +92,19 @@ static Document create(CharSequence fieldName1, Object value1, CharSequence fiel
         return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4);
     }
 
+    static Document create(CharSequence fieldName1, Object value1, CharSequence fieldName2, Object value2, CharSequence fieldName3,
+                           Object value3, CharSequence fieldName4, Object value4, CharSequence fieldName5, Object value5) {
+        return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4)
+                                  .set(fieldName5, value5);
+    }
+
+    static Document create(CharSequence fieldName1, Object value1, CharSequence fieldName2, Object value2, CharSequence fieldName3,
+                           Object value3, CharSequence fieldName4, Object value4, CharSequence fieldName5, Object value5,
+                           CharSequence fieldName6, Object value6) {
+        return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4)
+                                  .set(fieldName5, value5).set(fieldName6, value6);
+    }
+
     /**
      * Return the number of name-value fields in this object.
      * 
@@ -159,7 +172,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
             parent = find(parentPath, (missingPath, missingIndex) -> {
                 invalid.accept(missingPath); // invoke the invalid handler
                 return Optional.empty();
-            } , invalid);
+            }, invalid);
         } else {
             // Create any missing intermediaries using the segment after the missing segment to determine which
             // type of intermediate value to add ...
@@ -170,7 +183,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
                 } else {
                     return Optional.of(Value.create(Document.create()));
                 }
-            } , invalid);
+            }, invalid);
         }
         if (!parent.isPresent()) return Optional.empty();
         String lastSegment = path.lastSegment().get();
@@ -202,8 +215,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
      *         valid
      */
     default Optional<Value> find(Path path) {
-        return find(path, (missingPath, missingIndex) -> Optional.empty(), (invalidPath) -> {
-        });
+        return find(path, (missingPath, missingIndex) -> Optional.empty(), (invalidPath) -> {});
     }
 
     /**
@@ -719,7 +731,7 @@ default Value remove(Optional<? extends CharSequence> name) {
      * @return This document, to allow for chaining methods
      */
     Document removeAll();
-    
+
     /**
      * Sets on this object all name/value pairs from the supplied object. If the supplied object is null, this method does
      * nothing.",2016-11-09T14:11:41Z,74
"@@ -1169,8 +1169,9 @@ protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
      */
     protected Object handleUnknownData(Column column, Field fieldDefn, Object data) {
         if (column.isOptional() || fieldDefn.schema().isOptional()) {
+            Class<?> dataClass = data.getClass();
             logger.warn(""Unexpected value for JDBC type {} and column {}: class={}"", column.jdbcType(), column,
-                        data.getClass()); // don't include value in case its sensitive
+                        dataClass.isArray() ? dataClass.getSimpleName() : dataClass.getName()); // don't include value in case its sensitive
             return null;
         }
         throw new IllegalArgumentException(""Unexpected value for JDBC type "" + column.jdbcType() + "" and column "" + column +",2016-11-09T14:11:41Z,75
"@@ -46,14 +46,18 @@ public final void record(Map<String, ?> source, Map<String, ?> position, String
 
     @Override
     public final void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser) {
+        logger.debug(""Recovering DDL history for source partition {} and offset {}"",source,position);
         HistoryRecord stopPoint = new HistoryRecord(source, position, null, null);
         recoverRecords(schema,ddlParser,recovered->{
             if (comparator.isAtOrBefore(recovered,stopPoint)) {
                 String ddl = recovered.ddl();
                 if (ddl != null) {
                     ddlParser.setCurrentSchema(recovered.databaseName()); // may be null
                     ddlParser.parse(ddl, schema);
+                    logger.debug(""Applying: {}"", ddl);
                 }
+            } else {
+                logger.debug(""Skipping: {}"", recovered.ddl());
             }
         });
     }",2016-11-09T14:11:41Z,76
"@@ -281,7 +281,7 @@ public static void debug(SourceRecord record) {
      * @param record the record to validate; may not be null
      */
     public static void isValid(SourceRecord record) {
-        print(record);
+        //print(record);
 
         JsonNode keyJson = null;
         JsonNode valueJson = null;",2016-11-09T14:11:41Z,77
"@@ -8,6 +8,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
+import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Executor;
 import java.util.concurrent.ExecutorService;
@@ -35,7 +36,6 @@
 import org.apache.kafka.connect.storage.OffsetStorageReader;
 import org.apache.kafka.connect.storage.OffsetStorageReaderImpl;
 import org.apache.kafka.connect.storage.OffsetStorageWriter;
-import org.apache.kafka.connect.storage.StringConverter;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -134,7 +134,7 @@ public final class EmbeddedEngine implements Runnable {
 
     protected static final Field INTERNAL_KEY_CONVERTER_CLASS = Field.create(""internal.key.converter"")
                                                                      .withDescription(""The Converter class that should be used to serialize and deserialize key data for offsets."")
-                                                                     .withDefault(StringConverter.class.getName());
+                                                                     .withDefault(JsonConverter.class.getName());
 
     protected static final Field INTERNAL_VALUE_CONVERTER_CLASS = Field.create(""internal.value.converter"")
                                                                        .withDescription(""The Converter class that should be used to serialize and deserialize value data for offsets."")
@@ -159,14 +159,108 @@ public static interface CompletionCallback {
         /**
          * Handle the completion of the embedded connector engine.
          * 
-         * @param success true if the connector completed normally, or {@code false} if the connector produced an error that
-         *            prevented startup or premature termination.
+         * @param success {@code true} if the connector completed normally, or {@code false} if the connector produced an error
+         *            that prevented startup or premature termination.
          * @param message the completion message; never null
          * @param error the error, or null if there was no exception
          */
         void handle(boolean success, String message, Throwable error);
     }
 
+    /**
+     * A callback function to be notified when the connector completes.
+     */
+    public static class CompletionResult implements CompletionCallback {
+        private final CountDownLatch completed = new CountDownLatch(1);
+        private boolean success;
+        private String message;
+        private Throwable error;
+
+        @Override
+        public void handle(boolean success, String message, Throwable error) {
+            this.success = success;
+            this.message = message;
+            this.error = error;
+            this.completed.countDown();
+        }
+
+        /**
+         * Causes the current thread to wait until the {@link #handle(boolean, String, Throwable) completion occurs}
+         * or until the thread is {@linkplain Thread#interrupt interrupted}.
+         * <p>
+         * This method returns immediately if the connector has completed already.
+         * 
+         * @throws InterruptedException if the current thread is interrupted while waiting
+         */
+        public void await() throws InterruptedException {
+            this.completed.await();
+        }
+
+        /**
+         * Causes the current thread to wait until the {@link #handle(boolean, String, Throwable) completion occurs},
+         * unless the thread is {@linkplain Thread#interrupt interrupted}, or the specified waiting time elapses.
+         * <p>
+         * This method returns immediately if the connector has completed already.
+         * 
+         * @param timeout the maximum time to wait
+         * @param unit the time unit of the {@code timeout} argument
+         * @return {@code true} if the completion was received, or {@code false} if the waiting time elapsed before the completion
+         *         was received.
+         * @throws InterruptedException if the current thread is interrupted while waiting
+         */
+        public boolean await(long timeout, TimeUnit unit) throws InterruptedException {
+            return this.completed.await(timeout, unit);
+        }
+
+        /**
+         * Determine if the connector has completed.
+         * 
+         * @return {@code true} if the connector has completed, or {@code false} if the connector is still running and this
+         *         callback has not yet been {@link #handle(boolean, String, Throwable) notified}
+         */
+        public boolean hasCompleted() {
+            return completed.getCount() == 0;
+        }
+
+        /**
+         * Get whether the connector completed normally.
+         * 
+         * @return {@code true} if the connector completed normally, or {@code false} if the connector produced an error that
+         *         prevented startup or premature termination (or the connector has not yet {@link #hasCompleted() completed})
+         */
+        public boolean success() {
+            return success;
+        }
+
+        /**
+         * Get the completion message.
+         * 
+         * @return the completion message, or null if the connector has not yet {@link #hasCompleted() completed}
+         */
+        public String message() {
+            return message;
+        }
+
+        /**
+         * Get the completion error, if there is one.
+         * 
+         * @return the completion error, or null if there is no error or connector has not yet {@link #hasCompleted() completed}
+         */
+        public Throwable error() {
+            return error;
+        }
+
+        /**
+         * Determine if there is a completion error.
+         * 
+         * @return {@code true} if there is a {@link #error completion error}, or {@code false} if there is no error or
+         *         the connector has not yet {@link #hasCompleted() completed}
+         */
+        public boolean hasError() {
+            return error != null;
+        }
+    }
+
     /**
      * A builder to set up and create {@link EmbeddedEngine} instances.
      */
@@ -295,7 +389,7 @@ public EmbeddedEngine build() {
     private long timeSinceLastCommitMillis = 0;
 
     private EmbeddedEngine(Configuration config, ClassLoader classLoader, Clock clock, Consumer<SourceRecord> consumer,
-            CompletionCallback completionCallback) {
+                           CompletionCallback completionCallback) {
         this.config = config;
         this.consumer = consumer;
         this.classLoader = classLoader;
@@ -308,7 +402,7 @@ private EmbeddedEngine(Configuration config, ClassLoader classLoader, Clock cloc
         assert this.classLoader != null;
         assert this.clock != null;
         keyConverter = config.getInstance(INTERNAL_KEY_CONVERTER_CLASS, Converter.class, () -> this.classLoader);
-        keyConverter.configure(config.subset(INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), false);
+        keyConverter.configure(config.subset(INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), true);
         valueConverter = config.getInstance(INTERNAL_VALUE_CONVERTER_CLASS, Converter.class, () -> this.classLoader);
         Configuration valueConverterConfig = config;
         if (valueConverter instanceof JsonConverter) {
@@ -456,8 +550,9 @@ public void raiseError(Exception e) {
                     }
 
                     recordsSinceLastCommit = 0;
+                    Throwable handlerError = null;
                     timeSinceLastCommitMillis = clock.currentTimeInMillis();
-                    while (runningThread.get() != null) {
+                    while (runningThread.get() != null && handlerError == null) {
                         try {
                             logger.debug(""Embedded engine is polling task for records"");
                             List<SourceRecord> changeRecords = task.poll(); // blocks until there are values ...
@@ -469,17 +564,16 @@ public void raiseError(Exception e) {
                                     try {
                                         consumer.accept(record);
                                     } catch (Throwable t) {
-                                        logger.error(""Error in the application's handler method, but continuing anyway"", t);
+                                        handlerError = t;
+                                        break;
                                     }
-                                }
 
-                                // Only then do we write out the last partition to offset storage ...
-                                SourceRecord lastRecord = changeRecords.get(changeRecords.size() - 1);
-                                lastRecord.sourceOffset();
-                                offsetWriter.offset(lastRecord.sourcePartition(), lastRecord.sourceOffset());
+                                    // Record the offset for this record's partition
+                                    offsetWriter.offset(record.sourcePartition(), record.sourceOffset());
+                                    recordsSinceLastCommit += 1;
+                                }
 
                                 // Flush the offsets to storage if necessary ...
-                                recordsSinceLastCommit += changeRecords.size();
                                 maybeFlush(offsetWriter, offsetCommitPolicy, commitTimeoutMs);
                             } else {
                                 logger.debug(""Received no records from the task"");
@@ -501,7 +595,14 @@ public void raiseError(Exception e) {
                     } finally {
                         // Always commit offsets that were captured from the source records we actually processed ...
                         commitOffsets(offsetWriter, commitTimeoutMs);
-                        succeed(""Connector '"" + connectorClassName + ""' completed normally."");
+                        if (handlerError != null) {
+                            // There was an error in the handler ...
+                            fail(""Stopping connector after error in the application's handler method: "" + handlerError.getMessage(),
+                                 handlerError);
+                        } else {
+                            // We stopped normally ...
+                            succeed(""Connector '"" + connectorClassName + ""' completed normally."");
+                        }
                     }
                 } catch (Throwable t) {
                     fail(""Error while trying to run connector class '"" + connectorClassName + ""'"", t);",2016-11-09T14:11:41Z,78
"@@ -7,8 +7,11 @@
 
 import static org.junit.Assert.fail;
 
+import java.math.BigDecimal;
 import java.nio.file.Path;
 import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.LinkedList;
@@ -22,18 +25,25 @@
 import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 import java.util.function.Consumer;
+import java.util.function.Predicate;
 
 import org.apache.kafka.common.config.Config;
 import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.json.JsonConverter;
 import org.apache.kafka.connect.json.JsonDeserializer;
+import org.apache.kafka.connect.runtime.WorkerConfig;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;
 import org.apache.kafka.connect.source.SourceConnector;
 import org.apache.kafka.connect.source.SourceRecord;
+import org.apache.kafka.connect.storage.Converter;
+import org.apache.kafka.connect.storage.FileOffsetBackingStore;
+import org.apache.kafka.connect.storage.OffsetStorageReaderImpl;
+import org.fest.assertions.Delta;
 import org.junit.After;
 import org.junit.Before;
 import org.slf4j.Logger;
@@ -42,8 +52,10 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.data.SchemaUtil;
 import io.debezium.data.VerifyRecord;
 import io.debezium.embedded.EmbeddedEngine.CompletionCallback;
+import io.debezium.embedded.EmbeddedEngine.EmbeddedConfig;
 import io.debezium.function.BooleanConsumer;
 import io.debezium.relational.history.HistoryRecord;
 import io.debezium.util.LoggingContext;
@@ -162,20 +174,44 @@ protected int getMaximumEnqueuedRecordCount() {
     }
 
     /**
-     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
-     * logged.
+     * Create a {@link CompletionCallback} that logs when the engine fails to start the connector or when the connector
+     * stops running after completing successfully or due to an error
      * 
-     * @param connectorClass the connector class; may not be null
-     * @param connectorConfig the configuration for the connector; may not be null
+     * @return the logging {@link CompletionCallback}
      */
-    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig) {
-        start(connectorClass, connectorConfig, (success, msg, error) -> {
+    protected CompletionCallback loggingCompletion() {
+        return (success, msg, error) -> {
             if (success) {
                 logger.info(msg);
             } else {
                 logger.error(msg, error);
             }
-        });
+        };
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
+     * logged.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig) {
+        start(connectorClass, connectorConfig, loggingCompletion(), null);
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
+     * logged. The connector will stop immediately when the supplied predicate returns true.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     * @param isStopRecord the function that will be called to determine if the connector should be stopped before processing
+     *            this record; may be null if not needed
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         Predicate<SourceRecord> isStopRecord) {
+        start(connectorClass, connectorConfig, loggingCompletion(), isStopRecord);
     }
 
     /**
@@ -186,7 +222,23 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
      * @param callback the function that will be called when the engine fails to start the connector or when the connector
      *            stops running after completing successfully or due to an error; may be null
      */
-    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig, CompletionCallback callback) {
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         CompletionCallback callback) {
+        start(connectorClass, connectorConfig, callback, null);
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     * @param isStopRecord the function that will be called to determine if the connector should be stopped before processing
+     *            this record; may be null if not needed
+     * @param callback the function that will be called when the engine fails to start the connector or when the connector
+     *            stops running after completing successfully or due to an error; may be null
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         CompletionCallback callback, Predicate<SourceRecord> isStopRecord) {
         Configuration config = Configuration.copy(connectorConfig)
                                             .with(EmbeddedEngine.ENGINE_NAME, ""testing-connector"")
                                             .with(EmbeddedEngine.CONNECTOR_CLASS, connectorClass.getName())
@@ -202,11 +254,14 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
             }
             Testing.debug(""Stopped connector"");
         };
-
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
                                .notifying((record) -> {
+                                   if (isStopRecord != null && isStopRecord.test(record)) {
+                                       logger.error(""Stopping connector after record as requested"");
+                                       throw new ConnectException(""Stopping connector after record as requested"");
+                                   }
                                    try {
                                        consumedLines.put(record);
                                    } catch (InterruptedException e) {
@@ -306,7 +361,6 @@ protected SourceRecords consumeRecordsByTopic(int numRecords) throws Interrupted
         consumeRecords(numRecords, records::add);
         return records;
     }
-    
 
     protected class SourceRecords {
         private final List<SourceRecord> records = new ArrayList<>();
@@ -467,6 +521,55 @@ protected void assertTombstone(SourceRecord record, String pkField, int pk) {
     protected void assertTombstone(SourceRecord record) {
         VerifyRecord.isValidTombstone(record);
     }
+    
+    protected void assertOffset(SourceRecord record, Map<String,?> expectedOffset) {
+        Map<String,?> offset = record.sourceOffset();
+        assertThat(offset).isEqualTo(expectedOffset);
+    }
+    
+    protected void assertOffset(SourceRecord record, String offsetField, Object expectedValue) {
+        Map<String,?> offset = record.sourceOffset();
+        Object value = offset.get(offsetField);
+        assertSameValue(value,expectedValue);
+    }
+    
+    protected void assertValueField(SourceRecord record, String fieldPath, Object expectedValue) {
+        Object value = record.value();
+        String[] fieldNames = fieldPath.split(""/"");
+        String pathSoFar = null;
+        for (int i=0; i!=fieldNames.length; ++i) {
+            String fieldName = fieldNames[i];
+            if (value instanceof Struct) {
+                value = ((Struct)value).get(fieldName);
+            } else {
+                // We expected the value to be a struct ...
+                String path = pathSoFar == null ? ""record value"" : (""'"" + pathSoFar + ""'"");
+                String msg = ""Expected the "" + path + "" to be a Struct but was "" + value.getClass().getSimpleName() + "" in record: "" + SchemaUtil.asString(record);
+                fail(msg);
+            }
+            pathSoFar = pathSoFar == null ? fieldName : pathSoFar + ""/"" + fieldName;
+        }
+        assertSameValue(value,expectedValue);
+    }
+    
+    private void assertSameValue(Object actual, Object expected) {
+        if(expected instanceof Double || expected instanceof Float || expected instanceof BigDecimal) {
+            // Value should be within 1%
+            double expectedNumericValue = ((Number)expected).doubleValue();
+            double actualNumericValue = ((Number)actual).doubleValue();
+            assertThat(actualNumericValue).isEqualTo(expectedNumericValue, Delta.delta(0.01d*expectedNumericValue));
+        } else if (expected instanceof Integer || expected instanceof Long || expected instanceof Short) {
+            long expectedNumericValue = ((Number)expected).longValue();
+            long actualNumericValue = ((Number)actual).longValue();
+            assertThat(actualNumericValue).isEqualTo(expectedNumericValue);
+        } else if (expected instanceof Boolean) {
+            boolean expectedValue = ((Boolean)expected).booleanValue();
+            boolean actualValue = ((Boolean)expected).booleanValue();
+            assertThat(actualValue).isEqualTo(expectedValue);
+        } else {
+            assertThat(actual).isEqualTo(expected);
+        }
+    }
 
     /**
      * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
@@ -512,7 +615,8 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
         assertThat(value.errorMessages().size()).isEqualTo(numErrors);
     }
 
-    protected void assertConfigurationErrors(Config config, io.debezium.config.Field field, int minErrorsInclusive, int maxErrorsInclusive) {
+    protected void assertConfigurationErrors(Config config, io.debezium.config.Field field, int minErrorsInclusive,
+                                             int maxErrorsInclusive) {
         ConfigValue value = configValue(config, field.name());
         assertThat(value.errorMessages().size()).isGreaterThanOrEqualTo(minErrorsInclusive);
         assertThat(value.errorMessages().size()).isLessThanOrEqualTo(maxErrorsInclusive);
@@ -526,8 +630,8 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
     protected void assertNoConfigurationErrors(Config config, io.debezium.config.Field... fields) {
         for (io.debezium.config.Field field : fields) {
             ConfigValue value = configValue(config, field.name());
-            if ( value != null ) {
-                if ( !value.errorMessages().isEmpty() ) {
+            if (value != null) {
+                if (!value.errorMessages().isEmpty()) {
                     fail(""Error messages on field '"" + field.name() + ""': "" + value.errorMessages());
                 }
             }
@@ -538,4 +642,59 @@ protected ConfigValue configValue(Config config, String fieldName) {
         return config.configValues().stream().filter(value -> value.name().equals(fieldName)).findFirst().orElse(null);
     }
 
+    /**
+     * Utility to read the last committed offset for the specified partition.
+     * 
+     * @param config the configuration of the engine used to persist the offsets
+     * @param partition the partition
+     * @return the map of partitions to offsets; never null but possibly empty
+     */
+    protected <T> Map<String, Object> readLastCommittedOffset(Configuration config, Map<String, T> partition) {
+        return readLastCommittedOffsets(config, Arrays.asList(partition)).get(partition);
+    }
+
+    /**
+     * Utility to read the last committed offsets for the specified partitions.
+     * 
+     * @param config the configuration of the engine used to persist the offsets
+     * @param partitions the partitions
+     * @return the map of partitions to offsets; never null but possibly empty
+     */
+    protected <T> Map<Map<String, T>, Map<String, Object>> readLastCommittedOffsets(Configuration config,
+                                                                                    Collection<Map<String, T>> partitions) {
+        config = config.edit().with(EmbeddedEngine.ENGINE_NAME, ""testing-connector"")
+                       .with(StandaloneConfig.OFFSET_STORAGE_FILE_FILENAME_CONFIG, OFFSET_STORE_PATH)
+                       .with(EmbeddedEngine.OFFSET_FLUSH_INTERVAL_MS, 0)
+                       .build();
+
+        final String engineName = config.getString(EmbeddedEngine.ENGINE_NAME);
+        Converter keyConverter = config.getInstance(EmbeddedEngine.INTERNAL_KEY_CONVERTER_CLASS, Converter.class);
+        keyConverter.configure(config.subset(EmbeddedEngine.INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), true);
+        Converter valueConverter = config.getInstance(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS, Converter.class);
+        Configuration valueConverterConfig = config;
+        if (valueConverter instanceof JsonConverter) {
+            // Make sure that the JSON converter is configured to NOT enable schemas ...
+            valueConverterConfig = config.edit().with(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS + "".schemas.enable"", false).build();
+        }
+        valueConverter.configure(valueConverterConfig.subset(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS.name() + ""."", true).asMap(),
+                                 false);
+
+        // Create the worker config, adding extra fields that are required for validation of a worker config
+        // but that are not used within the embedded engine (since the source records are never serialized) ...
+        Map<String, String> embeddedConfig = config.asMap(EmbeddedEngine.ALL_FIELDS);
+        embeddedConfig.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, JsonConverter.class.getName());
+        embeddedConfig.put(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, JsonConverter.class.getName());
+        WorkerConfig workerConfig = new EmbeddedConfig(embeddedConfig);
+
+        FileOffsetBackingStore offsetStore = new FileOffsetBackingStore();
+        offsetStore.configure(workerConfig);
+        offsetStore.start();
+        try {
+            OffsetStorageReaderImpl offsetReader = new OffsetStorageReaderImpl(offsetStore, engineName, keyConverter, valueConverter);
+            return offsetReader.offsets(partitions);
+        } finally {
+            offsetStore.stop();
+        }
+    }
+
 }",2016-11-09T14:11:41Z,35
"@@ -63,6 +63,7 @@ public class MySqlDdlParser extends DdlParser {
     private final MySqlSystemVariables systemVariables = new MySqlSystemVariables();
     private final ConcurrentMap<String, String> charsetNameForDatabase = new ConcurrentHashMap<>();
 
+    public static final char ENUM_AND_SET_DELIMINATOR = ',';
     /**
      * Create a new DDL parser for MySQL that does not include view definitions.
      */
@@ -655,21 +656,21 @@ protected Column parseCreateColumn(Marker start, TableEditor table, String colum
      * @param typeExpression the data type expression
      * @return the string containing the character options allowed by the {@code ENUM} or {@code SET}; never null
      */
-    public static String parseSetAndEnumOptions(String typeExpression) {
+    public static List<String> parseSetAndEnumOptions(String typeExpression) {
         Matcher matcher = ENUM_AND_SET_LITERALS.matcher(typeExpression);
+        List<String> options = new ArrayList<>();
         if (matcher.matches()) {
             String literals = matcher.group(2);
             Matcher optionMatcher = ENUM_AND_SET_OPTIONS.matcher(literals);
             StringBuilder sb = new StringBuilder();
             while (optionMatcher.find()) {
                 String option = optionMatcher.group(1);
                 if (option.length() > 0) {
-                    sb.append(option.charAt(0));
+                    options.add(option);
                 }
             }
-            return sb.toString();
         }
-        return """";
+        return options;
     }
 
     protected void parseColumnDefinition(Marker start, String columnName, TokenStream tokens, TableEditor table, ColumnEditor column,
@@ -692,8 +693,9 @@ protected void parseColumnDefinition(Marker start, String columnName, TokenStrea
         if (""ENUM"".equals(dataType.name())) {
             column.length(1);
         } else if (""SET"".equals(dataType.name())) {
-            String options = parseSetAndEnumOptions(dataType.expression());
-            column.length(Math.max(0, options.length() * 2 - 1)); // number of options + number of commas
+            List<String> options = parseSetAndEnumOptions(dataType.expression());
+            //After DBZ-132, it will always be comma seperated
+            column.length(Math.max(0, options.size() * 2 - 1)); // number of options + number of commas
         } else {
             if (dataType.length() > -1) column.length((int) dataType.length());
             if (dataType.scale() > -1) column.scale(dataType.scale());",2016-10-11T19:11:08Z,49
"@@ -12,6 +12,7 @@
 import java.sql.Types;
 import java.time.OffsetDateTime;
 import java.time.ZoneOffset;
+import java.util.List;
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
@@ -85,12 +86,32 @@ public SchemaBuilder schemaBuilder(Column column) {
             return Year.builder();
         }
         if (matches(typeName, ""ENUM"")) {
-            String commaSeparatedOptions = extractEnumAndSetOptions(column, true);
-            return io.debezium.data.Enum.builder(commaSeparatedOptions);
+            List<String> options = extractEnumAndSetOptions(column);
+            StringBuilder commaSeparatedOptions = new StringBuilder();
+            boolean first = true;
+            for (String value:options) {
+                if (first) {
+                    first = false;
+                } else {
+                    commaSeparatedOptions.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+                }
+                commaSeparatedOptions.append(value);
+            }
+            return io.debezium.data.Enum.builder(commaSeparatedOptions.toString());
         }
         if (matches(typeName, ""SET"")) {
-            String commaSeparatedOptions = extractEnumAndSetOptions(column, true);
-            return io.debezium.data.EnumSet.builder(commaSeparatedOptions);
+            List<String> options = extractEnumAndSetOptions(column);
+            StringBuilder commaSeparatedOptions = new StringBuilder();
+            boolean first = true;
+            for (String value:options) {
+                if (first) {
+                    first = false;
+                } else {
+                    commaSeparatedOptions.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+                }
+                commaSeparatedOptions.append(value);
+            }
+            return io.debezium.data.EnumSet.builder(commaSeparatedOptions.toString());
         }
         // Otherwise, let the base class handle it ...
         return super.schemaBuilder(column);
@@ -105,12 +126,12 @@ public ValueConverter converter(Column column, Field fieldDefn) {
         }
         if (matches(typeName, ""ENUM"")) {
             // Build up the character array based upon the column's type ...
-            String options = extractEnumAndSetOptions(column, false);
+            List<String> options = extractEnumAndSetOptions(column);
             return (data) -> convertEnumToString(options, column, fieldDefn, data);
         }
         if (matches(typeName, ""SET"")) {
             // Build up the character array based upon the column's type ...
-            String options = extractEnumAndSetOptions(column, false);
+            List<String> options = extractEnumAndSetOptions(column);
             return (data) -> convertSetToString(options, column, fieldDefn, data);
         }
         
@@ -240,7 +261,7 @@ protected Object convertYearToInt(Column column, Field fieldDefn, Object data) {
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
-    protected Object convertEnumToString(String options, Column column, Field fieldDefn, Object data) {
+    protected Object convertEnumToString(List<String> options, Column column, Field fieldDefn, Object data) {
         if (data == null) {
             data = fieldDefn.schema().defaultValue();
         }
@@ -253,10 +274,13 @@ protected Object convertEnumToString(String options, Column column, Field fieldD
             return data;
         }
         if (data instanceof Integer) {
-            // The binlog will contain an int with the 1-based index of the option in the enum value ...
-            int index = ((Integer) data).intValue() - 1; // 'options' is 0-based
-            if (index < options.length()) {
-                return options.substring(index, index + 1);
+
+            if (options != null) {
+                // The binlog will contain an int with the 1-based index of the option in the enum value ...
+                int index = ((Integer) data).intValue() - 1; // 'options' is 0-based
+                if (index < options.size()) {
+                    return options.get(index);
+                }
             }
             return null;
         }
@@ -275,7 +299,7 @@ protected Object convertEnumToString(String options, Column column, Field fieldD
      * @return the converted value, or null if the conversion could not be made and the column allows nulls
      * @throws IllegalArgumentException if the value could not be converted but the column does not allow nulls
      */
-    protected Object convertSetToString(String options, Column column, Field fieldDefn, Object data) {
+    protected Object convertSetToString(List<String> options, Column column, Field fieldDefn, Object data) {
         if (data == null) {
             data = fieldDefn.schema().defaultValue();
         }
@@ -308,32 +332,25 @@ protected boolean matches(String upperCaseTypeName, String upperCaseMatch) {
         return upperCaseMatch.equals(upperCaseTypeName) || upperCaseTypeName.startsWith(upperCaseMatch + ""("");
     }
 
-    protected String extractEnumAndSetOptions(Column column, boolean commaSeparated) {
-        String options = MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
-        if (!commaSeparated) return options;
-        StringBuilder sb = new StringBuilder();
-        boolean first = true;
-        for (int i = 0; i != options.length(); ++i) {
-            if (first)
-                first = false;
-            else
-                sb.append(',');
-            sb.append(options.charAt(i));
-        }
-        return sb.toString();
+    protected List<String> extractEnumAndSetOptions(Column column) {
+        List<String> options = MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
+        return options;
     }
 
-    protected String convertSetValue(long indexes, String options) {
+    protected String convertSetValue(long indexes, List<String> options) {
         StringBuilder sb = new StringBuilder();
         int index = 0;
         boolean first = true;
+        int optionLen = options.size();
         while (indexes != 0L) {
             if (indexes % 2L != 0) {
                 if (first)
                     first = false;
                 else
-                    sb.append(',');
-                sb.append(options.substring(index, index + 1));
+                    sb.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+                if (index < optionLen) {
+                    sb.append(options.get(index));
+                }
             }
             ++index;
             indexes = indexes >>> 1;",2016-10-11T19:11:08Z,47
"@@ -10,6 +10,7 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.sql.Types;
+import java.util.List;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.junit.Before;
@@ -638,27 +639,37 @@ public void shouldParseTicketMonsterLiquibaseStatements() {
 
     @Test
     public void shouldParseEnumOptions() {
-        assertParseEnumAndSetOptions(""ENUM('a','b','c')"", ""abc"");
+        assertParseEnumAndSetOptions(""ENUM('a','b','c')"", ""a,b,c"");
         assertParseEnumAndSetOptions(""ENUM('a')"", ""a"");
         assertParseEnumAndSetOptions(""ENUM()"", """");
-        assertParseEnumAndSetOptions(""ENUM ('a','b','c') CHARACTER SET"", ""abc"");
+        assertParseEnumAndSetOptions(""ENUM ('a','b','c') CHARACTER SET"", ""a,b,c"");
         assertParseEnumAndSetOptions(""ENUM ('a') CHARACTER SET"", ""a"");
         assertParseEnumAndSetOptions(""ENUM () CHARACTER SET"", """");
     }
 
     @Test
     public void shouldParseSetOptions() {
-        assertParseEnumAndSetOptions(""SET('a','b','c')"", ""abc"");
+        assertParseEnumAndSetOptions(""SET('a','b','c')"", ""a,b,c"");
         assertParseEnumAndSetOptions(""SET('a')"", ""a"");
         assertParseEnumAndSetOptions(""SET()"", """");
-        assertParseEnumAndSetOptions(""SET ('a','b','c') CHARACTER SET"", ""abc"");
+        assertParseEnumAndSetOptions(""SET ('a','b','c') CHARACTER SET"", ""a,b,c"");
         assertParseEnumAndSetOptions(""SET ('a') CHARACTER SET"", ""a"");
         assertParseEnumAndSetOptions(""SET () CHARACTER SET"", """");
     }
 
     protected void assertParseEnumAndSetOptions(String typeExpression, String optionString) {
-        String options = MySqlDdlParser.parseSetAndEnumOptions(typeExpression);
-        assertThat(options).isEqualTo(optionString);
+        List<String> options = MySqlDdlParser.parseSetAndEnumOptions(typeExpression);
+        StringBuilder sb = new StringBuilder();
+        boolean first = true;
+        for (String value:options) {
+            if (first) {
+                first = false;
+            } else {
+                sb.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+            }
+            sb.append(value);
+        }
+        assertThat(optionString).isEqualTo(sb.toString());
     }
 
     protected void assertVariable(String name, String expectedValue) {",2016-10-11T19:11:08Z,26
"@@ -16,7 +16,6 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicLong;
-import java.util.function.Predicate;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -155,28 +154,21 @@ protected void doStart() {
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
 
-        // The 'source' object holds the starting point in the binlog where we should start reading,
-        // set set the client to start from that point ...
-        String gtidSetStr = source.gtidSet();
-        if (gtidSetStr != null) {
+        // Get the current GtidSet from MySQL so we can get a filtered/merged GtidSet based off of the last Debezium checkpoint.
+        String availableServerGtidStr = context.knownGtidSet();
+        GtidSet availableServerGtidSet = new GtidSet(availableServerGtidStr);
+        GtidSet filteredGtidSet = context.getFilteredGtidSet(availableServerGtidSet);
+        if (filteredGtidSet != null) {
             // Register the event handler ...
             eventHandlers.put(EventType.GTID, this::handleGtidEvent);
-
-            logger.info(""GTID set from previous recorded offset: {}"", gtidSetStr);
-            // Remove any of the GTID sources that are not required/acceptable ...
-            Predicate<String> gtidSourceFilter = context.gtidSourceFilter();
-            if (gtidSourceFilter != null) {
-                GtidSet gtidSet = new GtidSet(gtidSetStr).retainAll(gtidSourceFilter);
-                gtidSetStr = gtidSet.toString();
-                logger.info(""GTID set after applying GTID source includes/excludes: {}"", gtidSetStr);
-                source.setGtidSet(gtidSetStr);
-            }
-            client.setGtidSet(gtidSetStr);
-            gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(gtidSetStr);
+            logger.info(""Registering binlog reader with GTID set: {}"", filteredGtidSet);
+            String filteredGtidSetStr = filteredGtidSet.toString();
+            client.setGtidSet(filteredGtidSetStr);
+            source.setGtidSet(filteredGtidSetStr);
+            gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(filteredGtidSetStr);
         } else {
             client.setBinlogFilename(source.binlogFilename());
             client.setBinlogPosition(source.nextBinlogPosition());
-            gtidSet = null;
         }
 
         // Set the starting row number, which is the next row number to be read ...",2016-11-03T21:47:42Z,67
"@@ -8,6 +8,7 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
@@ -100,6 +101,19 @@ public boolean isContainedWithin(GtidSet other) {
         return true;
     }
 
+    /**
+     * Obtain a copy of this {@link GtidSet} except overwritten with all of the GTID ranges in the supplied {@link GtidSet}.
+     * @param other the other {@link GtidSet} with ranges to add/overwrite on top of those in this set;
+     * @return the new GtidSet, or this object if {@code other} is null or empty; never null
+     */
+    public GtidSet with(GtidSet other) {
+        if (other == null || other.uuidSetsByServerId.isEmpty()) return this;
+        Map<String, UUIDSet> newSet = new HashMap<>();
+        newSet.putAll(this.uuidSetsByServerId);
+        newSet.putAll(other.uuidSetsByServerId);
+        return new GtidSet(newSet);
+    }
+
     @Override
     public int hashCode() {
         return uuidSetsByServerId.keySet().hashCode();",2016-11-03T21:47:42Z,87
"@@ -241,7 +241,7 @@ protected boolean isBinlogAvailable() {
         String gtidStr = taskContext.source().gtidSet();
         if (gtidStr != null) {
             if (gtidStr.trim().isEmpty()) return true; // start at beginning ...
-            String availableGtidStr = knownGtidSet();
+            String availableGtidStr = taskContext.knownGtidSet();
             if (availableGtidStr == null || availableGtidStr.trim().isEmpty()) {
                 // Last offsets had GTIDs but the server does not use them ...
                 logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
@@ -250,7 +250,7 @@ protected boolean isBinlogAvailable() {
             // GTIDs are enabled, and we used them previously, but retain only those GTID ranges for the allowed source UUIDs ...
             GtidSet gtidSet = new GtidSet(gtidStr).retainAll(taskContext.gtidSourceFilter());
             // Get the GTID set that is available in the server ...
-            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            GtidSet availableGtidSet = new GtidSet(availableGtidStr);
             if (gtidSet.isContainedWithin(availableGtidSet)) {
                 logger.info(""MySQL current GTID set {} does contain the GTID set required by the connector {}"", availableGtidSet, gtidSet);
                 return true;
@@ -328,26 +328,6 @@ protected boolean isGtidModeEnabled() {
         return !""OFF"".equalsIgnoreCase(mode.get());
     }
 
-    /**
-     * Determine the available GTID set for MySQL.
-     * 
-     * @return the string representation of MySQL's GTID sets.
-     */
-    protected String knownGtidSet() {
-        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
-        try {
-            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
-                if (rs.next()) {
-                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
-                }
-            });
-        } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
-        }
-
-        return gtidSetStr.get();
-    }
-
     /**
      * Determine whether the MySQL server has the row-level binlog enabled.
      * ",2016-11-03T21:47:42Z,68
"@@ -123,6 +123,26 @@ public void close() {
         shutdown();
     }
 
+    /**
+     * Determine the available GTID set for MySQL.
+     *
+     * @return the string representation of MySQL's GTID sets.
+     */
+    public String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            jdbc.query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
+        }
+
+        return gtidSetStr.get();
+    }
+
     protected String connectionString() {
         return jdbc.connectionString(MYSQL_CONNECTION_URL);
     }",2016-11-03T21:47:42Z,61
"@@ -246,4 +246,40 @@ public ObjectName metricName(String contextName) throws MalformedObjectNameExcep
         return new ObjectName(""debezium.mysql:type=connector-metrics,context="" + contextName + "",server="" + serverName());
     }
 
+    /**
+     * Retrieve GTID set after applying include/exclude filters on the source. Also, merges the server GTID set with the
+     * filtered client (Debezium) set.
+     *
+     * The merging behavior of this method might seem a bit strange at first. It's required in order for Debezium to consume a
+     * MySQL binlog that has multi-source replication enabled, if a failover has to occur. In such a case, the server that
+     * Debezium is failed over to might have a different set of sources, but still include the sources required for Debezium
+     * to continue to function. MySQL does not allow downstream replicas to connect if the GTID set does not contain GTIDs for
+     * all channels that the server is replicating from, even if the server does have the data needed by the client. To get
+     * around this, we can have Debezium merge its GTID set with whatever is on the server, so that MySQL will allow it to
+     * connect. See <a href=""https://issues.jboss.org/browse/DBZ-143"">DBZ-143</a> for details.
+     *
+     * This method does not mutate any state in the context.
+     *
+     * @return A GTID set meant for consuming from a MySQL binlog; may return null if the SourceInfo has no GTIDs and therefore
+     *         none were filtered
+     */
+    public GtidSet getFilteredGtidSet(GtidSet availableServerGtidSet) {
+        logger.info(""Attempting to generate a filtered GTID set"");
+        String gtidStr = source.gtidSet();
+        if (gtidStr == null) {
+            return null;
+        }
+        logger.info(""GTID set from previous recorded offset: {}"", gtidStr);
+        GtidSet filteredGtidSet = new GtidSet(gtidStr);
+        Predicate<String> gtidSourceFilter = gtidSourceFilter();
+        if (gtidSourceFilter != null) {
+            filteredGtidSet = filteredGtidSet.retainAll(gtidSourceFilter);
+            logger.info(""GTID set after applying GTID source includes/excludes to previous recorded offset: {}"", filteredGtidSet);
+        }
+        logger.info(""GTID set available on server: {}"", availableServerGtidSet);
+        GtidSet mergedGtidSet = availableServerGtidSet.with(filteredGtidSet);
+        logger.info(""Final merged GTID set to use when connecting to MySQL: {}"", mergedGtidSet);
+        return mergedGtidSet;
+    }
+
 }",2016-11-03T21:47:42Z,69
"@@ -6,6 +6,7 @@
 package io.debezium.connector.mysql;
 
 import java.nio.file.Path;
+import java.util.Arrays;
 import java.util.function.Predicate;
 
 import org.junit.After;
@@ -201,4 +202,31 @@ public void shouldNotAllowBothGtidSetIncludesAndExcludes() throws Exception {
         boolean valid = config.validateAndRecord(MySqlConnectorConfig.ALL_FIELDS,msg->{});
         assertThat(valid).isFalse();
     }
+
+    @Test
+    public void shouldFilterAndMergeGtidSet() throws Exception {
+        String gtidStr = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,""
+          + ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41"";
+        String availableServerGtidStr = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-20,""
+          + ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3200,""
+          + ""123e4567-e89b-12d3-a456-426655440000:1-41"";
+        config = simpleConfig().with(MySqlConnectorConfig.GTID_SOURCE_INCLUDES,
+                                     ""036d85a9-64e5-11e6-9b48-42010af0000c"")
+                               .build();
+        context = new MySqlTaskContext(config);
+        context.start();
+        context.source().setGtidSet(gtidStr);
+
+        GtidSet mergedGtidSet = context.getFilteredGtidSet(new GtidSet(availableServerGtidStr));
+        assertThat(mergedGtidSet).isNotNull();
+        GtidSet.UUIDSet uuidSet1 = mergedGtidSet.forServerWithId(""036d85a9-64e5-11e6-9b48-42010af0000c"");
+        GtidSet.UUIDSet uuidSet2 = mergedGtidSet.forServerWithId(""7145bf69-d1ca-11e5-a588-0242ac110004"");
+        GtidSet.UUIDSet uuidSet3 = mergedGtidSet.forServerWithId(""123e4567-e89b-12d3-a456-426655440000"");
+        GtidSet.UUIDSet uuidSet4 = mergedGtidSet.forServerWithId(""7c1de3f2-3fd2-11e6-9cdc-42010af000bc"");
+
+        assertThat(uuidSet1.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 2)));
+        assertThat(uuidSet2.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 3200)));
+        assertThat(uuidSet3.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 41)));
+        assertThat(uuidSet4).isNull();
+    }
 }",2016-11-03T21:47:42Z,72
"@@ -1322,6 +1322,13 @@ protected void sequentially(Consumer<Marker>... functions) {
         parsingFailed(marker.position(), errors, ""Unable to parse statement"");
     }
 
+    /**
+     * Parse and consume the {@code DEFAULT} clause. Currently, this method does not capture the default in any way,
+     * since that will likely require parsing the default clause into a useful value (e.g., dealing with hexadecimals,
+     * bit-set literals, date-time literals, etc.).
+
+     * @param start the marker at the beginning of the clause
+     */
     protected void parseDefaultClause(Marker start) {
         tokens.consume(""DEFAULT"");
         if (isNextTokenQuotedIdentifier()) {
@@ -1342,7 +1349,6 @@ protected void parseDefaultClause(Marker start) {
                 // do nothing ...
             } else {
                 parseLiteral(start);
-                // do nothing ...
             }
         }
     }",2016-10-06T18:25:38Z,49
"@@ -588,6 +588,14 @@ public void shouldParseStatementForDbz106() {
         assertThat(listener.total()).isEqualTo(1);
     }
 
+    @Test
+    public void shouldParseStatementForDbz123() {
+        parser.parse(readFile(""ddl/mysql-dbz-123.ddl""), tables);
+        Testing.print(tables);
+        assertThat(tables.size()).isEqualTo(1);
+        assertThat(listener.total()).isEqualTo(1);
+    }
+
     @Test
     public void shouldParseTestStatements() {
         parser.parse(readFile(""ddl/mysql-test-statements.ddl""), tables);",2016-10-06T18:25:38Z,26
"@@ -0,0 +1,11 @@
+CREATE TABLE `DBZ123` (
+  `Id` bigint(20) NOT NULL AUTO_INCREMENT,
+  `Provider_ID` bigint(20) NOT NULL,
+  `External_ID` varchar(255) NOT NULL,
+  `Name` varchar(255) NOT NULL,
+  `Is_Enabled` bit(1) NOT NULL DEFAULT b'1',
+  `binaryRepresentation` BLOB NOT NULL DEFAULT x'cafe',
+  `BonusFactor` decimal(19,8) NOT NULL,
+  PRIMARY KEY (`Id`),
+  UNIQUE KEY `game_unq` (`Provider_ID`,`External_ID`)
+) ENGINE=InnoDB AUTO_INCREMENT=2374 DEFAULT CHARSET=utf8
\ No newline at end of file",2016-10-06T18:25:38Z,88
"@@ -664,15 +664,18 @@ protected Object parseLiteral(Marker start) {
             parseCharacterSetName(start);
             return parseCharacterLiteral(start);
         }
-        if (tokens.canConsume('N')) {
+        if (tokens.canConsume(""N"")) {
             return parseCharacterLiteral(start);
         }
         if (tokens.canConsume(""U"", ""&"")) {
             return parseCharacterLiteral(start);
         }
-        if (tokens.canConsume('X')) {
+        if (tokens.canConsume(""X"")) {
             return parseCharacterLiteral(start);
         }
+        if (tokens.canConsume(""B"")) {
+            return parseBitFieldLiteral(start);
+        }
         if (tokens.matchesAnyOf(DdlTokenizer.DOUBLE_QUOTED_STRING, DdlTokenizer.SINGLE_QUOTED_STRING)) {
             return tokens.consume();
         }
@@ -751,6 +754,10 @@ protected String parseCharacterSetName(Marker start) {
         return name;
     }
 
+    protected String parseBitFieldLiteral(Marker start) {
+        return consumeQuotedString();
+    }
+
     protected String parseDateLiteral(Marker start) {
         return consumeQuotedString();
     }",2016-10-06T18:25:38Z,65
"@@ -70,6 +70,8 @@ public class BinlogReader extends AbstractReader {
     private final ElapsedTimeStrategy pollOutputDelay;
     private long recordCounter = 0L;
     private long previousOutputMillis = 0L;
+    private long initialEventsToSkip = 0L;
+    private boolean skipEvent = false;
     private final AtomicLong totalRecordCounter = new AtomicLong();
     private volatile Map<String, ?> lastOffset = null;
     private com.github.shyiko.mysql.binlog.GtidSet gtidSet;
@@ -164,15 +166,21 @@ protected void doStart() {
             logger.info(""Registering binlog reader with GTID set: {}"", filteredGtidSet);
             String filteredGtidSetStr = filteredGtidSet.toString();
             client.setGtidSet(filteredGtidSetStr);
-            source.setGtidSet(filteredGtidSetStr);
+            source.setCompletedGtidSet(filteredGtidSetStr);
             gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(filteredGtidSetStr);
         } else {
             client.setBinlogFilename(source.binlogFilename());
-            client.setBinlogPosition(source.nextBinlogPosition());
+            client.setBinlogPosition(source.binlogPosition());
         }
 
+        // We may be restarting in the middle of a transaction, so see how far into the transaction we have already processed...
+        initialEventsToSkip = source.eventsToSkipUponRestart();
+
         // Set the starting row number, which is the next row number to be read ...
-        startingRowNumber = source.nextEventRowNumber();
+        startingRowNumber = source.rowsToSkipUponRestart();
+
+        // Only when we reach the first BEGIN event will we start to skip events ...
+        skipEvent = false;
 
         // Initial our poll output delay logic ...
         pollOutputDelay.hasElapsed();
@@ -287,8 +295,15 @@ protected void handleEvent(Event event) {
             // Forward the event to the handler ...
             eventHandlers.getOrDefault(eventType, this::ignoreEvent).accept(event);
 
-            // And after that event has been processed, always set the starting row number to 0 ...
-            startingRowNumber = 0;
+            // Capture that we've completed another event ...
+            source.completeEvent();
+
+            if (skipEvent) {
+                // We're in the mode of skipping events and we just skipped this one, so decrement our skip count ...
+                --initialEventsToSkip;
+                skipEvent = initialEventsToSkip > 0;
+            }
+
         } catch (RuntimeException e) {
             // There was an error in the event handler, so propagate the failure to Kafka Connect ...
             failed(e, ""Error processing binlog event"");
@@ -375,8 +390,7 @@ protected void handleGtidEvent(Event event) {
         GtidEventData gtidEvent = unwrapData(event);
         String gtid = gtidEvent.getGtid();
         gtidSet.add(gtid);
-        source.setGtid(gtid);
-        source.setGtidSet(gtidSet.toString()); // rather than use the client's GTID set
+        source.startGtid(gtid, gtidSet.toString()); // rather than use the client's GTID set
     }
 
     /**
@@ -387,14 +401,23 @@ protected void handleGtidEvent(Event event) {
      */
     protected void handleQueryEvent(Event event) {
         QueryEventData command = unwrapData(event);
-        logger.debug(""Received update table command: {}"", event);
+        logger.debug(""Received query command: {}"", event);
         String sql = command.getSql().trim();
         if (sql.equalsIgnoreCase(""BEGIN"")) {
-            // ignore these altogether ...
+            // We are starting a new transaction ...
+            source.startNextTransaction();
+            if (initialEventsToSkip != 0) {
+                logger.debug(""Restarting partially-processed transaction; change events will not be created for the first {} events plus {} more rows in the next event"",
+                             initialEventsToSkip, startingRowNumber);
+                // We are restarting, so we need to skip the events in this transaction that we processed previously...
+                skipEvent = true;
+            }
             return;
         }
         if (sql.equalsIgnoreCase(""COMMIT"")) {
-            // ignore these altogether ...
+            // We are completing the transaction ...
+            source.commitTransaction();
+            skipEvent = false;
             return;
         }
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
@@ -438,6 +461,11 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
@@ -447,13 +475,26 @@ protected void handleInsert(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                count += recordMaker.create(rows.get(row), ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    count += recordMaker.create(rows.get(row), ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} insert record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} insert record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed insert event: {}"", event);
             }
-            logger.debug(""Recorded {} insert records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping insert row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     /**
@@ -463,6 +504,11 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
@@ -473,16 +519,29 @@ protected void handleUpdate(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
-                Serializable[] before = changes.getKey();
-                Serializable[] after = changes.getValue();
-                count += recordMaker.update(before, after, ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
+                    Serializable[] before = changes.getKey();
+                    Serializable[] after = changes.getValue();
+                    count += recordMaker.update(before, after, ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} update record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} update record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed update event: {}"", event);
             }
-            logger.debug(""Recorded {} update records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping update row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     /**
@@ -492,6 +551,11 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
+        if (skipEvent) {
+            // We can skip this because we should already be at least this far ...
+            logger.debug(""Skipping previously processed row event: {}"", event);
+            return;
+        }
         DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
@@ -501,13 +565,26 @@ protected void handleDelete(Event event) throws InterruptedException {
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
             int numRows = rows.size();
-            for (int row = startingRowNumber; row != numRows; ++row) {
-                count += recordMaker.delete(rows.get(row), ts, row, numRows);
+            if (startingRowNumber < numRows) {
+                for (int row = startingRowNumber; row != numRows; ++row) {
+                    count += recordMaker.delete(rows.get(row), ts, row, numRows);
+                }
+                if (logger.isDebugEnabled()) {
+                    if (startingRowNumber != 0) {
+                        logger.debug(""Recorded {} delete record(s) for last {} row(s) in event: {}"",
+                                     count, numRows - startingRowNumber, event);
+                    } else {
+                        logger.debug(""Recorded {} delete record(s) for event: {}"", count, event);
+                    }
+                }
+            } else {
+                // All rows were previously processed ...
+                logger.debug(""Skipping previously processed delete event: {}"", event);
             }
-            logger.debug(""Recorded {} delete records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping delete row event: {}"", event);
         }
+        startingRowNumber = 0;
     }
 
     protected SSLMode sslModeFor(SecureConnectionMode mode) {",2016-11-09T14:11:41Z,67
"@@ -139,7 +139,7 @@ public synchronized void start(Map<String, String> props) {
             if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
                 // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
                 // previously used but the MySQL server has them enabled ...
-                source.setGtidSet("""");
+                source.setCompletedGtidSet("""");
             }
 
             // Check whether the row-level binlog is enabled ...",2016-11-09T14:11:41Z,68
"@@ -265,11 +265,11 @@ public ObjectName metricName(String contextName) throws MalformedObjectNameExcep
      *         none were filtered
      */
     public GtidSet filterGtidSet(GtidSet availableServerGtidSet) {
-        logger.info(""Attempting to generate a filtered GTID set"");
         String gtidStr = source.gtidSet();
         if (gtidStr == null) {
             return null;
         }
+        logger.info(""Attempting to generate a filtered GTID set"");
         logger.info(""GTID set from previous recorded offset: {}"", gtidStr);
         GtidSet filteredGtidSet = new GtidSet(gtidStr);
         Predicate<String> gtidSourceFilter = gtidSourceFilter();",2016-11-09T14:11:41Z,69
"@@ -222,7 +222,7 @@ protected void execute() {
                     if (rs.getMetaData().getColumnCount() > 4) {
                         // This column exists only in MySQL 5.6.5 or later ...
                         String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
-                        source.setGtidSet(gtidSet);
+                        source.setCompletedGtidSet(gtidSet);
                         logger.info(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
                                     gtidSet);
                     } else {",2016-11-09T14:11:41Z,62
"@@ -32,10 +32,13 @@
  * </pre>
  * 
  * <p>
- * The {@link #offset() source offset} information describes how much of the database's binary log the source the change detector
- * has already processed, and it includes the {@link #binlogFilename() binlog filename}, the {@link #nextBinlogPosition() next
- * position} in the binlog to start reading, and the {@link #nextEventRowNumber() next event row number}. Here's a JSON-like
- * representation of an example:
+ * The {@link #offset() source offset} information is included in each event and captures where the connector should restart
+ * if this event's offset is the last one recorded. The offset includes the {@link #binlogFilename() binlog filename},
+ * the {@link #binlogPosition() position of the first event} in the binlog, the
+ * {@link #eventsToSkipUponRestart() number of events to skip}, and the
+ * {@link #rowsToSkipUponRestart() number of rows to also skip}.
+ * <p>
+ * Here's a JSON-like representation of an example:
  * 
  * <pre>
  * {
@@ -44,22 +47,26 @@
  *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
  *     ""file"": ""mysql-bin.000003"",
  *     ""pos"" = 990,
+ *     ""event"" = 0,
  *     ""row"": 0,
  *     ""snapshot"": true
  * }
  * </pre>
- * 
+ * <p>
  * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
  * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
  * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
  * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
- * 
- * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
- * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), the
- * {@link #lastBinlogPosition() position} of the event (and {@link #lastEventRowNumber() row number} within the event) inside
- * the {@link #binlogFilename() binlog file}. When GTIDs are enabled, it also includes the GTID of the transaction in which the
- * event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced when the connector is in the
- * middle of a snapshot. Here's a JSON-like representation of the source for an event that corresponds to the above partition and
+ * <p>
+ * Each change event envelope also includes the {@link #struct() source} struct that contains MySQL information about that
+ * particular event, including a mixture the fields from the {@link #partition() partition} (which is renamed in the source to
+ * make more sense), the binlog filename and position where the event can be found, and when GTIDs are enabled the GTID of the
+ * transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced
+ * when the connector is in the middle of a snapshot. Note that this information is likely different than the offset information,
+ * since the connector may need to restart from either just after the most recently completed transaction or the beginning
+ * of the most recently started transaction (whichever appears later in the binlog).
+ * <p>
+ * Here's a JSON-like representation of the source for an event that corresponds to the above partition and
  * offset:
  * 
  * <pre>
@@ -88,9 +95,10 @@ final class SourceInfo {
     public static final String SERVER_PARTITION_KEY = ""server"";
     public static final String GTID_SET_KEY = ""gtids"";
     public static final String GTID_KEY = ""gtid"";
+    public static final String EVENTS_TO_SKIP_OFFSET_KEY = ""event"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
-    public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
+    public static final String BINLOG_ROW_IN_EVENT_OFFSET_KEY = ""row"";
     public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
@@ -105,17 +113,22 @@ final class SourceInfo {
                                                      .field(GTID_KEY, Schema.OPTIONAL_STRING_SCHEMA)
                                                      .field(BINLOG_FILENAME_OFFSET_KEY, Schema.STRING_SCHEMA)
                                                      .field(BINLOG_POSITION_OFFSET_KEY, Schema.INT64_SCHEMA)
-                                                     .field(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, Schema.INT32_SCHEMA)
+                                                     .field(BINLOG_ROW_IN_EVENT_OFFSET_KEY, Schema.INT32_SCHEMA)
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private String gtidSet;
-    private String binlogGtid;
-    private String binlogFilename;
-    private long lastBinlogPosition = 0;
-    private int lastEventRowNumber = 0;
-    private long nextBinlogPosition = 4;
-    private int nextEventRowNumber = 0;
+    private String currentGtidSet;
+    private String currentGtid;
+    private String currentBinlogFilename;
+    private long currentBinlogPosition = 0L;
+    private int currentRowNumber = 0;
+    private long currentEventLengthInBytes = 0;
+    private String restartGtidSet;
+    private String restartBinlogFilename;
+    private long restartBinlogPosition = 0L;
+    private long restartEventsToSkip = 0;
+    private int restartRowsToSkip = 0;
+    private boolean inTransaction = false;
     private String serverName;
     private long serverId = 0;
     private long binlogTimestampSeconds = 0;
@@ -149,50 +162,91 @@ public Map<String, String> partition() {
         return sourcePartition;
     }
 
+    /**
+     * Set the position in the MySQL binlog where we will start reading.
+     * 
+     * @param binlogFilename the name of the binary log file; may not be null
+     * @param positionOfFirstEvent the position in the binary log file to begin processing
+     */
+    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
+        if (binlogFilename != null) {
+            this.currentBinlogFilename = binlogFilename;
+            this.restartBinlogFilename = binlogFilename;
+        }
+        assert positionOfFirstEvent >= 0;
+        this.currentBinlogPosition = positionOfFirstEvent;
+        this.restartBinlogPosition = positionOfFirstEvent;
+        this.currentRowNumber = 0;
+        this.restartRowsToSkip = 0;
+    }
+
+    /**
+     * Set the position within the MySQL binary log file of the <em>current event</em>.
+     * 
+     * @param positionOfCurrentEvent the position within the binary log file of the current event
+     * @param eventSizeInBytes the size in bytes of this event
+     */
+    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
+        this.currentBinlogPosition = positionOfCurrentEvent;
+        this.currentEventLengthInBytes = eventSizeInBytes;
+        if (!inTransaction) {
+            this.restartBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
+        }
+        // Don't set anything else, since the row numbers are set in the offset(int,int) method called at least once
+        // for each processed event
+    }
+
     /**
      * Get the Kafka Connect detail about the source ""offset"", which describes the position within the source where we last
      * have last read.
      * 
      * @return a copy of the current offset; never null
      */
     public Map<String, ?> offset() {
-        return offsetUsingPosition(nextBinlogPosition);
+        return offsetUsingPosition(this.restartRowsToSkip);
     }
 
     /**
-     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
-     * describes the position within the source where we have last read.
+     * Given the row number within a binlog event and the total number of rows in that event, compute and return the
+     * Kafka Connect offset that is be included in the produced change event describing the row.
      * <p>
      * This method should always be called before {@link #struct()}.
      * 
-     * @param eventRowNumber the 0-based row number within the event being processed
+     * @param eventRowNumber the 0-based row number within the event for which the offset is to be produced
      * @param totalNumberOfRows the total number of rows within the event being processed
      * @return a copy of the current offset; never null
+     * @see #struct()
      */
     public Map<String, ?> offsetForRow(int eventRowNumber, int totalNumberOfRows) {
         if (eventRowNumber < (totalNumberOfRows - 1)) {
             // This is not the last row, so our offset should record the next row to be used ...
-            this.lastEventRowNumber = eventRowNumber;
-            this.nextEventRowNumber = eventRowNumber + 1;
+            this.currentRowNumber = eventRowNumber;
+            this.restartRowsToSkip = this.currentRowNumber + 1;
             // so write out the offset with the position of this event
-            return offsetUsingPosition(lastBinlogPosition);
+            return offsetUsingPosition(this.restartRowsToSkip);
         }
         // This is the last row, so write out the offset that has the position of the next event ...
-        this.lastEventRowNumber = this.nextEventRowNumber;
-        this.nextEventRowNumber = 0;
-        return offsetUsingPosition(nextBinlogPosition);
+        this.currentRowNumber = eventRowNumber;
+        this.restartRowsToSkip = 0;
+        return offsetUsingPosition(totalNumberOfRows);
     }
 
-    private Map<String, ?> offsetUsingPosition(long binlogPosition) {
+    private Map<String, ?> offsetUsingPosition(long rowsToSkip) {
         Map<String, Object> map = new HashMap<>();
         if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
-        if (gtidSet != null) {
-            map.put(GTID_SET_KEY, gtidSet);
+        if (restartGtidSet != null) {
+            // Put the previously-completed GTID set in the offset along with the event number ...
+            map.put(GTID_SET_KEY, restartGtidSet);
+        }
+        map.put(BINLOG_FILENAME_OFFSET_KEY, restartBinlogFilename);
+        map.put(BINLOG_POSITION_OFFSET_KEY, restartBinlogPosition);
+        if (restartEventsToSkip != 0) {
+            map.put(EVENTS_TO_SKIP_OFFSET_KEY, restartEventsToSkip);
         }
-        map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, nextEventRowNumber);
+        if (rowsToSkip != 0) {
+            map.put(BINLOG_ROW_IN_EVENT_OFFSET_KEY, rowsToSkip);
+        }
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             map.put(SNAPSHOT_KEY, true);
         }
@@ -223,13 +277,13 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        // Don't put the GTID Set into the struct; only the current GTID is fine ...
-        if (binlogGtid != null) {
-            result.put(GTID_KEY, binlogGtid);
+        if (currentGtid != null) {
+            // Don't put the GTID Set into the struct; only the current GTID is fine ...
+            result.put(GTID_KEY, currentGtid);
         }
-        result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        result.put(BINLOG_POSITION_OFFSET_KEY, lastBinlogPosition);
-        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, lastEventRowNumber);
+        result.put(BINLOG_FILENAME_OFFSET_KEY, currentBinlogFilename);
+        result.put(BINLOG_POSITION_OFFSET_KEY, currentBinlogPosition);
+        result.put(BINLOG_ROW_IN_EVENT_OFFSET_KEY, currentRowNumber);
         result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (lastSnapshot) {
             result.put(SNAPSHOT_KEY, true);
@@ -246,54 +300,73 @@ public boolean isSnapshotInEffect() {
         return nextSnapshot;
     }
 
+    public void startNextTransaction() {
+        // If we have to restart, then we'll start with this BEGIN transaction
+        this.restartRowsToSkip = 0;
+        this.restartEventsToSkip = 0;
+        this.restartBinlogFilename = this.currentBinlogFilename;
+        this.restartBinlogPosition = this.currentBinlogPosition;
+        this.inTransaction = true;
+    }
+
     /**
-     * Set the latest GTID from the MySQL binary log file.
-     * 
-     * @param gtid the string representation of a specific GTID; may not be null
+     * Capture that we're starting a new event.
      */
-    public void setGtid(String gtid) {
-        this.binlogGtid = gtid;
+    public void completeEvent() {
+        ++restartEventsToSkip;
     }
 
     /**
-     * Set the set of GTIDs known to the MySQL server.
+     * Get the number of events after the last transaction BEGIN that we've already processed.
      * 
-     * @param gtidSet the string representation of GTID set; may not be null
+     * @return the number of events in the transaction that have been processed completely
+     * @see #completeEvent()
+     * @see #startNextTransaction()
      */
-    public void setGtidSet(String gtidSet) {
-        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
-            this.gtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """"); // remove all of the newline chars if they exist
-        }
+    public long eventsToSkipUponRestart() {
+        return restartEventsToSkip;
+    }
+
+    public void commitTransaction() {
+        this.restartGtidSet = this.currentGtidSet;
+        this.restartBinlogFilename = this.currentBinlogFilename;
+        this.restartBinlogPosition = this.currentBinlogPosition + this.currentEventLengthInBytes;
+        this.restartRowsToSkip = 0;
+        this.restartEventsToSkip = 0;
+        this.inTransaction = false;
     }
 
     /**
-     * Set the name of the MySQL binary log file.
+     * Record that a new GTID transaction has been started and has been included in the set of GTIDs known to the MySQL server.
      * 
-     * @param binlogFilename the name of the binary log file; may not be null
-     * @param positionOfFirstEvent the position in the binary log file to begin processing
+     * @param gtid the string representation of a specific GTID that has been begun; may not be null
+     * @param gtidSet the string representation of GTID set that includes the newly begun GTID; may not be null
      */
-    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
-        if (binlogFilename != null) {
-            this.binlogFilename = binlogFilename;
+    public void startGtid(String gtid, String gtidSet) {
+        this.currentGtid = gtid;
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            // Remove all the newline chars that exist in the GTID set string ...
+            String trimmedGtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """");
+            // Set the GTID set that we'll use if restarting BEFORE successful completion of the events in this GTID ...
+            this.restartGtidSet = this.currentGtidSet != null ? this.currentGtidSet : trimmedGtidSet;
+            // Record the GTID set that includes the current transaction ...
+            this.currentGtidSet = trimmedGtidSet;
         }
-        assert positionOfFirstEvent >= 0;
-        this.nextBinlogPosition = positionOfFirstEvent;
-        this.lastBinlogPosition = this.nextBinlogPosition;
-        this.nextEventRowNumber = 0;
-        this.lastEventRowNumber = 0;
     }
 
     /**
-     * Set the position within the MySQL binary log file of the <em>current event</em>.
+     * Set the GTID set that captures all of the GTID transactions that have been completely processed.
      * 
-     * @param positionOfCurrentEvent the position within the binary log file of the current event
-     * @param eventSizeInBytes the size in bytes of this event
+     * @param gtidSet the string representation of the GTID set; may not be null, but may be an empty string if no GTIDs
+     *            have been previously processed
      */
-    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
-        this.lastBinlogPosition = positionOfCurrentEvent;
-        this.nextBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
-        // Don't set anything else, since the row numbers are set in the offset(int,int) method called at least once
-        // for each processed event
+    public void setCompletedGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            // Remove all the newline chars that exist in the GTID set string ...
+            String trimmedGtidSet = gtidSet.replaceAll(""\n"", """").replaceAll(""\r"", """");
+            this.currentGtidSet = trimmedGtidSet;
+            this.restartGtidSet = trimmedGtidSet;
+        }
     }
 
     /**
@@ -350,23 +423,23 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
-            binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
+            setCompletedGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
+            restartEventsToSkip = longOffsetValue(sourceOffset, EVENTS_TO_SKIP_OFFSET_KEY);
+            String binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
             }
-            nextBinlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
-            nextEventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
-            lastBinlogPosition = nextBinlogPosition;
-            lastEventRowNumber = nextEventRowNumber;
+            long binlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
+            setBinlogStartPoint(binlogFilename, binlogPosition);
+            this.restartRowsToSkip = (int) longOffsetValue(sourceOffset, BINLOG_ROW_IN_EVENT_OFFSET_KEY);
             nextSnapshot = booleanOffsetValue(sourceOffset, SNAPSHOT_KEY);
             lastSnapshot = nextSnapshot;
         }
     }
 
     private long longOffsetValue(Map<String, ?> values, String key) {
         Object obj = values.get(key);
-        if (obj == null) return 0;
+        if (obj == null) return 0L;
         if (obj instanceof Number) return ((Number) obj).longValue();
         try {
             return Long.parseLong(obj.toString());
@@ -388,56 +461,44 @@ private boolean booleanOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.gtidSet != null ? this.gtidSet : null;
+        return this.currentGtidSet != null ? this.currentGtidSet : null;
     }
 
     /**
-     * Get the name of the MySQL binary log file that has been processed.
+     * Get the name of the MySQL binary log file that has last been processed.
      * 
      * @return the name of the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
     public String binlogFilename() {
-        return binlogFilename;
+        return restartBinlogFilename;
     }
 
     /**
      * Get the position within the MySQL binary log file of the next event to be processed.
      * 
      * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
-    public long nextBinlogPosition() {
-        return nextBinlogPosition;
+    public long binlogPosition() {
+        return restartBinlogPosition;
     }
 
     /**
      * Get the position within the MySQL binary log file of the most recently processed event.
      * 
      * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
-    public long lastBinlogPosition() {
-        return lastBinlogPosition;
+    protected long restartBinlogPosition() {
+        return restartBinlogPosition;
     }
 
     /**
-     * Get the next row within the event at the {@link #nextBinlogPosition() position} within the {@link #binlogFilename() binary
-     * log file}
-     * .
+     * Get the number of rows beyond the {@link #eventsToSkipUponRestart() last completely processed event} to be skipped
+     * upon restart.
      * 
-     * @return the 0-based row number
+     * @return the number of rows to be skipped
      */
-    public int nextEventRowNumber() {
-        return nextEventRowNumber;
-    }
-
-    /**
-     * Get the previous row within the event at the {@link #lastBinlogPosition() position} within the {@link #binlogFilename()
-     * binary log file}
-     * .
-     * 
-     * @return the 0-based row number
-     */
-    public int lastEventRowNumber() {
-        return lastEventRowNumber;
+    public int rowsToSkipUponRestart() {
+        return restartRowsToSkip;
     }
 
     /**
@@ -452,22 +513,26 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (gtidSet != null) {
+        if (currentGtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(gtidSet);
-            sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
-            sb.append("", pos="").append(nextBinlogPosition());
-            sb.append("", row="").append(nextEventRowNumber());
+            sb.append(currentGtidSet);
+            sb.append("" and binlog file '"").append(restartBinlogFilename).append(""'"");
+            sb.append("", pos="").append(restartBinlogPosition);
+            sb.append("", skipping "").append(restartEventsToSkip);
+            sb.append("" events plus "").append(restartRowsToSkip);
+            sb.append("" rows"");
         } else {
-            if (binlogFilename == null) {
+            if (restartBinlogFilename == null) {
                 sb.append(""<latest>"");
             } else {
-                if ("""".equals(binlogFilename)) {
+                if ("""".equals(restartBinlogFilename)) {
                     sb.append(""earliest binlog file and position"");
                 } else {
-                    sb.append(""binlog file '"").append(binlogFilename).append(""'"");
-                    sb.append("", pos="").append(nextBinlogPosition());
-                    sb.append("", row="").append(nextEventRowNumber());
+                    sb.append(""binlog file '"").append(restartBinlogFilename).append(""'"");
+                    sb.append("", pos="").append(restartBinlogPosition);
+                    sb.append("", skipping "").append(restartEventsToSkip);
+                    sb.append("" events plus "").append(restartRowsToSkip);
+                    sb.append("" rows"");
                 }
             }
         }
@@ -505,7 +570,14 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
-                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired ...
+                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired GTID.
+                    // Now we need to compare how many events in that transaction we've already completed ...
+                    int recordedEventCount = recorded.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+                    int desiredEventCount = desired.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+                    int diff = recordedEventCount - desiredEventCount;
+                    if (diff > 0) return false;
+
+                    // Otherwise the recorded is definitely before or at the desired ...
                     return true;
                 }
                 // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
@@ -543,16 +615,25 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
         if (diff > 0) return false;
+        if (diff < 0) return true;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
         if (diff > 0) return false;
+        if (diff < 0) return true;
+
+        // The positions are the same, so compare the completed events in the transaction ...
+        int recordedEventCount = recorded.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+        int desiredEventCount = desired.getInteger(EVENTS_TO_SKIP_OFFSET_KEY, 0);
+        diff = recordedEventCount - desiredEventCount;
+        if (diff > 0) return false;
+        if (diff < 0) return true;
 
-        // The positions are the same, so compare the row number ...
-        int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
-        int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        // The completed events are the same, so compare the row number ...
+        int recordedRow = recorded.getInteger(BINLOG_ROW_IN_EVENT_OFFSET_KEY, -1);
+        int desiredRow = desired.getInteger(BINLOG_ROW_IN_EVENT_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
         if (diff > 0) return false;
 ",2016-11-09T14:11:41Z,70
"@@ -8,8 +8,10 @@
 import static org.junit.Assert.fail;
 
 import java.nio.file.Path;
+import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.util.List;
+import java.util.Map;
 import java.util.concurrent.TimeUnit;
 
 import org.apache.kafka.common.config.Config;
@@ -28,6 +30,7 @@
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
 import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.embedded.EmbeddedEngine.CompletionResult;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.relational.history.KafkaDatabaseHistory;
@@ -274,11 +277,12 @@ public void shouldValidateAcceptableConfiguration() {
     public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLException, InterruptedException {
         String masterPort = System.getProperty(""database.port"");
         String replicaPort = System.getProperty(""database.replica.port"");
-        if ( !masterPort.equals(replicaPort)) {
+        boolean replicaIsMaster = masterPort.equals(replicaPort);
+        if (!replicaIsMaster) {
             // Give time for the replica to catch up to the master ...
             Thread.sleep(5000L);
         }
-        
+
         // Use the DB configuration to define the connector's configuration to use the ""replica""
         // which may be the same as the ""master"" ...
         config = Configuration.create()
@@ -352,7 +356,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
             }
         }
 
-        //Testing.Print.enable();
+        // Testing.Print.enable();
 
         // Restart the connector and read the insert record ...
         Testing.print(""*** Restarting connector after inserts were made"");
@@ -388,7 +392,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         inserts = records.recordsForTopic(""myServer.connector_test.products"");
         assertInsert(inserts.get(0), ""id"", 1001);
 
-        Testing.print(""*** Done with simple insert"");
+        // Testing.print(""*** Done with simple insert"");
 
         // ---------------------------------------------------------------------------------------------------------------
         // Changing the primary key of a row should result in 3 events: INSERT, DELETE, and TOMBSTONE
@@ -433,13 +437,15 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
 
         Testing.print(""*** Done with simple update"");
 
+        //Testing.Print.enable();
+
         // ---------------------------------------------------------------------------------------------------------------
         // Change our schema with a fully-qualified name; we should still see this event
         // ---------------------------------------------------------------------------------------------------------------
         // Add a column with default to the 'products' table and explicitly update one record ...
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
-                connection.execute(""ALTER TABLE connector_test.products ADD COLUMN volume FLOAT NOT NULL, ADD COLUMN alias VARCHAR(30) NOT NULL AFTER description"");
+                connection.execute(""ALTER TABLE connector_test.products ADD COLUMN volume FLOAT, ADD COLUMN alias VARCHAR(30) NULL AFTER description"");
                 connection.execute(""UPDATE products SET volume=13.5 WHERE id=2001"");
                 connection.query(""SELECT * FROM products"", rs -> {
                     if (Testing.Print.isEnabled()) connection.print(rs);
@@ -508,6 +514,173 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // Stop the connector ...
         // ---------------------------------------------------------------------------------------------------------------
         stopConnector();
+
+        // ---------------------------------------------------------------------------------------------------------------
+        // Restart the connector to read only part of a transaction ...
+        // ---------------------------------------------------------------------------------------------------------------
+        Testing.print(""*** Restarting connector"");
+        CompletionResult completion = new CompletionResult();
+        start(MySqlConnector.class, config, completion, (record) -> {
+            // We want to stop before processing record 3003 ...
+            Struct key = (Struct) record.key();
+            Number id = (Number) key.get(""id"");
+            if (id.intValue() == 3003) {
+                return true;
+            }
+            return false;
+        });
+
+        BinlogPosition positionBeforeInserts = new BinlogPosition();
+        BinlogPosition positionAfterInserts = new BinlogPosition();
+        BinlogPosition positionAfterUpdate = new BinlogPosition();
+        try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
+            try (JdbcConnection connection = db.connect()) {
+                connection.query(""SHOW MASTER STATUS"", positionBeforeInserts::readFromDatabase);
+                connection.execute(""INSERT INTO products(id,name,description,weight,volume,alias) VALUES ""
+                        + ""(3001,'ashley','super robot',34.56,0.00,'ashbot'), ""
+                        + ""(3002,'arthur','motorcycle',87.65,0.00,'arcycle'), ""
+                        + ""(3003,'oak','tree',987.65,0.00,'oak');"");
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
+                connection.query(""SHOW MASTER STATUS"", positionAfterInserts::readFromDatabase);
+                // Change something else that is unrelated ...
+                connection.execute(""UPDATE products_on_hand SET quantity=40 WHERE product_id=109"");
+                connection.query(""SELECT * FROM products_on_hand"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
+                connection.query(""SHOW MASTER STATUS"", positionAfterUpdate::readFromDatabase);
+            }
+        }
+
+        //Testing.Print.enable();
+
+        // And consume the one insert ...
+        records = consumeRecordsByTopic(2);
+        assertThat(records.recordsForTopic(""myServer.connector_test.products"").size()).isEqualTo(2);
+        assertThat(records.topics().size()).isEqualTo(1);
+        inserts = records.recordsForTopic(""myServer.connector_test.products"");
+        assertInsert(inserts.get(0), ""id"", 3001);
+        assertInsert(inserts.get(1), ""id"", 3002);
+
+        // Verify that the connector has stopped ...
+        completion.await(10, TimeUnit.SECONDS);
+        assertThat(completion.hasCompleted()).isTrue();
+        assertThat(completion.hasError()).isTrue();
+        assertThat(completion.success()).isFalse();
+        assertNoRecordsToConsume();
+        assertConnectorNotRunning();
+
+        // ---------------------------------------------------------------------------------------------------------------
+        // Stop the connector ...
+        // ---------------------------------------------------------------------------------------------------------------
+        stopConnector();
+
+        // Read the last committed offsets, and verify the binlog coordinates ...
+        SourceInfo persistedOffsetSource = new SourceInfo();
+        persistedOffsetSource.setServerName(config.getString(MySqlConnectorConfig.SERVER_NAME));
+        Map<String, ?> lastCommittedOffset = readLastCommittedOffset(config, persistedOffsetSource.partition());
+        persistedOffsetSource.setOffset(lastCommittedOffset);
+        Testing.print(""Position before inserts: "" + positionBeforeInserts);
+        Testing.print(""Position after inserts:  "" + positionAfterInserts);
+        Testing.print(""Offset: "" + lastCommittedOffset);
+        Testing.print(""Position after update:  "" + positionAfterUpdate);
+        if (replicaIsMaster) {
+            // Same binlog filename ...
+            assertThat(persistedOffsetSource.binlogFilename()).isEqualTo(positionBeforeInserts.binlogFilename());
+            assertThat(persistedOffsetSource.binlogFilename()).isEqualTo(positionAfterInserts.binlogFilename());
+            // Binlog position in offset should be more than before the inserts, but less than the position after the inserts ...
+            assertThat(persistedOffsetSource.binlogPosition()).isGreaterThan(positionBeforeInserts.binlogPosition());
+            assertThat(persistedOffsetSource.binlogPosition()).isLessThan(positionAfterInserts.binlogPosition());
+        } else {
+            // the replica is not the same server as the master, so it will have a different binlog filename and position ...
+        }
+        // Event number is 2 ...
+        assertThat(persistedOffsetSource.eventsToSkipUponRestart()).isEqualTo(2);
+        // GTID set should match the before-inserts GTID set ...
+        // assertThat(persistedOffsetSource.gtidSet()).isEqualTo(positionBeforeInserts.gtidSet());
+
+        Testing.print(""*** Restarting connector, and should begin with inserting 3003 (not 109!)"");
+        start(MySqlConnector.class, config);
+
+        // And consume the insert for 3003 ...
+        records = consumeRecordsByTopic(1);
+        assertThat(records.topics().size()).isEqualTo(1);
+        inserts = records.recordsForTopic(""myServer.connector_test.products"");
+        if (inserts == null) {
+            updates = records.recordsForTopic(""myServer.connector_test.products_on_hand"");
+            if (updates != null) {
+                fail(""Restarted connector and missed the insert of product id=3003!"");
+            }
+        }
+        // Read the first record produced since we've restarted
+        SourceRecord prod3003 = inserts.get(0);
+        assertInsert(prod3003, ""id"", 3003);
+        
+        // Check that the offset has the correct/expected values ...
+        assertOffset(prod3003,""file"",lastCommittedOffset.get(""file""));
+        assertOffset(prod3003,""pos"",lastCommittedOffset.get(""pos""));
+        assertOffset(prod3003,""row"",3);
+        assertOffset(prod3003,""event"",lastCommittedOffset.get(""event""));
+
+        // Check that the record has all of the column values ...
+        assertValueField(prod3003,""after/id"",3003);
+        assertValueField(prod3003,""after/name"",""oak"");
+        assertValueField(prod3003,""after/description"",""tree"");
+        assertValueField(prod3003,""after/weight"",987.65d);
+        assertValueField(prod3003,""after/volume"",0.0d);
+        assertValueField(prod3003,""after/alias"",""oak"");
+        
+
+        // And make sure we consume that one extra update ...
+        records = consumeRecordsByTopic(1);
+        assertThat(records.topics().size()).isEqualTo(1);
+        updates = records.recordsForTopic(""myServer.connector_test.products_on_hand"");
+        assertThat(updates.size()).isEqualTo(1);
+        assertUpdate(updates.get(0), ""product_id"", 109);
+        updates.forEach(this::validate);
+
+        // Start the connector again, and we should see the next two
+        Testing.print(""*** Done with simple insert"");
+
+    }
+
+    protected static class BinlogPosition {
+        private String binlogFilename;
+        private long binlogPosition;
+        private String gtidSet;
+
+        public void readFromDatabase(ResultSet rs) throws SQLException {
+            if (rs.next()) {
+                binlogFilename = rs.getString(1);
+                binlogPosition = rs.getLong(2);
+                if (rs.getMetaData().getColumnCount() > 4) {
+                    // This column exists only in MySQL 5.6.5 or later ...
+                    gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                }
+            }
+        }
+
+        public String binlogFilename() {
+            return binlogFilename;
+        }
+
+        public long binlogPosition() {
+            return binlogPosition;
+        }
+
+        public String gtidSet() {
+            return gtidSet;
+        }
+
+        public boolean hasGtids() {
+            return gtidSet != null;
+        }
+
+        @Override
+        public String toString() {
+            return ""file="" + binlogFilename + "", pos="" + binlogPosition + "", gtids="" + (gtidSet != null ? gtidSet : """");
+        }
     }
 
     @Test",2016-11-09T14:11:41Z,71
"@@ -215,7 +215,7 @@ public void shouldFilterAndMergeGtidSet() throws Exception {
                                .build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setGtidSet(gtidStr);
+        context.source().setCompletedGtidSet(gtidStr);
 
         GtidSet mergedGtidSet = context.filterGtidSet(new GtidSet(availableServerGtidStr));
         assertThat(mergedGtidSet).isNotNull();",2016-11-09T14:11:41Z,72
"@@ -31,31 +31,34 @@ public class SourceInfoTest {
     private static final String SERVER_NAME = ""my-server""; // can technically be any string
 
     private SourceInfo source;
+    private boolean inTxn = false;
+    private long positionOfBeginEvent = 0L;
+    private int eventNumberInTxn = 0;
 
     @Before
     public void beforeEach() {
         source = new SourceInfo();
+        inTxn = false;
+        positionOfBeginEvent = 0L;
+        eventNumberInTxn = 0;
     }
 
     @Test
     public void shouldStartSourceInfoFromZeroBinlogCoordinates() {
         source.setBinlogStartPoint(FILENAME, 0);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.eventsToSkipUponRestart()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
     @Test
     public void shouldStartSourceInfoFromNonZeroBinlogCoordinates() {
         source.setBinlogStartPoint(FILENAME, 100);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -68,10 +71,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinates() {
         sourceWith(offset(0, 0));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -80,10 +81,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinates() {
         sourceWith(offset(100, 0));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -92,10 +91,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndNonZero
         sourceWith(offset(0, 5));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -104,10 +101,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndNonZ
         sourceWith(offset(100, 5));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -116,10 +111,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndSnapsho
         sourceWith(offset(0, 0, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -128,10 +121,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndSnap
         sourceWith(offset(100, 0, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -140,10 +131,8 @@ public void shouldRecoverSourceInfoFromOffsetWithZeroBinlogCoordinatesAndNonZero
         sourceWith(offset(0, 5, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -152,10 +141,8 @@ public void shouldRecoverSourceInfoFromOffsetWithNonZeroBinlogCoordinatesAndNonZ
         sourceWith(offset(100, 5, true));
         assertThat(source.gtidSet()).isNull();
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -164,10 +151,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 0, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -176,10 +161,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 5, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -188,10 +171,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 0, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -200,10 +181,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 5, false));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isFalse();
     }
 
@@ -212,10 +191,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 0, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -224,10 +201,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndZeroBinlogCoor
         sourceWith(offset(GTID_SET, 0, 5, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(0);
-        assertThat(source.lastBinlogPosition()).isEqualTo(0);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(0);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -236,10 +211,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 0, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(0);
-        assertThat(source.lastEventRowNumber()).isEqualTo(0);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -248,10 +221,8 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
         sourceWith(offset(GTID_SET, 100, 5, true));
         assertThat(source.gtidSet()).isEqualTo(GTID_SET);
         assertThat(source.binlogFilename()).isEqualTo(FILENAME);
-        assertThat(source.nextBinlogPosition()).isEqualTo(100);
-        assertThat(source.lastBinlogPosition()).isEqualTo(100);
-        assertThat(source.nextEventRowNumber()).isEqualTo(5);
-        assertThat(source.lastEventRowNumber()).isEqualTo(5);
+        assertThat(source.binlogPosition()).isEqualTo(100);
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(5);
         assertThat(source.isSnapshotInEffect()).isTrue();
     }
 
@@ -262,19 +233,89 @@ public void shouldStartSourceInfoFromBinlogCoordinatesWithGtidsAndNonZeroBinlogC
     @Test
     public void shouldAdvanceSourceInfoFromNonZeroPositionAndRowZeroForEventsWithOneRow() {
         sourceWith(offset(100, 0));
+
+        // Try a transactions with just one event ...
+        handleTransactionBegin(150, 2);
         handleNextEvent(200, 10, withRowCount(1));
+        handleTransactionCommit(210, 2);
+
+        handleTransactionBegin(210, 2);
         handleNextEvent(220, 10, withRowCount(1));
+        handleTransactionCommit(230, 3);
+
+        handleTransactionBegin(240, 2);
         handleNextEvent(250, 50, withRowCount(1));
+        handleTransactionCommit(300, 4);
+
+        // Try a transactions with multiple events ...
+        handleTransactionBegin(340, 2);
+        handleNextEvent(350, 20, withRowCount(1));
+        handleNextEvent(370, 30, withRowCount(1));
+        handleNextEvent(400, 40, withRowCount(1));
+        handleTransactionCommit(440, 4);
+
+        handleTransactionBegin(500, 2);
+        handleNextEvent(510, 20, withRowCount(1));
+        handleNextEvent(540, 15, withRowCount(1));
+        handleNextEvent(560, 10, withRowCount(1));
+        handleTransactionCommit(580, 4);
+
+        // Try another single event transaction ...
+        handleTransactionBegin(600, 2);
+        handleNextEvent(610, 50, withRowCount(1));
+        handleTransactionCommit(660, 4);
+
+        // Try event outside of a transaction ...
+        handleNextEvent(670, 10, withRowCount(1));
+
+        // Try another single event transaction ...
+        handleTransactionBegin(700, 2);
+        handleNextEvent(710, 50, withRowCount(1));
+        handleTransactionCommit(760, 4);
     }
 
     @Test
     public void shouldAdvanceSourceInfoFromNonZeroPositionAndRowZeroForEventsWithMultipleRow() {
         sourceWith(offset(100, 0));
+
+        // Try a transactions with just one event ...
+        handleTransactionBegin(150, 2);
         handleNextEvent(200, 10, withRowCount(3));
+        handleTransactionCommit(210, 2);
+
+        handleTransactionBegin(210, 2);
         handleNextEvent(220, 10, withRowCount(4));
-        handleNextEvent(250, 50, withRowCount(6));
-        handleNextEvent(300, 20, withRowCount(1));
-        handleNextEvent(350, 20, withRowCount(3));
+        handleTransactionCommit(230, 3);
+
+        handleTransactionBegin(240, 2);
+        handleNextEvent(250, 50, withRowCount(5));
+        handleTransactionCommit(300, 4);
+
+        // Try a transactions with multiple events ...
+        handleTransactionBegin(340, 2);
+        handleNextEvent(350, 20, withRowCount(6));
+        handleNextEvent(370, 30, withRowCount(1));
+        handleNextEvent(400, 40, withRowCount(3));
+        handleTransactionCommit(440, 4);
+
+        handleTransactionBegin(500, 2);
+        handleNextEvent(510, 20, withRowCount(8));
+        handleNextEvent(540, 15, withRowCount(9));
+        handleNextEvent(560, 10, withRowCount(1));
+        handleTransactionCommit(580, 4);
+
+        // Try another single event transaction ...
+        handleTransactionBegin(600, 2);
+        handleNextEvent(610, 50, withRowCount(1));
+        handleTransactionCommit(660, 4);
+
+        // Try event outside of a transaction ...
+        handleNextEvent(670, 10, withRowCount(5));
+
+        // Try another single event transaction ...
+        handleTransactionBegin(700, 2);
+        handleNextEvent(710, 50, withRowCount(3));
+        handleTransactionCommit(760, 4);
     }
 
     // -------------------------------------------------------------------------------------
@@ -285,33 +326,78 @@ protected int withRowCount(int rowCount) {
         return rowCount;
     }
 
+    protected void handleTransactionBegin(long positionOfEvent, int eventSize) {
+        source.setEventPosition(positionOfEvent, eventSize);
+        positionOfBeginEvent = positionOfEvent;
+        source.startNextTransaction();
+        inTxn = true;
+
+        assertThat(source.rowsToSkipUponRestart()).isEqualTo(0);
+    }
+
+    protected void handleTransactionCommit(long positionOfEvent, int eventSize) {
+        source.setEventPosition(positionOfEvent, eventSize);
+        source.commitTransaction();
+        eventNumberInTxn = 0;
+        inTxn = false;
+
+        // Verify the offset ...
+        Map<String, ?> offset = source.offset();
+
+        // The offset position should be the position of the next event
+        long position = (Long) offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY);
+        assertThat(position).isEqualTo(positionOfEvent + eventSize);
+        Long rowsToSkip = (Long) offset.get(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY);
+        if (rowsToSkip == null) rowsToSkip = 0L;
+        assertThat(rowsToSkip).isEqualTo(0);
+        assertThat(offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY)).isNull();
+        if (source.gtidSet() != null) {
+            assertThat(offset.get(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
+        }
+    }
+
     protected void handleNextEvent(long positionOfEvent, long eventSize, int rowCount) {
+        if (inTxn) ++eventNumberInTxn;
         source.setEventPosition(positionOfEvent, eventSize);
-        for (int i = 0; i != rowCount; ++i) {
+        for (int row = 0; row != rowCount; ++row) {
             // Get the offset for this row (always first!) ...
-            Map<String, ?> offset = source.offsetForRow(i, rowCount);
-            if ((i + 1) < rowCount) {
-                // This is not the last row, so the next binlog position should be for next row in this event ...
-                assertThat(offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent);
-                assertThat(offset.get(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(i+1);
-            } else {
-                // This is the last row, so the next binlog position should be for first row in next event ...
-                assertThat(offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent + eventSize);
-                assertThat(offset.get(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(0);
-            }
+            Map<String, ?> offset = source.offsetForRow(row, rowCount);
             assertThat(offset.get(SourceInfo.BINLOG_FILENAME_OFFSET_KEY)).isEqualTo(FILENAME);
-            if ( source.gtidSet() != null ) {
+            if (source.gtidSet() != null) {
                 assertThat(offset.get(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
             }
+            long position = (Long) offset.get(SourceInfo.BINLOG_POSITION_OFFSET_KEY);
+            if (inTxn) {
+                // regardless of the row count, the position is always the txn begin position ...
+                assertThat(position).isEqualTo(positionOfBeginEvent);
+                // and the number of the last completed event (the previous one) ...
+                Long eventsToSkip = (Long) offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY);
+                if (eventsToSkip == null) eventsToSkip = 0L;
+                assertThat(eventsToSkip).isEqualTo(eventNumberInTxn - 1);
+            } else {
+                // Matches the next event ...
+                assertThat(position).isEqualTo(positionOfEvent + eventSize);
+                assertThat(offset.get(SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY)).isNull();
+            }
+            Long rowsToSkip = (Long) offset.get(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY);
+            if (rowsToSkip == null) rowsToSkip = 0L;
+            if( (row+1) == rowCount) {
+                // This is the last row, so the next binlog position should be the number of rows in the event ...
+                assertThat(rowsToSkip).isEqualTo(rowCount);
+            } else {
+                // This is not the last row, so the next binlog position should be the row number ...
+                assertThat(rowsToSkip).isEqualTo(row+1);
+            }
             // Get the source struct for this row (always second), which should always reflect this row in this event ...
             Struct recordSource = source.struct();
             assertThat(recordSource.getInt64(SourceInfo.BINLOG_POSITION_OFFSET_KEY)).isEqualTo(positionOfEvent);
-            assertThat(recordSource.getInt32(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY)).isEqualTo(i);
+            assertThat(recordSource.getInt32(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY)).isEqualTo(row);
             assertThat(recordSource.getString(SourceInfo.BINLOG_FILENAME_OFFSET_KEY)).isEqualTo(FILENAME);
-            if ( source.gtidSet() != null ) {
+            if (source.gtidSet() != null) {
                 assertThat(recordSource.getString(SourceInfo.GTID_SET_KEY)).isEqualTo(source.gtidSet());
             }
         }
+        source.completeEvent();
     }
 
     protected Map<String, String> offset(long position, int row) {
@@ -326,7 +412,7 @@ protected Map<String, String> offset(String gtidSet, long position, int row, boo
         Map<String, String> offset = new HashMap<>();
         offset.put(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, FILENAME);
         offset.put(SourceInfo.BINLOG_POSITION_OFFSET_KEY, Long.toString(position));
-        offset.put(SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, Integer.toString(row));
+        offset.put(SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, Integer.toString(row));
         if (gtidSet != null) offset.put(SourceInfo.GTID_SET_KEY, gtidSet);
         if (snapshot) offset.put(SourceInfo.SNAPSHOT_KEY, Boolean.TRUE.toString());
         return offset;
@@ -393,12 +479,40 @@ public void shouldOrderPositionsWithSameServerButLowerUpperLimitAsBeforePosition
 
     @Test
     public void shouldOrderPositionWithoutGtidAsBeforePositionWithGtid() {
-        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0).isBefore(positionWithGtids(""IdA:1-5""));
+        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0, 0).isBefore(positionWithGtids(""IdA:1-5""));
     }
 
     @Test
     public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
-        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
+        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0, 0));
+    }
+
+    @Test
+    public void shouldComparePositionsWithoutGtids() {
+        // Same position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isAt(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAt(positionWithoutGtids(""fn.01"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.03"", 1, 0, 1).isAt(positionWithoutGtids(""fn.03"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isAt(positionWithoutGtids(""fn.01"", 1, 1, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAt(positionWithoutGtids(""fn.01"", 1, 1, 1));
+        assertPositionWithoutGtids(""fn.03"", 1, 1, 1).isAt(positionWithoutGtids(""fn.03"", 1, 1, 1));
+
+        // Before position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 0, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 0).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isBefore(positionWithoutGtids(""fn.01"", 1, 0, 2));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 1, 1));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 0).isBefore(positionWithoutGtids(""fn.01"", 1, 2, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isBefore(positionWithoutGtids(""fn.01"", 1, 2, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isBefore(positionWithoutGtids(""fn.01"", 2, 0, 0));
+
+        // After position ...
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAfter(positionWithoutGtids(""fn.01"", 0, 0, 99));
+        assertPositionWithoutGtids(""fn.01"", 1, 0, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 0, 0, 99));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 0, 0));
+        assertPositionWithoutGtids(""fn.01"", 1, 1, 1).isAfter(positionWithoutGtids(""fn.01"", 1, 1, 0));
     }
 
     @FixFor(""DBZ-107"")
@@ -410,21 +524,21 @@ public void shouldRemoveNewlinesFromGtidSet() {
         String gtidCleaned = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,"" +
                 ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3149,"" +
                 ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:1-39"";
-        source.setGtidSet(gtidExecuted);
+        source.setCompletedGtidSet(gtidExecuted);
         assertThat(source.gtidSet()).isEqualTo(gtidCleaned);
     }
 
     @FixFor(""DBZ-107"")
     @Test
     public void shouldNotSetBlankGtidSet() {
-        source.setGtidSet("""");
+        source.setCompletedGtidSet("""");
         assertThat(source.gtidSet()).isNull();
     }
 
     @FixFor(""DBZ-107"")
     @Test
     public void shouldNotSetNullGtidSet() {
-        source.setGtidSet(null);
+        source.setCompletedGtidSet(null);
         assertThat(source.gtidSet()).isNull();
     }
 
@@ -439,20 +553,22 @@ protected Document positionWithGtids(String gtids, boolean snapshot) {
         return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
-    protected Document positionWithoutGtids(String filename, int position, int row) {
-        return positionWithoutGtids(filename, position, row, false);
+    protected Document positionWithoutGtids(String filename, int position, int event, int row) {
+        return positionWithoutGtids(filename, position, event, row, false);
     }
 
-    protected Document positionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+    protected Document positionWithoutGtids(String filename, int position, int event, int row, boolean snapshot) {
         if (snapshot) {
             return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
                                    SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
-                                   SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row,
+                                   SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, row,
+                                   SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY, event,
                                    SourceInfo.SNAPSHOT_KEY, true);
         }
         return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
                                SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
-                               SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row);
+                               SourceInfo.BINLOG_ROW_IN_EVENT_OFFSET_KEY, row,
+                               SourceInfo.EVENTS_TO_SKIP_OFFSET_KEY, event);
     }
 
     protected PositionAssert assertThatDocument(Document position) {
@@ -467,12 +583,12 @@ protected PositionAssert assertPositionWithGtids(String gtids, boolean snapshot)
         return assertThatDocument(positionWithGtids(gtids, snapshot));
     }
 
-    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row) {
-        return assertPositionWithoutGtids(filename, position, row, false);
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int event, int row) {
+        return assertPositionWithoutGtids(filename, position, event, row, false);
     }
 
-    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row, boolean snapshot) {
-        return assertThatDocument(positionWithoutGtids(filename, position, row, snapshot));
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int event, int row, boolean snapshot) {
+        return assertThatDocument(positionWithoutGtids(filename, position, event, row, snapshot));
     }
 
     protected static class PositionAssert extends GenericAssert<PositionAssert, Document> {",2016-11-09T14:11:41Z,70
"@@ -11,4 +11,5 @@ log4j.rootLogger=INFO, stdout
 log4j.logger.io.debezium=INFO
 log4j.logger.io.debezium.embedded.EmbeddedEngine$EmbeddedConfig=WARN
 #log4j.logger.io.debezium.connector.mysql.BinlogReader=DEBUG
-#log4j.logger.io.debezium.connector.mysql.SnapshotReader=DEBUG
\ No newline at end of file
+#log4j.logger.io.debezium.connector.mysql.SnapshotReader=DEBUG
+#log4j.logger.io.debezium.relational.history=DEBUG",2016-11-09T14:11:41Z,73
"@@ -92,6 +92,19 @@ static Document create(CharSequence fieldName1, Object value1, CharSequence fiel
         return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4);
     }
 
+    static Document create(CharSequence fieldName1, Object value1, CharSequence fieldName2, Object value2, CharSequence fieldName3,
+                           Object value3, CharSequence fieldName4, Object value4, CharSequence fieldName5, Object value5) {
+        return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4)
+                                  .set(fieldName5, value5);
+    }
+
+    static Document create(CharSequence fieldName1, Object value1, CharSequence fieldName2, Object value2, CharSequence fieldName3,
+                           Object value3, CharSequence fieldName4, Object value4, CharSequence fieldName5, Object value5,
+                           CharSequence fieldName6, Object value6) {
+        return new BasicDocument().set(fieldName1, value1).set(fieldName2, value2).set(fieldName3, value3).set(fieldName4, value4)
+                                  .set(fieldName5, value5).set(fieldName6, value6);
+    }
+
     /**
      * Return the number of name-value fields in this object.
      * 
@@ -159,7 +172,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
             parent = find(parentPath, (missingPath, missingIndex) -> {
                 invalid.accept(missingPath); // invoke the invalid handler
                 return Optional.empty();
-            } , invalid);
+            }, invalid);
         } else {
             // Create any missing intermediaries using the segment after the missing segment to determine which
             // type of intermediate value to add ...
@@ -170,7 +183,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
                 } else {
                     return Optional.of(Value.create(Document.create()));
                 }
-            } , invalid);
+            }, invalid);
         }
         if (!parent.isPresent()) return Optional.empty();
         String lastSegment = path.lastSegment().get();
@@ -202,8 +215,7 @@ default Optional<Value> set(Path path, boolean addIntermediaries, Value value, C
      *         valid
      */
     default Optional<Value> find(Path path) {
-        return find(path, (missingPath, missingIndex) -> Optional.empty(), (invalidPath) -> {
-        });
+        return find(path, (missingPath, missingIndex) -> Optional.empty(), (invalidPath) -> {});
     }
 
     /**
@@ -719,7 +731,7 @@ default Value remove(Optional<? extends CharSequence> name) {
      * @return This document, to allow for chaining methods
      */
     Document removeAll();
-    
+
     /**
      * Sets on this object all name/value pairs from the supplied object. If the supplied object is null, this method does
      * nothing.",2016-11-09T14:11:41Z,74
"@@ -1169,8 +1169,9 @@ protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
      */
     protected Object handleUnknownData(Column column, Field fieldDefn, Object data) {
         if (column.isOptional() || fieldDefn.schema().isOptional()) {
+            Class<?> dataClass = data.getClass();
             logger.warn(""Unexpected value for JDBC type {} and column {}: class={}"", column.jdbcType(), column,
-                        data.getClass()); // don't include value in case its sensitive
+                        dataClass.isArray() ? dataClass.getSimpleName() : dataClass.getName()); // don't include value in case its sensitive
             return null;
         }
         throw new IllegalArgumentException(""Unexpected value for JDBC type "" + column.jdbcType() + "" and column "" + column +",2016-11-09T14:11:41Z,75
"@@ -46,14 +46,18 @@ public final void record(Map<String, ?> source, Map<String, ?> position, String
 
     @Override
     public final void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser) {
+        logger.debug(""Recovering DDL history for source partition {} and offset {}"",source,position);
         HistoryRecord stopPoint = new HistoryRecord(source, position, null, null);
         recoverRecords(schema,ddlParser,recovered->{
             if (comparator.isAtOrBefore(recovered,stopPoint)) {
                 String ddl = recovered.ddl();
                 if (ddl != null) {
                     ddlParser.setCurrentSchema(recovered.databaseName()); // may be null
                     ddlParser.parse(ddl, schema);
+                    logger.debug(""Applying: {}"", ddl);
                 }
+            } else {
+                logger.debug(""Skipping: {}"", recovered.ddl());
             }
         });
     }",2016-11-09T14:11:41Z,76
"@@ -281,7 +281,7 @@ public static void debug(SourceRecord record) {
      * @param record the record to validate; may not be null
      */
     public static void isValid(SourceRecord record) {
-        print(record);
+        //print(record);
 
         JsonNode keyJson = null;
         JsonNode valueJson = null;",2016-11-09T14:11:41Z,77
"@@ -8,6 +8,7 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
+import java.util.concurrent.CountDownLatch;
 import java.util.concurrent.ExecutionException;
 import java.util.concurrent.Executor;
 import java.util.concurrent.ExecutorService;
@@ -35,7 +36,6 @@
 import org.apache.kafka.connect.storage.OffsetStorageReader;
 import org.apache.kafka.connect.storage.OffsetStorageReaderImpl;
 import org.apache.kafka.connect.storage.OffsetStorageWriter;
-import org.apache.kafka.connect.storage.StringConverter;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -134,7 +134,7 @@ public final class EmbeddedEngine implements Runnable {
 
     protected static final Field INTERNAL_KEY_CONVERTER_CLASS = Field.create(""internal.key.converter"")
                                                                      .withDescription(""The Converter class that should be used to serialize and deserialize key data for offsets."")
-                                                                     .withDefault(StringConverter.class.getName());
+                                                                     .withDefault(JsonConverter.class.getName());
 
     protected static final Field INTERNAL_VALUE_CONVERTER_CLASS = Field.create(""internal.value.converter"")
                                                                        .withDescription(""The Converter class that should be used to serialize and deserialize value data for offsets."")
@@ -159,14 +159,108 @@ public static interface CompletionCallback {
         /**
          * Handle the completion of the embedded connector engine.
          * 
-         * @param success true if the connector completed normally, or {@code false} if the connector produced an error that
-         *            prevented startup or premature termination.
+         * @param success {@code true} if the connector completed normally, or {@code false} if the connector produced an error
+         *            that prevented startup or premature termination.
          * @param message the completion message; never null
          * @param error the error, or null if there was no exception
          */
         void handle(boolean success, String message, Throwable error);
     }
 
+    /**
+     * A callback function to be notified when the connector completes.
+     */
+    public static class CompletionResult implements CompletionCallback {
+        private final CountDownLatch completed = new CountDownLatch(1);
+        private boolean success;
+        private String message;
+        private Throwable error;
+
+        @Override
+        public void handle(boolean success, String message, Throwable error) {
+            this.success = success;
+            this.message = message;
+            this.error = error;
+            this.completed.countDown();
+        }
+
+        /**
+         * Causes the current thread to wait until the {@link #handle(boolean, String, Throwable) completion occurs}
+         * or until the thread is {@linkplain Thread#interrupt interrupted}.
+         * <p>
+         * This method returns immediately if the connector has completed already.
+         * 
+         * @throws InterruptedException if the current thread is interrupted while waiting
+         */
+        public void await() throws InterruptedException {
+            this.completed.await();
+        }
+
+        /**
+         * Causes the current thread to wait until the {@link #handle(boolean, String, Throwable) completion occurs},
+         * unless the thread is {@linkplain Thread#interrupt interrupted}, or the specified waiting time elapses.
+         * <p>
+         * This method returns immediately if the connector has completed already.
+         * 
+         * @param timeout the maximum time to wait
+         * @param unit the time unit of the {@code timeout} argument
+         * @return {@code true} if the completion was received, or {@code false} if the waiting time elapsed before the completion
+         *         was received.
+         * @throws InterruptedException if the current thread is interrupted while waiting
+         */
+        public boolean await(long timeout, TimeUnit unit) throws InterruptedException {
+            return this.completed.await(timeout, unit);
+        }
+
+        /**
+         * Determine if the connector has completed.
+         * 
+         * @return {@code true} if the connector has completed, or {@code false} if the connector is still running and this
+         *         callback has not yet been {@link #handle(boolean, String, Throwable) notified}
+         */
+        public boolean hasCompleted() {
+            return completed.getCount() == 0;
+        }
+
+        /**
+         * Get whether the connector completed normally.
+         * 
+         * @return {@code true} if the connector completed normally, or {@code false} if the connector produced an error that
+         *         prevented startup or premature termination (or the connector has not yet {@link #hasCompleted() completed})
+         */
+        public boolean success() {
+            return success;
+        }
+
+        /**
+         * Get the completion message.
+         * 
+         * @return the completion message, or null if the connector has not yet {@link #hasCompleted() completed}
+         */
+        public String message() {
+            return message;
+        }
+
+        /**
+         * Get the completion error, if there is one.
+         * 
+         * @return the completion error, or null if there is no error or connector has not yet {@link #hasCompleted() completed}
+         */
+        public Throwable error() {
+            return error;
+        }
+
+        /**
+         * Determine if there is a completion error.
+         * 
+         * @return {@code true} if there is a {@link #error completion error}, or {@code false} if there is no error or
+         *         the connector has not yet {@link #hasCompleted() completed}
+         */
+        public boolean hasError() {
+            return error != null;
+        }
+    }
+
     /**
      * A builder to set up and create {@link EmbeddedEngine} instances.
      */
@@ -295,7 +389,7 @@ public EmbeddedEngine build() {
     private long timeSinceLastCommitMillis = 0;
 
     private EmbeddedEngine(Configuration config, ClassLoader classLoader, Clock clock, Consumer<SourceRecord> consumer,
-            CompletionCallback completionCallback) {
+                           CompletionCallback completionCallback) {
         this.config = config;
         this.consumer = consumer;
         this.classLoader = classLoader;
@@ -308,7 +402,7 @@ private EmbeddedEngine(Configuration config, ClassLoader classLoader, Clock cloc
         assert this.classLoader != null;
         assert this.clock != null;
         keyConverter = config.getInstance(INTERNAL_KEY_CONVERTER_CLASS, Converter.class, () -> this.classLoader);
-        keyConverter.configure(config.subset(INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), false);
+        keyConverter.configure(config.subset(INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), true);
         valueConverter = config.getInstance(INTERNAL_VALUE_CONVERTER_CLASS, Converter.class, () -> this.classLoader);
         Configuration valueConverterConfig = config;
         if (valueConverter instanceof JsonConverter) {
@@ -456,8 +550,9 @@ public void raiseError(Exception e) {
                     }
 
                     recordsSinceLastCommit = 0;
+                    Throwable handlerError = null;
                     timeSinceLastCommitMillis = clock.currentTimeInMillis();
-                    while (runningThread.get() != null) {
+                    while (runningThread.get() != null && handlerError == null) {
                         try {
                             logger.debug(""Embedded engine is polling task for records"");
                             List<SourceRecord> changeRecords = task.poll(); // blocks until there are values ...
@@ -469,17 +564,16 @@ public void raiseError(Exception e) {
                                     try {
                                         consumer.accept(record);
                                     } catch (Throwable t) {
-                                        logger.error(""Error in the application's handler method, but continuing anyway"", t);
+                                        handlerError = t;
+                                        break;
                                     }
-                                }
 
-                                // Only then do we write out the last partition to offset storage ...
-                                SourceRecord lastRecord = changeRecords.get(changeRecords.size() - 1);
-                                lastRecord.sourceOffset();
-                                offsetWriter.offset(lastRecord.sourcePartition(), lastRecord.sourceOffset());
+                                    // Record the offset for this record's partition
+                                    offsetWriter.offset(record.sourcePartition(), record.sourceOffset());
+                                    recordsSinceLastCommit += 1;
+                                }
 
                                 // Flush the offsets to storage if necessary ...
-                                recordsSinceLastCommit += changeRecords.size();
                                 maybeFlush(offsetWriter, offsetCommitPolicy, commitTimeoutMs);
                             } else {
                                 logger.debug(""Received no records from the task"");
@@ -501,7 +595,14 @@ public void raiseError(Exception e) {
                     } finally {
                         // Always commit offsets that were captured from the source records we actually processed ...
                         commitOffsets(offsetWriter, commitTimeoutMs);
-                        succeed(""Connector '"" + connectorClassName + ""' completed normally."");
+                        if (handlerError != null) {
+                            // There was an error in the handler ...
+                            fail(""Stopping connector after error in the application's handler method: "" + handlerError.getMessage(),
+                                 handlerError);
+                        } else {
+                            // We stopped normally ...
+                            succeed(""Connector '"" + connectorClassName + ""' completed normally."");
+                        }
                     }
                 } catch (Throwable t) {
                     fail(""Error while trying to run connector class '"" + connectorClassName + ""'"", t);",2016-11-09T14:11:41Z,78
"@@ -7,8 +7,11 @@
 
 import static org.junit.Assert.fail;
 
+import java.math.BigDecimal;
 import java.nio.file.Path;
 import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.Collections;
 import java.util.HashMap;
 import java.util.LinkedList;
@@ -22,18 +25,25 @@
 import java.util.concurrent.Executors;
 import java.util.concurrent.TimeUnit;
 import java.util.function.Consumer;
+import java.util.function.Predicate;
 
 import org.apache.kafka.common.config.Config;
 import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.json.JsonConverter;
 import org.apache.kafka.connect.json.JsonDeserializer;
+import org.apache.kafka.connect.runtime.WorkerConfig;
 import org.apache.kafka.connect.runtime.standalone.StandaloneConfig;
 import org.apache.kafka.connect.source.SourceConnector;
 import org.apache.kafka.connect.source.SourceRecord;
+import org.apache.kafka.connect.storage.Converter;
+import org.apache.kafka.connect.storage.FileOffsetBackingStore;
+import org.apache.kafka.connect.storage.OffsetStorageReaderImpl;
+import org.fest.assertions.Delta;
 import org.junit.After;
 import org.junit.Before;
 import org.slf4j.Logger;
@@ -42,8 +52,10 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.data.SchemaUtil;
 import io.debezium.data.VerifyRecord;
 import io.debezium.embedded.EmbeddedEngine.CompletionCallback;
+import io.debezium.embedded.EmbeddedEngine.EmbeddedConfig;
 import io.debezium.function.BooleanConsumer;
 import io.debezium.relational.history.HistoryRecord;
 import io.debezium.util.LoggingContext;
@@ -162,20 +174,44 @@ protected int getMaximumEnqueuedRecordCount() {
     }
 
     /**
-     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
-     * logged.
+     * Create a {@link CompletionCallback} that logs when the engine fails to start the connector or when the connector
+     * stops running after completing successfully or due to an error
      * 
-     * @param connectorClass the connector class; may not be null
-     * @param connectorConfig the configuration for the connector; may not be null
+     * @return the logging {@link CompletionCallback}
      */
-    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig) {
-        start(connectorClass, connectorConfig, (success, msg, error) -> {
+    protected CompletionCallback loggingCompletion() {
+        return (success, msg, error) -> {
             if (success) {
                 logger.info(msg);
             } else {
                 logger.error(msg, error);
             }
-        });
+        };
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
+     * logged.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig) {
+        start(connectorClass, connectorConfig, loggingCompletion(), null);
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration, where upon completion the status of the connector is
+     * logged. The connector will stop immediately when the supplied predicate returns true.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     * @param isStopRecord the function that will be called to determine if the connector should be stopped before processing
+     *            this record; may be null if not needed
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         Predicate<SourceRecord> isStopRecord) {
+        start(connectorClass, connectorConfig, loggingCompletion(), isStopRecord);
     }
 
     /**
@@ -186,7 +222,23 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
      * @param callback the function that will be called when the engine fails to start the connector or when the connector
      *            stops running after completing successfully or due to an error; may be null
      */
-    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig, CompletionCallback callback) {
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         CompletionCallback callback) {
+        start(connectorClass, connectorConfig, callback, null);
+    }
+
+    /**
+     * Start the connector using the supplied connector configuration.
+     * 
+     * @param connectorClass the connector class; may not be null
+     * @param connectorConfig the configuration for the connector; may not be null
+     * @param isStopRecord the function that will be called to determine if the connector should be stopped before processing
+     *            this record; may be null if not needed
+     * @param callback the function that will be called when the engine fails to start the connector or when the connector
+     *            stops running after completing successfully or due to an error; may be null
+     */
+    protected void start(Class<? extends SourceConnector> connectorClass, Configuration connectorConfig,
+                         CompletionCallback callback, Predicate<SourceRecord> isStopRecord) {
         Configuration config = Configuration.copy(connectorConfig)
                                             .with(EmbeddedEngine.ENGINE_NAME, ""testing-connector"")
                                             .with(EmbeddedEngine.CONNECTOR_CLASS, connectorClass.getName())
@@ -202,11 +254,14 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
             }
             Testing.debug(""Stopped connector"");
         };
-
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
                                .notifying((record) -> {
+                                   if (isStopRecord != null && isStopRecord.test(record)) {
+                                       logger.error(""Stopping connector after record as requested"");
+                                       throw new ConnectException(""Stopping connector after record as requested"");
+                                   }
                                    try {
                                        consumedLines.put(record);
                                    } catch (InterruptedException e) {
@@ -306,7 +361,6 @@ protected SourceRecords consumeRecordsByTopic(int numRecords) throws Interrupted
         consumeRecords(numRecords, records::add);
         return records;
     }
-    
 
     protected class SourceRecords {
         private final List<SourceRecord> records = new ArrayList<>();
@@ -467,6 +521,55 @@ protected void assertTombstone(SourceRecord record, String pkField, int pk) {
     protected void assertTombstone(SourceRecord record) {
         VerifyRecord.isValidTombstone(record);
     }
+    
+    protected void assertOffset(SourceRecord record, Map<String,?> expectedOffset) {
+        Map<String,?> offset = record.sourceOffset();
+        assertThat(offset).isEqualTo(expectedOffset);
+    }
+    
+    protected void assertOffset(SourceRecord record, String offsetField, Object expectedValue) {
+        Map<String,?> offset = record.sourceOffset();
+        Object value = offset.get(offsetField);
+        assertSameValue(value,expectedValue);
+    }
+    
+    protected void assertValueField(SourceRecord record, String fieldPath, Object expectedValue) {
+        Object value = record.value();
+        String[] fieldNames = fieldPath.split(""/"");
+        String pathSoFar = null;
+        for (int i=0; i!=fieldNames.length; ++i) {
+            String fieldName = fieldNames[i];
+            if (value instanceof Struct) {
+                value = ((Struct)value).get(fieldName);
+            } else {
+                // We expected the value to be a struct ...
+                String path = pathSoFar == null ? ""record value"" : (""'"" + pathSoFar + ""'"");
+                String msg = ""Expected the "" + path + "" to be a Struct but was "" + value.getClass().getSimpleName() + "" in record: "" + SchemaUtil.asString(record);
+                fail(msg);
+            }
+            pathSoFar = pathSoFar == null ? fieldName : pathSoFar + ""/"" + fieldName;
+        }
+        assertSameValue(value,expectedValue);
+    }
+    
+    private void assertSameValue(Object actual, Object expected) {
+        if(expected instanceof Double || expected instanceof Float || expected instanceof BigDecimal) {
+            // Value should be within 1%
+            double expectedNumericValue = ((Number)expected).doubleValue();
+            double actualNumericValue = ((Number)actual).doubleValue();
+            assertThat(actualNumericValue).isEqualTo(expectedNumericValue, Delta.delta(0.01d*expectedNumericValue));
+        } else if (expected instanceof Integer || expected instanceof Long || expected instanceof Short) {
+            long expectedNumericValue = ((Number)expected).longValue();
+            long actualNumericValue = ((Number)actual).longValue();
+            assertThat(actualNumericValue).isEqualTo(expectedNumericValue);
+        } else if (expected instanceof Boolean) {
+            boolean expectedValue = ((Boolean)expected).booleanValue();
+            boolean actualValue = ((Boolean)expected).booleanValue();
+            assertThat(actualValue).isEqualTo(expectedValue);
+        } else {
+            assertThat(actual).isEqualTo(expected);
+        }
+    }
 
     /**
      * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
@@ -512,7 +615,8 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
         assertThat(value.errorMessages().size()).isEqualTo(numErrors);
     }
 
-    protected void assertConfigurationErrors(Config config, io.debezium.config.Field field, int minErrorsInclusive, int maxErrorsInclusive) {
+    protected void assertConfigurationErrors(Config config, io.debezium.config.Field field, int minErrorsInclusive,
+                                             int maxErrorsInclusive) {
         ConfigValue value = configValue(config, field.name());
         assertThat(value.errorMessages().size()).isGreaterThanOrEqualTo(minErrorsInclusive);
         assertThat(value.errorMessages().size()).isLessThanOrEqualTo(maxErrorsInclusive);
@@ -526,8 +630,8 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
     protected void assertNoConfigurationErrors(Config config, io.debezium.config.Field... fields) {
         for (io.debezium.config.Field field : fields) {
             ConfigValue value = configValue(config, field.name());
-            if ( value != null ) {
-                if ( !value.errorMessages().isEmpty() ) {
+            if (value != null) {
+                if (!value.errorMessages().isEmpty()) {
                     fail(""Error messages on field '"" + field.name() + ""': "" + value.errorMessages());
                 }
             }
@@ -538,4 +642,59 @@ protected ConfigValue configValue(Config config, String fieldName) {
         return config.configValues().stream().filter(value -> value.name().equals(fieldName)).findFirst().orElse(null);
     }
 
+    /**
+     * Utility to read the last committed offset for the specified partition.
+     * 
+     * @param config the configuration of the engine used to persist the offsets
+     * @param partition the partition
+     * @return the map of partitions to offsets; never null but possibly empty
+     */
+    protected <T> Map<String, Object> readLastCommittedOffset(Configuration config, Map<String, T> partition) {
+        return readLastCommittedOffsets(config, Arrays.asList(partition)).get(partition);
+    }
+
+    /**
+     * Utility to read the last committed offsets for the specified partitions.
+     * 
+     * @param config the configuration of the engine used to persist the offsets
+     * @param partitions the partitions
+     * @return the map of partitions to offsets; never null but possibly empty
+     */
+    protected <T> Map<Map<String, T>, Map<String, Object>> readLastCommittedOffsets(Configuration config,
+                                                                                    Collection<Map<String, T>> partitions) {
+        config = config.edit().with(EmbeddedEngine.ENGINE_NAME, ""testing-connector"")
+                       .with(StandaloneConfig.OFFSET_STORAGE_FILE_FILENAME_CONFIG, OFFSET_STORE_PATH)
+                       .with(EmbeddedEngine.OFFSET_FLUSH_INTERVAL_MS, 0)
+                       .build();
+
+        final String engineName = config.getString(EmbeddedEngine.ENGINE_NAME);
+        Converter keyConverter = config.getInstance(EmbeddedEngine.INTERNAL_KEY_CONVERTER_CLASS, Converter.class);
+        keyConverter.configure(config.subset(EmbeddedEngine.INTERNAL_KEY_CONVERTER_CLASS.name() + ""."", true).asMap(), true);
+        Converter valueConverter = config.getInstance(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS, Converter.class);
+        Configuration valueConverterConfig = config;
+        if (valueConverter instanceof JsonConverter) {
+            // Make sure that the JSON converter is configured to NOT enable schemas ...
+            valueConverterConfig = config.edit().with(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS + "".schemas.enable"", false).build();
+        }
+        valueConverter.configure(valueConverterConfig.subset(EmbeddedEngine.INTERNAL_VALUE_CONVERTER_CLASS.name() + ""."", true).asMap(),
+                                 false);
+
+        // Create the worker config, adding extra fields that are required for validation of a worker config
+        // but that are not used within the embedded engine (since the source records are never serialized) ...
+        Map<String, String> embeddedConfig = config.asMap(EmbeddedEngine.ALL_FIELDS);
+        embeddedConfig.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, JsonConverter.class.getName());
+        embeddedConfig.put(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, JsonConverter.class.getName());
+        WorkerConfig workerConfig = new EmbeddedConfig(embeddedConfig);
+
+        FileOffsetBackingStore offsetStore = new FileOffsetBackingStore();
+        offsetStore.configure(workerConfig);
+        offsetStore.start();
+        try {
+            OffsetStorageReaderImpl offsetReader = new OffsetStorageReaderImpl(offsetStore, engineName, keyConverter, valueConverter);
+            return offsetReader.offsets(partitions);
+        } finally {
+            offsetStore.stop();
+        }
+    }
+
 }",2016-11-09T14:11:41Z,35
"@@ -63,7 +63,6 @@ public class MySqlDdlParser extends DdlParser {
     private final MySqlSystemVariables systemVariables = new MySqlSystemVariables();
     private final ConcurrentMap<String, String> charsetNameForDatabase = new ConcurrentHashMap<>();
 
-    public static final String ENUM_AND_SET_DELIMINATOR = "","";
     /**
      * Create a new DDL parser for MySQL that does not include view definitions.
      */
@@ -662,10 +661,9 @@ public static List<String> parseSetAndEnumOptions(String typeExpression) {
         if (matcher.matches()) {
             String literals = matcher.group(2);
             Matcher optionMatcher = ENUM_AND_SET_OPTIONS.matcher(literals);
-            StringBuilder sb = new StringBuilder();
             while (optionMatcher.find()) {
                 String option = optionMatcher.group(1);
-                if (option.length() > 0) {
+                if (option != null && option.length() > 0) {
                     options.add(option);
                 }
             }",2016-10-11T20:38:14Z,49
"@@ -73,7 +73,7 @@ public MySqlValueConverters(boolean adaptiveTimePrecision) {
     public MySqlValueConverters(boolean adaptiveTimePrecision, ZoneOffset defaultOffset) {
         super(adaptiveTimePrecision, defaultOffset);
     }
-    
+
     @Override
     protected ByteOrder byteOrderOfBitType() {
         return ByteOrder.BIG_ENDIAN;
@@ -87,13 +87,11 @@ public SchemaBuilder schemaBuilder(Column column) {
             return Year.builder();
         }
         if (matches(typeName, ""ENUM"")) {
-            List<String> options = extractEnumAndSetOptions(column);
-            String commaSeperatedOptions = Strings.join(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR,options);
+            String commaSeperatedOptions = extractEnumAndSetOptionsAsString(column);
             return io.debezium.data.Enum.builder(commaSeperatedOptions);
         }
         if (matches(typeName, ""SET"")) {
-            List<String> options = extractEnumAndSetOptions(column);
-            String commaSeperatedOptions = Strings.join(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR,options);
+            String commaSeperatedOptions = extractEnumAndSetOptionsAsString(column);
             return io.debezium.data.EnumSet.builder(commaSeperatedOptions);
         }
         // Otherwise, let the base class handle it ...
@@ -117,7 +115,7 @@ public ValueConverter converter(Column column, Field fieldDefn) {
             List<String> options = extractEnumAndSetOptions(column);
             return (data) -> convertSetToString(options, column, fieldDefn, data);
         }
-        
+
         // We have to convert bytes encoded in the column's character set ...
         switch (column.jdbcType()) {
             case Types.CHAR: // variable-length
@@ -297,7 +295,7 @@ protected Object convertSetToString(List<String> options, Column column, Field f
         if (data instanceof Long) {
             // The binlog will contain a long with the indexes of the options in the set value ...
             long indexes = ((Long) data).longValue();
-            return convertSetValue(indexes, options);
+            return convertSetValue(column, indexes, options);
         }
         return handleUnknownData(column, fieldDefn, data);
     }
@@ -316,23 +314,29 @@ protected boolean matches(String upperCaseTypeName, String upperCaseMatch) {
     }
 
     protected List<String> extractEnumAndSetOptions(Column column) {
-        List<String> options = MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
-        return options;
+        return MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
     }
 
-    protected String convertSetValue(long indexes, List<String> options) {
+    protected String extractEnumAndSetOptionsAsString(Column column) {
+        return Strings.join("","", extractEnumAndSetOptions(column));
+    }
+
+    protected String convertSetValue(Column column, long indexes, List<String> options) {
         StringBuilder sb = new StringBuilder();
         int index = 0;
         boolean first = true;
         int optionLen = options.size();
         while (indexes != 0L) {
             if (indexes % 2L != 0) {
-                if (first)
+                if (first) {
                     first = false;
-                else
-                    sb.append(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR);
+                } else {
+                    sb.append(',');
+                }
                 if (index < optionLen) {
                     sb.append(options.get(index));
+                } else {
+                    logger.warn(""Found unexpected index '{}' on column {}"", index, column);
                 }
             }
             ++index;",2016-10-11T20:38:14Z,47
"@@ -25,8 +25,8 @@
 import io.debezium.relational.ddl.DdlParserListener.Event;
 import io.debezium.relational.ddl.SimpleDdlParserListener;
 import io.debezium.util.IoUtil;
-import io.debezium.util.Testing;
 import io.debezium.util.Strings;
+import io.debezium.util.Testing;
 
 public class MySqlDdlParserTest {
 
@@ -660,7 +660,7 @@ public void shouldParseSetOptions() {
 
     protected void assertParseEnumAndSetOptions(String typeExpression, String optionString) {
         List<String> options = MySqlDdlParser.parseSetAndEnumOptions(typeExpression);
-        String commaSeperatedOptions = Strings.join(MySqlDdlParser.ENUM_AND_SET_DELIMINATOR,options);
+        String commaSeperatedOptions = Strings.join("","", options);
         assertThat(optionString).isEqualTo(commaSeperatedOptions);
     }
 ",2016-10-11T20:38:14Z,26
"@@ -10,6 +10,8 @@
 import java.util.Collection;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicReference;
 import java.util.function.Consumer;
@@ -25,6 +27,7 @@
 import io.debezium.relational.ddl.DataType;
 import io.debezium.relational.ddl.DataTypeParser;
 import io.debezium.relational.ddl.DdlParser;
+import io.debezium.relational.ddl.DdlParserListener.SetVariableEvent;
 import io.debezium.relational.ddl.DdlTokenizer;
 import io.debezium.text.ParsingException;
 import io.debezium.text.TokenStream;
@@ -51,6 +54,15 @@ public class MySqlDdlParser extends DdlParser {
      */
     private static final Pattern ENUM_AND_SET_OPTIONS = Pattern.compile(""'([^']*)'"");
 
+    /**
+     * The system variable name for the name of the character set that the server uses by default.
+     * See http://dev.mysql.com/doc/refman/5.7/en/server-options.html#option_mysqld_character-set-server
+     */
+    private static final String SERVER_CHARSET_NAME = MySqlSystemVariables.CHARSET_NAME_SERVER;
+
+    private final MySqlSystemVariables systemVariables = new MySqlSystemVariables();
+    private final ConcurrentMap<String, String> charsetNameForDatabase = new ConcurrentHashMap<>();
+
     /**
      * Create a new DDL parser for MySQL that does not include view definitions.
      */
@@ -67,6 +79,10 @@ public MySqlDdlParser(boolean includeViews) {
         super("";"", includeViews);
     }
 
+    protected MySqlSystemVariables systemVariables() {
+        return systemVariables;
+    }
+
     @Override
     protected void initializeDataTypes(DataTypeParser dataTypes) {
         dataTypes.register(Types.BIT, ""BIT[(L)]"");
@@ -90,40 +106,26 @@ protected void initializeDataTypes(DataTypeParser dataTypes) {
         dataTypes.register(Types.TIMESTAMP_WITH_TIMEZONE, ""TIMESTAMP[(L)]""); // includes timezone information
         dataTypes.register(Types.TIMESTAMP, ""DATETIME[(L)]"");
         dataTypes.register(Types.INTEGER, ""YEAR[(2|4)]"");
-        dataTypes.register(Types.BLOB, ""CHAR[(L)] BINARY [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""VARCHAR(L) BINARY [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""CHAR[(L)] [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""VARCHAR(L) [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""CHAR[(L)] BINARY [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""VARCHAR(L) BINARY [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""CHAR[(L)] [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""VARCHAR(L) [CHARSET charset_name] [COLLATE collation_name]"");
+        dataTypes.register(Types.BLOB, ""CHAR[(L)] BINARY"");
+        dataTypes.register(Types.BLOB, ""VARCHAR(L) BINARY"");
+        dataTypes.register(Types.CHAR, ""CHAR[(L)]"");
+        dataTypes.register(Types.VARCHAR, ""VARCHAR(L)"");
         dataTypes.register(Types.CHAR, ""BINARY[(L)]"");
         dataTypes.register(Types.VARBINARY, ""VARBINARY(L)"");
         dataTypes.register(Types.BLOB, ""TINYBLOB"");
         dataTypes.register(Types.BLOB, ""BLOB"");
         dataTypes.register(Types.BLOB, ""MEDIUMBLOB"");
         dataTypes.register(Types.BLOB, ""LONGBLOB"");
-        dataTypes.register(Types.BLOB, ""TINYTEXT BINARY [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""TEXT BINARY [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""MEDIUMTEXT BINARY [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""LONGTEXT BINARY [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""TINYTEXT [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""TEXT [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""MEDIUMTEXT [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""LONGTEXT [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.CHAR, ""ENUM(...) [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.CHAR, ""SET(...) [CHARACTER SET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""TINYTEXT BINARY [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""TEXT BINARY [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""MEDIUMTEXT BINARY [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.BLOB, ""LONGTEXT BINARY [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""TINYTEXT [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""TEXT [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""MEDIUMTEXT [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.VARCHAR, ""LONGTEXT [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.CHAR, ""ENUM(...) [CHARSET charset_name] [COLLATE collation_name]"");
-        dataTypes.register(Types.CHAR, ""SET(...) [CHARSET charset_name] [COLLATE collation_name]"");
+        dataTypes.register(Types.BLOB, ""TINYTEXT BINARY"");
+        dataTypes.register(Types.BLOB, ""TEXT BINARY"");
+        dataTypes.register(Types.BLOB, ""MEDIUMTEXT BINARY"");
+        dataTypes.register(Types.BLOB, ""LONGTEXT BINARY"");
+        dataTypes.register(Types.VARCHAR, ""TINYTEXT"");
+        dataTypes.register(Types.VARCHAR, ""TEXT"");
+        dataTypes.register(Types.VARCHAR, ""MEDIUMTEXT"");
+        dataTypes.register(Types.VARCHAR, ""LONGTEXT"");
+        dataTypes.register(Types.CHAR, ""ENUM(...)"");
+        dataTypes.register(Types.CHAR, ""SET(...)"");
         dataTypes.register(Types.OTHER, ""JSON"");
     }
 
@@ -133,7 +135,7 @@ protected void initializeKeywords(TokenSet keywords) {
 
     @Override
     protected void initializeStatementStarts(TokenSet statementStartTokens) {
-        statementStartTokens.add(""CREATE"", ""ALTER"", ""DROP"", ""INSERT"", ""SET"", ""GRANT"", ""REVOKE"", ""FLUSH"", ""TRUNCATE"", ""COMMIT"", ""USE"");
+        statementStartTokens.add(""CREATE"", ""ALTER"", ""DROP"", ""INSERT"", ""GRANT"", ""REVOKE"", ""FLUSH"", ""TRUNCATE"", ""COMMIT"", ""USE"");
     }
 
     @Override
@@ -150,11 +152,117 @@ protected void parseNextStatement(Marker marker) {
             parseRename(marker);
         } else if (tokens.matches(""USE"")) {
             parseUse(marker);
+        } else if (tokens.matches(""SET"")) {
+            parseSet(marker);
         } else {
             parseUnknownStatement(marker);
         }
     }
 
+    protected void parseSet(Marker start) {
+        tokens.consume(""SET"");
+        AtomicReference<MySqlSystemVariables.Scope> scope = new AtomicReference<>();
+        parseSetVariable(start, scope);
+        while (tokens.canConsume(',')) {
+            parseSetVariable(start, scope);
+        }
+        consumeRemainingStatement(start);
+        debugParsed(start);
+    }
+
+    protected void parseSetVariable(Marker start, AtomicReference<MySqlSystemVariables.Scope> scope) {
+        // First, use the modifier to set the scope ...
+        if (tokens.canConsume(""GLOBAL"") || tokens.canConsume(""@@GLOBAL"", ""."")) {
+            scope.set(MySqlSystemVariables.Scope.GLOBAL);
+        } else if (tokens.canConsume(""SESSION"") || tokens.canConsume(""@@SESSION"", ""."")) {
+            scope.set(MySqlSystemVariables.Scope.SESSION);
+        } else if (tokens.canConsume(""LOCAL"") || tokens.canConsume(""@@LOCAL"", ""."")) {
+            scope.set(MySqlSystemVariables.Scope.LOCAL);
+        }
+
+        // Now handle the remainder of the variable assignment ...
+        if (tokens.canConsume(""PASSWORD"")) {
+            // ignore
+        } else if (tokens.canConsume(""TRANSACTION"", ""ISOLATION"", ""LEVEL"")) {
+            // ignore
+        } else if (tokens.canConsume(""CHARACTER"", ""SET"") || tokens.canConsume(""CHARSET"")) {
+            // Sets two variables plus the current character set for the current database
+            // See https://dev.mysql.com/doc/refman/5.7/en/set-statement.html
+            String charsetName = tokens.consume();
+            if (""DEFAULT"".equalsIgnoreCase(charsetName)) {
+                charsetName = currentDatabaseCharset();
+            }
+            systemVariables.setVariable(scope.get(), ""character_set_client"", charsetName);
+            systemVariables.setVariable(scope.get(), ""character_set_results"", charsetName);
+            // systemVariables.setVariable(scope.get(), ""collation_connection"", ...);
+        } else if (tokens.canConsume(""NAMES"")) {
+            // https://dev.mysql.com/doc/refman/5.7/en/set-statement.html
+            String charsetName = tokens.consume();
+            if (""DEFAULT"".equalsIgnoreCase(charsetName)) {
+                charsetName = currentDatabaseCharset();
+            }
+            systemVariables.setVariable(scope.get(), ""character_set_client"", charsetName);
+            systemVariables.setVariable(scope.get(), ""character_set_results"", charsetName);
+            systemVariables.setVariable(scope.get(), ""character_set_connection"", charsetName);
+            // systemVariables.setVariable(scope.get(), ""collation_connection"", ...);
+
+            if (tokens.canConsume(""COLLATION"")) {
+                tokens.consume(); // consume the collation name but do nothing with it
+            }
+        } else {
+            // This is a global, session, or local system variable, or a user variable.
+            String variableName = parseVariableName();
+            tokens.canConsume("":""); // := is for user variables
+            tokens.consume(""="");
+            String value = parseVariableValue();
+
+            if (variableName.startsWith(""@"")) {
+                // This is a user variable, so do nothing with it ...
+            } else {
+                systemVariables.setVariable(scope.get(), variableName, value);
+
+                // If this is setting 'character_set_database', then we need to record the character set for
+                // the given database ...
+                if (""character_set_database"".equalsIgnoreCase(variableName)) {
+                    String currentDatabaseName = currentSchema();
+                    if (currentDatabaseName != null) {
+                        charsetNameForDatabase.put(currentDatabaseName, value);
+                    }
+                }
+                
+                // Signal that the variable was set ...
+                signalEvent(new SetVariableEvent(variableName,value,statement(start)));
+            }
+        }
+    }
+
+    protected String parseVariableName() {
+        String variableName = tokens.consume();
+        while (tokens.canConsume(""-"")) {
+            variableName = variableName + ""-"" + tokens.consume();
+        }
+        return variableName;
+    }
+
+    protected String parseVariableValue() {
+        if ( tokens.canConsumeAnyOf("","","";"")) {
+            // The variable is blank ...
+            return """";
+        }
+        Marker start = tokens.mark();
+        tokens.consumeUntilEndOrOneOf("","", "";"");
+        String value = tokens.getContentFrom(start);
+        if ( value.startsWith(""'"") && value.endsWith(""'"")) {
+            // Remove the single quotes around the value ...
+            if ( value.length() <= 2 ) {
+                value = """";
+            } else {
+                value = value.substring(1, value.length()-2);
+            }
+        }
+        return value;
+    }
+
     @SuppressWarnings(""unchecked"")
     @Override
     protected void parseCreate(Marker marker) {
@@ -188,6 +296,7 @@ protected void parseCreateDatabase(Marker start) {
         tokens.consumeAnyOf(""DATABASE"", ""SCHEMA"");
         tokens.canConsume(""IF"", ""NOT"", ""EXISTS"");
         String dbName = tokens.consume();
+        parseDatabaseOptions(start, dbName);
         consumeRemainingStatement(start);
         signalCreateDatabase(dbName, start);
         debugParsed(start);
@@ -196,6 +305,7 @@ protected void parseCreateDatabase(Marker start) {
     protected void parseAlterDatabase(Marker start) {
         tokens.consumeAnyOf(""DATABASE"", ""SCHEMA"");
         String dbName = tokens.consume();
+        parseDatabaseOptions(start, dbName);
         consumeRemainingStatement(start);
         signalAlterDatabase(dbName, null, start);
         debugParsed(start);
@@ -209,6 +319,23 @@ protected void parseDropDatabase(Marker start) {
         debugParsed(start);
     }
 
+    protected void parseDatabaseOptions(Marker start, String dbName) {
+        // Handle the default character set and collation ...
+        tokens.canConsume(""DEFAULT"");
+        if (tokens.canConsume(""CHARACTER"", ""SET"") || tokens.canConsume(""CHARSET"")) {
+            tokens.canConsume(""="");
+            String charsetName = tokens.consume();
+            if (""DEFAULT"".equalsIgnoreCase(charsetName)) {
+                charsetName = systemVariables.getVariable(SERVER_CHARSET_NAME);
+            }
+            charsetNameForDatabase.put(dbName, charsetName);
+        }
+        if (tokens.canConsume(""COLLATE"")) {
+            tokens.canConsume(""="");
+            tokens.consume(); // collation name
+        }
+    }
+
     protected void parseCreateTable(Marker start) {
         tokens.canConsume(""TEMPORARY"");
         tokens.consume(""TABLE"");
@@ -218,7 +345,7 @@ protected void parseCreateTable(Marker start) {
             TableId originalId = parseQualifiedTableName(start);
             Table original = databaseTables.forTable(originalId);
             if (original != null) {
-                databaseTables.overwriteTable(tableId, original.columns(), original.primaryKeyColumnNames());
+                databaseTables.overwriteTable(tableId, original.columns(), original.primaryKeyColumnNames(), original.defaultCharsetName());
             }
             consumeRemainingStatement(start);
             signalCreateTable(tableId, start);
@@ -247,6 +374,11 @@ protected void parseCreateTable(Marker start) {
             parseAsSelectStatement(start, table);
         }
 
+        // Make sure that the table's character set has been set ...
+        if (!table.hasDefaultCharsetName()) {
+            table.setDefaultCharsetName(currentDatabaseCharset());
+        }
+
         // Update the table definition ...
         databaseTables.overwriteTable(table.create());
         signalCreateTable(tableId, start);
@@ -273,7 +405,12 @@ protected boolean parseTableOption(Marker start, TableEditor table) {
         } else if (tokens.canConsume(""DEFAULT"", ""CHARACTER"", ""SET"") || tokens.canConsume(""CHARACTER"", ""SET"") ||
                 tokens.canConsume(""DEFAULT"", ""CHARSET"") || tokens.canConsume(""CHARSET"")) {
             tokens.canConsume('=');
-            tokens.consume();
+            String charsetName = tokens.consume();
+            if (""DEFAULT"".equalsIgnoreCase(charsetName)) {
+                // The table's default character set is set to the character set of the current database ...
+                charsetName = currentDatabaseCharset();
+            }
+            table.setDefaultCharsetName(charsetName);
             return true;
         } else if (tokens.canConsume(""DEFAULT"", ""COLLATE"") || tokens.canConsume(""COLLATE"")) {
             tokens.canConsume('=');
@@ -562,6 +699,17 @@ protected void parseColumnDefinition(Marker start, String columnName, TokenStrea
             if (dataType.scale() > -1) column.scale(dataType.scale());
         }
 
+        if (tokens.canConsume(""CHARSET"") || tokens.canConsume(""CHARACTER"", ""SET"")) {
+            String charsetName = tokens.consume();
+            if (!""DEFAULT"".equalsIgnoreCase(charsetName)) {
+                // Only record it if not inheriting the character set from the table
+                column.charsetName(charsetName);
+            }
+        }
+        if (tokens.canConsume(""COLLATE"")) {
+            tokens.consume(); // name of collation
+        }
+
         if (tokens.canConsume(""AS"") || tokens.canConsume(""GENERATED"", ""ALWAYS"", ""AS"")) {
             consumeExpression(start);
             tokens.canConsumeAnyOf(""VIRTUAL"", ""STORED"");
@@ -952,10 +1100,11 @@ protected void parseAlterSpecification(Marker start, TableEditor table, Consumer
                 || tokens.canConsume(""DEFAULT"", ""CHARACTER"", ""SET"")
                 || tokens.canConsume(""DEFAULT"", ""CHARSET"")) {
             tokens.canConsume('=');
-            tokens.consume(); // charset name
+            String charsetName = tokens.consume(); // charset name
+            table.setDefaultCharsetName(charsetName);
             if (tokens.canConsume(""COLLATE"")) {
                 tokens.canConsume('=');
-                tokens.consume(); // collation name
+                tokens.consume(); // collation name (ignored)
             }
         } else if (tokens.canConsume(""DISCARD"", ""TABLESPACE"") || tokens.canConsume(""IMPORT"", ""TABLESPACE"")) {
             // nothing
@@ -1092,6 +1241,25 @@ protected void parseUse(Marker marker) {
         tokens.consume(""USE"");
         String dbName = tokens.consume();
         setCurrentSchema(dbName);
+
+        // Every time MySQL switches to a different database, it sets the ""character_set_database"" and ""collation_database""
+        // system variables. We replicate that behavior here (or the variable we care about) so that these variables are always
+        // right for the current database.
+        String charsetForDb = charsetNameForDatabase.get(dbName);
+        systemVariables.setVariable(MySqlSystemVariables.Scope.GLOBAL, ""character_set_database"", charsetForDb);
+    }
+
+    /**
+     * Get the name of the character set for the current database, via the ""character_set_database"" system property.
+     * 
+     * @return the name of the character set for the current database, or null if not known ...
+     */
+    protected String currentDatabaseCharset() {
+        String charsetName = systemVariables.getVariable(""character_set_database"");
+        if (charsetName == null || ""DEFAULT"".equalsIgnoreCase(charsetName)) {
+            charsetName = systemVariables.getVariable(SERVER_CHARSET_NAME);
+        }
+        return charsetName;
     }
 
     protected List<String> parseColumnNameList(Marker start) {",2016-08-29T20:14:05Z,49
"@@ -0,0 +1,100 @@
+/*
+ * Copyright Debezium Authors.
+ *
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentMap;
+
+/**
+ * Encapsulates a set of the MySQL system variables.
+ * 
+ * @author Randall Hauch
+ */
+public class MySqlSystemVariables {
+
+    public static enum Scope {
+        GLOBAL, SESSION, LOCAL;
+    }
+    
+    /**
+     * The system variable name for the name of the character set that the server uses by default.
+     * See http://dev.mysql.com/doc/refman/5.7/en/server-options.html#option_mysqld_character-set-server
+     */
+    public static final String CHARSET_NAME_SERVER = ""character_set_server"";
+
+    private final ConcurrentMap<String, String> global = new ConcurrentHashMap<>();
+    private final ConcurrentMap<String, String> session = new ConcurrentHashMap<>();
+
+    /**
+     * Create an instance.
+     */
+    public MySqlSystemVariables() {
+    }
+
+    /**
+     * Set the variable with the specified scope.
+     * 
+     * @param scope the variable scope; may be null if the session scope is to be used
+     * @param name the name of the variable; may not be null
+     * @param value the variable value; may be null if the value for the named variable is to be removed
+     * @return this object for method chaining purposes; never null
+     */
+    public MySqlSystemVariables setVariable(Scope scope, String name, String value) {
+        name = variableName(name);
+        if (value != null) {
+            forScope(scope).put(name, value);
+        } else {
+            forScope(scope).remove(name);
+        }
+        return this;
+    }
+
+    /**
+     * Get the variable with the specified name and scope.
+     * 
+     * @param name the name of the variable; may not be null
+     * @param scope the variable scope; may not be null
+     * @return the variable value; may be null if the variable is not currently set
+     */
+    public String getVariable(String name, Scope scope) {
+        name = variableName(name);
+        return forScope(scope).get(name);
+    }
+
+    /**
+     * Get the variable with the specified name, first checking the {@link Scope#SESSION session} (or {@link Scope#LOCAL local})
+     * variables and then the {@link Scope#GLOBAL global} variables.
+     * 
+     * @param name the name of the variable; may not be null
+     * @return the variable value; may be null if the variable is not currently set
+     */
+    public String getVariable(String name) {
+        name = variableName(name);
+        String value = session.get(name);
+        if (value == null) {
+            value = global.get(name);
+        }
+        return value;
+    }
+
+    private String variableName(String name) {
+        return name.toLowerCase();
+    }
+
+    private ConcurrentMap<String, String> forScope(Scope scope) {
+        if (scope != null) {
+            switch (scope) {
+                case GLOBAL:
+                    return global;
+                case SESSION:
+                case LOCAL:
+                    return session;
+            }
+        }
+        return session;
+    }
+
+}",2016-08-29T20:14:05Z,89
"@@ -21,15 +21,14 @@
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.relational.Tables;
-import io.debezium.relational.ddl.DdlParser;
 import io.debezium.relational.ddl.DdlParserListener.Event;
 import io.debezium.relational.ddl.SimpleDdlParserListener;
 import io.debezium.util.IoUtil;
 import io.debezium.util.Testing;
 
 public class MySqlDdlParserTest {
 
-    private DdlParser parser;
+    private MySqlDdlParser parser;
     private Tables tables;
     private SimpleDdlParserListener listener;
 
@@ -212,7 +211,7 @@ public void shouldParseCreateTableStatementWithCharacterSetForColumns() {
         assertThat(t).isNotNull();
         assertThat(t.columnNames()).containsExactly(""col1"");
         assertThat(t.primaryKeyColumnNames()).isEmpty();
-        assertColumn(t, ""col1"", ""VARCHAR CHARACTER SET greek"", Types.VARCHAR, 25, -1, true, false, false);
+        assertColumn(t, ""col1"", ""VARCHAR"", Types.VARCHAR, 25, -1, true, false, false);
     }
 
     @Test
@@ -224,23 +223,192 @@ public void shouldParseAlterTableStatementThatAddsCharacterSetForColumns() {
         assertThat(t).isNotNull();
         assertThat(t.columnNames()).containsExactly(""col1"");
         assertThat(t.primaryKeyColumnNames()).isEmpty();
-        assertColumn(t, ""col1"", ""VARCHAR"", Types.VARCHAR, 25, -1, true, false, false);
+        assertColumn(t, ""col1"", ""VARCHAR"", Types.VARCHAR, 25, null, true);
 
         ddl = ""ALTER TABLE t MODIFY col1 VARCHAR(50) CHARACTER SET greek;"";
         parser.parse(ddl, tables);
         Table t2 = tables.forTable(new TableId(null, null, ""t""));
         assertThat(t2).isNotNull();
         assertThat(t2.columnNames()).containsExactly(""col1"");
         assertThat(t2.primaryKeyColumnNames()).isEmpty();
-        assertColumn(t2, ""col1"", ""VARCHAR CHARACTER SET greek"", Types.VARCHAR, 50, -1, true, false, false);
+        assertColumn(t2, ""col1"", ""VARCHAR"", Types.VARCHAR, 50, ""greek"", true);
 
         ddl = ""ALTER TABLE t MODIFY col1 VARCHAR(75) CHARSET utf8;"";
         parser.parse(ddl, tables);
         Table t3 = tables.forTable(new TableId(null, null, ""t""));
         assertThat(t3).isNotNull();
         assertThat(t3.columnNames()).containsExactly(""col1"");
         assertThat(t3.primaryKeyColumnNames()).isEmpty();
-        assertColumn(t3, ""col1"", ""VARCHAR CHARSET utf8"", Types.VARCHAR, 75, -1, true, false, false);
+        assertColumn(t3, ""col1"", ""VARCHAR"", Types.VARCHAR, 75, ""utf8"", true);
+    }
+
+    @Test
+    public void shouldParseCreateDatabaseAndTableThatUsesDefaultCharacterSets() {
+        String ddl = ""SET character_set_server=utf8;"" + System.lineSeparator()
+                + ""CREATE DATABASE db1 CHARACTER SET utf8mb4;"" + System.lineSeparator()
+                + ""USE db1;"" + System.lineSeparator()
+                + ""CREATE TABLE t1 ("" + System.lineSeparator()
+                + "" id int(11) not null auto_increment,"" + System.lineSeparator()
+                + "" c1 varchar(255) default null,"" + System.lineSeparator()
+                + "" c2 varchar(255) charset default not null,"" + System.lineSeparator()
+                + "" c3 varchar(255) charset latin2 not null,"" + System.lineSeparator()
+                + "" primary key ('id')"" + System.lineSeparator()
+                + "") engine=InnoDB auto_increment=1006 default charset=latin1;"" + System.lineSeparator();
+        parser.parse(ddl, tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", ""utf8mb4""); // changes when we use a different database
+        assertThat(tables.size()).isEqualTo(1);
+        Table t = tables.forTable(new TableId(""db1"", null, ""t1""));
+        assertThat(t).isNotNull();
+        assertThat(t.columnNames()).containsExactly(""id"", ""c1"", ""c2"", ""c3"");
+        assertThat(t.primaryKeyColumnNames()).containsExactly(""id"");
+        assertColumn(t, ""id"", ""INT"", Types.INTEGER, 11, -1, false, true, true);
+        assertColumn(t, ""c1"", ""VARCHAR"", Types.VARCHAR, 255, ""latin1"", true);
+        assertColumn(t, ""c2"", ""VARCHAR"", Types.VARCHAR, 255, ""latin1"", false);
+        assertColumn(t, ""c3"", ""VARCHAR"", Types.VARCHAR, 255, ""latin2"", false);
+
+        // Create a similar table but without a default charset for the table ...
+        ddl = ""CREATE TABLE t2 ("" + System.lineSeparator()
+                + "" id int(11) not null auto_increment,"" + System.lineSeparator()
+                + "" c1 varchar(255) default null,"" + System.lineSeparator()
+                + "" c2 varchar(255) charset default not null,"" + System.lineSeparator()
+                + "" c3 varchar(255) charset latin2 not null,"" + System.lineSeparator()
+                + "" primary key ('id')"" + System.lineSeparator()
+                + "") engine=InnoDB auto_increment=1006;"" + System.lineSeparator();
+        parser.parse(ddl, tables);
+        assertThat(tables.size()).isEqualTo(2);
+        Table t2 = tables.forTable(new TableId(""db1"", null, ""t2""));
+        assertThat(t2).isNotNull();
+        assertThat(t2.columnNames()).containsExactly(""id"", ""c1"", ""c2"", ""c3"");
+        assertThat(t2.primaryKeyColumnNames()).containsExactly(""id"");
+        assertColumn(t2, ""id"", ""INT"", Types.INTEGER, 11, -1, false, true, true);
+        assertColumn(t2, ""c1"", ""VARCHAR"", Types.VARCHAR, 255, ""utf8mb4"", true);
+        assertColumn(t2, ""c2"", ""VARCHAR"", Types.VARCHAR, 255, ""utf8mb4"", false);
+        assertColumn(t2, ""c3"", ""VARCHAR"", Types.VARCHAR, 255, ""latin2"", false);
+    }
+
+    @Test
+    public void shouldParseCreateDatabaseAndUseDatabaseStatementsAndHaveCharacterEncodingVariablesUpdated() {
+        parser.parse(""SET character_set_server=utf8;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", null);
+
+        parser.parse(""CREATE DATABASE db1 CHARACTER SET utf8mb4;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", null); // changes when we USE a different database
+
+        parser.parse(""USE db1;"", tables);// changes the ""character_set_database"" system variable ...
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", ""utf8mb4"");
+
+        parser.parse(""CREATE DATABASE db2 CHARACTER SET latin1;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", ""utf8mb4"");
+
+        parser.parse(""USE db2;"", tables);// changes the ""character_set_database"" system variable ...
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", ""latin1"");
+
+        parser.parse(""USE db1;"", tables);// changes the ""character_set_database"" system variable ...
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", ""utf8mb4"");
+    }
+
+    @Test
+    public void shouldParseSetCharacterSetStatement() {
+        parser.parse(""SET character_set_server=utf8;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_connection"", null);
+        assertVariable(""character_set_database"", null);
+
+        parser.parse(""SET CHARACTER SET utf8mb4;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""utf8mb4"");
+        assertVariable(""character_set_results"", ""utf8mb4"");
+        assertVariable(""character_set_connection"", null);
+        assertVariable(""character_set_database"", null);
+
+        // Set the character set to the default for the current database, or since there is none then that of the server ...
+        parser.parse(""SET CHARACTER SET default;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""utf8"");
+        assertVariable(""character_set_results"", ""utf8"");
+        assertVariable(""character_set_connection"", null);
+        assertVariable(""character_set_database"", null);
+
+        parser.parse(""SET CHARSET utf16;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""utf16"");
+        assertVariable(""character_set_results"", ""utf16"");
+        assertVariable(""character_set_connection"", null);
+        assertVariable(""character_set_database"", null);
+
+        parser.parse(""SET CHARSET default;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""utf8"");
+        assertVariable(""character_set_results"", ""utf8"");
+        assertVariable(""character_set_connection"", null);
+        assertVariable(""character_set_database"", null);
+
+        parser.parse(""CREATE DATABASE db1 CHARACTER SET cs1;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", null); // changes when we USE a different database
+
+        parser.parse(""USE db1;"", tables);// changes the ""character_set_database"" system variable ...
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", ""cs1"");
+
+        parser.parse(""SET CHARSET default;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""cs1"");
+        assertVariable(""character_set_results"", ""cs1"");
+        assertVariable(""character_set_connection"", null);
+        assertVariable(""character_set_database"", ""cs1"");
+    }
+
+    @Test
+    public void shouldParseSetNamesStatement() {
+        parser.parse(""SET character_set_server=utf8;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_connection"", null);
+        assertVariable(""character_set_database"", null);
+
+        parser.parse(""SET NAMES utf8mb4 COLLATE junk;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""utf8mb4"");
+        assertVariable(""character_set_results"", ""utf8mb4"");
+        assertVariable(""character_set_connection"", ""utf8mb4"");
+        assertVariable(""character_set_database"", null);
+
+        // Set the character set to the default for the current database, or since there is none then that of the server ...
+        parser.parse(""SET NAMES default;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""utf8"");
+        assertVariable(""character_set_results"", ""utf8"");
+        assertVariable(""character_set_connection"", ""utf8"");
+        assertVariable(""character_set_database"", null);
+
+        parser.parse(""SET NAMES utf16;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""utf16"");
+        assertVariable(""character_set_results"", ""utf16"");
+        assertVariable(""character_set_connection"", ""utf16"");
+        assertVariable(""character_set_database"", null);
+
+        parser.parse(""CREATE DATABASE db1 CHARACTER SET cs1;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", null); // changes when we USE a different database
+
+        parser.parse(""USE db1;"", tables);// changes the ""character_set_database"" system variable ...
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_database"", ""cs1"");
+
+        parser.parse(""SET NAMES default;"", tables);
+        assertVariable(""character_set_server"", ""utf8"");
+        assertVariable(""character_set_client"", ""cs1"");
+        assertVariable(""character_set_results"", ""cs1"");
+        assertVariable(""character_set_connection"", ""cs1"");
+        assertVariable(""character_set_database"", ""cs1"");
     }
 
     @Test
@@ -259,7 +427,7 @@ public void shouldParseAlterTableStatementAddColumns() {
         parser.parse(ddl, tables);
         Table t2 = tables.forTable(new TableId(null, null, ""t""));
         assertThat(t2).isNotNull();
-        assertThat(t2.columnNames()).containsExactly(""col1"",""col2"");
+        assertThat(t2.columnNames()).containsExactly(""col1"", ""col2"");
         assertThat(t2.primaryKeyColumnNames()).isEmpty();
         assertColumn(t2, ""col1"", ""VARCHAR"", Types.VARCHAR, 25, -1, true, false, false);
         assertColumn(t2, ""col2"", ""VARCHAR"", Types.VARCHAR, 50, -1, false, false, false);
@@ -270,7 +438,7 @@ public void shouldParseAlterTableStatementAddColumns() {
         parser.parse(ddl, tables);
         Table t3 = tables.forTable(new TableId(null, null, ""t""));
         assertThat(t3).isNotNull();
-        assertThat(t3.columnNames()).containsExactly(""col1"",""col3"", ""col2"");
+        assertThat(t3.columnNames()).containsExactly(""col1"", ""col3"", ""col2"");
         assertThat(t3.primaryKeyColumnNames()).isEmpty();
         assertColumn(t3, ""col1"", ""VARCHAR"", Types.VARCHAR, 25, -1, true, false, false);
         assertColumn(t3, ""col3"", ""FLOAT"", Types.FLOAT, -1, -1, false, false, false);
@@ -303,16 +471,97 @@ public void shouldParseGrantStatement() {
         assertThat(listener.total()).isEqualTo(0);
     }
 
+    @Test
+    public void shouldParseSetOfOneVariableStatementWithoutTerminator() {
+        String ddl = ""set character_set_client=utf8"";
+        parser.parse(ddl, tables);
+        assertVariable(""character_set_client"", ""utf8"");
+    }
+
+    @Test
+    public void shouldParseSetOfOneVariableStatementWithTerminator() {
+        String ddl = ""set character_set_client = utf8;"";
+        parser.parse(ddl, tables);
+        assertVariable(""character_set_client"", ""utf8"");
+    }
+
+    @Test
+    public void shouldParseSetOfSameVariableWithDifferentScope() {
+        String ddl = ""SET GLOBAL sort_buffer_size=1000000, SESSION sort_buffer_size=1000000"";
+        parser.parse(ddl, tables);
+        assertGlobalVariable(""sort_buffer_size"", ""1000000"");
+        assertSessionVariable(""sort_buffer_size"", ""1000000"");
+    }
+
+    @Test
+    public void shouldParseSetOfMultipleVariablesWithInferredScope() {
+        String ddl = ""SET GLOBAL v1=1, v2=2"";
+        parser.parse(ddl, tables);
+        assertGlobalVariable(""v1"", ""1"");
+        assertGlobalVariable(""v2"", ""2"");
+        assertSessionVariable(""v2"", null);
+    }
+
+    @Test
+    public void shouldParseSetOfGlobalVariable() {
+        String ddl = ""SET GLOBAL v1=1; SET @@global.v2=2"";
+        parser.parse(ddl, tables);
+        assertGlobalVariable(""v1"", ""1"");
+        assertGlobalVariable(""v2"", ""2"");
+        assertSessionVariable(""v1"", null);
+        assertSessionVariable(""v2"", null);
+    }
+
+    @Test
+    public void shouldParseSetOfLocalVariable() {
+        String ddl = ""SET LOCAL v1=1; SET @@local.v2=2"";
+        parser.parse(ddl, tables);
+        assertLocalVariable(""v1"", ""1"");
+        assertLocalVariable(""v2"", ""2"");
+        assertSessionVariable(""v1"", ""1"");
+        assertSessionVariable(""v2"", ""2"");
+        assertGlobalVariable(""v1"", null);
+        assertGlobalVariable(""v2"", null);
+    }
+
+    @Test
+    public void shouldParseSetOfSessionVariable() {
+        String ddl = ""SET SESSION v1=1; SET @@session.v2=2"";
+        parser.parse(ddl, tables);
+        assertLocalVariable(""v1"", ""1"");
+        assertLocalVariable(""v2"", ""2"");
+        assertSessionVariable(""v1"", ""1"");
+        assertSessionVariable(""v2"", ""2"");
+        assertGlobalVariable(""v1"", null);
+        assertGlobalVariable(""v2"", null);
+    }
+
+    @Test
+    public void shouldParseButNotSetUserVariableWithHyphenDelimiter() {
+        String ddl = ""SET @a-b-c-d:=1"";
+        parser.parse(ddl, tables);
+        assertLocalVariable(""a-b-c-d"", null);
+        assertSessionVariable(""a-b-c-d"", null);
+        assertGlobalVariable(""a-b-c-d"", null);
+    }
+
+    @Test
+    public void shouldParseVariableWithHyphenDelimiter() {
+        String ddl = ""SET a-b-c-d=1"";
+        parser.parse(ddl, tables);
+        assertSessionVariable(""a-b-c-d"", ""1"");
+    }
+
     @Test
     public void shouldParseStatementsWithQuotedIdentifiers() {
         parser.parse(readFile(""ddl/mysql-quoted.ddl""), tables);
         Testing.print(tables);
         assertThat(tables.size()).isEqualTo(4);
         assertThat(listener.total()).isEqualTo(10);
-        assertThat(tables.forTable(""connector_test_ro"",null,""products"")).isNotNull();
-        assertThat(tables.forTable(""connector_test_ro"",null,""products_on_hand"")).isNotNull();
-        assertThat(tables.forTable(""connector_test_ro"",null,""customers"")).isNotNull();
-        assertThat(tables.forTable(""connector_test_ro"",null,""orders"")).isNotNull();
+        assertThat(tables.forTable(""connector_test_ro"", null, ""products"")).isNotNull();
+        assertThat(tables.forTable(""connector_test_ro"", null, ""products_on_hand"")).isNotNull();
+        assertThat(tables.forTable(""connector_test_ro"", null, ""customers"")).isNotNull();
+        assertThat(tables.forTable(""connector_test_ro"", null, ""orders"")).isNotNull();
     }
 
     @Test
@@ -345,7 +594,7 @@ public void shouldParseTestStatements() {
         Testing.print(tables);
         assertThat(tables.size()).isEqualTo(6);
         assertThat(listener.total()).isEqualTo(62);
-        // listener.forEach(this::printEvent);
+        listener.forEach(this::printEvent);
     }
 
     @Test
@@ -378,32 +627,62 @@ public void shouldParseTicketMonsterLiquibaseStatements() {
         assertThat(listener.total()).isEqualTo(16);
         listener.forEach(this::printEvent);
     }
-    
+
     @Test
     public void shouldParseEnumOptions() {
-        assertParseEnumAndSetOptions(""ENUM('a','b','c')"",""abc"");
-        assertParseEnumAndSetOptions(""ENUM('a')"",""a"");
-        assertParseEnumAndSetOptions(""ENUM()"","""");
-        assertParseEnumAndSetOptions(""ENUM ('a','b','c') CHARACTER SET"",""abc"");
-        assertParseEnumAndSetOptions(""ENUM ('a') CHARACTER SET"",""a"");
-        assertParseEnumAndSetOptions(""ENUM () CHARACTER SET"","""");
+        assertParseEnumAndSetOptions(""ENUM('a','b','c')"", ""abc"");
+        assertParseEnumAndSetOptions(""ENUM('a')"", ""a"");
+        assertParseEnumAndSetOptions(""ENUM()"", """");
+        assertParseEnumAndSetOptions(""ENUM ('a','b','c') CHARACTER SET"", ""abc"");
+        assertParseEnumAndSetOptions(""ENUM ('a') CHARACTER SET"", ""a"");
+        assertParseEnumAndSetOptions(""ENUM () CHARACTER SET"", """");
     }
-    
+
     @Test
     public void shouldParseSetOptions() {
-        assertParseEnumAndSetOptions(""SET('a','b','c')"",""abc"");
-        assertParseEnumAndSetOptions(""SET('a')"",""a"");
-        assertParseEnumAndSetOptions(""SET()"","""");
-        assertParseEnumAndSetOptions(""SET ('a','b','c') CHARACTER SET"",""abc"");
-        assertParseEnumAndSetOptions(""SET ('a') CHARACTER SET"",""a"");
-        assertParseEnumAndSetOptions(""SET () CHARACTER SET"","""");
+        assertParseEnumAndSetOptions(""SET('a','b','c')"", ""abc"");
+        assertParseEnumAndSetOptions(""SET('a')"", ""a"");
+        assertParseEnumAndSetOptions(""SET()"", """");
+        assertParseEnumAndSetOptions(""SET ('a','b','c') CHARACTER SET"", ""abc"");
+        assertParseEnumAndSetOptions(""SET ('a') CHARACTER SET"", ""a"");
+        assertParseEnumAndSetOptions(""SET () CHARACTER SET"", """");
     }
 
-    protected void assertParseEnumAndSetOptions( String typeExpression, String optionString ) {
+    protected void assertParseEnumAndSetOptions(String typeExpression, String optionString) {
         String options = MySqlDdlParser.parseSetAndEnumOptions(typeExpression);
         assertThat(options).isEqualTo(optionString);
     }
-    
+
+    protected void assertVariable(String name, String expectedValue) {
+        String actualValue = parser.systemVariables().getVariable(name);
+        if (expectedValue == null) {
+            assertThat(actualValue).isNull();
+        } else {
+            assertThat(actualValue).isEqualToIgnoringCase(expectedValue);
+        }
+    }
+
+    protected void assertVariable(MySqlSystemVariables.Scope scope, String name, String expectedValue) {
+        String actualValue = parser.systemVariables().getVariable(name, scope);
+        if (expectedValue == null) {
+            assertThat(actualValue).isNull();
+        } else {
+            assertThat(actualValue).isEqualToIgnoringCase(expectedValue);
+        }
+    }
+
+    protected void assertGlobalVariable(String name, String expectedValue) {
+        assertVariable(MySqlSystemVariables.Scope.GLOBAL, name, expectedValue);
+    }
+
+    protected void assertSessionVariable(String name, String expectedValue) {
+        assertVariable(MySqlSystemVariables.Scope.SESSION, name, expectedValue);
+    }
+
+    protected void assertLocalVariable(String name, String expectedValue) {
+        assertVariable(MySqlSystemVariables.Scope.LOCAL, name, expectedValue);
+    }
+
     protected void printEvent(Event event) {
         Testing.print(event);
     }
@@ -444,6 +723,20 @@ protected String readLines(int startingLineNumber, String classpathResource) {
         return null;
     }
 
+    protected void assertColumn(Table table, String name, String typeName, int jdbcType, int length,
+                                String charsetName, boolean optional) {
+        Column column = table.columnWithName(name);
+        assertThat(column.name()).isEqualTo(name);
+        assertThat(column.typeName()).isEqualTo(typeName);
+        assertThat(column.jdbcType()).isEqualTo(jdbcType);
+        assertThat(column.length()).isEqualTo(length);
+        assertThat(column.charsetName()).isEqualTo(charsetName);
+        assertThat(column.scale()).isEqualTo(-1);
+        assertThat(column.isOptional()).isEqualTo(optional);
+        assertThat(column.isGenerated()).isFalse();
+        assertThat(column.isAutoIncremented()).isFalse();
+    }
+
     protected void assertColumn(Table table, String name, String typeName, int jdbcType, int length, int scale,
                                 boolean optional, boolean generated, boolean autoIncremented) {
         Column column = table.columnWithName(name);",2016-08-29T20:14:05Z,26
"@@ -646,7 +646,8 @@ public void readSchema(Tables tables, String databaseCatalog, String schemaNameP
             // Then define the table ...
             List<Column> columns = columnsByTable.get(id);
             Collections.sort(columns);
-            tables.overwriteTable(id, columns, pkColumnNames);
+            String defaultCharsetName = null; // JDBC does not expose character sets
+            tables.overwriteTable(id, columns, pkColumnNames, defaultCharsetName);
         }
 
         if (removeTablesNotFoundInJdbc) {",2016-08-29T20:14:05Z,9
"@@ -20,6 +20,7 @@ public interface Column extends Comparable<Column> {
 
     /**
      * Obtain an column definition editor that can be used to define a column.
+     * 
      * @return the editor; never null
      */
     public static ColumnEditor editor() {
@@ -62,6 +63,14 @@ public static ColumnEditor editor() {
      */
     String typeExpression();
 
+    /**
+     * Get the database-specific name of the character set used by this column.
+     * 
+     * @return the database-specific character set name, or null if the column's data type doesn't {@link #typeUsesCharset() use
+     *         character sets} or no character set is specified
+     */
+    String charsetName();
+
     /**
      * Get the maximum length of this column's values. For numeric columns, this represents the precision.
      * 
@@ -83,7 +92,7 @@ public static ColumnEditor editor() {
      * @see #isRequired()
      */
     boolean isOptional();
-    
+
     /**
      * Determine whether this column is required. This is equivalent to calling {@code !isOptional()}.
      * 
@@ -119,16 +128,28 @@ default int compareTo(Column that) {
      * 
      * @return the editor; never null
      */
-    default ColumnEditor edit() {
-        return Column.editor()
-                               .name(name())
-                               .type(typeName(),typeExpression())
-                               .jdbcType(jdbcType())
-                               .length(length())
-                               .scale(scale())
-                               .position(position())
-                               .optional(isOptional())
-                               .autoIncremented(isAutoIncremented())
-                               .generated(isGenerated());
+    ColumnEditor edit();
+
+    /**
+     * Determine whether this column has a {@link #typeName()} or {@link #jdbcType()} to which a character set applies.
+     * 
+     * @return {@code true} if a character set applies the column's type, or {@code false} otherwise
+     */
+    default boolean typeUsesCharset() {
+        switch (jdbcType()) {
+            case Types.CHAR:
+            case Types.VARCHAR:
+            case Types.LONGVARCHAR:
+            case Types.CLOB:
+            case Types.NCHAR:
+            case Types.NVARCHAR:
+            case Types.LONGNVARCHAR:
+            case Types.NCLOB:
+            case Types.DATALINK:
+            case Types.SQLXML:
+                return true;
+            default:
+                return false;
+        }
     }
 }",2016-08-29T20:14:05Z,90
"@@ -53,6 +53,22 @@ public interface ColumnEditor extends Comparable<Column> {
      */
     String typeExpression();
 
+    /**
+     * Get the database-specific name of the character set used by this column.
+     * 
+     * @return the database-specific character set name, or null if the column's data type doesn't use character sets or no
+     * character set is specified
+     */
+    String charsetName();
+
+    /**
+     * Get the database-specific name of the character set defined by this column's table, which is used if a character set is
+     * not explicitly set on this column.
+     * 
+     * @return the database-specific character set name defined for this column's table, or null if not defined
+     */
+    String charsetNameOfTable();
+
     /**
      * Get the maximum length of this column's values. For numeric columns, this represents the precision.
      * 
@@ -123,6 +139,22 @@ public interface ColumnEditor extends Comparable<Column> {
      */
     ColumnEditor jdbcType(int jdbcType);
 
+    /**
+     * Set the database-specific name of the character set used by this column.
+     * 
+     * @param charsetName the database-specific character set name; may be null
+     * @return this editor so callers can chain methods together
+     */
+    ColumnEditor charsetName(String charsetName);
+
+    /**
+     * Set the database-specific name of the character set defined by this column's table.
+     * 
+     * @param charsetName the database-specific character set name; may be null
+     * @return this editor so callers can chain methods together
+     */
+    ColumnEditor charsetNameOfTable(String charsetName);
+
     /**
      * Set the maximum length of this column's values. For numeric columns, this represents the precision.
      * ",2016-08-29T20:14:05Z,91
"@@ -13,6 +13,8 @@ final class ColumnEditorImpl implements ColumnEditor {
     private int jdbcType = Types.INTEGER;
     private String typeName;
     private String typeExpression;
+    private String charsetName;
+    private String tableCharsetName;
     private int length = -1;
     private int scale = -1;
     private int position = 1;
@@ -42,7 +44,17 @@ public String typeExpression() {
     public int jdbcType() {
         return jdbcType;
     }
-
+    
+    @Override
+    public String charsetName() {
+        return charsetName;
+    }
+    
+    @Override
+    public String charsetNameOfTable() {
+        return tableCharsetName;
+    }
+    
     @Override
     public int length() {
         return length;
@@ -99,6 +111,18 @@ public ColumnEditorImpl jdbcType(int jdbcType) {
         return this;
     }
 
+    @Override
+    public ColumnEditor charsetName(String charsetName) {
+        this.charsetName = charsetName;
+        return this;
+    }
+    
+    @Override
+    public ColumnEditor charsetNameOfTable(String charsetName) {
+        this.tableCharsetName = charsetName;
+        return this;
+    }
+
     @Override
     public ColumnEditorImpl length(int length) {
         assert length >= -1;
@@ -139,7 +163,7 @@ public ColumnEditorImpl position(int position) {
 
     @Override
     public Column create() {
-        return new ColumnImpl(name, position, jdbcType, typeName, typeExpression, length, scale, optional, autoIncremented, generated);
+        return new ColumnImpl(name, position, jdbcType, typeName, typeExpression, charsetName, tableCharsetName, length, scale, optional, autoIncremented, generated);
     }
 
     @Override",2016-08-29T20:14:05Z,92
"@@ -5,25 +5,35 @@
  */
 package io.debezium.relational;
 
+import io.debezium.util.Strings;
+
 final class ColumnImpl implements Column, Comparable<Column> {
     private final String name;
     private final int position;
     private final int jdbcType;
     private final String typeName;
     private final String typeExpression;
+    private final String charsetName;
     private final int length;
     private final int scale;
     private final boolean optional;
     private final boolean autoIncremented;
     private final boolean generated;
 
-    protected ColumnImpl(String columnName, int position, int jdbcType, String typeName, String typeExpression, int columnLength,
-            int columnScale, boolean optional, boolean autoIncremented, boolean generated) {
+    protected ColumnImpl(String columnName, int position, int jdbcType, String typeName, String typeExpression,
+                         String charsetName, String defaultCharsetName, int columnLength, int columnScale,
+                         boolean optional, boolean autoIncremented, boolean generated) {
         this.name = columnName;
         this.position = position;
         this.jdbcType = jdbcType;
         this.typeName = typeName;
         this.typeExpression = typeExpression;
+        // We want to always capture the charset name for the column (if the column needs one) ...
+        if ( typeUsesCharset() && (charsetName == null || ""DEFAULT"".equalsIgnoreCase(charsetName)) ) {
+            // Use the default charset name ...
+            charsetName = defaultCharsetName;
+        }
+        this.charsetName = charsetName;
         this.length = columnLength;
         this.scale = columnScale;
         this.optional = optional;
@@ -58,6 +68,11 @@ public String typeExpression() {
         return typeExpression;
     }
 
+    @Override
+    public String charsetName() {
+        return charsetName;
+    }
+    
     @Override
     public int length() {
         return length;
@@ -97,6 +112,7 @@ public boolean equals(Object obj) {
                     this.typeExpression().equalsIgnoreCase(that.typeExpression()) &&
                     this.typeName().equalsIgnoreCase(that.typeName()) &&
                     this.jdbcType() == that.jdbcType() &&
+                    Strings.equalsIgnoreCase(this.charsetName(),that.charsetName()) &&
                     this.position() == that.position() &&
                     this.length() == that.length() &&
                     this.scale() == that.scale() &&
@@ -118,10 +134,28 @@ public String toString() {
             }
             sb.append(')');
         }
+        if (charsetName != null && !charsetName.isEmpty()) {
+            sb.append("" CHARSET "").append(charsetName);
+        }
         if (!optional) sb.append("" NOT NULL"");
         if (autoIncremented) sb.append("" AUTO_INCREMENTED"");
         if (generated) sb.append("" GENERATED"");
         return sb.toString();
     }
+    
+    @Override
+    public ColumnEditor edit()  {
+        return Column.editor()
+                .name(name())
+                .type(typeName(), typeExpression())
+                .jdbcType(jdbcType())
+                .charsetName(charsetName)
+                .length(length())
+                .scale(scale())
+                .position(position())
+                .optional(isOptional())
+                .autoIncremented(isAutoIncremented())
+                .generated(isGenerated());
+}
 
 }
\ No newline at end of file",2016-08-29T20:14:05Z,93
"@@ -94,6 +94,14 @@ default List<String> filterColumnNames( Predicate<Column> predicate ) {
      */
     Column columnWithName(String name);
 
+    /**
+     * Get the database-specific name of the default character set used by columns in this table.
+     * 
+     * @return the database-specific character set name used by default in columns of this table, or {@code null} if there is no
+     * such default character set name defined on the table
+     */
+    String defaultCharsetName();
+
     /**
      * Determine if the named column is part of the primary key.
      * ",2016-08-29T20:14:05Z,94
"@@ -186,6 +186,20 @@ default TableEditor addColumn(Column column) {
      */
     TableEditor setUniqueValues();
     
+    /**
+     * Set the name of the character set that should be used by default in the columns that require a character set but have
+     * not defined one.
+     * @param charsetName the name of the character set that should be used by default
+     * @return this editor so callers can chain methods together
+     */
+    TableEditor setDefaultCharsetName(String charsetName);
+
+    /**
+     * Determine if a {@link #setDefaultCharsetName(String) default character set} has been set on this table.
+     * @return {@code true} if this has a default character set, or {@code false} if one has not yet been set
+     */
+    boolean hasDefaultCharsetName();
+    
     /**
      * Determine whether this table's primary key contains all columns (via {@link #setUniqueValues()}) such that all rows
      * within the table are unique.",2016-08-29T20:14:05Z,95
"@@ -18,6 +18,7 @@ final class TableEditorImpl implements TableEditor {
     private LinkedHashMap<String, Column> sortedColumns = new LinkedHashMap<>();
     private final List<String> pkColumnNames = new ArrayList<>();
     private boolean uniqueValues = false;
+    private String defaultCharsetName;
 
     protected TableEditorImpl() {
     }
@@ -142,6 +143,17 @@ public TableEditor setUniqueValues() {
     public boolean hasUniqueValues() {
         return uniqueValues;
     }
+    
+    @Override
+    public TableEditor setDefaultCharsetName(String charsetName) {
+        this.defaultCharsetName = charsetName;
+        return this;
+    }
+    
+    @Override
+    public boolean hasDefaultCharsetName() {
+        return this.defaultCharsetName != null && !this.defaultCharsetName.trim().isEmpty();
+    }
 
     @Override
     public TableEditor removeColumn(String columnName) {
@@ -231,6 +243,11 @@ public String toString() {
     @Override
     public Table create() {
         if (id == null) throw new IllegalStateException(""Unable to create a table from an editor that has no table ID"");
-        return new TableImpl(id, new ArrayList<>(sortedColumns.values()), primaryKeyColumnNames());
+        List<Column> columns = new ArrayList<>();
+        sortedColumns.values().forEach(column->{
+            column = column.edit().charsetNameOfTable(defaultCharsetName).create();
+            columns.add(column);
+        });
+        return new TableImpl(id, columns, primaryKeyColumnNames(), defaultCharsetName);
     }
 }",2016-08-29T20:14:05Z,96
"@@ -11,19 +11,22 @@
 import java.util.List;
 import java.util.Map;
 
+import io.debezium.util.Strings;
+
 final class TableImpl implements Table {
 
     private final TableId id;
     private final List<Column> columnDefs;
     private final List<String> pkColumnNames;
     private final List<String> columnNames;
     private final Map<String, Column> columnsByLowercaseName;
+    private final String defaultCharsetName;
 
     protected TableImpl(Table table) {
-        this(table.id(), table.columns(), table.primaryKeyColumnNames());
+        this(table.id(), table.columns(), table.primaryKeyColumnNames(), table.defaultCharsetName());
     }
 
-    protected TableImpl(TableId id, List<Column> sortedColumns, List<String> pkColumnNames) {
+    protected TableImpl(TableId id, List<Column> sortedColumns, List<String> pkColumnNames, String defaultCharsetName) {
         this.id = id;
         this.columnDefs = Collections.unmodifiableList(sortedColumns);
         this.pkColumnNames = pkColumnNames == null ? Collections.emptyList() : Collections.unmodifiableList(pkColumnNames);
@@ -35,8 +38,9 @@ protected TableImpl(TableId id, List<Column> sortedColumns, List<String> pkColum
         }
         this.columnsByLowercaseName = Collections.unmodifiableMap(defsByLowercaseName);
         this.columnNames = Collections.unmodifiableList(columnNames);
+        this.defaultCharsetName = defaultCharsetName;
     }
-    
+
     @Override
     public TableId id() {
         return id;
@@ -61,20 +65,26 @@ public List<Column> columns() {
     public Column columnWithName(String name) {
         return columnsByLowercaseName.get(name.toLowerCase());
     }
-    
+
+    @Override
+    public String defaultCharsetName() {
+        return defaultCharsetName;
+    }
+
     @Override
     public int hashCode() {
         return id.hashCode();
     }
-    
+
     @Override
     public boolean equals(Object obj) {
-        if ( obj== this) return true;
-        if ( obj instanceof Table ) {
-            Table that = (Table)obj;
-            return  this.id().equals(that.id())
+        if (obj == this) return true;
+        if (obj instanceof Table) {
+            Table that = (Table) obj;
+            return this.id().equals(that.id())
                     && this.columns().equals(that.columns())
-                    && this.primaryKeyColumnNames().equals(that.primaryKeyColumnNames());
+                    && this.primaryKeyColumnNames().equals(that.primaryKeyColumnNames())
+                    && Strings.equalsIgnoreCase(this.defaultCharsetName(), that.defaultCharsetName());
         }
         return false;
     }
@@ -85,7 +95,7 @@ public String toString() {
         toString(sb, """");
         return sb.toString();
     }
-    
+
     protected void toString(StringBuilder sb, String prefix) {
         if (prefix == null) prefix = """";
         sb.append(prefix).append(""columns: {"").append(System.lineSeparator());
@@ -94,10 +104,14 @@ protected void toString(StringBuilder sb, String prefix) {
         }
         sb.append(prefix).append(""}"").append(System.lineSeparator());
         sb.append(prefix).append(""primary key: "").append(primaryKeyColumnNames()).append(System.lineSeparator());
+        sb.append(prefix).append(""default charset: "").append(defaultCharsetName()).append(System.lineSeparator());
     }
 
     @Override
     public TableEditor edit() {
-        return new TableEditorImpl().tableId(id).setColumns(columnDefs).setPrimaryKeyNames(pkColumnNames);
+        return new TableEditorImpl().tableId(id)
+                                    .setColumns(columnDefs)
+                                    .setPrimaryKeyNames(pkColumnNames)
+                                    .setDefaultCharsetName(defaultCharsetName);
     }
 }
\ No newline at end of file",2016-08-29T20:14:05Z,97
"@@ -120,11 +120,13 @@ public Set<TableId> drainChanges() {
      * @param tableId the identifier of the table
      * @param columnDefs the list of column definitions; may not be null or empty
      * @param primaryKeyColumnNames the list of the column names that make up the primary key; may be null or empty
+     * @param defaultCharsetName the name of the character set that should be used by default
      * @return the previous table definition, or null if there was no prior table definition
      */
-    public Table overwriteTable(TableId tableId, List<Column> columnDefs, List<String> primaryKeyColumnNames) {
+    public Table overwriteTable(TableId tableId, List<Column> columnDefs, List<String> primaryKeyColumnNames,
+                                String defaultCharsetName) {
         return lock.write(() -> {
-            TableImpl updated = new TableImpl(tableId, columnDefs, primaryKeyColumnNames);
+            TableImpl updated = new TableImpl(tableId, columnDefs, primaryKeyColumnNames, defaultCharsetName);
             TableImpl existing = tablesByTableId.get(tableId);
             if ( existing == null || !existing.equals(updated) ) {
                 // Our understanding of the table has changed ...
@@ -164,7 +166,8 @@ public Table renameTable(TableId existingTableId, TableId newTableId) {
             Table existing = forTable(existingTableId);
             if (existing == null) return null;
             tablesByTableId.remove(existing);
-            TableImpl updated = new TableImpl(newTableId, existing.columns(), existing.primaryKeyColumnNames());
+            TableImpl updated = new TableImpl(newTableId, existing.columns(),
+                                              existing.primaryKeyColumnNames(), existing.defaultCharsetName());
             try {
                 return tablesByTableId.put(updated.id(), updated);
             } finally {
@@ -187,7 +190,8 @@ public Table updateTable(TableId tableId, Function<Table, Table> changer) {
             TableImpl existing = tablesByTableId.get(tableId);
             Table updated = changer.apply(existing);
             if (updated != existing) {
-                tablesByTableId.put(tableId, new TableImpl(tableId, updated.columns(), updated.primaryKeyColumnNames()));
+                tablesByTableId.put(tableId, new TableImpl(tableId, updated.columns(),
+                                                           updated.primaryKeyColumnNames(), updated.defaultCharsetName()));
             }
             changes.add(tableId);
             return existing;
@@ -208,7 +212,7 @@ public Table updateTable(TableId tableId, TableChanger changer) {
             List<Column> columns = new ArrayList<>(existing.columns());
             List<String> pkColumnNames = new ArrayList<>(existing.primaryKeyColumnNames());
             changer.rewrite(columns, pkColumnNames);
-            TableImpl updated = new TableImpl(tableId, columns, pkColumnNames);
+            TableImpl updated = new TableImpl(tableId, columns, pkColumnNames, existing.defaultCharsetName());
             tablesByTableId.put(tableId, updated);
             changes.add(tableId);
             return existing;",2016-08-29T20:14:05Z,98
"@@ -128,6 +128,8 @@ protected String getDatabase(Event event) {
             case DROP_DATABASE:
                 DatabaseEvent dbEvent = (DatabaseEvent) event;
                 return dbEvent.databaseName();
+            case SET_VARIABLE:
+                return """";
         }
         assert false : ""Should never happen"";
         return null;",2016-08-29T20:14:05Z,99
"@@ -577,7 +577,9 @@ protected void consumeStatement() throws ParsingException {
      */
     protected void consumeRemainingStatement(Marker start) {
         while (tokens.hasNext()) {
-            if (tokens.matches(DdlTokenizer.STATEMENT_KEY)) break;
+            if (tokens.matches(DdlTokenizer.STATEMENT_KEY)) {
+                break;
+            }
             if (tokens.canConsume(""BEGIN"")) {
                 tokens.consumeThrough(""END"");
             } else if (tokens.matches(DdlTokenizer.STATEMENT_TERMINATOR)) {",2016-08-29T20:14:05Z,65
"@@ -34,6 +34,7 @@ public static enum EventType {
         CREATE_TABLE, ALTER_TABLE, DROP_TABLE,
         CREATE_INDEX, DROP_INDEX,
         CREATE_DATABASE, ALTER_DATABASE, DROP_DATABASE,
+        SET_VARIABLE,
     }
 
     /**
@@ -283,4 +284,40 @@ public DatabaseDroppedEvent(String databaseName, String ddlStatement) {
         }
     }
 
+    /**
+     * An event describing the setting of a variable.
+     */
+    @Immutable
+    public static class SetVariableEvent extends Event {
+        
+        private final String variableName;
+        private final String value;
+
+        public SetVariableEvent(String variableName, String value, String ddlStatement) {
+            super(EventType.SET_VARIABLE, ddlStatement);
+            this.variableName = variableName;
+            this.value = value;
+        }
+
+        /**
+         * Get the name of the variable that was set.
+         * @return the variable name; never null
+         */
+        public String variableName() {
+            return variableName;
+        }
+        
+        /**
+         * Get the value of the variable that was set.
+         * @return the variable value; may be null
+         */
+        public String variableValue() {
+            return value;
+        }
+        
+        @Override
+        public String toString() {
+            return statement();
+        }
+    }
 }",2016-08-29T20:14:05Z,100
"@@ -213,7 +213,8 @@
  * <pre>
  * public class BasicTokenizer implements Tokenizer {
  *     public void tokenize(CharacterStream input,
- *                          Tokens tokens) throws ParsingException {
+ *                          Tokens tokens)
+ *             throws ParsingException {
  *         while (input.hasNext()) {
  *             char c = input.next();
  *             switch (c) {
@@ -481,7 +482,7 @@ public void rewind() {
      * @throws NoSuchElementException if there are no more tokens
      */
     public Marker mark() {
-        if ( completed ) {
+        if (completed) {
             return new Marker(null, tokenIterator.previousIndex());
         }
         Token currentToken = currentToken();
@@ -741,7 +742,8 @@ public TokenStream consume(int expectedType) throws ParsingException, IllegalSta
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public TokenStream consume(String expected,
-                               String... expectedForNextTokens) throws ParsingException, IllegalStateException {
+                               String... expectedForNextTokens)
+            throws ParsingException, IllegalStateException {
         consume(expected);
         for (String nextExpected : expectedForNextTokens) {
             consume(nextExpected);
@@ -850,7 +852,7 @@ public String consumeAnyOf(String... options) throws IllegalStateException {
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public TokenStream consumeThrough(char expected) throws ParsingException, IllegalStateException {
-        return consumeThrough(String.valueOf(expected),null);
+        return consumeThrough(String.valueOf(expected), null);
     }
 
     /**
@@ -862,13 +864,13 @@ public TokenStream consumeThrough(char expected) throws ParsingException, Illega
      * 
      * @param expected the token that is to be found
      * @param skipMatchingTokens the token that, if found, should result in skipping {@code expected} once for each occurrence
-     * of {@code skipMatchingTokens}; may be null
+     *            of {@code skipMatchingTokens}; may be null
      * @return this token stream instance so callers can chain together methods; never null
      * @throws ParsingException if the specified token cannot be found
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public TokenStream consumeThrough(char expected, char skipMatchingTokens) throws ParsingException, IllegalStateException {
-        return consumeThrough(String.valueOf(expected),String.valueOf(skipMatchingTokens));
+        return consumeThrough(String.valueOf(expected), String.valueOf(skipMatchingTokens));
     }
 
     /**
@@ -884,8 +886,9 @@ public TokenStream consumeThrough(char expected, char skipMatchingTokens) throws
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public TokenStream consumeThrough(String expected) throws ParsingException, IllegalStateException {
-        return consumeThrough(expected,null);
+        return consumeThrough(expected, null);
     }
+
     /**
      * Attempt to consume all tokens until the specified token is consumed, and then stop. If it is not found, then the token
      * stream is left untouched and a ParsingException is thrown.
@@ -895,7 +898,7 @@ public TokenStream consumeThrough(String expected) throws ParsingException, Ille
      * 
      * @param expected the token that is to be found
      * @param skipMatchingTokens the token that, if found, should result in skipping {@code expected} once for each occurrence
-     * of {@code skipMatchingTokens}; may be null
+     *            of {@code skipMatchingTokens}; may be null
      * @return this token stream instance so callers can chain together methods; never null
      * @throws ParsingException if the specified token cannot be found
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
@@ -905,7 +908,7 @@ public TokenStream consumeThrough(String expected, String skipMatchingTokens) th
             consume();
             return this;
         }
-        consumeUntil(expected,skipMatchingTokens);
+        consumeUntil(expected, skipMatchingTokens);
         consume(expected);
         return this;
     }
@@ -935,7 +938,7 @@ public TokenStream consumeUntil(char expected) throws ParsingException, IllegalS
      * 
      * @param expected the token that is to be found
      * @param skipMatchingTokens the token that, if found, should result in skipping {@code expected} once for each occurrence
-     * of {@code skipMatchingTokens}
+     *            of {@code skipMatchingTokens}
      * @return this token stream instance so callers can chain together methods; never null
      * @throws ParsingException if the specified token cannot be found
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
@@ -957,7 +960,7 @@ public TokenStream consumeUntil(char expected, char skipMatchingTokens) throws P
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public TokenStream consumeUntil(String expected) throws ParsingException, IllegalStateException {
-        return consumeUntil(expected,null);
+        return consumeUntil(expected, null);
     }
 
     /**
@@ -969,7 +972,7 @@ public TokenStream consumeUntil(String expected) throws ParsingException, Illega
      * 
      * @param expected the token that is to be found
      * @param skipMatchingTokens the token that, if found, should result in skipping {@code expected} once for each occurrence
-     * of {@code skipMatchingTokens}; may be null
+     *            of {@code skipMatchingTokens}; may be null
      * @return this token stream instance so callers can chain together methods; never null
      * @throws ParsingException if the specified token cannot be found
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
@@ -982,9 +985,9 @@ public TokenStream consumeUntil(String expected, String skipMatchingTokens) thro
         Marker start = mark();
         int remaining = 0;
         while (hasNext()) {
-            if ( skipMatchingTokens != null && matches(skipMatchingTokens)) ++remaining;
-            if ( matches(expected) ) {
-                if ( remaining == 0 ) {
+            if (skipMatchingTokens != null && matches(skipMatchingTokens)) ++remaining;
+            if (matches(expected)) {
+                if (remaining == 0) {
                     break;
                 }
                 --remaining;
@@ -999,6 +1002,25 @@ public TokenStream consumeUntil(String expected, String skipMatchingTokens) thro
         return this;
     }
 
+    /**
+     * Consume the token stream until one of the stop tokens or the end of the stream is found.
+     * 
+     * @param stopTokens the stop tokens; may not be null
+     * @return this token stream instance so callers can chain together methods; never null
+     * @throws ParsingException if none of the specified tokens can be found
+     * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
+     */
+    public TokenStream consumeUntilEndOrOneOf(String... stopTokens)
+            throws ParsingException, IllegalStateException {
+        while (hasNext()) {
+            if (matchesAnyOf(stopTokens)) {
+                break;
+            }
+            consume();
+        }
+        return this;
+    }
+
     /**
      * Attempt to consume this current token if it can be parsed as an integer, and return whether this method was indeed able to
      * consume the token.
@@ -1149,7 +1171,8 @@ public boolean canConsume(int expectedType) throws IllegalStateException {
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public boolean canConsume(String currentExpected,
-                              String... expectedForNextTokens) throws IllegalStateException {
+                              String... expectedForNextTokens)
+            throws IllegalStateException {
         if (completed) return false;
         ListIterator<Token> iter = tokens.listIterator(tokenIterator.previousIndex());
         if (!iter.hasNext()) return false;
@@ -1264,7 +1287,8 @@ public boolean canConsume(Iterable<String> nextTokens) throws IllegalStateExcept
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public boolean canConsumeAnyOf(String firstOption,
-                                   String... additionalOptions) throws IllegalStateException {
+                                   String... additionalOptions)
+            throws IllegalStateException {
         if (completed) return false;
         if (canConsume(firstOption)) return true;
         for (String nextOption : additionalOptions) {
@@ -1312,7 +1336,8 @@ public boolean canConsumeAnyOf(Iterable<String> options) throws IllegalStateExce
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public boolean canConsumeAnyOf(int firstTypeOption,
-                                   int... additionalTypeOptions) throws IllegalStateException {
+                                   int... additionalTypeOptions)
+            throws IllegalStateException {
         if (completed) return false;
         if (canConsume(firstTypeOption)) return true;
         for (int nextTypeOption : additionalTypeOptions) {
@@ -1384,7 +1409,8 @@ public boolean matches(int expectedType) throws IllegalStateException {
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public boolean matches(String currentExpected,
-                           String... expectedForNextTokens) throws IllegalStateException {
+                           String... expectedForNextTokens)
+            throws IllegalStateException {
         if (completed) return false;
         ListIterator<Token> iter = tokens.listIterator(tokenIterator.previousIndex());
         if (!iter.hasNext()) return false;
@@ -1457,7 +1483,8 @@ public boolean matches(Iterable<String> nextTokens) throws IllegalStateException
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public boolean matches(int currentExpectedType,
-                           int... expectedTypeForNextTokens) throws IllegalStateException {
+                           int... expectedTypeForNextTokens)
+            throws IllegalStateException {
         if (completed) return false;
         ListIterator<Token> iter = tokens.listIterator(tokenIterator.previousIndex());
         if (!iter.hasNext()) return false;
@@ -1504,7 +1531,8 @@ public boolean matches(int[] typesForNextTokens) throws IllegalStateException {
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public boolean matchesAnyOf(String firstOption,
-                                String... additionalOptions) throws IllegalStateException {
+                                String... additionalOptions)
+            throws IllegalStateException {
         if (completed) return false;
         Token current = currentToken();
         if (current.matches(firstOption)) return true;
@@ -1555,7 +1583,8 @@ public boolean matchesAnyOf(Iterable<String> options) throws IllegalStateExcepti
      * @throws IllegalStateException if this method was called before the stream was {@link #start() started}
      */
     public boolean matchesAnyOf(int firstTypeOption,
-                                int... additionalTypeOptions) throws IllegalStateException {
+                                int... additionalTypeOptions)
+            throws IllegalStateException {
         if (completed) return false;
         Token current = currentToken();
         if (current.matches(firstTypeOption)) return true;
@@ -1762,7 +1791,8 @@ public static interface Tokenizer {
          * @throws ParsingException if there is an error while processing the character stream (e.g., a quote is not closed, etc.)
          */
         void tokenize(CharacterStream input,
-                      Tokens tokens) throws ParsingException;
+                      Tokens tokens)
+                throws ParsingException;
     }
 
     /**
@@ -2363,7 +2393,8 @@ protected BasicTokenizer(boolean useComments) {
 
         @Override
         public void tokenize(CharacterStream input,
-                             Tokens tokens) throws ParsingException {
+                             Tokens tokens)
+                throws ParsingException {
             while (input.hasNext()) {
                 char c = input.next();
                 switch (c) {",2016-08-29T20:14:05Z,101
"@@ -142,6 +142,20 @@ public static int compareTo(CharSequence str1, CharSequence str2) {
         return str1.toString().compareTo(str2.toString());
     }
 
+    /**
+     * Check whether the two {@link String} instances are equal ignoring case.
+     * 
+     * @param str1 the first character sequence; may be null
+     * @param str2 the second character sequence; may be null
+     * @return {@code true} if both are null or if the two strings are equal to each other ignoring case, or {@code false}
+     *         otherwise
+     */
+    public static boolean equalsIgnoreCase(String str1, String str2) {
+        if (str1 == str2) return true;
+        if (str1 == null) return str2 == null;
+        return str1.equalsIgnoreCase(str2);
+    }
+
     /**
      * Returns a new String composed of the supplied integer values joined together
      * with a copy of the specified {@code delimiter}.
@@ -274,11 +288,11 @@ public static String createString(final char charToRepeat,
      * @see #justifyLeft(String, int, char)
      */
     public static String pad(String original,
-                                   int length,
-                                   char padChar) {
-        if ( original.length() >= length ) return original;
+                             int length,
+                             char padChar) {
+        if (original.length() >= length) return original;
         StringBuilder sb = new StringBuilder(original);
-        while ( sb.length() < length ) {
+        while (sb.length() < length) {
             sb.append(padChar);
         }
         return sb.toString();
@@ -482,7 +496,7 @@ public static String getStackTrace(Throwable throwable) {
      * @return the number, or {@code null} if the value is not a number
      */
     public static Number asNumber(String value) {
-        return asNumber(value,null);
+        return asNumber(value, null);
     }
 
     /**",2016-08-29T20:14:05Z,102
"@@ -12,7 +12,7 @@ August XX, 2016 - [Detailed release notes](https://issues.jboss.org/browse/DBZ/f
 
 ### Backwards-incompatible changes since 0.3.0
 
-None
+* MySQL connector now properly decodes string values from the binlog based upon the column's character set encoding as read by the DDL statement. Upon upgrade and restart, the connector will re-read the recorded database history and now associate the columns with their the character sets, and any newly processed events will use properly encoded strings values. As expected, previously generated events are never altered. Force a snapshot to regenerate events for the servers. [DBZ-102](https://issues.jboss.org/projects/DBZ/issues/DBZ-102)
 
 ### Fixes since 0.3.0
 ",2016-08-29T17:19:24Z,66
"@@ -114,11 +114,13 @@ public void start(Map<String, String> props) {
                     // full history of the database.
                     logger.info(""Found no existing offset and snapshots disallowed, so starting at beginning of binlog"");
                     source.setBinlogStartPoint("""", 0L);// start from the beginning of the binlog
+                    taskContext.initializeHistory();
                 } else {
                     // We are allowed to use snapshots, and that is the best way to start ...
                     startWithSnapshot = true;
                     // The snapshot will determine if GTIDs are set
                     logger.info(""Found no existing offset, so preparing to perform a snapshot"");
+                    // The snapshot will also initialize history ...
                 }
             }
 ",2016-08-29T17:19:24Z,68
"@@ -5,9 +5,14 @@
  */
 package io.debezium.connector.mysql;
 
+import java.nio.charset.StandardCharsets;
 import java.sql.SQLException;
+import java.util.ArrayList;
+import java.util.Collections;
 import java.util.HashMap;
+import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.slf4j.Logger;
@@ -18,6 +23,7 @@
 import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
+import io.debezium.util.Strings;
 
 /**
  * A context for a JDBC connection to MySQL.
@@ -32,19 +38,21 @@ public class MySqlJdbcContext implements AutoCloseable {
     protected final Logger logger = LoggerFactory.getLogger(getClass());
     protected final Configuration config;
     protected final JdbcConnection jdbc;
-    private final Map<String,String> originalSystemProperties = new HashMap<>();
+    private final Map<String, String> originalSystemProperties = new HashMap<>();
 
     public MySqlJdbcContext(Configuration config) {
         this.config = config; // must be set before most methods are used
 
         // Set up the JDBC connection without actually connecting, with extra MySQL-specific properties
-        // to give us better JDBC database metadata behavior ...
+        // to give us better JDBC database metadata behavior, including using UTF-8 for the client-side character encoding
+        // per https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-reference-charsets.html
         boolean useSSL = sslModeEnabled();
         Configuration jdbcConfig = config.subset(""database."", true)
                                          .edit()
                                          .with(""useInformationSchema"", ""true"")
                                          .with(""nullCatalogMeansCurrent"", ""false"")
                                          .with(""useSSL"", Boolean.toString(useSSL))
+                                         .with(""characterEncoding"", StandardCharsets.UTF_8.name())
                                          .build();
         this.jdbc = new JdbcConnection(jdbcConfig, FACTORY);
     }
@@ -81,11 +89,11 @@ public SecureConnectionMode sslMode() {
         String mode = config.getString(MySqlConnectorConfig.SSL_MODE);
         return SecureConnectionMode.parse(mode);
     }
-    
+
     public boolean sslModeEnabled() {
         return sslMode() != SecureConnectionMode.DISABLED;
     }
-    
+
     public void start() {
         if (sslModeEnabled()) {
             originalSystemProperties.clear();
@@ -104,9 +112,9 @@ public void shutdown() {
             logger.error(""Unexpected error shutting down the database connection"", e);
         } finally {
             // Reset the system properties to their original value ...
-            originalSystemProperties.forEach((name,value)->{
-                if ( value != null ) {
-                    System.setProperty(name,value);
+            originalSystemProperties.forEach((name, value) -> {
+                if (value != null) {
+                    System.setProperty(name, value);
                 } else {
                     System.clearProperty(name);
                 }
@@ -123,6 +131,59 @@ protected String connectionString() {
         return jdbc.connectionString(MYSQL_CONNECTION_URL);
     }
 
+    /**
+     * Read the MySQL charset-related system variables.
+     * 
+     * @param sql the reference that should be set to the SQL statement; may be null if not needed
+     * @return the system variables that are related to server character sets; never null
+     */
+    protected Map<String, String> readMySqlCharsetSystemVariables(AtomicReference<String> sql) {
+        // Read the system variables from the MySQL instance and get the current database name ...
+        Map<String, String> variables = new HashMap<>();
+        try (JdbcConnection mysql = jdbc.connect()) {
+            logger.debug(""Reading MySQL charset-related system variables before parsing DDL history."");
+            String statement = ""SHOW VARIABLES WHERE Variable_name IN ('character_set_server','collation_server')"";
+            if (sql != null) sql.set(statement);
+            mysql.query(statement, rs -> {
+                while (rs.next()) {
+                    String varName = rs.getString(1);
+                    String value = rs.getString(2);
+                    if (varName != null && value != null) {
+                        variables.put(varName, value);
+                        logger.debug(""\t{} = {}"",
+                                     Strings.pad(varName, 45, ' '),
+                                     Strings.pad(value, 45, ' '));
+                    }
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Error reading MySQL variables: "" + e.getMessage(), e);
+        }
+        return variables;
+    }
+
+    protected String setStatementFor(Map<String, String> variables) {
+        StringBuilder sb = new StringBuilder(""SET "");
+        boolean first = true;
+        List<String> varNames = new ArrayList<>(variables.keySet());
+        Collections.sort(varNames);
+        for (String varName : varNames) {
+            if (first) {
+                first = false;
+            } else {
+                sb.append("", "");
+            }
+            sb.append(varName).append(""="");
+            String value = variables.get(varName);
+            if (value == null) value = """";
+            if (value.contains("","") || value.contains("";"")) {
+                value = ""'"" + value + ""'"";
+            }
+            sb.append(value);
+        }
+        return sb.append("";"").toString();
+    }
+
     protected void setSystemProperty(String property, Field field, boolean showValueInError) {
         String value = config.getString(field);
         if (value != null) {",2016-08-29T17:19:24Z,61
"@@ -24,6 +24,7 @@
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.TemporalPrecisionMode;
+import io.debezium.connector.mysql.MySqlSystemVariables.Scope;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.TimeZoneAdapter;
 import io.debezium.relational.Table;
@@ -106,7 +107,7 @@ public MySqlSchema(Configuration config, String serverName) {
         boolean adaptiveTimePrecision = TemporalPrecisionMode.ADAPTIVE.equals(timePrecisionMode);
         MySqlValueConverters valueConverters = new MySqlValueConverters(adaptiveTimePrecision);
         this.schemaBuilder = new TableSchemaBuilder(valueConverters, schemaNameValidator::validate);
-        
+
         // Set up the server name and schema prefix ...
         if (serverName != null) serverName = serverName.trim();
         this.serverName = serverName;
@@ -197,6 +198,26 @@ public String historyLocation() {
         return dbHistory.toString();
     }
 
+    /**
+     * Set the system variables on the DDL parser.
+     * 
+     * @param variables the system variables; may not be null but may be empty
+     */
+    public void setSystemVariables(Map<String, String> variables) {
+        variables.forEach((varName, value) -> {
+            ddlParser.systemVariables().setVariable(Scope.SESSION, varName, value);
+        });
+    }
+    
+    /**
+     * Get the system variables as known by the DDL parser.
+     * 
+     * @return the system variables; never null
+     */
+    public MySqlSystemVariables systemVariables() {
+        return ddlParser.systemVariables();
+    }
+    
     /**
      * Load the schema for the databases using JDBC database metadata. If there are changes relative to any
      * table definitions that existed when this method is called, those changes are recorded in the database history
@@ -335,12 +356,12 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
                     // the same order they were read for each _affected_ database, grouped together if multiple apply
                     // to the same _affected_ database...
                     ddlChanges.groupStatementStringsByDatabase((dbName, ddl) -> {
-                        if (filters.databaseFilter().test(dbName)) {
+                        if (filters.databaseFilter().test(dbName) || dbName == null || """".equals(dbName)) {
                             if (dbName == null) dbName = """";
                             statementConsumer.consume(dbName, ddlStatements);
                         }
                     });
-                } else if (filters.databaseFilter().test(databaseName)) {
+                } else if (filters.databaseFilter().test(databaseName) || databaseName == null || """".equals(databaseName)) {
                     if (databaseName == null) databaseName = """";
                     statementConsumer.consume(databaseName, ddlStatements);
                 }",2016-08-29T17:19:24Z,19
"@@ -5,11 +5,14 @@
  */
 package io.debezium.connector.mysql;
 
+import java.util.Map;
+
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.util.Clock;
 import io.debezium.util.LoggingContext;
 import io.debezium.util.LoggingContext.PreviousContext;
+import io.debezium.util.Strings;
 
 /**
  * A Kafka Connect source task reads the MySQL binary log and generate the corresponding data change events.
@@ -59,6 +62,19 @@ public RecordMakers makeRecord() {
         return recordProcessor;
     }
 
+    /**
+     * Initialize the database history with any server-specific information. This should be done only upon connector startup
+     * when the connector has no prior history.
+     */
+    public void initializeHistory() {
+        // Read the system variables from the MySQL instance and get the current database name ...
+        Map<String, String> variables = readMySqlCharsetSystemVariables(null);
+        String ddlStatement = setStatementFor(variables);
+
+        // And write them into the database history ...
+        dbSchema.applyDdl(source, """", ddlStatement, null);
+    }
+
     /**
      * Load the database schema information using the previously-recorded history, and stop reading the history when the
      * the history reaches the supplied starting point.
@@ -67,7 +83,23 @@ public RecordMakers makeRecord() {
      *            offset} at which the database schemas are to reflect; may not be null
      */
     public void loadHistory(SourceInfo startingPoint) {
+        // Read the system variables from the MySQL instance and load them into the DDL parser as defaults ...
+        Map<String, String> variables = readMySqlCharsetSystemVariables(null);
+        dbSchema.setSystemVariables(variables);
+
+        // And then load the history ...
         dbSchema.loadHistory(startingPoint);
+
+        // The server's default character set may have changed since we last recorded it in the history,
+        // so we need to see if the history's state does not match ...
+        String systemCharsetName = variables.get(MySqlSystemVariables.CHARSET_NAME_SERVER);
+        String systemCharsetNameFromHistory = dbSchema.systemVariables().getVariable(MySqlSystemVariables.CHARSET_NAME_SERVER);
+        if (!Strings.equalsIgnoreCase(systemCharsetName, systemCharsetNameFromHistory)) {
+            // The history's server character set is NOT the same as the server's current default,
+            // so record the change in the history ...
+            String ddlStatement = setStatementFor(variables);
+            dbSchema.applyDdl(source, """", ddlStatement, null);
+        }
         recordProcessor.regenerate();
     }
 
@@ -81,7 +113,7 @@ public long serverId() {
 
     public String serverName() {
         String serverName = config.getString(MySqlConnectorConfig.SERVER_NAME);
-        if ( serverName == null ) {
+        if (serverName == null) {
             serverName = hostname() + "":"" + port();
         }
         return serverName;
@@ -102,7 +134,7 @@ public long timeoutInMilliseconds() {
     public long pollIntervalInMillseconds() {
         return config.getLong(MySqlConnectorConfig.POLL_INTERVAL_MS);
     }
-    
+
     public long rowCountForLargeTable() {
         return config.getLong(MySqlConnectorConfig.ROW_COUNT_FOR_STREAMING_RESULT_SETS);
     }
@@ -150,14 +182,15 @@ public void shutdown() {
 
     /**
      * Configure the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * 
      * @param contextName the name of the context; may not be null
      * @return the previous MDC context; never null
      * @throws IllegalArgumentException if {@code contextName} is null
      */
     public PreviousContext configureLoggingContext(String contextName) {
         return LoggingContext.forConnector(""MySQL"", serverName(), contextName);
     }
-    
+
     /**
      * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
      * state before this method was called.",2016-08-29T17:19:24Z,69
"@@ -5,14 +5,20 @@
  */
 package io.debezium.connector.mysql;
 
+import java.nio.charset.Charset;
+import java.nio.charset.IllegalCharsetNameException;
+import java.nio.charset.StandardCharsets;
 import java.sql.Types;
 import java.time.OffsetDateTime;
 import java.time.ZoneOffset;
 
 import org.apache.kafka.connect.data.Field;
+import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaBuilder;
+import org.apache.kafka.connect.source.SourceRecord;
 
 import com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer;
+import com.mysql.jdbc.CharsetMapping;
 
 import io.debezium.annotation.Immutable;
 import io.debezium.jdbc.JdbcValueConverters;
@@ -73,11 +79,11 @@ public SchemaBuilder schemaBuilder(Column column) {
             return Year.builder();
         }
         if (matches(typeName, ""ENUM"")) {
-            String commaSeparatedOptions = extractEnumAndSetOptions(column,true);
+            String commaSeparatedOptions = extractEnumAndSetOptions(column, true);
             return io.debezium.data.Enum.builder(commaSeparatedOptions);
         }
         if (matches(typeName, ""SET"")) {
-            String commaSeparatedOptions = extractEnumAndSetOptions(column,true);
+            String commaSeparatedOptions = extractEnumAndSetOptions(column, true);
             return io.debezium.data.EnumSet.builder(commaSeparatedOptions);
         }
         // Otherwise, let the base class handle it ...
@@ -93,18 +99,87 @@ public ValueConverter converter(Column column, Field fieldDefn) {
         }
         if (matches(typeName, ""ENUM"")) {
             // Build up the character array based upon the column's type ...
-            String options = extractEnumAndSetOptions(column,false);
+            String options = extractEnumAndSetOptions(column, false);
             return (data) -> convertEnumToString(options, column, fieldDefn, data);
         }
         if (matches(typeName, ""SET"")) {
             // Build up the character array based upon the column's type ...
-            String options = extractEnumAndSetOptions(column,false);
+            String options = extractEnumAndSetOptions(column, false);
             return (data) -> convertSetToString(options, column, fieldDefn, data);
         }
+        
+        // We have to convert bytes encoded in the column's character set ...
+        switch (column.jdbcType()) {
+            case Types.CHAR: // variable-length
+            case Types.VARCHAR: // variable-length
+            case Types.LONGVARCHAR: // variable-length
+            case Types.CLOB: // variable-length
+            case Types.NCHAR: // fixed-length
+            case Types.NVARCHAR: // fixed-length
+            case Types.LONGNVARCHAR: // fixed-length
+            case Types.NCLOB: // fixed-length
+            case Types.DATALINK:
+            case Types.SQLXML:
+                Charset charset = charsetFor(column);
+                if (charset != null) {
+                    return (data) -> convertString(column, fieldDefn, charset, data);
+                }
+                logger.warn(""Using UTF-8 charset by default for column without charset: {}"", column);
+                return (data) -> convertString(column, fieldDefn, StandardCharsets.UTF_8, data);
+            default:
+                break;
+        }
+
         // Otherwise, let the base class handle it ...
         return super.converter(column, fieldDefn);
     }
 
+    /**
+     * Return the {@link Charset} instance with the MySQL-specific character set name used by the given column.
+     * 
+     * @param column the column in which the character set is used; never null
+     * @return the Java {@link Charset}, or null if there is no mapping
+     */
+    protected Charset charsetFor(Column column) {
+        String mySqlCharsetName = column.charsetName();
+        if (mySqlCharsetName == null) {
+            logger.warn(""Column is missing a character set: {}"", column);
+            return null;
+        }
+        String encoding = CharsetMapping.getJavaEncodingForMysqlCharset(mySqlCharsetName);
+        if (encoding == null) {
+            logger.warn(""Column uses MySQL character set '{}', which has no mapping to a Java character set"", mySqlCharsetName);
+        } else {
+            try {
+                return Charset.forName(encoding);
+            } catch (IllegalCharsetNameException e) {
+                logger.error(""Unable to load Java charset '{}' for column with MySQL character set '{}'"", encoding, mySqlCharsetName);
+            }
+        }
+        return null;
+    }
+
+    /**
+     * Convert the {@link String} or {@code byte[]} value to a string value used in a {@link SourceRecord}.
+     * 
+     * @param column the column in which the value appears
+     * @param fieldDefn the field definition for the {@link SourceRecord}'s {@link Schema}; never null
+     * @param columnCharset the Java character set in which column byte[] values are encoded; may not be null
+     * @param data the data; may be null
+     * @return the string value; may be null if the value is null or is an unknown input type
+     */
+    protected Object convertString(Column column, Field fieldDefn, Charset columnCharset, Object data) {
+        if (data == null) return null;
+        if (data instanceof byte[]) {
+            // Decode the binary representation using the given character encoding ...
+            return new String((byte[]) data, columnCharset);
+        }
+        if (data instanceof String) {
+            return data;
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
     /**
      * Converts a value object for a MySQL {@code YEAR}, which appear in the binlog as an integer though returns from
      * the MySQL JDBC driver as either a short or a {@link java.sql.Date}.
@@ -151,7 +226,7 @@ protected Object convertEnumToString(String options, Column column, Field fieldD
         }
         if (data instanceof Integer) {
             // The binlog will contain an int with the 1-based index of the option in the enum value ...
-            int index = ((Integer) data).intValue() - 1;    // 'options' is 0-based
+            int index = ((Integer) data).intValue() - 1; // 'options' is 0-based
             if (index < options.length()) {
                 return options.substring(index, index + 1);
             }
@@ -180,7 +255,7 @@ protected Object convertSetToString(String options, Column column, Field fieldDe
         if (data instanceof Long) {
             // The binlog will contain a long with the indexes of the options in the set value ...
             long indexes = ((Long) data).longValue();
-            return convertSetValue(indexes,options);
+            return convertSetValue(indexes, options);
         }
         return handleUnknownData(column, fieldDefn, data);
     }
@@ -200,25 +275,29 @@ protected boolean matches(String upperCaseTypeName, String upperCaseMatch) {
 
     protected String extractEnumAndSetOptions(Column column, boolean commaSeparated) {
         String options = MySqlDdlParser.parseSetAndEnumOptions(column.typeExpression());
-        if ( !commaSeparated ) return options;
+        if (!commaSeparated) return options;
         StringBuilder sb = new StringBuilder();
         boolean first = true;
-        for ( int i=0; i!=options.length(); ++i ) {
-            if ( first ) first = false;
-            else sb.append(',');
+        for (int i = 0; i != options.length(); ++i) {
+            if (first)
+                first = false;
+            else
+                sb.append(',');
             sb.append(options.charAt(i));
         }
         return sb.toString();
     }
-    
-    protected String convertSetValue( long indexes, String options ) {
+
+    protected String convertSetValue(long indexes, String options) {
         StringBuilder sb = new StringBuilder();
         int index = 0;
         boolean first = true;
         while (indexes != 0L) {
             if (indexes % 2L != 0) {
-                if ( first ) first = false;
-                else sb.append(',');
+                if (first)
+                    first = false;
+                else
+                    sb.append(',');
                 sb.append(options.substring(index, index + 1));
             }
             ++index;",2016-08-29T17:19:24Z,47
"@@ -56,6 +56,16 @@ public static class DeleteRowsDeserializer extends DeleteRowsEventDataDeserializ
         public DeleteRowsDeserializer(Map<Long, TableMapEventData> tableMapEventByTableId) {
             super(tableMapEventByTableId);
         }
+        
+        @Override
+        protected Serializable deserializeString(int length, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeString(length, inputStream);
+        }
+        
+        @Override
+        protected Serializable deserializeVarString(int meta, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeVarString(meta, inputStream);
+        }
 
         @Override
         protected Serializable deserializeDate(ByteArrayInputStream inputStream) throws IOException {
@@ -104,6 +114,16 @@ public UpdateRowsDeserializer(Map<Long, TableMapEventData> tableMapEventByTableI
             super(tableMapEventByTableId);
         }
 
+        @Override
+        protected Serializable deserializeString(int length, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeString(length, inputStream);
+        }
+        
+        @Override
+        protected Serializable deserializeVarString(int meta, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeVarString(meta, inputStream);
+        }
+
         @Override
         protected Serializable deserializeDate(ByteArrayInputStream inputStream) throws IOException {
             return RowDeserializers.deserializeDate(inputStream);
@@ -151,6 +171,16 @@ public WriteRowsDeserializer(Map<Long, TableMapEventData> tableMapEventByTableId
             super(tableMapEventByTableId);
         }
 
+        @Override
+        protected Serializable deserializeString(int length, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeString(length, inputStream);
+        }
+        
+        @Override
+        protected Serializable deserializeVarString(int meta, ByteArrayInputStream inputStream) throws IOException {
+            return RowDeserializers.deserializeVarString(meta, inputStream);
+        }
+
         @Override
         protected Serializable deserializeDate(ByteArrayInputStream inputStream) throws IOException {
             return RowDeserializers.deserializeDate(inputStream);
@@ -192,6 +222,34 @@ protected Serializable deserializeYear(ByteArrayInputStream inputStream) throws
         }
     }
 
+    /**
+     * Converts a MySQL string to a {@code byte[]}.
+     * 
+     * @param length the number of bytes used to store the length of the string
+     * @param inputStream the binary stream containing the raw binlog event data for the value
+     * @return the {@code byte[]} object
+     * @throws IOException if there is an error reading from the binlog event data
+     */
+    protected static Serializable deserializeString(int length, ByteArrayInputStream inputStream) throws IOException {
+        // charset is not present in the binary log (meaning there is no way to distinguish between CHAR / BINARY)
+        // as a result - return byte[] instead of an actual String
+        int stringLength = length < 256 ? inputStream.readInteger(1) : inputStream.readInteger(2);
+        return inputStream.read(stringLength);
+    }
+
+    /**
+     * Converts a MySQL string to a {@code byte[]}.
+     * 
+     * @param meta the {@code meta} value containing the number of bytes in the length field
+     * @param inputStream the binary stream containing the raw binlog event data for the value
+     * @return the {@code byte[]} object
+     * @throws IOException if there is an error reading from the binlog event data
+     */
+    protected static Serializable deserializeVarString(int meta, ByteArrayInputStream inputStream) throws IOException {
+        int varcharLength = meta < 256 ? inputStream.readInteger(1) : inputStream.readInteger(2);
+        return inputStream.read(varcharLength);
+    }
+
     /**
      * Converts a MySQL {@code DATE} value to a {@link LocalDate}.
      * ",2016-08-29T17:19:24Z,103
"@@ -19,6 +19,7 @@
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
+
 import org.apache.kafka.connect.source.SourceRecord;
 
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
@@ -168,6 +169,10 @@ protected void execute() {
             sql.set(""SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"");
             mysql.execute(sql.get());
 
+            // Generate the DDL statements that set the charset-related system variables ...
+            Map<String, String> systemVariables = context.readMySqlCharsetSystemVariables(sql);
+            String setSystemVariablesStatement = context.setStatementFor(systemVariables);
+
             // ------
             // STEP 1
             // ------
@@ -206,13 +211,14 @@ protected void execute() {
                         String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
                         source.setGtidSet(gtidSet);
                         logger.info(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
-                                     gtidSet);
+                                    gtidSet);
                     } else {
                         logger.info(""\t using binlog '{}' at position '{}'"", binlogFilename, binlogPosition);
                     }
                     source.startSnapshot();
                 } else {
-                    throw new IllegalStateException(""Cannot read the binlog filename and position via '"" + showMasterStmt + ""'. Make sure your server is correctly configured"");    
+                    throw new IllegalStateException(""Cannot read the binlog filename and position via '"" + showMasterStmt
+                            + ""'. Make sure your server is correctly configured"");
                 }
             });
 
@@ -264,14 +270,18 @@ protected void execute() {
             // Transform the current schema so that it reflects the *current* state of the MySQL server's contents.
             // First, get the DROP TABLE and CREATE TABLE statement (with keys and constraint definitions) for our tables ...
             logger.info(""Step 6: generating DROP and CREATE statements to reflect current database schemas:"");
+            schema.applyDdl(source, null, setSystemVariablesStatement, this::enqueueSchemaChanges);
+
             // Add DROP TABLE statements for all tables that we knew about AND those tables found in the databases ...
             Set<TableId> allTableIds = new HashSet<>(schema.tables().tableIds());
             allTableIds.addAll(tableIds);
-            allTableIds.forEach(tableId -> schema.applyDdl(source, tableId.schema(), ""DROP TABLE IF EXISTS "" + tableId, this::enqueueSchemaChanges));
+            allTableIds.forEach(tableId -> schema.applyDdl(source, tableId.schema(), ""DROP TABLE IF EXISTS "" + tableId,
+                                                           this::enqueueSchemaChanges));
             // Add a DROP DATABASE statement for each database that we no longer know about ...
             schema.tables().tableIds().stream().map(TableId::catalog)
                   .filter(Predicates.not(databaseNames::contains))
-                  .forEach(missingDbName -> schema.applyDdl(source, missingDbName, ""DROP DATABASE IF EXISTS "" + missingDbName, this::enqueueSchemaChanges));
+                  .forEach(missingDbName -> schema.applyDdl(source, missingDbName, ""DROP DATABASE IF EXISTS "" + missingDbName,
+                                                            this::enqueueSchemaChanges));
             // Now process all of our tables for each database ...
             for (Map.Entry<String, List<TableId>> entry : tableIdsByDbName.entrySet()) {
                 String dbName = entry.getKey();
@@ -472,7 +482,7 @@ private Statement createStatement(Connection connection) throws SQLException {
     private void logServerInformation(JdbcConnection mysql) {
         try {
             logger.info(""MySQL server variables related to change data capture:"");
-            mysql.query(""SHOW VARIABLES WHERE Variable_name REGEXP 'version|binlog|tx_|gtid'"", rs -> {
+            mysql.query(""SHOW VARIABLES WHERE Variable_name REGEXP 'version|binlog|tx_|gtid|character_set|collation'"", rs -> {
                 while (rs.next()) {
                     logger.info(""\t{} = {}"",
                                 Strings.pad(rs.getString(1), 45, ' '),",2016-08-29T17:19:24Z,62
"@@ -116,6 +116,7 @@ public void shouldCreateSnapshotOfSingleDatabase() throws Exception {
         context = new MySqlTaskContext(config);
         context.start();
         context.source().setBinlogStartPoint("""",0L); // start from beginning
+        context.initializeHistory();
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...
@@ -175,6 +176,7 @@ public void shouldCreateSnapshotOfSingleDatabaseWithSchemaChanges() throws Excep
         context = new MySqlTaskContext(config);
         context.start();
         context.source().setBinlogStartPoint("""",0L); // start from beginning
+        context.initializeHistory();
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...",2016-08-29T17:19:24Z,67
"@@ -286,16 +286,17 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // ---------------------------------------------------------------------------------------------------------------
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
-        SourceRecords records = consumeRecordsByTopic(5 + 9 + 9 + 4 + 11); // 11 schema change records
-        assertThat(records.recordsForTopic(""myServer"").size()).isEqualTo(11);
+        SourceRecords records = consumeRecordsByTopic(5 + 9 + 9 + 4 + 11 + 1); // 11 schema change records + 1 SET statement
+        assertThat(records.recordsForTopic(""myServer"").size()).isEqualTo(12);
         assertThat(records.recordsForTopic(""myServer.connector_test.products"").size()).isEqualTo(9);
         assertThat(records.recordsForTopic(""myServer.connector_test.products_on_hand"").size()).isEqualTo(9);
         assertThat(records.recordsForTopic(""myServer.connector_test.customers"").size()).isEqualTo(4);
         assertThat(records.recordsForTopic(""myServer.connector_test.orders"").size()).isEqualTo(5);
         assertThat(records.topics().size()).isEqualTo(5);
-        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.databaseNames().size()).isEqualTo(2);
         assertThat(records.ddlRecordsForDatabase(""connector_test"").size()).isEqualTo(11);
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase("""").size()).isEqualTo(1);
         records.ddlRecordsForDatabase(""connector_test"").forEach(this::print);
 
         // Check that all records are valid, can be serialized and deserialized ...",2016-08-29T17:19:24Z,71
"@@ -109,11 +109,11 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
                 String c1 = after.getString(""c1"");
                 String c2 = after.getString(""c2"");
-                if ( c1.equals(""a"") ) {
+                if (c1.equals(""a"")) {
                     assertThat(c2).isEqualTo(""a,b,c"");
-                } else if ( c1.equals(""b"") ) {
+                } else if (c1.equals(""b"")) {
                     assertThat(c2).isEqualTo(""a,b"");
-                } else if ( c1.equals(""c"") ) {
+                } else if (c1.equals(""c"")) {
                     assertThat(c2).isEqualTo(""a"");
                 } else {
                     fail(""c1 didn't match expected value"");
@@ -230,11 +230,11 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
                 String c1 = after.getString(""c1"");
                 String c2 = after.getString(""c2"");
-                if ( c1.equals(""a"") ) {
+                if (c1.equals(""a"")) {
                     assertThat(c2).isEqualTo(""a,b,c"");
-                } else if ( c1.equals(""b"") ) {
+                } else if (c1.equals(""b"")) {
                     assertThat(c2).isEqualTo(""a,b"");
-                } else if ( c1.equals(""c"") ) {
+                } else if (c1.equals(""c"")) {
                     assertThat(c2).isEqualTo(""a"");
                 } else {
                     fail(""c1 didn't match expected value"");
@@ -325,20 +325,23 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
         // Testing.Debug.enable();
-        // 11 schema change records = 5 drop tables, 1 drop database, 1 create database, 1 use database, 5 create tables
-        SourceRecords records = consumeRecordsByTopic(11 + 6); // plus 6 data records ...
+        // 12 schema change records = 1 set variables, 5 drop tables, 1 drop database, 1 create database, 1 use database, 5 create
+        // tables
+        SourceRecords records = consumeRecordsByTopic(12 + 6); // plus 6 data records ...
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(11);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(12);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_100_enumsettest"").size()).isEqualTo(3);
         assertThat(records.topics().size()).isEqualTo(5);
-        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.databaseNames().size()).isEqualTo(2);
+        assertThat(records.databaseNames()).containsOnly(""regression_test"","""");
         assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(11);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase("""").size()).isEqualTo(1); // SET statement
         records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
 
         // Check that all records are valid, can be serialized and deserialized ...
@@ -349,11 +352,11 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
                 String c1 = after.getString(""c1"");
                 String c2 = after.getString(""c2"");
-                if ( c1.equals(""a"") ) {
+                if (c1.equals(""a"")) {
                     assertThat(c2).isEqualTo(""a,b,c"");
-                } else if ( c1.equals(""b"") ) {
+                } else if (c1.equals(""b"")) {
                     assertThat(c2).isEqualTo(""a,b"");
-                } else if ( c1.equals(""c"") ) {
+                } else if (c1.equals(""c"")) {
                     assertThat(c2).isEqualTo(""a"");
                 } else {
                     fail(""c1 didn't match expected value"");",2016-08-29T17:19:24Z,48
"@@ -61,7 +61,10 @@ public void shouldApplyDdlStatementsAndRecover() {
         mysql.start();
 
         // Testing.Print.enable();
+
+        // Set up the server ...
         source.setBinlogStartPoint(""binlog-001"",400);
+        mysql.applyDdl(source, ""db1"", ""SET "" + MySqlSystemVariables.CHARSET_NAME_SERVER + ""=utf8mb4"", this::printStatements);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -82,6 +85,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
         mysql.start();
 
         source.setBinlogStartPoint(""binlog-001"",400);
+        mysql.applyDdl(source, ""mysql"", ""SET "" + MySqlSystemVariables.CHARSET_NAME_SERVER + ""=utf8mb4"", this::printStatements);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
         source.setBinlogStartPoint(""binlog-001"",1000);
@@ -107,6 +111,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
         mysql.start();
 
         source.setBinlogStartPoint(""binlog-001"",400);
+        mysql.applyDdl(source, ""mysql"", ""SET "" + MySqlSystemVariables.CHARSET_NAME_SERVER + ""=utf8mb4"", this::printStatements);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
         source.setBinlogStartPoint(""binlog-001"",1000);",2016-08-29T17:19:24Z,19
"@@ -268,10 +268,10 @@ public void shouldCreateSnapshotOfSingleDatabaseWithSchemaChanges() throws Excep
         // The last poll should always return null ...
         assertThat(records).isNull();
         
-        // There should be 11 schema changes ...
-        assertThat(schemaChanges.recordCount()).isEqualTo(11);
-        assertThat(schemaChanges.databaseCount()).isEqualTo(1);
-        assertThat(schemaChanges.databases()).containsOnly(DB_NAME);
+        // There should be 11 schema changes plus 1 SET statement ...
+        assertThat(schemaChanges.recordCount()).isEqualTo(12);
+        assertThat(schemaChanges.databaseCount()).isEqualTo(2);
+        assertThat(schemaChanges.databases()).containsOnly(DB_NAME,"""");
         
         // Check the records via the store ...
         assertThat(store.collectionCount()).isEqualTo(4);",2016-08-29T17:19:24Z,62
"@@ -61,7 +61,7 @@ public class JdbcValueConverters implements ValueConverterProvider {
     private static final Double DOUBLE_TRUE = new Double(1.0d);
     private static final Double DOUBLE_FALSE = new Double(0.0d);
 
-    private final Logger logger = LoggerFactory.getLogger(getClass());
+    protected final Logger logger = LoggerFactory.getLogger(getClass());
     private final ZoneOffset defaultOffset;
     private final boolean adaptiveTimePrecision;
 ",2016-08-29T17:19:24Z,75
"@@ -16,7 +16,6 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicLong;
-import java.util.function.Predicate;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -155,28 +154,21 @@ protected void doStart() {
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
 
-        // The 'source' object holds the starting point in the binlog where we should start reading,
-        // set set the client to start from that point ...
-        String gtidSetStr = source.gtidSet();
-        if (gtidSetStr != null) {
+        // Get the current GtidSet from MySQL so we can get a filtered/merged GtidSet based off of the last Debezium checkpoint.
+        String availableServerGtidStr = context.knownGtidSet();
+        GtidSet availableServerGtidSet = new GtidSet(availableServerGtidStr);
+        GtidSet filteredGtidSet = context.getFilteredGtidSet(availableServerGtidSet);
+        if (filteredGtidSet != null) {
             // Register the event handler ...
             eventHandlers.put(EventType.GTID, this::handleGtidEvent);
-
-            logger.info(""GTID set from previous recorded offset: {}"", gtidSetStr);
-            // Remove any of the GTID sources that are not required/acceptable ...
-            Predicate<String> gtidSourceFilter = context.gtidSourceFilter();
-            if (gtidSourceFilter != null) {
-                GtidSet gtidSet = new GtidSet(gtidSetStr).retainAll(gtidSourceFilter);
-                gtidSetStr = gtidSet.toString();
-                logger.info(""GTID set after applying GTID source includes/excludes: {}"", gtidSetStr);
-                source.setGtidSet(gtidSetStr);
-            }
-            client.setGtidSet(gtidSetStr);
-            gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(gtidSetStr);
+            logger.info(""Registering binlog reader with GTID set: {}"", filteredGtidSet);
+            String filteredGtidSetStr = filteredGtidSet.toString();
+            client.setGtidSet(filteredGtidSetStr);
+            source.setGtidSet(filteredGtidSetStr);
+            gtidSet = new com.github.shyiko.mysql.binlog.GtidSet(filteredGtidSetStr);
         } else {
             client.setBinlogFilename(source.binlogFilename());
             client.setBinlogPosition(source.nextBinlogPosition());
-            gtidSet = null;
         }
 
         // Set the starting row number, which is the next row number to be read ...",2016-11-03T17:14:54Z,67
"@@ -8,6 +8,7 @@
 import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
+import java.util.HashMap;
 import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.List;
@@ -100,6 +101,19 @@ public boolean isContainedWithin(GtidSet other) {
         return true;
     }
 
+    /**
+     * Obtain a copy of this {@link GtidSet} except overwritten with all of the GTID ranges in the supplied {@link GtidSet}.
+     * @param other the other {@link GtidSet} with ranges to add/overwrite on top of those in this set;
+     * @return the new GtidSet, or this object if {@code other} is null or empty; never null
+     */
+    public GtidSet with(GtidSet other) {
+        if (other == null || other.uuidSetsByServerId.isEmpty()) return this;
+        Map<String, UUIDSet> newSet = new HashMap<>();
+        newSet.putAll(this.uuidSetsByServerId);
+        newSet.putAll(other.uuidSetsByServerId);
+        return new GtidSet(newSet);
+    }
+
     @Override
     public int hashCode() {
         return uuidSetsByServerId.keySet().hashCode();",2016-11-03T17:14:54Z,87
"@@ -241,7 +241,7 @@ protected boolean isBinlogAvailable() {
         String gtidStr = taskContext.source().gtidSet();
         if (gtidStr != null) {
             if (gtidStr.trim().isEmpty()) return true; // start at beginning ...
-            String availableGtidStr = knownGtidSet();
+            String availableGtidStr = taskContext.knownGtidSet();
             if (availableGtidStr == null || availableGtidStr.trim().isEmpty()) {
                 // Last offsets had GTIDs but the server does not use them ...
                 logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
@@ -250,7 +250,7 @@ protected boolean isBinlogAvailable() {
             // GTIDs are enabled, and we used them previously, but retain only those GTID ranges for the allowed source UUIDs ...
             GtidSet gtidSet = new GtidSet(gtidStr).retainAll(taskContext.gtidSourceFilter());
             // Get the GTID set that is available in the server ...
-            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            GtidSet availableGtidSet = new GtidSet(availableGtidStr);
             if (gtidSet.isContainedWithin(availableGtidSet)) {
                 logger.info(""MySQL current GTID set {} does contain the GTID set required by the connector {}"", availableGtidSet, gtidSet);
                 return true;
@@ -328,26 +328,6 @@ protected boolean isGtidModeEnabled() {
         return !""OFF"".equalsIgnoreCase(mode.get());
     }
 
-    /**
-     * Determine the available GTID set for MySQL.
-     * 
-     * @return the string representation of MySQL's GTID sets.
-     */
-    protected String knownGtidSet() {
-        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
-        try {
-            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
-                if (rs.next()) {
-                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
-                }
-            });
-        } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
-        }
-
-        return gtidSetStr.get();
-    }
-
     /**
      * Determine whether the MySQL server has the row-level binlog enabled.
      * ",2016-11-03T17:14:54Z,68
"@@ -123,6 +123,26 @@ public void close() {
         shutdown();
     }
 
+    /**
+     * Determine the available GTID set for MySQL.
+     *
+     * @return the string representation of MySQL's GTID sets.
+     */
+    public String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            jdbc.query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
+        }
+
+        return gtidSetStr.get();
+    }
+
     protected String connectionString() {
         return jdbc.connectionString(MYSQL_CONNECTION_URL);
     }",2016-11-03T17:14:54Z,61
"@@ -246,4 +246,40 @@ public ObjectName metricName(String contextName) throws MalformedObjectNameExcep
         return new ObjectName(""debezium.mysql:type=connector-metrics,context="" + contextName + "",server="" + serverName());
     }
 
+    /**
+     * Retrieve GTID set after applying include/exclude filters on the source. Also, merges the server GTID set with the
+     * filtered client (Debezium) set.
+     *
+     * The merging behavior of this method might seem a bit strange at first. It's required in order for Debezium to consume a
+     * MySQL binlog that has multi-source replication enabled, if a failover has to occur. In such a case, the server that
+     * Debezium is failed over to might have a different set of sources, but still include the sources required for Debezium
+     * to continue to function. MySQL does not allow downstream replicas to connect if the GTID set does not contain GTIDs for
+     * all channels that the server is replicating from, even if the server does have the data needed by the client. To get
+     * around this, we can have Debezium merge its GTID set with whatever is on the server, so that MySQL will allow it to
+     * connect. See <a href=""https://issues.jboss.org/browse/DBZ-143"">DBZ-143</a> for details.
+     *
+     * This method does not mutate any state in the context.
+     *
+     * @return A GTID set meant for consuming from a MySQL binlog; may return null if the SourceInfo has no GTIDs and therefore
+     *         none were filtered
+     */
+    public GtidSet getFilteredGtidSet(GtidSet availableServerGtidSet) {
+        logger.info(""Attempting to generate a filtered GTID set"");
+        String gtidStr = source.gtidSet();
+        if (gtidStr == null) {
+            return null;
+        }
+        logger.info(""GTID set from previous recorded offset: {}"", gtidStr);
+        GtidSet filteredGtidSet = new GtidSet(gtidStr);
+        Predicate<String> gtidSourceFilter = gtidSourceFilter();
+        if (gtidSourceFilter != null) {
+            filteredGtidSet = filteredGtidSet.retainAll(gtidSourceFilter);
+            logger.info(""GTID set after applying GTID source includes/excludes to previous recorded offset: {}"", filteredGtidSet);
+        }
+        logger.info(""GTID set available on server: {}"", availableServerGtidSet);
+        GtidSet mergedGtidSet = availableServerGtidSet.with(filteredGtidSet);
+        logger.info(""Final merged GTID set to use when connecting to MySQL: {}"", mergedGtidSet);
+        return mergedGtidSet;
+    }
+
 }",2016-11-03T17:14:54Z,69
"@@ -6,6 +6,7 @@
 package io.debezium.connector.mysql;
 
 import java.nio.file.Path;
+import java.util.Arrays;
 import java.util.function.Predicate;
 
 import org.junit.After;
@@ -201,4 +202,31 @@ public void shouldNotAllowBothGtidSetIncludesAndExcludes() throws Exception {
         boolean valid = config.validateAndRecord(MySqlConnectorConfig.ALL_FIELDS,msg->{});
         assertThat(valid).isFalse();
     }
+
+    @Test
+    public void shouldFilterAndMergeGtidSet() throws Exception {
+        String gtidStr = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,""
+          + ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:5-41"";
+        String availableServerGtidStr = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-20,""
+          + ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3200,""
+          + ""123e4567-e89b-12d3-a456-426655440000:1-41"";
+        config = simpleConfig().with(MySqlConnectorConfig.GTID_SOURCE_INCLUDES,
+                                     ""036d85a9-64e5-11e6-9b48-42010af0000c"")
+                               .build();
+        context = new MySqlTaskContext(config);
+        context.start();
+        context.source().setGtidSet(gtidStr);
+
+        GtidSet mergedGtidSet = context.getFilteredGtidSet(new GtidSet(availableServerGtidStr));
+        assertThat(mergedGtidSet).isNotNull();
+        GtidSet.UUIDSet uuidSet1 = mergedGtidSet.forServerWithId(""036d85a9-64e5-11e6-9b48-42010af0000c"");
+        GtidSet.UUIDSet uuidSet2 = mergedGtidSet.forServerWithId(""7145bf69-d1ca-11e5-a588-0242ac110004"");
+        GtidSet.UUIDSet uuidSet3 = mergedGtidSet.forServerWithId(""123e4567-e89b-12d3-a456-426655440000"");
+        GtidSet.UUIDSet uuidSet4 = mergedGtidSet.forServerWithId(""7c1de3f2-3fd2-11e6-9cdc-42010af000bc"");
+
+        assertThat(uuidSet1.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 2)));
+        assertThat(uuidSet2.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 3200)));
+        assertThat(uuidSet3.getIntervals()).isEqualTo(Arrays.asList(new GtidSet.Interval(1, 41)));
+        assertThat(uuidSet4).isNull();
+    }
 }",2016-11-03T17:14:54Z,72
"@@ -146,6 +146,53 @@
                       </assembly>
                     </build>
                   </image>
+                  <image>
+                    <!-- A Docker image using a partial MySQL installation maintained by MySQL team. -->
+                    <name>debezium/mysql-server-gtids-test-databases</name>
+                    <alias>database</alias>
+                    <run>
+                      <namingStrategy>alias</namingStrategy>
+                      <env>
+                        <MYSQL_ROOT_PASSWORD>debezium-rocks</MYSQL_ROOT_PASSWORD>
+                        <MYSQL_DATABASE>mysql</MYSQL_DATABASE> <!-- database created upon init -->
+                        <MYSQL_USER>${database.user}</MYSQL_USER>
+                        <MYSQL_PASSWORD>${database.password}</MYSQL_PASSWORD>
+                      </env>
+                      <ports>
+                        <port>${database.port}:3306</port>
+                      </ports>
+                      <log>
+                        <prefix>mysql</prefix>
+                        <enabled>true</enabled>
+                        <color>yellow</color>
+                      </log>
+                      <wait>
+                        <log>MySQL init process done. Ready for start up.</log>
+                        <time>30000</time> <!-- 30 seconds max -->
+                      </wait>
+                    </run>
+                    <build>
+                      <from>mysql/mysql-server:${version.mysql.server}</from>
+                      <assembly>
+                        <inline>
+                          <fileSets>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/server-gtids</directory>
+                              <includes>
+                                <include>my.cnf</include>
+                              </includes>
+                              <outputDirectory>etc/mysql</outputDirectory>
+                            </fileSet>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/init</directory>
+                              <outputDirectory>docker-entrypoint-initdb.d</outputDirectory>
+                            </fileSet>
+                          </fileSets>
+                        </inline>
+                        <basedir>/</basedir>
+                      </assembly>
+                    </build>
+                  </image>
                   <image>
                     <!-- A Docker image using a complete MySQL installation maintained by Docker team. -->
                     <name>debezium/mysql-test--databases</name>
@@ -350,5 +397,22 @@
           <docker.skip>false</docker.skip>
         </properties>
       </profile>
+      <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+            Use the Docker image for MySQL configured to use GTIDs.
+            To use, specify ""-Dgtid-mysql"" or -Pgtid-mysql on the Maven command line.
+            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+      <profile>
+        <id>gtid-mysql</id>
+        <activation>
+          <activeByDefault>false</activeByDefault>
+          <property>
+            <name>gtid-mysql</name>
+          </property>
+        </activation>
+        <properties>
+          <docker.image>debezium/mysql-server-gtids-test-databases</docker.image>
+          <docker.skip>false</docker.skip>
+        </properties>
+      </profile>
     </profiles>
 </project>",2016-06-07T17:01:51Z,83
"@@ -88,6 +88,7 @@ protected void doStart() {
         eventHandlers.put(EventType.ROTATE, this::handleRotateLogsEvent);
         eventHandlers.put(EventType.TABLE_MAP, this::handleUpdateTableMetadata);
         eventHandlers.put(EventType.QUERY, this::handleQueryEvent);
+        eventHandlers.put(EventType.GTID, this::handleGtidEvent);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, this::handleInsert);
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
@@ -142,9 +143,10 @@ protected void ignoreEvent(Event event) {
     protected void handleEvent(Event event) {
         if (event == null) return;
 
-        // Update the source offset info ...
+        // Update the source offset info. Note that the client returns the value in *milliseconds*, even though the binlog
+        // contains only *seconds* precision ...
         EventHeader eventHeader = event.getHeader();
-        source.setBinlogTimestamp(eventHeader.getTimestamp());
+        source.setBinlogTimestampSeconds(eventHeader.getTimestamp()/1000L); // client returns milliseconds, we record seconds
         source.setBinlogServerId(eventHeader.getServerId());
         EventType eventType = eventHeader.getEventType();
         if (eventType == EventType.ROTATE) {
@@ -166,16 +168,6 @@ protected void handleEvent(Event event) {
                 source.setRowInEvent(0);
             }
         }
-        if (eventType == EventType.GTID) {
-            EventData eventData = event.getData();
-            GtidEventData gtidEventData;
-            if (eventData instanceof EventDeserializer.EventDataWrapper) {
-                gtidEventData = (GtidEventData) ((EventDeserializer.EventDataWrapper) eventData).getInternal();
-            } else {
-                gtidEventData = (GtidEventData) eventData;
-            }
-            source.setGtids(gtidEventData.getGtid());
-        }
 
         // If there is a handler for this event, forward the event to it ...
         try {
@@ -188,6 +180,15 @@ protected void handleEvent(Event event) {
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    protected <T extends EventData> T unwrapData(Event event) {
+        EventData eventData = event.getData();
+        if (eventData instanceof EventDeserializer.EventDataWrapper) {
+            eventData = ((EventDeserializer.EventDataWrapper) eventData).getInternal();
+        }
+        return (T)eventData;
+    }
+
     /**
      * Handle the supplied event that signals that mysqld has stopped.
      * 
@@ -226,19 +227,31 @@ protected void handleServerIncident(Event event) {
      */
     protected void handleRotateLogsEvent(Event event) {
         logger.debug(""Rotating logs: {}"", event);
-        RotateEventData command = event.getData();
+        RotateEventData command = unwrapData(event);
         assert command != null;
         recordMakers.clear();
     }
 
+    /**
+     * Handle the supplied event with a {@link GtidEventData} that signals the beginning of a GTID transaction.
+     * 
+     * @param event the GTID event to be processed; may not be null
+     */
+    protected void handleGtidEvent(Event event) {
+        logger.debug(""GTID transaction: {}"", event);
+        GtidEventData gtidEvent = unwrapData(event);
+        source.setGtid(gtidEvent.getGtid());
+        source.setGtidSet(client.getGtidSet());
+    }
+
     /**
      * Handle the supplied event with an {@link QueryEventData} by possibly recording the DDL statements as changes in the
      * MySQL schemas.
      * 
      * @param event the database change data event to be processed; may not be null
      */
     protected void handleQueryEvent(Event event) {
-        QueryEventData command = event.getData();
+        QueryEventData command = unwrapData(event);
         logger.debug(""Received update table command: {}"", event);
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords && recordMakers.schemaChanges(dbName, statements, super::enqueueRecord) > 0) {
@@ -262,7 +275,7 @@ protected void handleQueryEvent(Event event) {
      * @param event the update event; never null
      */
     protected void handleUpdateTableMetadata(Event event) {
-        TableMapEventData metadata = event.getData();
+        TableMapEventData metadata = unwrapData(event);
         long tableNumber = metadata.getTableId();
         String databaseName = metadata.getDatabase();
         String tableName = metadata.getTable();
@@ -281,7 +294,7 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
-        WriteRowsEventData write = event.getData();
+        WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);
@@ -302,7 +315,7 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
-        UpdateRowsEventData update = event.getData();
+        UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
         // BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
@@ -330,7 +343,7 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
-        DeleteRowsEventData deleted = event.getData();
+        DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);",2016-06-07T17:01:51Z,67
"@@ -7,6 +7,7 @@
 
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.TreeMap;
@@ -111,11 +112,17 @@ protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
                 intervals.add(new Interval(interval.getStart(), interval.getEnd()));
             });
             Collections.sort(this.intervals);
-        }
-
-        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
-            this.uuid = uuid;
-            this.intervals = intervals;
+            if ( this.intervals.size() > 1 ) {
+                // Collapse adjacent intervals ...
+                for ( int i=intervals.size()-1; i!=0; --i) {
+                    Interval before = this.intervals.get(i-1);
+                    Interval after = this.intervals.get(i);
+                    if ( (before.getEnd() + 1) == after.getStart() ) {
+                        this.intervals.set(i-1,new Interval(before.getStart(),after.getEnd()));
+                        this.intervals.remove(i);
+                    }
+                }
+            }
         }
 
         /**
@@ -218,17 +225,41 @@ public String toString() {
             StringBuilder sb = new StringBuilder();
             if (sb.length() != 0) sb.append(',');
             sb.append(uuid).append(':');
-            sb.append(intervals.getFirst().getStart());
-            sb.append(intervals.getLast().getEnd());
+            Iterator<Interval> iter = intervals.iterator();
+            if ( iter.hasNext() ) sb.append(iter.next());
+            while ( iter.hasNext() ) {
+                sb.append(':');
+                sb.append(iter.next());
+            }
             return sb.toString();
         }
     }
 
     @Immutable
-    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+    public static class Interval implements Comparable<Interval> {
+
+        private final long start;
+        private final long end;
 
         public Interval(long start, long end) {
-            super(start, end);
+            this.start = start;
+            this.end = end;
+        }
+
+        /**
+         * Get the starting transaction number in this interval.
+         * @return this interval's first transaction number
+         */
+        public long getStart() {
+            return start;
+        }
+
+        /**
+         * Get the ending transaction number in this interval.
+         * @return this interval's last transaction number
+         */
+        public long getEnd() {
+            return end;
         }
 
         /**
@@ -244,6 +275,15 @@ public boolean isSubsetOf(Interval other) {
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
         }
+        
+        @Override
+        public int compareTo(Interval that) {
+            if ( that == this ) return 0;
+            long diff = this.start - that.start;
+            if ( diff > Integer.MAX_VALUE ) return Integer.MAX_VALUE;
+            if ( diff < Integer.MIN_VALUE ) return Integer.MIN_VALUE;
+            return (int) diff;
+        }
 
         @Override
         public int hashCode() {",2016-06-07T17:01:51Z,87
"@@ -9,6 +9,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -110,9 +111,16 @@ public void start(Map<String, String> props) {
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;
+                // The snapshot will determine if GTIDs are set
             }
         }
 
+        if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
+            // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
+            // previously used but the MySQL server has them enabled ...
+            source.setGtidSet("""");
+        }
+
         // Set up the readers ...
         this.binlogReader = new BinlogReader(taskContext);
         if (startWithSnapshot) {
@@ -174,13 +182,33 @@ protected void transitionToReadBinlog() {
      * @return {@code true} if the server has the binlog coordinates, or {@code false} otherwise
      */
     protected boolean isBinlogAvailable() {
+        String gtidStr = taskContext.source().gtidSet();
+        if ( gtidStr != null) {
+            if ( gtidStr.trim().isEmpty() ) return true; // start at beginning ...
+            String availableGtidStr = knownGtidSet();
+            if ( availableGtidStr == null || availableGtidStr.trim().isEmpty() ) {
+                // Last offsets had GTIDs but the server does not use them ...
+                logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
+                return false;
+            }
+            // GTIDs are enabled, and we used them previously ...
+            GtidSet gtidSet = new GtidSet(gtidStr);
+            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+                return true;
+            }
+            logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);
+            return false;
+        }
+        
         String binlogFilename = taskContext.source().binlogFilename();
         if (binlogFilename == null) return true; // start at current position
         if (binlogFilename.equals("""")) return true; // start at beginning
 
         // Accumulate the available binlog filenames ...
         List<String> logNames = new ArrayList<>();
         try {
+            logger.info(""Stop 0: Get all known binlogs from MySQL"");
             taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
                 while (rs.next()) {
                     logNames.add(rs.getString(1));
@@ -191,6 +219,50 @@ protected boolean isBinlogAvailable() {
         }
 
         // And compare with the one we're supposed to use ...
-        return logNames.stream().anyMatch(binlogFilename::equals);
+        boolean found = logNames.stream().anyMatch(binlogFilename::equals);
+        if ( !found ) {
+            logger.info(""Connector requires binlog file '{}', but MySQL only has {}"",binlogFilename,String.join("", "",logNames));
+        }
+        return found;
+    }
+
+    /**
+     * Determine whether the MySQL server has GTIDs enabled.
+     * 
+     * @return {@code false} if the server's {@code gtid_mode} is set and is {@code OFF}, or {@code true} otherwise
+     */
+    protected boolean isGtidModeEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>(""off"");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'GTID_MODE'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return !""OFF"".equalsIgnoreCase(mode.get());
+    }
+
+    /**
+     * Determine the available GTID set for MySQL.
+     * 
+     * @return the string representation of MySQL's GTID sets.
+     */
+    protected String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return gtidSetStr.get();
     }
 }",2016-06-07T17:01:51Z,68
"@@ -188,7 +188,7 @@ protected void execute() {
                 if (rs.next()) {
                     source.setBinlogFilename(rs.getString(1));
                     source.setBinlogPosition(rs.getLong(2));
-                    source.setGtids(rs.getString(5));// GTIDs
+                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
                     source.startSnapshot();
                 }
             });",2016-06-07T17:01:51Z,62
"@@ -14,6 +14,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.data.Envelope;
 import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
@@ -26,7 +27,7 @@
  * 
  * <pre>
  * {
- *     ""db"" : ""myDatabase""
+ *     ""server"" : ""production-server""
  * }
  * </pre>
  * 
@@ -36,14 +37,40 @@
  * 
  * <pre>
  * {
- *         ""gtids"" = ""3E11FA47-71CA-11E1-9E33-C80AA9429562:1-5"",
- *         ""file"" = ""mysql-bin.000003"",
- *         ""pos"" = 105586,
- *         ""row"" = 0
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
+ *     ""file"" = ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"" = 0,
+ *     ""snapshot"": true
  * }
  * </pre>
  * 
- * Note that the ""{@code gtid}"" field is present only when GTIDs are enabled.
+ * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
+ * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
+ * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
+ * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
+ * 
+ * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
+ * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
+ * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
+ * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
+ * an event that corresponds to the above partition and offset:
+ * 
+ * <pre>
+ * {
+ *     ""name"": ""production-server"",
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"": 0,
+ *     ""snapshot"": true
+ * }
+ * </pre>
  * 
  * @author Randall Hauch
  */
@@ -56,11 +83,12 @@ final class SourceInfo {
 
     public static final String SERVER_NAME_KEY = ""name"";
     public static final String SERVER_PARTITION_KEY = ""server"";
-    public static final String GTID_KEY = ""gtids"";
+    public static final String GTID_SET_KEY = ""gtids"";
+    public static final String GTID_KEY = ""gtid"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
     public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
-    public static final String TIMESTAMP_KEY = ""ts"";
+    public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
     /**
@@ -78,13 +106,14 @@ final class SourceInfo {
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private GtidSet binlogGtids;
+    private String gtidSet;
+    private String binlogGtid;
     private String binlogFilename;
     private long binlogPosition = 4;
     private int eventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
-    private long binlogTs = 0;
+    private long binlogTimestampSeconds = 0;
     private Map<String, String> sourcePartition;
     private boolean snapshot = false;
 
@@ -122,11 +151,11 @@ public Map<String, String> partition() {
      */
     public Map<String, ?> offset() {
         Map<String, Object> map = new HashMap<>();
-        if (binlogGtids != null) {
-            map.put(GTID_KEY, binlogGtids.toString());
+        if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
+        if (gtidSet != null) {
+            map.put(GTID_SET_KEY, gtidSet);
         }
-        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTs != 0 ) map.put(TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -158,13 +187,14 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        if (binlogGtids != null) {
-            result.put(GTID_KEY, binlogGtids.toString());
+        // Don't put the GTID Set into the struct; only the current GTID is fine ...
+        if (binlogGtid != null) {
+            result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
-        result.put(TIMESTAMP_KEY, binlogTs);
+        result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
@@ -193,12 +223,23 @@ public boolean isSnapshotInEffect() {
     }
 
     /**
-     * Set the GTID range for the MySQL binary log file.
+     * Set the latest GTID from the MySQL binary log file.
+     * 
+     * @param gtid the string representation of a specific GTID; may not be null
+     */
+    public void setGtid(String gtid) {
+        this.binlogGtid = gtid;
+    }
+
+    /**
+     * Set the set of GTIDs known to the MySQL server.
      * 
-     * @param gtids the string representation of the binlog GTIDs; may not be null
+     * @param gtidSet the string representation of GTID set; may not be null
      */
-    public void setGtids(String gtids) {
-        this.binlogGtids = gtids != null && !gtids.trim().isEmpty() ? new GtidSet(gtids) : null;
+    public void setGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            this.gtidSet = gtidSet;
+        }
     }
 
     /**
@@ -239,12 +280,12 @@ public void setBinlogServerId(long serverId) {
     }
 
     /**
-     * Set the timestamp as found within the MySQL binary log file.
+     * Set the number of <em>seconds</em> since Unix epoch (January 1, 1970) as found within the MySQL binary log file.
      * 
-     * @param timestamp the timestamp found within the binary log file
+     * @param timestampInSeconds the timestamp in <em>seconds</em> found within the binary log file
      */
-    public void setBinlogTimestamp(long timestamp) {
-        this.binlogTs = timestamp;
+    public void setBinlogTimestampSeconds(long timestampInSeconds) {
+        this.binlogTimestampSeconds = timestampInSeconds / 1000;
     }
 
     /**
@@ -270,7 +311,7 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtids((String) sourceOffset.get(GTID_KEY)); // may be null
+            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
             binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
@@ -297,7 +338,7 @@ private long longOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.binlogGtids != null ? this.binlogGtids.toString() : null;
+        return this.gtidSet != null ? this.gtidSet.toString() : null;
     }
 
     /**
@@ -340,9 +381,9 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (binlogGtids != null) {
+        if (gtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(binlogGtids);
+            sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
             sb.append("", pos="").append(binlogPosition());
             sb.append("", row="").append(eventRowNumber());
@@ -379,17 +420,17 @@ public String toString() {
      * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
      */
     public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
-        String recordedGtidSetStr = recorded.getString(GTID_KEY);
-        String desiredGtidSetStr = desired.getString(GTID_KEY);
+        String recordedGtidSetStr = recorded.getString(GTID_SET_KEY);
+        String desiredGtidSetStr = desired.getString(GTID_SET_KEY);
         if (desiredGtidSetStr != null) {
             // The desired position uses GTIDs, so we ideally compare using GTIDs ...
             if (recordedGtidSetStr != null) {
                 // Both have GTIDs, so base the comparison entirely on the GTID sets.
                 GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
                 GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
-                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                if (recordedGtidSet.equals(desiredGtidSet)) {
                     // They are exactly the same, which means the recorded position exactly matches the desired ...
-                    if ( !recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
+                    if (!recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
@@ -414,35 +455,35 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         }
 
         // Both positions are missing GTIDs. Look at the servers ...
-        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        if ( recordedServerId != desiredServerId ) {
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        if (recordedServerId != desiredServerId) {
             // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
             // is compare timestamps, and we have to assume that the server timestamps can be compared ...
-            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
-            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
+            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
+            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
             return recordedTimestamp <= desiredTimestamp;
         }
-        
+
         // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
         // comparable ...
         String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
         String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
-        if ( diff > 0 ) return false;
-        
+        if (diff > 0) return false;
+
         // The positions are the same, so compare the row number ...
         int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The binlog coordinates are the same ...
         return true;",2016-06-07T17:01:51Z,70
"@@ -0,0 +1,52 @@
+# For advice on how to change settings please see
+# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
+
+[mysqld]
+#
+# Remove leading # and set to the amount of RAM for the most important data
+# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
+# innodb_buffer_pool_size = 128M
+#
+# Remove leading # to turn on a very important data integrity option: logging
+# changes to the binary log between backups.
+# log_bin
+#
+# Remove leading # to set options mainly useful for reporting servers.
+# The server defaults are faster for transactions and fast SELECTs.
+# Adjust sizes as needed, experiment to find the optimal values.
+# join_buffer_size = 128M
+# sort_buffer_size = 2M
+# read_rnd_buffer_size = 2M
+skip-host-cache
+skip-name-resolve
+datadir=/var/lib/mysql
+socket=/var/lib/mysql/mysql.sock
+secure-file-priv=/var/lib/mysql-files
+user=mysql
+
+# Disabling symbolic-links is recommended to prevent assorted security risks
+symbolic-links=0
+
+log-error=/var/log/mysqld.log
+pid-file=/var/run/mysqld/mysqld.pid
+
+# ----------------------------------------------
+# Enable GTIDs on this master
+# ----------------------------------------------
+gtid_mode                 = on
+enforce_gtid_consistency  = on
+
+# ----------------------------------------------
+# Debezium ingest
+# ----------------------------------------------
+
+# Enable binary replication log and set the prefix, expiration, and log format.
+# The prefix is arbitrary, expiration can be short for integration tests but would
+# be longer on a production system. Row-level info is required for ingest to work.
+# Server ID is required, but this will vary on production systems
+server-id         = 112233
+log_bin           = mysql-bin
+expire_logs_days  = 1
+binlog_format     = row
+
+",2016-06-07T17:01:51Z,104
"@@ -0,0 +1,107 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.connector.mysql.GtidSet.Interval;
+import io.debezium.connector.mysql.GtidSet.UUIDSet;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class GtidSetTest {
+    
+    private static final String UUID1 = ""24bc7850-2c16-11e6-a073-0242ac110002"";
+
+    private GtidSet gtids;
+    
+    @Test
+    public void shouldCreateSetWithSingleInterval() {
+        gtids = new GtidSet(UUID1 + "":1-191"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,191);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,1,191);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191"");
+    }
+    
+    @Test
+    public void shouldCollapseAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,199);
+        asertFirstInterval(UUID1,1,199);
+        asertLastInterval(UUID1,1,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199"");
+    }
+
+    
+    @Test
+    public void shouldNotCollapseNonAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199"");
+        asertIntervalCount(UUID1,2);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,193,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199:1000-1033"");
+        asertIntervalCount(UUID1,3);
+        asertFirstInterval(UUID1,1,191);
+        asertIntervalExists(UUID1,193,199);
+        asertLastInterval(UUID1,1000,1033);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199:1000-1033"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervalsThatMayBeAdjacent() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199:1000-1033:1035-1036:1038-1039"");
+        asertIntervalCount(UUID1, 4);
+        asertFirstInterval(UUID1, 1, 199);
+        asertIntervalExists(UUID1, 1000, 1033);
+        asertIntervalExists(UUID1, 1035, 1036);
+        asertLastInterval(UUID1, 1038, 1039);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199:1000-1033:1035-1036:1038-1039""); // ??
+    }
+    
+    protected void asertIntervalCount( String uuid, int count) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        assertThat(set.getIntervals().size()).isEqualTo(count);
+    }
+    
+    protected void asertIntervalExists( String uuid, int start, int end) {
+        assertThat(hasInterval(uuid,start,end)).isTrue();
+    }
+    
+    protected void asertFirstInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getFirstInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected void asertLastInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getLastInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected boolean hasInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        for ( Interval interval : set.getIntervals() ) {
+            if ( interval.getStart() == start && interval.getEnd() == end ) return true;
+        }
+        return false;
+    }
+
+}",2016-06-07T17:01:51Z,105
"@@ -86,9 +86,9 @@ protected Document positionWithGtids(String gtids) {
 
     protected Document positionWithGtids(String gtids, boolean snapshot) {
         if (snapshot) {
-            return Document.create(SourceInfo.GTID_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
+            return Document.create(SourceInfo.GTID_SET_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
         }
-        return Document.create(SourceInfo.GTID_KEY, gtids);
+        return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
     protected Document positionWithoutGtids(String filename, int position, int row) {",2016-06-07T17:01:51Z,70
"@@ -45,15 +45,18 @@ public static enum Operation {
          */
         DELETE(""d"");
         private final String code;
+
         private Operation(String code) {
             this.code = code;
         }
-        public static Operation forCode( String code ) {
-            for ( Operation op : Operation.values()) {
-                if ( op.code().equalsIgnoreCase(code)) return op;
+
+        public static Operation forCode(String code) {
+            for (Operation op : Operation.values()) {
+                if (op.code().equalsIgnoreCase(code)) return op;
             }
             return null;
         }
+
         public String code() {
             return code;
         }
@@ -81,11 +84,13 @@ public static final class FieldName {
          */
         public static final String SOURCE = ""source"";
         /**
-         * The {@code ts} field is used to store the information about the local time at which the connector processed/generated
-         * the event. Note that the accuracy of the timestamp is not defined, and the values may not always be monotonically
-         * increasing.
+         * The {@code ts_ms} field is used to store the information about the local time at which the connector
+         * processed/generated the event. The timestamp values are the number of milliseconds past epoch (January 1, 1970), and
+         * determined by the {@link System#currentTimeMillis() JVM current time in milliseconds}. Note that the <em>accuracy</em>
+         * of the timestamp value depends on the JVM's system clock and all of its assumptions, limitations, conditions, and
+         * variations.
          */
-        public static final String TIMESTAMP = ""ts"";
+        public static final String TIMESTAMP = ""ts_ms"";
     }
 
     /**
@@ -304,6 +309,7 @@ public Struct delete(Struct before, Struct source, Long timestamp) {
 
     /**
      * Obtain the operation for the given source record.
+     * 
      * @param record the source record; may not be null
      * @return the operation, or null if no valid operation was found in the record
      */",2016-06-07T17:01:51Z,106
"@@ -91,10 +91,27 @@ public static TemporalPrecisionMode parse(String value, String defaultValue) {
      * The set of predefined SnapshotMode options or aliases.
      */
     public static enum SnapshotMode {
+
+        /**
+         * Perform a snapshot when it is needed.
+         */
+        WHEN_NEEDED(""when_needed""),
+        
+        /**
+         * Perform a snapshot only upon initial startup of a connector.
+         */
+        INITIAL(""initial""),
+
+        /**
+         * Never perform a snapshot and only read the binlog. This assumes the binlog contains all the history of those
+         * databases and tables that will be captured.
+         */
+        NEVER(""never""),
+
         /**
-         * Forwards each event as a structured Kafka Connect message.
+         * Perform a snapshot and then stop before attempting to read the binlog.
          */
-        WHEN_NEEDED(""when_needed""), INITIAL(""initial""), NEVER(""never"");
+        INITIAL_ONLY(""initial_only"");
 
         private final String value;
 
@@ -474,7 +491,8 @@ public static SecureConnectionMode parse(String value, String defaultValue) {
                                                    .withDescription(""The criteria for running a snapshot upon startup of the connector. ""
                                                            + ""Options include: ""
                                                            + ""'when_needed' to specify that the connector run a snapshot upon startup whenever it deems it necessary; ""
-                                                           + ""'initial' (the default) to specify the connector can run a snapshot only when no offsets are available for the logical server name; and ""
+                                                           + ""'initial' (the default) to specify the connector can run a snapshot only when no offsets are available for the logical server name; ""
+                                                           + ""'initial_only' same as 'initial' except the connector should stop after completing the snapshot and before it would normally read the binlog; and""
                                                            + ""'never' to specify the connector should never run a snapshot and that upon first startup the connector should read from the beginning of the binlog. ""
                                                            + ""The 'never' mode should be used with care, and only when the binlog is known to contain all history."");
 ",2016-09-22T22:09:11Z,40
"@@ -115,6 +115,15 @@ public void start(Map<String, String> props) {
                     logger.info(""Found no existing offset and snapshots disallowed, so starting at beginning of binlog"");
                     source.setBinlogStartPoint("""", 0L);// start from the beginning of the binlog
                     taskContext.initializeHistory();
+
+                    // Look to see what the first available binlog file is called, and whether it looks like binlog files have
+                    // been purged. If so, then output a warning ...
+                    String earliestBinlogFilename = earliestBinlogFilename();
+                    if (earliestBinlogFilename == null) {
+                        logger.warn(""No binlog appears to be available. Ensure that the MySQL row-level binlog is enabled."");
+                    } else if (!earliestBinlogFilename.endsWith(""00001"")) {
+                        logger.warn(""It is possible the server has purged some binlogs. If this is the case, then using snapshot mode may be required."");
+                    }
                 } else {
                     // We are allowed to use snapshots, and that is the best way to start ...
                     startWithSnapshot = true;
@@ -130,22 +139,44 @@ public void start(Map<String, String> props) {
                 source.setGtidSet("""");
             }
 
+            // Check whether the row-level binlog is enabled ...
+            final boolean rowBinlogEnabled = isRowBinlogEnabled();
+
             // Set up the readers ...
             this.binlogReader = new BinlogReader(taskContext);
             if (startWithSnapshot) {
                 // We're supposed to start with a snapshot, so set that up ...
                 this.snapshotReader = new SnapshotReader(taskContext);
-                this.snapshotReader.onSuccessfulCompletion(this::transitionToReadBinlog);
+                if (!taskContext.isInitialSnapshotOnly()) {
+                    logger.warn(""This connector will only perform a snapshot, and will stop after that completes."");
+                    this.snapshotReader.onSuccessfulCompletion(this::skipReadBinlog);
+                } else if (rowBinlogEnabled) {
+                    // This is the normal mode ...
+                    this.snapshotReader.onSuccessfulCompletion(this::transitionToReadBinlog);
+                } else {
+                    assert !rowBinlogEnabled;
+                    assert !taskContext.isInitialSnapshotOnly();
+                    throw new ConnectException(""The MySQL server is not configured to use a row-level binlog, which is ""
+                            + ""required for this connector to work properly. Change the MySQL configuration to use a ""
+                            + ""row-level binlog and restart the connector."");
+                }
                 this.snapshotReader.useMinimalBlocking(taskContext.useMinimalSnapshotLocking());
                 if (snapshotEventsAreInserts) this.snapshotReader.generateInsertEvents();
                 this.currentReader = this.snapshotReader;
             } else {
+                if (!rowBinlogEnabled) {
+                    throw new ConnectException(
+                            ""The MySQL server does not appear to be using a row-level binlog, which is required for this connector to work properly. Enable this mode and restart the connector."");
+                }
                 // Just starting to read the binlog ...
                 this.currentReader = this.binlogReader;
             }
 
             // And start our first reader ...
             this.currentReader.start();
+        } catch (RuntimeException e) {
+            this.taskContext.shutdown();
+            throw e;
         } finally {
             prevLoggingContext.restore();
         }
@@ -198,6 +229,10 @@ protected void transitionToReadBinlog() {
         this.currentReader = this.binlogReader;
     }
 
+    protected void skipReadBinlog() {
+        logger.info(""Connector configured to only perform snapshot, and snapshot completed successfully. Connector will terminate."");
+    }
+
     /**
      * Determine whether the binlog position as set on the {@link MySqlTaskContext#source() SourceInfo} is available in the
      * server.
@@ -251,6 +286,29 @@ protected boolean isBinlogAvailable() {
         return found;
     }
 
+    /**
+     * Determine the earliest binlog filename that is still available in the server.
+     * 
+     * @return the name of the earliest binlog filename, or null if there are none.
+     */
+    protected String earliestBinlogFilename() {
+        // Accumulate the available binlog filenames ...
+        List<String> logNames = new ArrayList<>();
+        try {
+            logger.info(""Checking all known binlogs from MySQL"");
+            taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
+                while (rs.next()) {
+                    logNames.add(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking for binary logs: "", e);
+        }
+
+        if (logNames.isEmpty()) return null;
+        return logNames.get(0);
+    }
+
     /**
      * Determine whether the MySQL server has GTIDs enabled.
      * 
@@ -290,4 +348,25 @@ protected String knownGtidSet() {
 
         return gtidSetStr.get();
     }
+
+    /**
+     * Determine whether the MySQL server has the row-level binlog enabled.
+     * 
+     * @return {@code true} if the server's {@code binlog_format} is set to {@code ROW}, or {@code false} otherwise
+     */
+    protected boolean isRowBinlogEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>("""");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'binlog_format'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(2));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at BINLOG mode: "", e);
+        }
+
+        logger.info(""binlog_format={}"" + mode.get());
+        return ""ROW"".equalsIgnoreCase(mode.get());
+    }
 }",2016-09-22T22:09:11Z,68
"@@ -150,6 +150,10 @@ public boolean isSnapshotNeverAllowed() {
         return snapshotMode() == SnapshotMode.NEVER;
     }
 
+    public boolean isInitialSnapshotOnly() {
+        return snapshotMode() == SnapshotMode.INITIAL_ONLY;
+    }
+
     protected SnapshotMode snapshotMode() {
         String value = config.getString(MySqlConnectorConfig.SNAPSHOT_MODE);
         return SnapshotMode.parse(value, MySqlConnectorConfig.SNAPSHOT_MODE.defaultValueAsString());",2016-09-22T22:09:11Z,69
"@@ -20,6 +20,7 @@
 import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
 
+import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
 
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
@@ -130,8 +131,10 @@ protected void doCleanup() {
         try {
             // Call the completion function to say that we've successfully completed
             if (onSuccessfulCompletion != null) onSuccessfulCompletion.run();
+        } catch (RuntimeException e) {
+            throw e;
         } catch (Throwable e) {
-            logger.error(""Error calling completion function after completing snapshot"", e);
+            throw new ConnectException(""Error calling completion function after completing snapshot"", e);
         }
     }
 ",2016-09-22T22:09:11Z,62
"@@ -262,7 +262,7 @@ public void setGtid(String gtid) {
      */
     public void setGtidSet(String gtidSet) {
         if (gtidSet != null && !gtidSet.trim().isEmpty()) {
-            this.gtidSet = gtidSet;
+            this.gtidSet = gtidSet.replaceAll(""\n"", """"); // remove all of the newline chars if they exist
         }
     }
 ",2016-08-23T22:47:19Z,70
"@@ -19,6 +19,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.confluent.connect.avro.AvroData;
+import io.debezium.doc.FixFor;
 import io.debezium.document.Document;
 
 public class SourceInfoTest {
@@ -400,6 +401,33 @@ public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
         assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
     }
 
+    @FixFor(""DBZ-107"")
+    @Test
+    public void shouldRemoveNewlinesFromGtidSet() {
+        String gtidExecuted = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,\n"" +
+                ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3149,\n"" +
+                ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:1-39"";
+        String gtidCleaned = ""036d85a9-64e5-11e6-9b48-42010af0000c:1-2,"" +
+                ""7145bf69-d1ca-11e5-a588-0242ac110004:1-3149,"" +
+                ""7c1de3f2-3fd2-11e6-9cdc-42010af000bc:1-39"";
+        source.setGtidSet(gtidExecuted);
+        assertThat(source.gtidSet()).isEqualTo(gtidCleaned);
+    }
+
+    @FixFor(""DBZ-107"")
+    @Test
+    public void shouldNotSetBlankGtidSet() {
+        source.setGtidSet("""");
+        assertThat(source.gtidSet()).isNull();
+    }
+
+    @FixFor(""DBZ-107"")
+    @Test
+    public void shouldNotSetNullGtidSet() {
+        source.setGtidSet(null);
+        assertThat(source.gtidSet()).isNull();
+    }
+
     protected Document positionWithGtids(String gtids) {
         return positionWithGtids(gtids, false);
     }",2016-08-23T22:47:19Z,70
"@@ -53,3 +53,9 @@ This will use the `mysql:5.7` image to start a new container named `database` wh
 The second volume mount, namely `-v src/test/docker/init:/docker-entrypoint-initdb.d`, makes available all of our existing scripts inside the `src/test/docker/init` directory so that they are run upon server initialization.
 
 The command also defines the same `mysql` database and uses the same username and password(s) as our integration test MySQL container.
+
+### Use MySQL client
+
+The following command can be used to manually start up a Docker container to run the MySQL command line client:
+
+    $ docker run -it --link database:mysql --rm mysql:5.7 sh -c 'exec mysql -h""$MYSQL_PORT_3306_TCP_ADDR"" -P""$MYSQL_PORT_3306_TCP_PORT"" -uroot -p""$MYSQL_ENV_MYSQL_ROOT_PASSWORD""'",2016-05-26T20:58:58Z,107
"@@ -0,0 +1,114 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.Set;
+import java.util.function.Predicate;
+
+import io.debezium.annotation.Immutable;
+import io.debezium.config.Configuration;
+import io.debezium.relational.ColumnId;
+import io.debezium.relational.Selectors;
+import io.debezium.relational.TableId;
+import io.debezium.relational.mapping.ColumnMappers;
+import io.debezium.util.Collect;
+
+/**
+ * A utility that is contains various filters for acceptable database names, {@link TableId}s, and columns.
+ * 
+ * @author Randall Hauch
+ */
+@Immutable
+public class Filters {
+
+    protected static final Set<String> BUILT_IN_TABLE_NAMES = Collect.unmodifiableSet(""db"", ""user"", ""func"", ""plugin"", ""tables_priv"",
+                                                                                      ""columns_priv"", ""help_topic"", ""help_category"",
+                                                                                      ""help_relation"", ""help_keyword"",
+                                                                                      ""time_zone_name"", ""time_zone"", ""time_zone_transition"",
+                                                                                      ""time_zone_transition_type"", ""time_zone_leap_second"",
+                                                                                      ""proc"", ""procs_priv"", ""general_log"", ""event"",
+                                                                                      ""ndb_binlog_index"",
+                                                                                      ""innodb_table_stats"", ""innodb_index_stats"",
+                                                                                      ""slave_relay_log_info"", ""slave_master_info"",
+                                                                                      ""slave_worker_info"", ""gtid_executed"",
+                                                                                      ""server_cost"", ""engine_cost"");
+    protected static final Set<String> BUILT_IN_DB_NAMES = Collect.unmodifiableSet(""mysql"", ""performance_schema"");
+
+    private final Predicate<String> dbFilter;
+    private final Predicate<TableId> tableFilter;
+    private final Predicate<String> isBuiltInDb;
+    private final Predicate<TableId> isBuiltInTable;
+    private final Predicate<ColumnId> columnFilter;
+    private final ColumnMappers columnMappers;
+
+    /**
+     * @param config the configuration; may not be null
+     */
+    public Filters(Configuration config) {
+        this.isBuiltInDb = (dbName) -> {
+            return BUILT_IN_DB_NAMES.contains(dbName.toLowerCase());
+        };
+        this.isBuiltInTable = (id) -> {
+            return isBuiltInDb.test(id.catalog()) || BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase());
+        };
+
+        // Define the filter used for database names ...
+        Predicate<String> dbFilter = Selectors.databaseSelector()
+                                              .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
+                                              .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
+                                              .build();
+
+        // Define the filter using the whitelists and blacklists for tables and database names ...
+        Predicate<TableId> tableFilter = Selectors.tableSelector()
+                                                  .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
+                                                  .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
+                                                  .includeTables(config.getString(MySqlConnectorConfig.TABLE_WHITELIST))
+                                                  .excludeTables(config.getString(MySqlConnectorConfig.TABLE_BLACKLIST))
+                                                  .build();
+
+        // Ignore built-in databases and tables ...
+        if (config.getBoolean(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN)) {
+            this.tableFilter = tableFilter.and(isBuiltInTable.negate());
+            this.dbFilter = dbFilter.and(isBuiltInDb.negate());
+        } else {
+            this.tableFilter = tableFilter.or(isBuiltInTable);
+            this.dbFilter = dbFilter.or(isBuiltInDb);
+        }
+
+        // Define the filter that excludes blacklisted columns, truncated columns, and masked columns ...
+        this.columnFilter = Selectors.excludeColumns(config.getString(MySqlConnectorConfig.COLUMN_BLACKLIST));
+
+        // Define the truncated, masked, and mapped columns ...
+        ColumnMappers.Builder columnMapperBuilder = ColumnMappers.create();
+        config.forEachMatchingFieldNameWithInteger(""column\\.truncate\\.to\\.(\\d+)\\.chars"", columnMapperBuilder::truncateStrings);
+        config.forEachMatchingFieldNameWithInteger(""column\\.mask\\.with\\.(\\d+)\\.chars"", columnMapperBuilder::maskStrings);
+        this.columnMappers = columnMapperBuilder.build();
+    }
+
+    public Predicate<String> databaseFilter() {
+        return dbFilter;
+    }
+
+    public Predicate<TableId> tableFilter() {
+        return tableFilter;
+    }
+
+    public Predicate<TableId> builtInTableFilter() {
+        return isBuiltInTable;
+    }
+
+    public Predicate<String> builtInDatabaseFilter() {
+        return isBuiltInDb;
+    }
+
+    public Predicate<ColumnId> columnFilter() {
+        return columnFilter;
+    }
+
+    public ColumnMappers columnMappers() {
+        return columnMappers;
+    }
+}",2016-05-26T20:58:58Z,108
"@@ -12,14 +12,12 @@
 import java.util.List;
 import java.util.Map;
 import java.util.Queue;
-import java.util.Set;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.LinkedBlockingDeque;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Consumer;
-import java.util.function.Predicate;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -40,15 +38,7 @@
 
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
-import io.debezium.relational.ColumnId;
-import io.debezium.relational.Selectors;
-import io.debezium.relational.TableId;
-import io.debezium.relational.Tables;
-import io.debezium.relational.ddl.DdlParser;
-import io.debezium.relational.history.DatabaseHistory;
-import io.debezium.relational.mapping.ColumnMappers;
 import io.debezium.util.Clock;
-import io.debezium.util.Collect;
 import io.debezium.util.Metronome;
 
 /**
@@ -60,35 +50,21 @@
 @NotThreadSafe
 public final class MySqlConnectorTask extends SourceTask {
 
-    private final Set<String> BUILT_IN_TABLE_NAMES = Collect.unmodifiableSet(""db"", ""user"", ""func"", ""plugin"", ""tables_priv"",
-                                                                             ""columns_priv"", ""help_topic"", ""help_category"",
-                                                                             ""help_relation"", ""help_keyword"",
-                                                                             ""time_zone_name"", ""time_zone"", ""time_zone_transition"",
-                                                                             ""time_zone_transition_type"", ""time_zone_leap_second"",
-                                                                             ""proc"", ""procs_priv"", ""general_log"", ""event"",
-                                                                             ""ndb_binlog_index"",
-                                                                             ""innodb_table_stats"", ""innodb_index_stats"",
-                                                                             ""slave_relay_log_info"", ""slave_master_info"",
-                                                                             ""slave_worker_info"", ""gtid_executed"",
-                                                                             ""server_cost"", ""engine_cost"");
-    private final Set<String> BUILT_IN_DB_NAMES = Collect.unmodifiableSet(""mysql"", ""performance_schema"");
-
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final TopicSelector topicSelector;
+    private final Clock clock = Clock.system();
+    private final AtomicBoolean running = new AtomicBoolean(false);
 
     // These are all effectively constants between start(...) and stop(...)
-    private DatabaseHistory dbHistory;
-    private EnumMap<EventType, EventHandler> eventHandlers = new EnumMap<>(EventType.class);
-    private Tables tables;
-    private TableConverters tableConverters;
-    private BinaryLogClient client;
+    private MySqlSchema dbSchema;
+    private String serverName;
+    private int maxBatchSize;
     private BlockingQueue<Event> events;
     private Queue<Event> batchEvents;
-    private int maxBatchSize;
-    private String serverName;
+    private EnumMap<EventType, EventHandler> eventHandlers = new EnumMap<>(EventType.class);
+    private TableConverters tableConverters;
     private Metronome metronome;
-    private final Clock clock = Clock.system();
-    private final AtomicBoolean running = new AtomicBoolean(false);
+    private BinaryLogClient client;
 
     // Used in the methods that process events ...
     private final SourceInfo source = new SourceInfo();
@@ -100,7 +76,7 @@ public final class MySqlConnectorTask extends SourceTask {
      */
     public MySqlConnectorTask() {
         this.topicSelector = TopicSelector.defaultSelector();
-        this.dbHistory = null; // delay creating the history until startup, which is only allowed by default constructor
+        this.dbSchema = null; // delay creating the history until startup, which is only allowed by default constructor
     }
 
     @Override
@@ -121,15 +97,8 @@ public void start(Map<String, String> props) {
         }
 
         // Create and configure the database history ...
-        this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
-        if (this.dbHistory == null) {
-            throw new ConnectException(""Unable to instantiate the database history class "" +
-                    config.getString(MySqlConnectorConfig.DATABASE_HISTORY));
-        }
-        Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false); // do not remove
-                                                                                                                 // prefix
-        this.dbHistory.configure(dbHistoryConfig); // validates
-        this.dbHistory.start();
+        this.dbSchema = new MySqlSchema(config);
+        this.dbSchema.start();
         this.running.set(true);
 
         // Read the configuration ...
@@ -148,43 +117,12 @@ public void start(Map<String, String> props) {
         maxBatchSize = config.getInteger(MySqlConnectorConfig.MAX_BATCH_SIZE);
         metronome = Metronome.parker(pollIntervalMs, TimeUnit.MILLISECONDS, Clock.SYSTEM);
 
-        // Define the filter used for database names ...
-        Predicate<String> dbFilter = Selectors.databaseSelector()
-                                              .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
-                                              .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
-                                              .build();
-
-        // Define the filter using the whitelists and blacklists for tables and database names ...
-        Predicate<TableId> tableFilter = Selectors.tableSelector()
-                                                  .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
-                                                  .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
-                                                  .includeTables(config.getString(MySqlConnectorConfig.TABLE_WHITELIST))
-                                                  .excludeTables(config.getString(MySqlConnectorConfig.TABLE_BLACKLIST))
-                                                  .build();
-        if (config.getBoolean(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN)) {
-            Predicate<TableId> isBuiltin = (id) -> {
-                return BUILT_IN_DB_NAMES.contains(id.catalog().toLowerCase()) || BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase());
-            };
-            tableFilter = tableFilter.and(isBuiltin.negate());
-        }
-
-        // Define the filter that excludes blacklisted columns, truncated columns, and masked columns ...
-        Predicate<ColumnId> columnFilter = Selectors.excludeColumns(config.getString(MySqlConnectorConfig.COLUMN_BLACKLIST));
-
-        // Define the truncated, masked, and mapped columns ...
-        ColumnMappers.Builder columnMapperBuilder = ColumnMappers.create();
-        config.forEachMatchingFieldNameWithInteger(""column\\.truncate\\.to\\.(\\d+)\\.chars"", columnMapperBuilder::truncateStrings);
-        config.forEachMatchingFieldNameWithInteger(""column\\.mask\\.with\\.(\\d+)\\.chars"", columnMapperBuilder::maskStrings);
-        ColumnMappers columnMappers = columnMapperBuilder.build();
-
         // Create the queue ...
         events = new LinkedBlockingDeque<>(maxQueueSize);
         batchEvents = new ArrayDeque<>(maxBatchSize);
 
         // Set up our handlers for specific kinds of events ...
-        tables = new Tables();
-        tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, clock,
-                                              dbFilter, tables, tableFilter, columnFilter, columnMappers);
+        tableConverters = new TableConverters(topicSelector, dbSchema, clock, includeSchemaChanges);
         eventHandlers.put(EventType.ROTATE, tableConverters::rotateLogs);
         eventHandlers.put(EventType.TABLE_MAP, tableConverters::updateTableMetadata);
         eventHandlers.put(EventType.QUERY, tableConverters::updateTableCommand);
@@ -207,6 +145,13 @@ public void start(Map<String, String> props) {
 
         // Check if we've already processed some of the log for this database ...
         source.setServerName(serverName);
+        
+        // We use the initial binlog filename configuration property to know whether to perform a snapshot
+        // or to start with that (or the previous) binlog position...
+        if ( initialBinLogFilename == null || initialBinLogFilename.trim().isEmpty() ) {
+            // No initial binlog filename was specified, so perform a snapshot ...
+        }
+        
         // Get the offsets for our partition ...
         Map<String, ?> offsets = context.offsetStorageReader().offset(source.partition());
         if (offsets != null) {
@@ -224,11 +169,9 @@ public void start(Map<String, String> props) {
             // to our Tables object. Each of those DDL messages is keyed by the database name, and contains a single string
             // of DDL. However, we should consume no further than offset we recovered above.
             try {
-                logger.info(""Recovering MySQL connector '{}' database schemas from history stored in {}"", serverName, dbHistory);
-                DdlParser ddlParser = new MySqlDdlParser();
-                dbHistory.recover(source.partition(), source.offset(), tables, ddlParser);
-                tableConverters.loadTables();
-                logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, tables.subset(tableFilter));
+                logger.info(""Recovering MySQL connector '{}' database schemas from history stored in {}"", serverName, dbSchema.historyLocation());
+                dbSchema.loadHistory(source);
+                logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, dbSchema.tables());
             } catch (Throwable t) {
                 throw new ConnectException(""Failure while recovering database schemas"", t);
             }
@@ -327,7 +270,7 @@ public void stop() {
 
             // Flush and stop the database history ...
             logger.debug(""Stopping database history for MySQL server '{}'"", serverName);
-            dbHistory.stop();
+            dbSchema.shutdown();
         } catch (Throwable e) {
             logger.error(""Unexpected error shutting down the database history"", e);
         } finally {",2016-05-26T20:58:58Z,68
"@@ -0,0 +1,250 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.errors.ConnectException;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import io.debezium.annotation.NotThreadSafe;
+import io.debezium.config.Configuration;
+import io.debezium.relational.Table;
+import io.debezium.relational.TableId;
+import io.debezium.relational.TableSchema;
+import io.debezium.relational.TableSchemaBuilder;
+import io.debezium.relational.Tables;
+import io.debezium.relational.ddl.DdlChanges;
+import io.debezium.relational.ddl.DdlChanges.DatabaseStatementStringConsumer;
+import io.debezium.relational.history.DatabaseHistory;
+import io.debezium.text.ParsingException;
+import io.debezium.util.Collect;
+
+/**
+ * Component that records the schema history for databases hosted by a MySQL database server. The schema information includes
+ * the {@link Tables table definitions} and the Kafka Connect {@link #schemaFor(TableId) Schema}s for each table, where the
+ * {@link Schema} excludes any columns that have been {@link MySqlConnectorConfig#COLUMN_BLACKLIST specified} in the
+ * configuration.
+ * <p>
+ * The history is changed by {@link #applyDdl(SourceInfo, String, String, DatabaseStatementStringConsumer) applying DDL
+ * statements}, and every change is {@link DatabaseHistory persisted} as defined in the supplied {@link MySqlConnectorConfig MySQL
+ * connector configuration}. This component can be reconstructed (e.g., on connector restart) and the history
+ * {@link #loadHistory(SourceInfo) loaded} from persisted storage.
+ * <p>
+ * Note that when {@link #applyDdl(SourceInfo, String, String, DatabaseStatementStringConsumer) applying DDL statements}, the
+ * caller is able to supply a {@link DatabaseStatementStringConsumer consumer function} that will be called with the DDL
+ * statements and the database to which they apply, grouped by database names. However, these will only be called based when the
+ * databases are included by the database filters defined in the {@link MySqlConnectorConfig MySQL connector configuration}.
+ * 
+ * @author Randall Hauch
+ */
+@NotThreadSafe
+public class MySqlSchema {
+
+    private final Logger logger = LoggerFactory.getLogger(getClass());
+    private final Set<String> ignoredQueryStatements = Collect.unmodifiableSet(""BEGIN"", ""END"", ""FLUSH PRIVILEGES"");
+    private final MySqlDdlParser ddlParser;
+    private final Map<TableId, TableSchema> tableSchemaByTableId = new HashMap<>();
+    private final Filters filters;
+    private final DatabaseHistory dbHistory;
+    private final TableSchemaBuilder schemaBuilder;
+    private final DdlChanges ddlChanges;
+    private Tables tables;
+
+    /**
+     * Create a schema component given the supplied {@link MySqlConnectorConfig MySQL connector configuration}.
+     * 
+     * @param config the connector configuration, which is presumed to be valid
+     */
+    public MySqlSchema(Configuration config) {
+        this.filters = new Filters(config);
+        this.ddlParser = new MySqlDdlParser(false);
+        this.tables = new Tables();
+        this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
+        this.ddlParser.addListener(ddlChanges);
+        this.schemaBuilder = new TableSchemaBuilder();
+
+        // Create and configure the database history ...
+        this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
+        if (this.dbHistory == null) {
+            throw new ConnectException(""Unable to instantiate the database history class "" +
+                    config.getString(MySqlConnectorConfig.DATABASE_HISTORY));
+        }
+        // Do not remove the prefix from the subset of config properties ...
+        Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false);
+        this.dbHistory.configure(dbHistoryConfig); // validates
+    }
+
+    /**
+     * Start by acquiring resources needed to persist the database history
+     */
+    public void start() {
+        this.dbHistory.start();
+    }
+
+    /**
+     * Stop recording history and release any resources acquired since {@link #start()}.
+     */
+    public void shutdown() {
+        this.dbHistory.stop();
+    }
+
+    /**
+     * Get the {@link Filters database and table filters} defined by the configuration.
+     * 
+     * @return the filters; never null
+     */
+    public Filters filters() {
+        return filters;
+    }
+
+    /**
+     * Get all of the table definitions for all database tables as defined by
+     * {@link #applyDdl(SourceInfo, String, String, DatabaseStatementStringConsumer) applied DDL statements}, including those
+     * that have been excluded by the {@link #filters() filters}.
+     * 
+     * @return the table definitions; never null
+     */
+    public Tables tables() {
+        return tables.subset(filters.tableFilter());
+    }
+
+    /**
+     * Get the {@link TableSchema Schema information} for the table with the given identifier, if that table exists and is
+     * included by the {@link #filters() filter}.
+     * 
+     * @param id the fully-qualified table identifier; may be null
+     * @return the current table definition, or null if there is no table with the given identifier, if the identifier is null,
+     *         or if the table has been excluded by the filters
+     */
+    public Table tableFor(TableId id) {
+        return filters.tableFilter().test(id) ? tables.forTable(id) : null;
+    }
+
+    /**
+     * Get the {@link TableSchema Schema information} for the table with the given identifier, if that table exists and is
+     * included by the {@link #filters() filter}.
+     * <p>
+     * Note that the {@link Schema} will not contain any columns that have been {@link MySqlConnectorConfig#COLUMN_BLACKLIST
+     * filtered out}.
+     * 
+     * @param id the fully-qualified table identifier; may be null
+     * @return the schema information, or null if there is no table with the given identifier, if the identifier is null,
+     *         or if the table has been excluded by the filters
+     */
+    public TableSchema schemaFor(TableId id) {
+        return filters.tableFilter().test(id) ? tableSchemaByTableId.get(id) : null;
+    }
+
+    /**
+     * Get the information about where the DDL statement history is recorded.
+     * 
+     * @return the history description; never null
+     */
+    public String historyLocation() {
+        return dbHistory.toString();
+    }
+
+    /**
+     * Load the database schema information using the previously-recorded history, and stop reading the history when the
+     * the history reaches the supplied starting point.
+     * 
+     * @param startingPoint the source information with the current {@link SourceInfo#partition()} and {@link SourceInfo#offset()
+     *            offset} at which the database schemas are to reflect; may not be null
+     */
+    public void loadHistory(SourceInfo startingPoint) {
+        tables = new Tables();
+        dbHistory.recover(startingPoint.partition(), startingPoint.offset(), tables, ddlParser);
+        refreshSchemas();
+    }
+
+    /**
+     * Discard any currently-cached schemas and rebuild them using the filters.
+     */
+    protected void refreshSchemas() {
+        tableSchemaByTableId.clear();
+        // Create TableSchema instances for any existing table ...
+        this.tables.tableIds().forEach(id -> {
+            Table table = this.tables.forTable(id);
+            TableSchema schema = schemaBuilder.create(table, filters.columnFilter(), filters.columnMappers());
+            tableSchemaByTableId.put(id, schema);
+        });
+    }
+
+    /**
+     * Apply the supplied DDL statements to this database schema and record the history. If a {@code statementConsumer} is
+     * supplied, then call it for each sub-sequence of the DDL statements that all apply to the same database.
+     * <p>
+     * Typically DDL statements are applied using a connection to a single database, and unless the statements use fully-qualified
+     * names, the DDL statements apply to this database.
+     * 
+     * @param source the current {@link SourceInfo#partition()} and {@link SourceInfo#offset() offset} at which these changes are
+     *            found; may not be null
+     * @param databaseName the name of the default database under which these statements are applied; may not be null
+     * @param ddlStatements the {@code ;}-separated DDL statements; may be null or empty
+     * @param statementConsumer the consumer that should be called with each sub-sequence of DDL statements that apply to
+     *            a single database; may be null if no action is to be performed with the changes
+     * @return {@code true} if changes were made to the database schema, or {@code false} if the DDL statements had no
+     *         effect on the database schema
+     */
+    public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatements,
+                            DatabaseStatementStringConsumer statementConsumer) {
+        if (ignoredQueryStatements.contains(ddlStatements)) return false;
+        try {
+            this.ddlChanges.reset();
+            this.ddlParser.setCurrentSchema(databaseName);
+            this.ddlParser.parse(ddlStatements, tables);
+        } catch (ParsingException e) {
+            logger.error(""Error parsing DDL statement and updating tables: {}"", ddlStatements, e);
+        } finally {
+            if (statementConsumer != null) {
+
+                // We are supposed to _also_ record the schema changes as SourceRecords, but these need to be filtered
+                // by database. Unfortunately, the databaseName on the event might not be the same database as that
+                // being modified by the DDL statements (since the DDL statements can have fully-qualified names).
+                // Therefore, we have to look at each statement to figure out which database it applies and then
+                // record the DDL statements (still in the same order) to those databases.
+
+                if (!ddlChanges.isEmpty() && ddlChanges.applyToMoreDatabasesThan(databaseName)) {
+
+                    // We understood at least some of the DDL statements and can figure out to which database they apply.
+                    // They also apply to more databases than 'databaseName', so we need to apply the DDL statements in
+                    // the same order they were read for each _affected_ database, grouped together if multiple apply
+                    // to the same _affected_ database...
+                    ddlChanges.groupStatementStringsByDatabase((dbName, ddl) -> {
+                        if (filters.databaseFilter().test(dbName)) {
+                            statementConsumer.consume(databaseName, ddlStatements);
+                        }
+                    });
+                } else if (filters.databaseFilter().test(databaseName)) {
+                    statementConsumer.consume(databaseName, ddlStatements);
+                }
+            }
+
+            // Record the DDL statement so that we can later recover them if needed. We do this _after_ writing the
+            // schema change records so that failure recovery (which is based on of the history) won't lose
+            // schema change records.
+            dbHistory.record(source.partition(), source.offset(), databaseName, tables, ddlStatements);
+        }
+
+        // Figure out what changed ...
+        Set<TableId> changes = tables.drainChanges();
+        changes.forEach(tableId -> {
+            Table table = tables.forTable(tableId);
+            if (table == null) { // removed
+                tableSchemaByTableId.remove(tableId);
+            } else {
+                TableSchema schema = schemaBuilder.create(table, filters.columnFilter(), filters.columnMappers());
+                tableSchemaByTableId.put(tableId, schema);
+            }
+        });
+        return true;
+    }
+}",2016-05-26T20:58:58Z,19
"@@ -8,14 +8,11 @@
 import java.io.Serializable;
 import java.util.BitSet;
 import java.util.HashMap;
-import java.util.HashSet;
 import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Objects;
-import java.util.Set;
 import java.util.function.Consumer;
-import java.util.function.Predicate;
 
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.SchemaBuilder;
@@ -34,19 +31,10 @@
 
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.data.Envelope;
-import io.debezium.relational.ColumnId;
-import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.relational.TableSchema;
-import io.debezium.relational.TableSchemaBuilder;
-import io.debezium.relational.Tables;
-import io.debezium.relational.ddl.DdlChanges;
-import io.debezium.relational.history.DatabaseHistory;
 import io.debezium.relational.history.HistoryRecord.Fields;
-import io.debezium.relational.mapping.ColumnMappers;
-import io.debezium.text.ParsingException;
 import io.debezium.util.Clock;
-import io.debezium.util.Collect;
 
 /**
  * @author Randall Hauch
@@ -82,54 +70,22 @@ public Struct schemaChangeRecordValue(SourceInfo source, String databaseName, St
     }
 
     private final Logger logger = LoggerFactory.getLogger(getClass());
-    private final DatabaseHistory dbHistory;
+    private final MySqlSchema dbSchema;
     private final TopicSelector topicSelector;
-    private final MySqlDdlParser ddlParser;
-    private final DdlChanges ddlChanges;
-    private final Tables tables;
-    private final TableSchemaBuilder schemaBuilder = new TableSchemaBuilder();
-    private final Map<TableId, TableSchema> tableSchemaByTableId = new HashMap<>();
     private final Map<Long, Converter> convertersByTableId = new HashMap<>();
     private final Map<String, Long> tableNumbersByTableName = new HashMap<>();
     private final boolean recordSchemaChangesInSourceRecords;
-    private final Predicate<String> dbFilter;
-    private final Predicate<TableId> tableFilter;
-    private final Predicate<ColumnId> columnFilter;
-    private final ColumnMappers columnMappers;
-    private final Set<String> ignoredQueryStatements = Collect.unmodifiableSet(""BEGIN"", ""END"", ""FLUSH PRIVILEGES"");
-    private final Set<TableId> unknownTableIds = new HashSet<>();
     private final Clock clock;
 
-    public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
-            boolean recordSchemaChangesInSourceRecords, Clock clock, Predicate<String> dbFilter, Tables tables,
-            Predicate<TableId> tableFilter, Predicate<ColumnId> columnFilter, ColumnMappers columnSelectors) {
+    public TableConverters(TopicSelector topicSelector, MySqlSchema dbSchema, Clock clock,
+            boolean recordSchemaChangesInSourceRecords) {
         Objects.requireNonNull(topicSelector, ""A topic selector is required"");
-        Objects.requireNonNull(dbHistory, ""Database history storage is required"");
-        Objects.requireNonNull(tables, ""A Tables object is required"");
         Objects.requireNonNull(clock, ""A Clock object is required"");
-        Objects.requireNonNull(dbFilter, ""A database filter object is required"");
+        Objects.requireNonNull(dbSchema, ""A DatabaseSchema object is required"");
         this.topicSelector = topicSelector;
-        this.dbHistory = dbHistory;
+        this.dbSchema = dbSchema;
         this.clock = clock;
-        this.dbFilter = dbFilter;
-        this.tables = tables;
-        this.columnFilter = columnFilter;
-        this.columnMappers = columnSelectors;
-        this.ddlParser = new MySqlDdlParser(false); // don't include views
-        this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
-        this.ddlParser.addListener(ddlChanges);
         this.recordSchemaChangesInSourceRecords = recordSchemaChangesInSourceRecords;
-        Predicate<TableId> knownTables = (id) -> !unknownTableIds.contains(id); // known if not unknown
-        this.tableFilter = tableFilter != null ? tableFilter.and(knownTables) : knownTables;
-    }
-
-    public void loadTables() {
-        // Create TableSchema instances for any existing table ...
-        this.tables.tableIds().forEach(id -> {
-            Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(table, columnFilter, columnMappers);
-            tableSchemaByTableId.put(id, schema);
-        });
     }
 
     public void rotateLogs(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
@@ -147,76 +103,19 @@ public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRe
         QueryEventData command = event.getData();
         // The command's database is the one that the client was using when submitting the DDL statements,
         // and that might not be the database(s) affected by the DDL statements ...
-        String databaseName = command.getDatabase();
-        String ddlStatements = command.getSql();
-        if (ignoredQueryStatements.contains(ddlStatements)) return;
         logger.debug(""Received update table command: {}"", event);
-        try {
-            this.ddlChanges.reset();
-            this.ddlParser.setCurrentSchema(databaseName);
-            this.ddlParser.parse(ddlStatements, tables);
-        } catch (ParsingException e) {
-            logger.error(""Error parsing DDL statement and updating tables: {}"", ddlStatements, e);
-        } finally {
+        dbSchema.applyDdl(source, command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords) {
-
-                // We are supposed to _also_ record the schema changes as SourceRecords, but these need to be filtered
-                // by database. Unfortunately, the databaseName on the event might not be the same database as that
-                // being modified by the DDL statements (since the DDL statements can have fully-qualified names).
-                // Therefore, we have to look at each statement to figure out which database it applies and then
-                // record the DDL statements (still in the same order) to those databases.
-                
-                if ( !ddlChanges.isEmpty() && ddlChanges.applyToMoreDatabasesThan(databaseName) ) {
-                    
-                    // We understood at least some of the DDL statements and can figure out to which database they apply.
-                    // They also apply to more databases than 'databaseName', so we need to apply the DDL statements in
-                    // the same order they were read for each _affected_ database, grouped together if multiple apply
-                    // to the same _affected_ database...
-                    ddlChanges.groupStatementStringsByDatabase((dbName, statements) -> {
-                        if (dbFilter.test(dbName)) {
-                            String serverName = source.serverName();
-                            String topicName = topicSelector.getTopic(serverName);
-                            Integer partition = 0;
-                            Struct key = schemaChangeRecordKey(databaseName);
-                            Struct value = schemaChangeRecordValue(source, dbName, statements);
-                            SourceRecord record = new SourceRecord(source.partition(), source.offset(),
-                                    topicName, partition,
-                                    SCHEMA_CHANGE_RECORD_KEY_SCHEMA, key,
-                                    SCHEMA_CHANGE_RECORD_VALUE_SCHEMA, value);
-                            recorder.accept(record);
-                        }
-                    });
-                } else if (dbFilter.test(databaseName)) {
-                    // Either all of the statements applied to 'databaseName', or we didn't understand any of the statements.
-                    // But the database filter includes 'databaseName' so we should forward all of the statements ...
-                    String serverName = source.serverName();
-                    String topicName = topicSelector.getTopic(serverName);
-                    Integer partition = 0;
-                    Struct key = schemaChangeRecordKey(databaseName);
-                    Struct value = schemaChangeRecordValue(source, databaseName, ddlStatements);
-                    SourceRecord record = new SourceRecord(source.partition(), source.offset(),
-                            topicName, partition,
-                            SCHEMA_CHANGE_RECORD_KEY_SCHEMA, key,
-                            SCHEMA_CHANGE_RECORD_VALUE_SCHEMA, value);
-                    recorder.accept(record);
-                }
-            }
-
-            // Record the DDL statement so that we can later recover them if needed. We do this _after_ writing the
-            // schema change records so that failure recovery (which is based on of the history) won't lose
-            // schema change records.
-            dbHistory.record(source.partition(), source.offset(), databaseName, tables, ddlStatements);
-        }
-
-        // Figure out what changed ...
-        Set<TableId> changes = tables.drainChanges();
-        changes.forEach(tableId -> {
-            Table table = tables.forTable(tableId);
-            if (table == null) { // removed
-                tableSchemaByTableId.remove(tableId);
-            } else {
-                TableSchema schema = schemaBuilder.create(table, columnFilter, columnMappers);
-                tableSchemaByTableId.put(tableId, schema);
+                String serverName = source.serverName();
+                String topicName = topicSelector.getTopic(serverName);
+                Integer partition = 0;
+                Struct key = schemaChangeRecordKey(dbName);
+                Struct value = schemaChangeRecordValue(source, dbName, statements);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(),
+                        topicName, partition,
+                        SCHEMA_CHANGE_RECORD_KEY_SCHEMA, key,
+                        SCHEMA_CHANGE_RECORD_VALUE_SCHEMA, value);
+                recorder.accept(record);
             }
         });
     }
@@ -250,15 +149,12 @@ public void updateTableMetadata(Event event, SourceInfo source, Consumer<SourceR
 
             // Just get the current schema, which should be up-to-date ...
             TableId tableId = new TableId(databaseName, null, tableName);
-            TableSchema tableSchema = tableSchemaByTableId.get(tableId);
+            TableSchema tableSchema = dbSchema.schemaFor(tableId);
             logger.debug(""Registering metadata for table {} with table #{}"", tableId, tableNumber);
             if (tableSchema == null) {
-                // We are seeing an event for a row that's in a table we don't know about, meaning the table
-                // was created before the binlog was enabled (or before the point we started reading it).
-                if (unknownTableIds.add(tableId)) {
-                    logger.warn(""Transaction affects rows in {}, for which no metadata exists. All subsequent changes to rows in this table will be ignored."",
-                                tableId);
-                }
+                // We are seeing an event for a row that's in a table we don't know about or that has been filtered out ...
+                logger.debug(""Skipping update table metadata event: {}"", event);
+                return;
             }
             // Specify the envelope structure for this table's messages ...
             Envelope envelope = Envelope.defineSchema()
@@ -328,32 +224,28 @@ public void handleInsert(Event event, SourceInfo source, Consumer<SourceRecord>
         Converter converter = convertersByTableId.get(tableNumber);
         if (converter != null) {
             TableId tableId = converter.tableId();
-            if (tableFilter.test(tableId)) {
-                logger.debug(""Processing insert row event for {}: {}"", tableId, event);
-                String topic = converter.topic();
-                Integer partitionNum = converter.partition();
-                List<Serializable[]> rows = write.getRows();
-                Long ts = clock.currentTimeInMillis();
-                for (int row = 0; row != rows.size(); ++row) {
-                    Serializable[] values = rows.get(row);
-                    Schema keySchema = converter.keySchema();
-                    Object key = converter.createKey(values, includedColumns);
-                    Struct value = converter.createValue(values, includedColumns);
-                    if (value != null || key != null) {
-                        Envelope envelope = converter.envelope();
-                        Map<String, ?> partition = source.partition();
-                        Map<String, ?> offset = source.offset(row);
-                        Struct origin = source.struct();
-                        SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
-                                keySchema, key, envelope.schema(), envelope.create(value, origin, ts));
-                        recorder.accept(record);
-                    }
+            logger.debug(""Processing insert row event for {}: {}"", tableId, event);
+            String topic = converter.topic();
+            Integer partitionNum = converter.partition();
+            List<Serializable[]> rows = write.getRows();
+            Long ts = clock.currentTimeInMillis();
+            for (int row = 0; row != rows.size(); ++row) {
+                Serializable[] values = rows.get(row);
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(values, includedColumns);
+                Struct value = converter.createValue(values, includedColumns);
+                if (value != null || key != null) {
+                    Envelope envelope = converter.envelope();
+                    Map<String, ?> partition = source.partition();
+                    Map<String, ?> offset = source.offset(row);
+                    Struct origin = source.struct();
+                    SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
+                            keySchema, key, envelope.schema(), envelope.create(value, origin, ts));
+                    recorder.accept(record);
                 }
-            } else if (logger.isDebugEnabled()) {
-                logger.debug(""Skipping insert row event: {}"", event);
             }
         } else {
-            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
+            logger.debug(""Skipping insert row event: {}"", event);
         }
     }
 
@@ -372,53 +264,49 @@ public void handleUpdate(Event event, SourceInfo source, Consumer<SourceRecord>
         Converter converter = convertersByTableId.get(tableNumber);
         if (converter != null) {
             TableId tableId = converter.tableId();
-            if (tableFilter.test(tableId)) {
-                logger.debug(""Processing update row event for {}: {}"", tableId, event);
-                String topic = converter.topic();
-                Integer partitionNum = converter.partition();
-                Long ts = clock.currentTimeInMillis();
-                List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
-                for (int row = 0; row != rows.size(); ++row) {
-                    Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
-                    Serializable[] before = changes.getKey();
-                    Serializable[] after = changes.getValue();
-                    Schema keySchema = converter.keySchema();
-                    Object key = converter.createKey(after, includedColumns);
-                    Object oldKey = converter.createKey(before, includedColumns);
-                    Struct valueBefore = converter.createValue(before, includedColumnsBefore);
-                    Struct valueAfter = converter.createValue(after, includedColumns);
-                    if (valueAfter != null || key != null) {
-                        Envelope envelope = converter.envelope();
-                        Map<String, ?> partition = source.partition();
-                        Map<String, ?> offset = source.offset(row);
-                        Struct origin = source.struct();
-                        if (key != null && !Objects.equals(key, oldKey)) {
-                            // The key has indeed changed, so first send a create event ...
-                            SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
-                                    keySchema, key, envelope.schema(), envelope.create(valueAfter, origin, ts));
-                            recorder.accept(record);
-
-                            // then send a delete event for the old key ...
-                            record = new SourceRecord(partition, offset, topic, partitionNum,
-                                    keySchema, oldKey, envelope.schema(), envelope.delete(valueBefore, origin, ts));
-                            recorder.accept(record);
-
-                            // Send a tombstone event for the old key ...
-                            record = new SourceRecord(partition, offset, topic, partitionNum, keySchema, oldKey, null, null);
-                            recorder.accept(record);
-                        } else {
-                            // The key has not changed, so a simple update is fine ...
-                            SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
-                                    keySchema, key, envelope.schema(), envelope.update(valueBefore, valueAfter, origin, ts));
-                            recorder.accept(record);
-                        }
+            logger.debug(""Processing update row event for {}: {}"", tableId, event);
+            String topic = converter.topic();
+            Integer partitionNum = converter.partition();
+            Long ts = clock.currentTimeInMillis();
+            List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
+            for (int row = 0; row != rows.size(); ++row) {
+                Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
+                Serializable[] before = changes.getKey();
+                Serializable[] after = changes.getValue();
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(after, includedColumns);
+                Object oldKey = converter.createKey(before, includedColumns);
+                Struct valueBefore = converter.createValue(before, includedColumnsBefore);
+                Struct valueAfter = converter.createValue(after, includedColumns);
+                if (valueAfter != null || key != null) {
+                    Envelope envelope = converter.envelope();
+                    Map<String, ?> partition = source.partition();
+                    Map<String, ?> offset = source.offset(row);
+                    Struct origin = source.struct();
+                    if (key != null && !Objects.equals(key, oldKey)) {
+                        // The key has indeed changed, so first send a create event ...
+                        SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
+                                keySchema, key, envelope.schema(), envelope.create(valueAfter, origin, ts));
+                        recorder.accept(record);
+
+                        // then send a delete event for the old key ...
+                        record = new SourceRecord(partition, offset, topic, partitionNum,
+                                keySchema, oldKey, envelope.schema(), envelope.delete(valueBefore, origin, ts));
+                        recorder.accept(record);
+
+                        // Send a tombstone event for the old key ...
+                        record = new SourceRecord(partition, offset, topic, partitionNum, keySchema, oldKey, null, null);
+                        recorder.accept(record);
+                    } else {
+                        // The key has not changed, so a simple update is fine ...
+                        SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
+                                keySchema, key, envelope.schema(), envelope.update(valueBefore, valueAfter, origin, ts));
+                        recorder.accept(record);
                     }
                 }
-            } else if (logger.isDebugEnabled()) {
-                logger.debug(""Skipping update row event: {}"", event);
             }
-        } else {
-            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
+        } else if (logger.isDebugEnabled()) {
+            logger.debug(""Skipping update row event: {}"", event);
         }
     }
 
@@ -429,36 +317,32 @@ public void handleDelete(Event event, SourceInfo source, Consumer<SourceRecord>
         Converter converter = convertersByTableId.get(tableNumber);
         if (converter != null) {
             TableId tableId = converter.tableId();
-            if (tableFilter.test(tableId)) {
-                logger.debug(""Processing delete row event for {}: {}"", tableId, event);
-                String topic = converter.topic();
-                Integer partitionNum = converter.partition();
-                Long ts = clock.currentTimeInMillis();
-                List<Serializable[]> rows = deleted.getRows();
-                for (int row = 0; row != rows.size(); ++row) {
-                    Serializable[] values = rows.get(row);
-                    Schema keySchema = converter.keySchema();
-                    Object key = converter.createKey(values, includedColumns);
-                    Struct value = converter.createValue(values, includedColumns);
-                    if (value != null || key != null) {
-                        Envelope envelope = converter.envelope();
-                        Map<String, ?> partition = source.partition();
-                        Map<String, ?> offset = source.offset(row);
-                        Struct origin = source.struct();
-                        SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
-                                keySchema, key, envelope.schema(), envelope.delete(value, origin, ts));
-                        recorder.accept(record);
-                        // And send a tombstone ...
-                        record = new SourceRecord(partition, offset, topic, partitionNum,
-                                keySchema, key, null, null);
-                        recorder.accept(record);
-                    }
+            logger.debug(""Processing delete row event for {}: {}"", tableId, event);
+            String topic = converter.topic();
+            Integer partitionNum = converter.partition();
+            Long ts = clock.currentTimeInMillis();
+            List<Serializable[]> rows = deleted.getRows();
+            for (int row = 0; row != rows.size(); ++row) {
+                Serializable[] values = rows.get(row);
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(values, includedColumns);
+                Struct value = converter.createValue(values, includedColumns);
+                if (value != null || key != null) {
+                    Envelope envelope = converter.envelope();
+                    Map<String, ?> partition = source.partition();
+                    Map<String, ?> offset = source.offset(row);
+                    Struct origin = source.struct();
+                    SourceRecord record = new SourceRecord(partition, offset, topic, partitionNum,
+                            keySchema, key, envelope.schema(), envelope.delete(value, origin, ts));
+                    recorder.accept(record);
+                    // And send a tombstone ...
+                    record = new SourceRecord(partition, offset, topic, partitionNum,
+                            keySchema, key, null, null);
+                    recorder.accept(record);
                 }
-            } else if (logger.isDebugEnabled()) {
-                logger.debug(""Skipping delete row event: {}"", event);
             }
-        } else {
-            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
+        } else if (logger.isDebugEnabled()) {
+            logger.debug(""Skipping delete row event: {}"", event);
         }
     }
 ",2016-05-26T20:58:58Z,109
"@@ -0,0 +1,83 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.nio.file.Path;
+
+import io.debezium.config.Configuration;
+import io.debezium.config.Field;
+import io.debezium.relational.history.FileDatabaseHistory;
+
+/**
+ * A helper for easily building connector configurations for testing.
+ * 
+ * @author Randall Hauch
+ */
+public class Configurator {
+
+    private Configuration.Builder configBuilder = Configuration.create();
+
+    public Configurator with(Field field, String value) {
+        configBuilder.with(field, value);
+        return this;
+    }
+
+    public Configurator with(Field field, boolean value) {
+        configBuilder.with(field, value);
+        return this;
+    }
+
+    public Configurator includeDatabases(String regexList) {
+        return with(MySqlConnectorConfig.DATABASE_WHITELIST, regexList);
+    }
+
+    public Configurator excludeDatabases(String regexList) {
+        return with(MySqlConnectorConfig.DATABASE_BLACKLIST, regexList);
+    }
+
+    public Configurator includeTables(String regexList) {
+        return with(MySqlConnectorConfig.TABLE_WHITELIST, regexList);
+    }
+
+    public Configurator excludeTables(String regexList) {
+        return with(MySqlConnectorConfig.TABLE_BLACKLIST, regexList);
+    }
+
+    public Configurator excludeColumns(String regexList) {
+        return with(MySqlConnectorConfig.COLUMN_BLACKLIST, regexList);
+    }
+
+    public Configurator truncateColumns(int length, String fullyQualifiedTableNames) {
+        return with(MySqlConnectorConfig.TRUNCATE_COLUMN(length), fullyQualifiedTableNames);
+    }
+
+    public Configurator maskColumns(int length, String fullyQualifiedTableNames) {
+        return with(MySqlConnectorConfig.MASK_COLUMN(length), fullyQualifiedTableNames);
+    }
+
+    public Configurator excludeBuiltInTables() {
+        return with(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN, true);
+    }
+
+    public Configurator includeBuiltInTables() {
+        return with(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN, false);
+    }
+
+    public Configurator storeDatabaseHistoryInFile(Path path) {
+        with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class.getName());
+        with(FileDatabaseHistory.FILE_PATH,path.toAbsolutePath().toString());
+        return this;
+    }
+
+    public Filters createFilters() {
+        return new Filters(configBuilder.build());
+    }
+
+    public MySqlSchema createSchemas() {
+        return new MySqlSchema(configBuilder.build());
+    }
+
+}
\ No newline at end of file",2016-05-26T20:58:58Z,110
"@@ -0,0 +1,306 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.relational.TableId;
+
+/**
+ * @author Randall Hauch
+ */
+public class FiltersTest {
+
+    private Configurator build;
+    private Filters filters;
+
+    @Before
+    public void beforeEach() {
+        build = new Configurator();
+        filters = null;
+    }
+    
+    @Test
+    public void shouldAllowDatabaseListedWithLiteralInWhitelistAndNoDatabaseBlacklist() {
+        filters = build.includeDatabases(""connector_test"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseExcluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowDatabaseListedWithMultipleLiteralsInWhitelistAndNoDatabaseBlacklist() {
+        filters = build.includeDatabases(""connector_test,another_included"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""another_included"");
+        assertDatabaseExcluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowDatabaseListedWithMultipleRegexInWhitelistAndNoDatabaseBlacklist() {
+        filters = build.includeDatabases(""connector.*_test,another_{1}.*"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""another_included"");
+        assertDatabaseIncluded(""another__test"");
+        assertDatabaseExcluded(""conn_test"");
+        assertDatabaseExcluded(""connector-test"");
+        assertDatabaseExcluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowDatabaseListedWithWildcardInWhitelistAndNoDatabaseBlacklist() {
+        filters = build.includeDatabases("".*"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""another_included"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowAllDatabaseExceptSystemWhenWhitelistIsBlank() {
+        filters = build.includeDatabases("""").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+
+    @Test
+    public void shouldNotAllowDatabaseListedWithLiteralInBlacklistAndNoDatabaseWhitelist() {
+        filters = build.excludeDatabases(""connector_test"").createFilters();
+        assertDatabaseExcluded(""connector_test"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldNotAllowDatabaseListedWithMultipleLiteralsInBlacklistAndNoDatabaseWhitelist() {
+        filters = build.excludeDatabases(""connector_test,another_included"").createFilters();
+        assertDatabaseExcluded(""connector_test"");
+        assertDatabaseExcluded(""another_included"");
+        assertDatabaseIncluded(""other"");
+        assertDatabaseIncluded(""something-else"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldNotAllowDatabaseListedWithMultipleRegexInBlacklistAndNoDatabaseWhitelist() {
+        filters = build.excludeDatabases(""connector.*_test,another_{1}.*"").createFilters();
+        assertDatabaseExcluded(""connector_test"");
+        assertDatabaseExcluded(""another_included"");
+        assertDatabaseExcluded(""another__test"");
+        assertDatabaseIncluded(""conn_test"");
+        assertDatabaseIncluded(""connector-test"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldNotAllowDatabaseListedWithWildcardInBlacklistAndNoDatabaseWhitelist() {
+        filters = build.excludeDatabases("".*"").createFilters();
+        assertDatabaseExcluded(""connector_test"");
+        assertDatabaseExcluded(""another_included"");
+        assertDatabaseExcluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldRespectOnlyDatabaseWhitelistWithDatabaseBlacklistAlsoSpecified() {
+        filters = build.includeDatabases(""A,B,C,D.*"").excludeDatabases(""C,B,E"").createFilters();
+        assertDatabaseIncluded(""A"");
+        assertDatabaseIncluded(""B"");
+        assertDatabaseIncluded(""C"");
+        assertDatabaseIncluded(""D"");
+        assertDatabaseIncluded(""D1"");
+        assertDatabaseIncluded(""D_3"");
+        assertDatabaseExcluded(""E"");
+        assertDatabaseExcluded(""another__test"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowAllDatabaseWhenBlacklistIsBlank() {
+        filters = build.excludeDatabases("""").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other"");
+        assertSystemDatabasesExcluded();
+    }
+
+    @Test
+    public void shouldIgnoreDatabaseBlacklistWhenDatabaseWhitelistIsNonEmpty() {
+        filters = build.includeDatabases("".*"").excludeDatabases(""connector_test,other"").createFilters();
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other"");
+        assertDatabaseIncluded(""something_else"");
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithLiteralInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test.table1"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableExcluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithLiteralWithEscapedPeriodInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test[.]table1"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableExcluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+
+        filters = build.includeTables(""connector_test\\.table1"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableExcluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithMultipleLiteralsInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test.table1,connector_test.table2"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableIncluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithMultipleRegexInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test.table[x]?1,connector_test[.](.*)2"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableIncluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithWildcardInWhitelistAndNoTableBlacklistWhenDatabaseIncluded() {
+        filters = build.includeTables(""connector_test[.](.*)"").createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableIncluded(""connector_test.table2"");
+        assertTableIncluded(""connector_test.table3"");
+        assertTableIncluded(""connector_test.ABC"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldAllowTableListedWithLiteralInWhitelistAndNoTableBlacklistWhenDatabaseIncludedButSystemTablesIncluded() {
+        filters = build.includeTables(""connector_test.table1,connector_test.table2"").includeBuiltInTables().createFilters();
+        assertTableIncluded(""connector_test.table1"");
+        assertTableIncluded(""connector_test.table2"");
+        assertTableExcluded(""connector_test.table3"");
+        assertTableExcluded(""other.table1"");
+        assertDatabaseIncluded(""connector_test"");
+        assertDatabaseIncluded(""other_test"");
+        assertSystemTablesIncluded();
+        assertSystemDatabasesIncluded();
+    }
+    
+    @Test
+    public void shouldNotAllowTableWhenNotIncludedInDatabaseWhitelist() {
+        filters = build.includeTables(""db1.table1,db2.table1,db3.*"").includeDatabases(""db1,db3"").createFilters();
+        assertTableIncluded(""db1.table1"");
+        assertTableExcluded(""db1.table2"");
+        assertTableExcluded(""db2.table1"");
+        assertTableExcluded(""db2.table2"");
+        assertTableIncluded(""db3.table1"");
+        assertTableIncluded(""db3.table2"");
+        assertTableExcluded(""db4.table1"");
+        assertTableExcluded(""db4.table2"");
+        assertDatabaseIncluded(""db1"");
+        assertDatabaseIncluded(""db3"");
+        assertDatabaseExcluded(""db2"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    @Test
+    public void shouldNotAllowTableWhenExcludedInDatabaseWhitelist() {
+        filters = build.includeTables(""db1.table1,db2.table1,db3.*"").excludeDatabases(""db2"").createFilters();
+        assertTableIncluded(""db1.table1"");
+        assertTableExcluded(""db1.table2""); // not explicitly included in the tables
+        assertTableExcluded(""db2.table1"");
+        assertTableExcluded(""db2.table2"");
+        assertTableIncluded(""db3.table1"");
+        assertTableIncluded(""db3.table2"");
+        assertTableExcluded(""db4.table1""); // not explicitly included in the tables
+        assertTableExcluded(""db4.table2""); // not explicitly included in the tables
+        assertDatabaseIncluded(""db1"");
+        assertDatabaseIncluded(""db3"");
+        assertDatabaseExcluded(""db2"");
+        assertSystemTablesExcluded();
+        assertSystemDatabasesExcluded();
+    }
+    
+    protected void assertDatabaseIncluded( String databaseName ) {
+        assertThat(filters.databaseFilter().test(databaseName)).isTrue();
+    }
+
+    protected void assertDatabaseExcluded( String databaseName ) {
+        assertThat(filters.databaseFilter().test(databaseName)).isFalse();
+    }
+
+    protected void assertSystemDatabasesExcluded() {
+        Filters.BUILT_IN_DB_NAMES.forEach(this::assertDatabaseExcluded);
+    }
+
+    protected void assertSystemDatabasesIncluded() {
+        Filters.BUILT_IN_DB_NAMES.forEach(this::assertDatabaseIncluded);
+    }
+
+    protected void assertSystemTablesExcluded() {
+        Filters.BUILT_IN_TABLE_NAMES.forEach(tableName->{
+            Filters.BUILT_IN_DB_NAMES.forEach(dbName->{
+                assertTableExcluded(dbName + ""."" + tableName);
+            });
+        });
+    }
+
+    protected void assertSystemTablesIncluded() {
+        Filters.BUILT_IN_TABLE_NAMES.forEach(tableName->{
+            Filters.BUILT_IN_DB_NAMES.forEach(dbName->{
+                assertTableIncluded(dbName + ""."" + tableName);
+            });
+        });
+    }
+
+    protected void assertTableIncluded( String fullyQualifiedTableName ) {
+        TableId id = TableId.parse(fullyQualifiedTableName);
+        assertThat(filters.tableFilter().test(id)).isTrue();
+    }
+
+    protected void assertTableExcluded( String fullyQualifiedTableName ) {
+        TableId id = TableId.parse(fullyQualifiedTableName);
+        assertThat(filters.tableFilter().test(id)).isFalse();
+    }
+
+}",2016-05-26T20:58:58Z,108
"@@ -0,0 +1,185 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Path;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.relational.Table;
+import io.debezium.relational.TableId;
+import io.debezium.relational.TableSchema;
+import io.debezium.util.IoUtil;
+import io.debezium.util.Testing;
+
+/**
+ * @author Randall Hauch
+ */
+public class MySqlSchemaTest {
+
+    private static final Path TEST_FILE_PATH = Testing.Files.createTestingPath(""dbHistory.log"");
+
+    private Configurator build;
+    private MySqlSchema mysql;
+    private SourceInfo source;
+
+    @Before
+    public void beforeEach() {
+        Testing.Files.delete(TEST_FILE_PATH);
+        build = new Configurator();
+        mysql = null;
+        source = new SourceInfo();
+    }
+
+    @After
+    public void afterEach() {
+        if (mysql != null) {
+            try {
+                mysql.shutdown();
+            } finally {
+                mysql = null;
+            }
+        }
+    }
+
+    @Test
+    public void shouldApplyDdlStatementsAndRecover() {
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        mysql.start();
+
+        // Testing.Print.enable();
+        source.setBinlogFilename(""binlog-001"");
+        source.setBinlogPosition(400);
+        mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
+
+        // Check that we have tables ...
+        assertTableIncluded(""connector_test.products"");
+        assertTableIncluded(""connector_test.products_on_hand"");
+        assertTableIncluded(""connector_test.customers"");
+        assertTableIncluded(""connector_test.orders"");
+        assertHistoryRecorded();
+    }
+
+    @Test
+    public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases() {
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .includeDatabases(""connector_test"")
+                     .excludeBuiltInTables()
+                     .createSchemas();
+        mysql.start();
+
+        source.setBinlogFilename(""binlog-001"");
+        source.setBinlogPosition(400);
+        mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
+
+        source.setBinlogPosition(1000);
+        mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
+
+        // Check that we have tables ...
+        assertTableIncluded(""connector_test.products"");
+        assertTableIncluded(""connector_test.products_on_hand"");
+        assertTableIncluded(""connector_test.customers"");
+        assertTableIncluded(""connector_test.orders"");
+        assertTableExcluded(""mysql.columns_priv"");
+        assertNoTablesExistForDatabase(""mysql"");
+        assertHistoryRecorded();
+    }
+
+    @Test
+    public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .includeDatabases(""connector_test"")
+                     .includeBuiltInTables()
+                     .createSchemas();
+        mysql.start();
+
+        source.setBinlogFilename(""binlog-001"");
+        source.setBinlogPosition(400);
+        mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
+
+        source.setBinlogPosition(1000);
+        mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
+
+        // Check that we have tables ...
+        assertTableIncluded(""connector_test.products"");
+        assertTableIncluded(""connector_test.products_on_hand"");
+        assertTableIncluded(""connector_test.customers"");
+        assertTableIncluded(""connector_test.orders"");
+        assertTableIncluded(""mysql.columns_priv"");
+        assertTablesExistForDatabase(""mysql"");
+        assertHistoryRecorded();
+    }
+
+    protected void assertTableIncluded(String fullyQualifiedTableName) {
+        TableId tableId = TableId.parse(fullyQualifiedTableName);
+        assertThat(mysql.tables().forTable(tableId)).isNotNull();
+        assertThat(mysql.schemaFor(tableId)).isNotNull();
+    }
+
+    protected void assertTableExcluded(String fullyQualifiedTableName) {
+        TableId tableId = TableId.parse(fullyQualifiedTableName);
+        assertThat(mysql.tables().forTable(tableId)).isNull();
+        assertThat(mysql.schemaFor(tableId)).isNull();
+    }
+    
+    protected void assertNoTablesExistForDatabase(String dbName) {
+        assertThat(mysql.tables().tableIds().stream().filter(id->id.catalog().equals(dbName)).count()).isEqualTo(0);
+    }
+    protected void assertTablesExistForDatabase(String dbName) {
+        assertThat(mysql.tables().tableIds().stream().filter(id->id.catalog().equals(dbName)).count()).isGreaterThan(0);
+    }
+
+    protected void assertHistoryRecorded() {
+        MySqlSchema duplicate = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        duplicate.loadHistory(source);
+
+        // Make sure table is defined in each ...
+        assertThat(duplicate.tables()).isEqualTo(mysql.tables());
+        for (int i = 0; i != 2; ++i) {
+            duplicate.tables().tableIds().forEach(tableId -> {
+                TableSchema dupSchema = duplicate.schemaFor(tableId);
+                TableSchema schema = mysql.schemaFor(tableId);
+                assertThat(schema).isEqualTo(dupSchema);
+                Table dupTable = duplicate.tables().forTable(tableId);
+                Table table = mysql.tables().forTable(tableId);
+                assertThat(table).isEqualTo(dupTable);
+            });
+            mysql.tables().tableIds().forEach(tableId -> {
+                TableSchema dupSchema = duplicate.schemaFor(tableId);
+                TableSchema schema = mysql.schemaFor(tableId);
+                assertThat(schema).isEqualTo(dupSchema);
+                Table dupTable = duplicate.tables().forTable(tableId);
+                Table table = mysql.tables().forTable(tableId);
+                assertThat(table).isEqualTo(dupTable);
+            });
+            duplicate.refreshSchemas();
+        }
+    }
+
+    protected void printStatements(String dbName, String ddlStatements) {
+        Testing.print(""Running DDL for '"" + dbName + ""': "" + ddlStatements);
+    }
+
+    protected String readFile(String classpathResource) {
+        try (InputStream stream = getClass().getClassLoader().getResourceAsStream(classpathResource);) {
+            assertThat(stream).isNotNull();
+            return IoUtil.read(stream);
+        } catch (IOException e) {
+            fail(""Unable to read '"" + classpathResource + ""'"");
+        }
+        assert false : ""should never get here"";
+        return null;
+    }
+
+}",2016-05-26T20:58:58Z,19
"@@ -0,0 +1,37 @@
+# Create the database that we'll use to populate data and watch the effect in the binlog
+CREATE DATABASE connector_test;
+
+# Create and populate our products using a single insert with many rows
+CREATE TABLE connector_test.products (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  name VARCHAR(255) NOT NULL,
+  description VARCHAR(512),
+  weight FLOAT
+);
+ALTER TABLE connector_test.products AUTO_INCREMENT = 101;
+
+# Create and populate the products on hand using multiple inserts
+CREATE TABLE connector_test.products_on_hand (
+  product_id INTEGER NOT NULL PRIMARY KEY,
+  quantity INTEGER NOT NULL,
+  FOREIGN KEY (product_id) REFERENCES products(id)
+);
+
+# Create some customers ...
+CREATE TABLE connector_test.customers (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  first_name VARCHAR(255) NOT NULL,
+  last_name VARCHAR(255) NOT NULL,
+  email VARCHAR(255) NOT NULL UNIQUE KEY
+) AUTO_INCREMENT=1001;
+
+# Create some veyr simple orders
+CREATE TABLE connector_test.orders (
+  order_number INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  order_date DATE NOT NULL,
+  purchaser INTEGER NOT NULL,
+  quantity INTEGER NOT NULL,
+  product_id INTEGER NOT NULL,
+  FOREIGN KEY order_customer (purchaser) REFERENCES customers(id),
+  FOREIGN KEY ordered_product (product_id) REFERENCES products(id)
+) AUTO_INCREMENT = 10001;
\ No newline at end of file",2016-05-26T20:58:58Z,111
"@@ -54,8 +54,12 @@ public static <T> Predicate<T> includes(String regexPatterns, Function<T, String
         Set<Pattern> patterns = Strings.listOfRegex(regexPatterns,Pattern.CASE_INSENSITIVE);
         return (t) -> {
             String str = conversion.apply(t);
-            for ( Pattern p : patterns ) {
-                if ( p.matcher(str).matches()) return true;
+            if ( str != null ) {
+                for ( Pattern p : patterns ) {
+                    if ( p.matcher(str).matches()) return true;
+                }
+            } else {
+                int x =0;
             }
             return false;
         };",2016-05-26T20:58:58Z,112
"@@ -15,6 +15,18 @@
 @Immutable
 public final class TableId implements Comparable<TableId> {
 
+    /**
+     * Parse the supplied string, extracting up to the first 3 parts into a TableID.
+     * 
+     * @param str the string representation of the table identifier; may not be null
+     * @return the table ID, or null if it could not be parsed
+     */
+    public static TableId parse(String str) {
+        String[] parts = str.split(""[\\"" + '.' + ""]"");
+        if ( parts.length < 0 ) return null;
+        return TableId.parse(parts, parts.length, true);
+    }
+
     /**
      * Parse the supplied string, extracting up to the first 3 parts into a TableID.
      * ",2016-05-26T20:58:58Z,113
"@@ -5,12 +5,14 @@
  */
 package io.debezium.relational;
 
+import java.util.Objects;
 import java.util.function.Function;
 
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.Struct;
 
 import io.debezium.annotation.Immutable;
+import io.debezium.data.SchemaUtil;
 
 /**
  * Defines the Kafka Connect {@link Schema} functionality associated with a given {@link Table table definition}, and which can
@@ -108,4 +110,24 @@ public Object keyFromColumnData(Object[] columnData) {
     public Struct valueFromColumnData(Object[] columnData) {
         return columnData == null ? null : valueGenerator.apply(columnData);
     }
+    
+    @Override
+    public int hashCode() {
+        return valueSchema().hashCode();
+    }
+    
+    @Override
+    public boolean equals(Object obj) {
+        if ( obj == this ) return true;
+        if ( obj instanceof TableSchema ) {
+            TableSchema that = (TableSchema)obj;
+            return Objects.equals(this.keySchema(),that.keySchema()) && Objects.equals(this.valueSchema(),that.valueSchema());
+        }
+        return false;
+    }
+    
+    @Override
+    public String toString() {
+        return ""{ key : "" + SchemaUtil.asString(keySchema()) + "", value : "" + SchemaUtil.asString(valueSchema()) + "" }"";
+    }
 }",2016-05-26T20:58:58Z,114
"@@ -19,15 +19,16 @@
  * @author Randall Hauch
  */
 public interface DatabaseHistory {
-    
+
     public static final String CONFIGURATION_FIELD_PREFIX_STRING = ""database.history."";
-    
+
     /**
      * Configure this instance.
+     * 
      * @param config the configuration for this history store
      */
     void configure(Configuration config);
-    
+
     /**
      * Start the history.
      */
@@ -54,11 +55,12 @@ public interface DatabaseHistory {
      * 
      * @param source the information about the source database; may not be null
      * @param position the point in history at which the {@link Tables database schema} should be recovered; may not be null
-     * @param schema the definition of the schema for the named {@code database}; may not be null
+     * @param schema the table definitions that should be changed to reflect the database schema at the desired point in history;
+     *            may not be null
      * @param ddlParser the DDL parser that can be used to apply DDL statements to the given {@code schema}; may not be null
      */
     void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser);
-    
+
     /**
      * Stop recording history and release any resources acquired since {@link #configure(Configuration)}.
      */",2016-05-26T20:58:58Z,41
"@@ -150,10 +150,10 @@ public void start(Map<String, String> props) {
                                                         config.getString(MySqlConnectorConfig.TABLE_WHITELIST),
                                                         config.getString(MySqlConnectorConfig.TABLE_BLACKLIST));
         if (config.getBoolean(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN)) {
-            Predicate<TableId> ignoreBuiltins = (id) -> {
-                return !BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase()) && !BUILT_IN_DB_NAMES.contains(id.catalog().toLowerCase());
+            Predicate<TableId> isBuiltin = (id) -> {
+                return BUILT_IN_DB_NAMES.contains(id.catalog().toLowerCase()) || BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase());
             };
-            tableFilter = ignoreBuiltins.or(tableFilter);
+            tableFilter = tableFilter.and(isBuiltin.negate());
         }
 
         // Create the queue ...
@@ -163,6 +163,7 @@ public void start(Map<String, String> props) {
         // Set up our handlers for specific kinds of events ...
         tables = new Tables();
         tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, tables, tableFilter);
+        eventHandlers.put(EventType.ROTATE, tableConverters::rotateLogs);
         eventHandlers.put(EventType.TABLE_MAP, tableConverters::updateTableMetadata);
         eventHandlers.put(EventType.QUERY, tableConverters::updateTableCommand);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, tableConverters::handleInsert);",2016-03-17T16:03:28Z,68
"@@ -26,6 +26,7 @@
 import com.github.shyiko.mysql.binlog.event.DeleteRowsEventData;
 import com.github.shyiko.mysql.binlog.event.Event;
 import com.github.shyiko.mysql.binlog.event.QueryEventData;
+import com.github.shyiko.mysql.binlog.event.RotateEventData;
 import com.github.shyiko.mysql.binlog.event.TableMapEventData;
 import com.github.shyiko.mysql.binlog.event.UpdateRowsEventData;
 import com.github.shyiko.mysql.binlog.event.WriteRowsEventData;
@@ -86,6 +87,17 @@ public void loadTables() {
         });
     }
 
+    public void rotateLogs(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
+        logger.debug(""Rotating logs: {}"", event);
+        RotateEventData command = event.getData();
+        if (command != null) {
+            // The logs are being rotated, which means the server was either restarted, or the binlog has transitioned to a new
+            // file. In either case, the table numbers will change, so we need to discard the cache of converters by the table IDs
+            // (e.g., the Map<Long,Converter>). Note, however, that we're NOT clearing out the Map<TableId,TableSchema>.
+            convertersByTableId.clear();
+        }
+    }
+
     public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
         QueryEventData command = event.getData();
         String databaseName = command.getDatabase();
@@ -142,8 +154,8 @@ public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRe
     public void updateTableMetadata(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
         TableMapEventData metadata = event.getData();
         long tableNumber = metadata.getTableId();
+        logger.debug(""Received update table metadata event: {}"", event);
         if (!convertersByTableId.containsKey(tableNumber)) {
-            logger.debug(""Received update table metadata event: {}"", event);
             // We haven't seen this table ID, so we need to rebuild our converter functions ...
             String serverName = source.serverName();
             String databaseName = metadata.getDatabase();
@@ -153,6 +165,7 @@ public void updateTableMetadata(Event event, SourceInfo source, Consumer<SourceR
             // Just get the current schema, which should be up-to-date ...
             TableId tableId = new TableId(databaseName, null, tableName);
             TableSchema tableSchema = tableSchemaByTableId.get(tableId);
+            logger.debug(""Registering metadata for table {} with table #{}"", tableId, tableNumber);
             if (tableSchema == null) {
                 // We are seeing an event for a row that's in a table we don't know about, meaning the table
                 // was created before the binlog was enabled (or before the point we started reading it).
@@ -201,7 +214,7 @@ public Struct inserted(Serializable[] row, BitSet includedColumns) {
                 }
 
                 @Override
-                public Struct updated(Serializable[] after, BitSet includedColumns, Serializable[] before,
+                public Struct updated(Serializable[] before, BitSet includedColumns, Serializable[] after,
                                       BitSet includedColumnsBeforeUpdate) {
                     // assume all columns in the table are included, and we'll write out only the after state ...
                     return tableSchema.valueFromColumnData(after);
@@ -218,6 +231,8 @@ public Struct deleted(Serializable[] deleted, BitSet includedColumns) {
             if (previousTableNumber != null) {
                 convertersByTableId.remove(previousTableNumber);
             }
+        } else if (logger.isDebugEnabled()) {
+            logger.debug(""Skipping update table metadata event: {}"", event);
         }
     }
 
@@ -226,25 +241,30 @@ public void handleInsert(Event event, SourceInfo source, Consumer<SourceRecord>
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         Converter converter = convertersByTableId.get(tableNumber);
-        if (tableFilter.test(converter.tableId())) {
-            logger.debug(""Received insert row event: {}"", event);
-            String topic = converter.topic();
-            Integer partition = converter.partition();
-            List<Serializable[]> rows = write.getRows();
-            for (int row = 0; row != rows.size(); ++row) {
-                Serializable[] values = rows.get(row);
-                Schema keySchema = converter.keySchema();
-                Object key = converter.createKey(values, includedColumns);
-                Schema valueSchema = converter.valueSchema();
-                Struct value = converter.inserted(values, includedColumns);
-                if (value != null || key != null) {
-                    SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                            keySchema, key, valueSchema, value);
-                    recorder.accept(record);
+        if (converter != null) {
+            TableId tableId = converter.tableId();
+            if (tableFilter.test(tableId)) {
+                logger.debug(""Processing insert row event for {}: {}"", tableId, event);
+                String topic = converter.topic();
+                Integer partition = converter.partition();
+                List<Serializable[]> rows = write.getRows();
+                for (int row = 0; row != rows.size(); ++row) {
+                    Serializable[] values = rows.get(row);
+                    Schema keySchema = converter.keySchema();
+                    Object key = converter.createKey(values, includedColumns);
+                    Schema valueSchema = converter.valueSchema();
+                    Struct value = converter.inserted(values, includedColumns);
+                    if (value != null || key != null) {
+                        SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                                keySchema, key, valueSchema, value);
+                        recorder.accept(record);
+                    }
                 }
+            } else if (logger.isDebugEnabled()) {
+                logger.debug(""Skipping insert row event: {}"", event);
             }
-        } else if (logger.isDebugEnabled()) {
-            logger.debug(""Skipping insert row event: {}"", event);
+        } else {
+            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
         }
     }
 
@@ -261,27 +281,32 @@ public void handleUpdate(Event event, SourceInfo source, Consumer<SourceRecord>
         BitSet includedColumns = update.getIncludedColumns();
         BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
         Converter converter = convertersByTableId.get(tableNumber);
-        if (tableFilter.test(converter.tableId())) {
-            logger.debug(""Received update row event: {}"", event);
-            String topic = converter.topic();
-            Integer partition = converter.partition();
-            List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
-            for (int row = 0; row != rows.size(); ++row) {
-                Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
-                Serializable[] before = changes.getKey();
-                Serializable[] after = changes.getValue();
-                Schema keySchema = converter.keySchema();
-                Object key = converter.createKey(after, includedColumns);
-                Schema valueSchema = converter.valueSchema();
-                Struct value = converter.updated(before, includedColumnsBefore, after, includedColumns);
-                if (value != null || key != null) {
-                    SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                            keySchema, key, valueSchema, value);
-                    recorder.accept(record);
+        if (converter != null) {
+            TableId tableId = converter.tableId();
+            if (tableFilter.test(tableId)) {
+                logger.debug(""Processing update row event for {}: {}"", tableId, event);
+                String topic = converter.topic();
+                Integer partition = converter.partition();
+                List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
+                for (int row = 0; row != rows.size(); ++row) {
+                    Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
+                    Serializable[] before = changes.getKey();
+                    Serializable[] after = changes.getValue();
+                    Schema keySchema = converter.keySchema();
+                    Object key = converter.createKey(after, includedColumns);
+                    Schema valueSchema = converter.valueSchema();
+                    Struct value = converter.updated(before, includedColumnsBefore, after, includedColumns);
+                    if (value != null || key != null) {
+                        SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                                keySchema, key, valueSchema, value);
+                        recorder.accept(record);
+                    }
                 }
+            } else if (logger.isDebugEnabled()) {
+                logger.debug(""Skipping update row event: {}"", event);
             }
-        } else if (logger.isDebugEnabled()) {
-            logger.debug(""Skipping update row event: {}"", event);
+        } else {
+            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
         }
     }
 
@@ -290,25 +315,30 @@ public void handleDelete(Event event, SourceInfo source, Consumer<SourceRecord>
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         Converter converter = convertersByTableId.get(tableNumber);
-        if (tableFilter.test(converter.tableId())) {
-            logger.debug(""Received delete row event: {}"", event);
-            String topic = converter.topic();
-            Integer partition = converter.partition();
-            List<Serializable[]> rows = deleted.getRows();
-            for (int row = 0; row != rows.size(); ++row) {
-                Serializable[] values = rows.get(row);
-                Schema keySchema = converter.keySchema();
-                Object key = converter.createKey(values, includedColumns);
-                Schema valueSchema = converter.valueSchema();
-                Struct value = converter.deleted(values, includedColumns);
-                if (value != null || key != null) {
-                    SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                            keySchema, key, valueSchema, value);
-                    recorder.accept(record);
+        if (converter != null) {
+            TableId tableId = converter.tableId();
+            if (tableFilter.test(tableId)) {
+                logger.debug(""Processing delete row event for {}: {}"", tableId, event);
+                String topic = converter.topic();
+                Integer partition = converter.partition();
+                List<Serializable[]> rows = deleted.getRows();
+                for (int row = 0; row != rows.size(); ++row) {
+                    Serializable[] values = rows.get(row);
+                    Schema keySchema = converter.keySchema();
+                    Object key = converter.createKey(values, includedColumns);
+                    Schema valueSchema = converter.valueSchema();
+                    Struct value = converter.deleted(values, includedColumns);
+                    if (value != null || key != null) {
+                        SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                                keySchema, key, valueSchema, value);
+                        recorder.accept(record);
+                    }
                 }
+            } else if (logger.isDebugEnabled()) {
+                logger.debug(""Skipping delete row event: {}"", event);
             }
-        } else if (logger.isDebugEnabled()) {
-            logger.debug(""Skipping delete row event: {}"", event);
+        } else {
+            logger.warn(""Unable to find converter for table #{} in {}"", tableNumber, convertersByTableId);
         }
     }
 
@@ -327,7 +357,7 @@ protected static interface Converter {
 
         Struct inserted(Serializable[] row, BitSet includedColumns);
 
-        Struct updated(Serializable[] after, BitSet includedColumns, Serializable[] before, BitSet includedColumnsBeforeUpdate);
+        Struct updated(Serializable[] before, BitSet includedColumns, Serializable[] after, BitSet includedColumnsBeforeUpdate);
 
         Struct deleted(Serializable[] deleted, BitSet includedColumns);
     }",2016-03-17T16:03:28Z,109
"@@ -70,6 +70,9 @@ public class KafkaDatabaseHistory extends AbstractDatabaseHistory {
     public static Collection<Field> ALL_FIELDS = Collect.arrayListOf(TOPIC, BOOTSTRAP_SERVERS,
                                                                      RECOVERY_POLL_INTERVAL_MS, RECOVERY_POLL_ATTEMPTS);
 
+    private static final String CONSUMER_PREFIX = CONFIGURATION_FIELD_PREFIX_STRING + ""consumer."";
+    private static final String PRODUCER_PREFIX = CONFIGURATION_FIELD_PREFIX_STRING + ""producer."";
+    
     private final DocumentReader reader = DocumentReader.defaultReader();
     private final Integer partition = new Integer(0);
     private String topicName;
@@ -92,7 +95,7 @@ public void configure(Configuration config) {
         String bootstrapServers = config.getString(BOOTSTRAP_SERVERS);
         // Copy the relevant portions of the configuration and add useful defaults ...
         String clientAndGroupId = UUID.randomUUID().toString();
-        this.consumerConfig = config.subset(""consumer."", true).edit()
+        this.consumerConfig = config.subset(CONSUMER_PREFIX, true).edit()
                                     .withDefault(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
                                     .withDefault(ConsumerConfig.CLIENT_ID_CONFIG, clientAndGroupId)
                                     .withDefault(ConsumerConfig.GROUP_ID_CONFIG, clientAndGroupId)
@@ -104,7 +107,7 @@ public void configure(Configuration config) {
                                     .withDefault(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class)
                                     .withDefault(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class)
                                     .build();
-        this.producerConfig = config.subset(""producer."", true).edit()
+        this.producerConfig = config.subset(PRODUCER_PREFIX, true).edit()
                                     .withDefault(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers)
                                     .withDefault(ProducerConfig.CLIENT_ID_CONFIG, UUID.randomUUID().toString())
                                     .withDefault(ProducerConfig.ACKS_CONFIG, 1)
@@ -116,6 +119,8 @@ public void configure(Configuration config) {
                                     .withDefault(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class)
                                     .withDefault(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class)
                                     .build();
+        logger.info(""KafkaDatabaseHistory Consumer config: "" + consumerConfig);
+        logger.info(""KafkaDatabaseHistory Producer config: "" + producerConfig);
     }
 
     @Override",2016-03-17T16:03:28Z,115
"@@ -5,7 +5,6 @@
  */
 package io.debezium.connector.mysql;
 
-import java.nio.charset.StandardCharsets;
 import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -32,7 +31,7 @@
  */
 public class MySqlJdbcContext implements AutoCloseable {
 
-    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false&useSSL=${useSSL}"";
+    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false&useSSL=${useSSL}&useUnicode=true&characterEncoding=UTF-8&characterSetResults=UTF-8"";
     protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(MYSQL_CONNECTION_URL);
 
     protected final Logger logger = LoggerFactory.getLogger(getClass());
@@ -49,10 +48,7 @@ public MySqlJdbcContext(Configuration config) {
         boolean useSSL = sslModeEnabled();
         Configuration jdbcConfig = config.subset(""database."", true)
                                          .edit()
-                                         .with(""useInformationSchema"", ""true"")
-                                         .with(""nullCatalogMeansCurrent"", ""false"")
                                          .with(""useSSL"", Boolean.toString(useSSL))
-                                         .with(""characterEncoding"", StandardCharsets.UTF_8.name())
                                          .build();
         this.jdbc = new JdbcConnection(jdbcConfig, FACTORY);
     }",2016-08-29T20:14:25Z,61
"@@ -122,6 +122,7 @@ public ValueConverter converter(Column column, Field fieldDefn) {
             case Types.SQLXML:
                 Charset charset = charsetFor(column);
                 if (charset != null) {
+                    logger.debug(""Using {} charset by default for column: {}"", charset, column);
                     return (data) -> convertString(column, fieldDefn, charset, data);
                 }
                 logger.warn(""Using UTF-8 charset by default for column without charset: {}"", column);",2016-08-29T20:14:25Z,47
"@@ -237,4 +237,16 @@ CREATE TABLE dbz_100_enumsettest (
 );
 INSERT INTO dbz_100_enumsettest VALUES ('a', 'a,b,c');
 INSERT INTO dbz_100_enumsettest VALUES ('b', 'b,a');
-INSERT INTO dbz_100_enumsettest VALUES ('c', 'a');
\ No newline at end of file
+INSERT INTO dbz_100_enumsettest VALUES ('c', 'a');
+
+-- DBZ-102 handle character sets
+-- Use session variables to dictate the character sets used by the client running these commands so
+-- the literal value is interpretted correctly...
+set character_set_client=utf8;
+set character_set_connection=utf8;
+CREATE TABLE dbz_102_charsettest (
+  id INT(11) NOT NULL AUTO_INCREMENT,
+  text VARCHAR(255) DEFAULT NULL,
+  PRIMARY KEY (`id`)
+) ENGINE=InnoDB AUTO_INCREMENT=2001 DEFAULT CHARSET=utf8;
+INSERT INTO dbz_102_charsettest VALUES (default, """");
\ No newline at end of file",2016-08-29T20:14:25Z,79
"@@ -86,17 +86,18 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
         // Testing.Debug.enable();
-        SourceRecords records = consumeRecordsByTopic(5 + 6); // 5 schema change record, 6 inserts
+        SourceRecords records = consumeRecordsByTopic(6 + 7); // 5 schema change record, 7 inserts
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(6);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_100_enumsettest"").size()).isEqualTo(3);
-        assertThat(records.topics().size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_102_charsettest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(6);
         assertThat(records.databaseNames().size()).isEqualTo(1);
-        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(5);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(6);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
@@ -118,6 +119,10 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 } else {
                     fail(""c1 didn't match expected value"");
                 }
+            } else if (record.topic().endsWith(""dbz_102_charsettest"")) {
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                String text = after.getString(""text"");
+                assertThat(text).isEqualTo("""");
             } else if (record.topic().endsWith(""dbz_85_fractest"")) {
                 // The microseconds of all three should be exactly 780
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
@@ -207,17 +212,18 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
         // Testing.Debug.enable();
-        SourceRecords records = consumeRecordsByTopic(5 + 6); // 5 schema change record, 6 inserts
+        SourceRecords records = consumeRecordsByTopic(6 + 7); // 6 schema change record, 7 inserts
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(6);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_100_enumsettest"").size()).isEqualTo(3);
-        assertThat(records.topics().size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_102_charsettest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(6);
         assertThat(records.databaseNames().size()).isEqualTo(1);
-        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(5);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(6);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
@@ -239,6 +245,10 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                 } else {
                     fail(""c1 didn't match expected value"");
                 }
+            } else if (record.topic().endsWith(""dbz_102_charsettest"")) {
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                String text = after.getString(""text"");
+                assertThat(text).isEqualTo("""");
             } else if (record.topic().endsWith(""dbz_85_fractest"")) {
                 // The microseconds of all three should be exactly 780
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);
@@ -324,21 +334,27 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         // ---------------------------------------------------------------------------------------------------------------
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
-        // Testing.Debug.enable();
-        // 12 schema change records = 1 set variables, 5 drop tables, 1 drop database, 1 create database, 1 use database, 5 create
-        // tables
-        SourceRecords records = consumeRecordsByTopic(12 + 6); // plus 6 data records ...
+        Testing.Debug.enable();
+        // We expect a total of 14 schema change records:
+        // 1 set variables
+        // 6 drop tables
+        // 1 drop database
+        // 1 create database
+        // 1 use database
+        // 6 create tables
+        SourceRecords records = consumeRecordsByTopic(14 + 7); // plus 7 data records ...
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(12);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(14);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz_100_enumsettest"").size()).isEqualTo(3);
-        assertThat(records.topics().size()).isEqualTo(5);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_102_charsettest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(6);
         assertThat(records.databaseNames().size()).isEqualTo(2);
         assertThat(records.databaseNames()).containsOnly(""regression_test"","""");
-        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(11);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(13);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase("""").size()).isEqualTo(1); // SET statement
@@ -361,6 +377,10 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                 } else {
                     fail(""c1 didn't match expected value"");
                 }
+            } else if (record.topic().endsWith(""dbz_102_charsettest"")) {
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                String text = after.getString(""text"");
+                assertThat(text).isEqualTo("""");
             } else if (record.topic().endsWith(""dbz_85_fractest"")) {
                 // The microseconds of all three should be exactly 780
                 Struct after = value.getStruct(Envelope.FieldName.AFTER);",2016-08-29T20:14:25Z,48
"@@ -36,7 +36,9 @@
 import com.github.shyiko.mysql.binlog.event.deserialization.GtidEventDataDeserializer;
 import com.github.shyiko.mysql.binlog.io.ByteArrayInputStream;
 import com.github.shyiko.mysql.binlog.network.AuthenticationException;
+import com.github.shyiko.mysql.binlog.network.SSLMode;
 
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
 import io.debezium.function.BlockingConsumer;
 import io.debezium.relational.TableId;
@@ -70,6 +72,7 @@ public BinlogReader(MySqlTaskContext context) {
         // Set up the log reader ...
         client = new BinaryLogClient(context.hostname(), context.port(), context.username(), context.password());
         client.setServerId(context.serverId());
+        client.setSSLMode(sslModeFor(context.sslMode()));
         client.setKeepAlive(context.config().getBoolean(MySqlConnectorConfig.KEEP_ALIVE));
         client.registerEventListener(this::handleEvent);
         client.registerLifecycleListener(new ReaderThreadLifecycleListener());
@@ -407,6 +410,22 @@ protected void handleDelete(Event event) throws InterruptedException {
             logger.debug(""Skipping delete row event: {}"", event);
         }
     }
+    
+    protected SSLMode sslModeFor( SecureConnectionMode mode ) {
+        switch(mode) {
+            case DISABLED:
+                return SSLMode.DISABLED;
+            case PREFERRED:
+                return SSLMode.PREFERRED;
+            case REQUIRED:
+                return SSLMode.REQUIRED;
+            case VERIFY_CA:
+                return SSLMode.VERIFY_CA;
+            case VERIFY_IDENTITY:
+                return SSLMode.VERIFY_IDENTITY;
+        }
+        return null;
+    }
 
     protected final class ReaderThreadLifecycleListener implements LifecycleListener {
         @Override",2016-08-24T18:27:35Z,67
"@@ -17,6 +17,8 @@
 import org.apache.kafka.common.config.ConfigValue;
 import org.apache.kafka.connect.connector.Task;
 import org.apache.kafka.connect.source.SourceConnector;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import io.debezium.config.Configuration;
 import io.debezium.jdbc.JdbcConnection;
@@ -32,7 +34,8 @@
  * @author Randall Hauch
  */
 public class MySqlConnector extends SourceConnector {
-
+    
+    private Logger logger = LoggerFactory.getLogger(getClass());
     private Map<String, String> props;
 
     public MySqlConnector() {
@@ -88,10 +91,17 @@ public Config validate(Map<String, String> connectorConfigs) {
                 && passwordValue.errorMessages().isEmpty()) {
             // Try to connect to the database ...
             try (MySqlJdbcContext jdbcContext = new MySqlJdbcContext(config)) {
+                jdbcContext.start();
                 JdbcConnection mysql = jdbcContext.jdbc();
-                mysql.execute(""SELECT version()"");
-            } catch (SQLException e) {
-                hostnameValue.addErrorMessage(""Unable to connect: "" + e.getMessage());
+                try {
+                    mysql.execute(""SELECT version()"");
+                    logger.info(""Successfully tested connection for {} with user '{}'"", jdbcContext.connectionString(), mysql.username());
+                } catch (SQLException e) {
+                    logger.info(""Failed testing connection for {} with user '{}'"", jdbcContext.connectionString(), mysql.username());
+                    hostnameValue.addErrorMessage(""Unable to connect: "" + e.getMessage());
+                } finally {
+                    jdbcContext.shutdown();
+                }
             }
         }
         return new Config(new ArrayList<>(results.values()));",2016-08-24T18:27:35Z,116
"@@ -135,7 +135,75 @@ public static SnapshotMode parse(String value, String defaultValue) {
         }
     }
 
-    private static final String DATABASE_LIST_NAME = ""database.list"";
+    /**
+     * The set of predefined SecureConnectionMode options or aliases.
+     */
+    public static enum SecureConnectionMode {
+        /**
+         * Establish an unencrypted connection.
+         */
+        DISABLED(""disabled""),
+        
+        /**
+         * Establish a secure (encrypted) connection if the server supports secure connections.
+         * Fall back to an unencrypted connection otherwise.
+         */
+        PREFERRED(""preferred""),
+        /**
+         * Establish a secure connection if the server supports secure connections.
+         * The connection attempt fails if a secure connection cannot be established.
+         */
+        REQUIRED(""required""),
+        /**
+         * Like REQUIRED, but additionally verify the server TLS certificate against the configured Certificate Authority
+         * (CA) certificates. The connection attempt fails if no valid matching CA certificates are found.
+         */
+        VERIFY_CA(""verify_ca""),
+        /**
+         * Like VERIFY_CA, but additionally verify that the server certificate matches the host to which the connection is
+         * attempted.
+         */
+        VERIFY_IDENTITY(""verify_identity"");
+        
+        private final String value;
+
+        private SecureConnectionMode(String value) {
+            this.value = value;
+        }
+
+        public String getValue() {
+            return value;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         * 
+         * @param value the configuration property value; may not be null
+         * @return the matching option, or null if no match is found
+         */
+        public static SecureConnectionMode parse(String value) {
+            if (value == null) return null;
+            value = value.trim();
+            for (SecureConnectionMode option : SecureConnectionMode.values()) {
+                if (option.getValue().equalsIgnoreCase(value)) return option;
+            }
+            return null;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         * 
+         * @param value the configuration property value; may not be null
+         * @param defaultValue the default value; may be null
+         * @return the matching option, or null if no match is found and the non-null default is invalid
+         */
+        public static SecureConnectionMode parse(String value, String defaultValue) {
+            SecureConnectionMode mode = parse(value);
+            if (mode == null && defaultValue != null) mode = parse(defaultValue);
+            return mode;
+        }
+    }
+
     private static final String DATABASE_WHITELIST_NAME = ""database.whitelist"";
     private static final String TABLE_WHITELIST_NAME = ""table.whitelist"";
     private static final String TABLE_IGNORE_BUILTIN_NAME = ""table.ignore.builtin"";
@@ -147,7 +215,6 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                               .withType(Type.STRING)
                                               .withWidth(Width.MEDIUM)
                                               .withImportance(Importance.HIGH)
-                                              .withDependents(DATABASE_LIST_NAME)
                                               .withValidation(Field::isRequired)
                                               .withDescription(""Resolvable hostname or IP address of the MySQL database server."");
 
@@ -157,7 +224,6 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                           .withWidth(Width.SHORT)
                                           .withDefault(3306)
                                           .withImportance(Importance.HIGH)
-                                          .withDependents(DATABASE_LIST_NAME)
                                           .withValidation(Field::isInteger)
                                           .withDescription(""Port of the MySQL database server."");
 
@@ -166,7 +232,6 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                           .withType(Type.STRING)
                                           .withWidth(Width.SHORT)
                                           .withImportance(Importance.HIGH)
-                                          .withDependents(DATABASE_LIST_NAME)
                                           .withValidation(Field::isRequired)
                                           .withDescription(""Name of the MySQL database user to be used when connecting to the database."");
 
@@ -175,7 +240,6 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                               .withType(Type.PASSWORD)
                                               .withWidth(Width.SHORT)
                                               .withImportance(Importance.HIGH)
-                                              .withDependents(DATABASE_LIST_NAME)
                                               .withValidation(Field::isRequired)
                                               .withDescription(""Password of the MySQL database user to be used when connecting to the database."");
 
@@ -202,6 +266,49 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                                        + ""MySQL database cluster as another server (with this unique ID) so it can read ""
                                                        + ""the binlog. By default, a random number is generated between 5400 and 6400."");
 
+    public static final Field SSL_MODE = Field.create(""database.ssl.mode"")
+                                              .withDisplayName(""SSL mode"")
+                                              .withEnum(SecureConnectionMode.class, SecureConnectionMode.DISABLED)
+                                              .withWidth(Width.MEDIUM)
+                                              .withImportance(Importance.MEDIUM)
+                                              .withDescription(""Whether to use an encrypted connection to MySQL. Options include""
+                                                      + ""'disabled' (the default) to use an unencrypted connection; ""
+                                                      + ""'preferred' to establish a secure (encrypted) connection if the server supports secure connections, ""
+                                                      + ""but fall back to an unencrypted connection otherwise; ""
+                                                      + ""'required' to use a secure (encrypted) connection, and fail if one cannot be established; ""
+                                                      + ""'verify_ca' like 'required' but additionally verify the server TLS certificate against the configured Certificate Authority ""
+                                                      + ""(CA) certificates, or fail if no valid matching CA certificates are found; or""
+                                                      + ""'verify_identity' like 'verify_ca' but additionally verify that the server certificate matches the host to which the connection is attempted."");
+
+    public static final Field SSL_KEYSTORE = Field.create(""database.ssl.keystore"")
+                                                  .withDisplayName(""SSL Keystore"")
+                                                  .withType(Type.STRING)
+                                                  .withWidth(Width.LONG)
+                                                  .withImportance(Importance.MEDIUM)
+                                                  .withDescription(""Location of the Java keystore file containing an application process's own certificate and private key."");
+
+    public static final Field SSL_KEYSTORE_PASSWORD = Field.create(""database.ssl.keystore.password"")
+                                                           .withDisplayName(""SSL Keystore Password"")
+                                                           .withType(Type.PASSWORD)
+                                                           .withWidth(Width.MEDIUM)
+                                                           .withImportance(Importance.MEDIUM)
+                                                           .withDescription(""Password to access the private key from the keystore file specified by 'ssl.keystore' configuration property or the 'javax.net.ssl.keyStore' system or JVM property. ""
+                                                                   + ""This password is used to unlock the keystore file (store password), and to decrypt the private key stored in the keystore (key password)."");
+
+    public static final Field SSL_TRUSTSTORE = Field.create(""database.ssl.truststore"")
+                                                    .withDisplayName(""SSL Truststore"")
+                                                    .withType(Type.STRING)
+                                                    .withWidth(Width.LONG)
+                                                    .withImportance(Importance.MEDIUM)
+                                                    .withDescription(""Location of the Java truststore file containing the collection of CA certificates trusted by this application process (trust store)."");
+
+    public static final Field SSL_TRUSTSTORE_PASSWORD = Field.create(""database.ssl.truststore.password"")
+                                                             .withDisplayName(""SSL Truststore Password"")
+                                                             .withType(Type.PASSWORD)
+                                                             .withWidth(Width.MEDIUM)
+                                                             .withImportance(Importance.MEDIUM)
+                                                             .withDescription(""Password to unlock the keystore file (store password) specified by 'ssl.trustore' configuration property or the 'javax.net.ssl.trustStore' system or JVM property."");
+
     public static final Field TABLES_IGNORE_BUILTIN = Field.create(TABLE_IGNORE_BUILTIN_NAME)
                                                            .withDisplayName(""Ignore system databases"")
                                                            .withType(Type.BOOLEAN)
@@ -361,16 +468,15 @@ public static SnapshotMode parse(String value, String defaultValue) {
 
     public static final Field SNAPSHOT_MODE = Field.create(""snapshot.mode"")
                                                    .withDisplayName(""Snapshot mode"")
-                                                   .withEnum(SnapshotMode.class)
+                                                   .withEnum(SnapshotMode.class, SnapshotMode.INITIAL)
                                                    .withWidth(Width.SHORT)
                                                    .withImportance(Importance.LOW)
                                                    .withDescription(""The criteria for running a snapshot upon startup of the connector. ""
                                                            + ""Options include: ""
                                                            + ""'when_needed' to specify that the connector run a snapshot upon startup whenever it deems it necessary; ""
                                                            + ""'initial' (the default) to specify the connector can run a snapshot only when no offsets are available for the logical server name; and ""
                                                            + ""'never' to specify the connector should never run a snapshot and that upon first startup the connector should read from the beginning of the binlog. ""
-                                                           + ""The 'never' mode should be used with care, and only when the binlog is known to contain all history."")
-                                                   .withDefault(SnapshotMode.INITIAL.getValue());
+                                                           + ""The 'never' mode should be used with care, and only when the binlog is known to contain all history."");
 
     public static final Field SNAPSHOT_MINIMAL_LOCKING = Field.create(""snapshot.minimal.locks"")
                                                               .withDisplayName(""Use shortest database locking for snapshots"")
@@ -387,14 +493,13 @@ public static SnapshotMode parse(String value, String defaultValue) {
 
     public static final Field TIME_PRECISION_MODE = Field.create(""time.precision.mode"")
                                                          .withDisplayName(""Time Precision"")
-                                                         .withEnum(TemporalPrecisionMode.class)
+                                                         .withEnum(TemporalPrecisionMode.class, TemporalPrecisionMode.ADAPTIVE)
                                                          .withWidth(Width.SHORT)
                                                          .withImportance(Importance.MEDIUM)
                                                          .withDescription(""Time, date, and timestamps can be represented with different kinds of precisions, including:""
                                                                  + ""'adaptive' (the default) bases the precision of time, date, and timestamp values on the database column's precision; ""
                                                                  + ""'connect' always represents time, date, and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, ""
-                                                                 + ""which uses millisecond precision regardless of the database columns' precision ."")
-                                                         .withDefault(TemporalPrecisionMode.ADAPTIVE.getValue());
+                                                                 + ""which uses millisecond precision regardless of the database columns' precision ."");
 
     /**
      * Method that generates a Field for specifying that string columns whose names match a set of regular expressions should
@@ -438,7 +543,9 @@ public static final Field MASK_COLUMN(int length) {
                                                      TABLE_WHITELIST, TABLE_BLACKLIST, TABLES_IGNORE_BUILTIN,
                                                      DATABASE_WHITELIST, DATABASE_BLACKLIST,
                                                      COLUMN_BLACKLIST, SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING,
-                                                     TIME_PRECISION_MODE);
+                                                     TIME_PRECISION_MODE,
+                                                     SSL_MODE, SSL_KEYSTORE, SSL_KEYSTORE_PASSWORD,
+                                                     SSL_TRUSTSTORE, SSL_TRUSTSTORE_PASSWORD);
 
     /**
      * The set of {@link Field}s that are included in the {@link #configDef() configuration definition}. This includes
@@ -453,10 +560,11 @@ public static final Field MASK_COLUMN(int length) {
 
     protected static ConfigDef configDef() {
         ConfigDef config = new ConfigDef();
-        Field.group(config, ""MySQL"", HOSTNAME, PORT, USER, PASSWORD, SERVER_NAME, SERVER_ID);
-        Field.group(config, ""History Storage"", KafkaDatabaseHistory.BOOTSTRAP_SERVERS, KafkaDatabaseHistory.TOPIC,
-                    KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS, KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS,
-                    DATABASE_HISTORY);
+        Field.group(config, ""MySQL"", HOSTNAME, PORT, USER, PASSWORD, SERVER_NAME, SERVER_ID,
+                    SSL_MODE, SSL_KEYSTORE, SSL_KEYSTORE_PASSWORD, SSL_TRUSTSTORE, SSL_TRUSTSTORE_PASSWORD);
+        Field.group(config, ""History Storage"", KafkaDatabaseHistory.BOOTSTRAP_SERVERS,
+                    KafkaDatabaseHistory.TOPIC, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS,
+                    KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS, DATABASE_HISTORY);
         Field.group(config, ""Events"", INCLUDE_SCHEMA_CHANGES, TABLES_IGNORE_BUILTIN, DATABASE_WHITELIST, TABLE_WHITELIST,
                     COLUMN_BLACKLIST, TABLE_BLACKLIST, DATABASE_BLACKLIST);
         Field.group(config, ""Connector"", CONNECTION_TIMEOUT_MS, KEEP_ALIVE, MAX_QUEUE_SIZE, MAX_BATCH_SIZE, POLL_INTERVAL_MS,",2016-08-24T18:27:35Z,40
"@@ -6,9 +6,16 @@
 package io.debezium.connector.mysql;
 
 import java.sql.SQLException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.kafka.connect.errors.ConnectException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import io.debezium.config.Configuration;
+import io.debezium.config.Field;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
 
@@ -19,22 +26,25 @@
  */
 public class MySqlJdbcContext implements AutoCloseable {
 
-    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false"";
+    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false&useSSL=${useSSL}"";
     protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(MYSQL_CONNECTION_URL);
 
     protected final Logger logger = LoggerFactory.getLogger(getClass());
     protected final Configuration config;
     protected final JdbcConnection jdbc;
+    private final Map<String,String> originalSystemProperties = new HashMap<>();
 
     public MySqlJdbcContext(Configuration config) {
         this.config = config; // must be set before most methods are used
 
         // Set up the JDBC connection without actually connecting, with extra MySQL-specific properties
         // to give us better JDBC database metadata behavior ...
+        boolean useSSL = sslModeEnabled();
         Configuration jdbcConfig = config.subset(""database."", true)
                                          .edit()
                                          .with(""useInformationSchema"", ""true"")
                                          .with(""nullCatalogMeansCurrent"", ""false"")
+                                         .with(""useSSL"", Boolean.toString(useSSL))
                                          .build();
         this.jdbc = new JdbcConnection(jdbcConfig, FACTORY);
     }
@@ -67,23 +77,68 @@ public int port() {
         return config.getInteger(MySqlConnectorConfig.PORT);
     }
 
+    public SecureConnectionMode sslMode() {
+        String mode = config.getString(MySqlConnectorConfig.SSL_MODE);
+        return SecureConnectionMode.parse(mode);
+    }
+    
+    public boolean sslModeEnabled() {
+        return sslMode() != SecureConnectionMode.DISABLED;
+    }
+    
     public void start() {
+        if (sslModeEnabled()) {
+            originalSystemProperties.clear();
+            // Set the System properties for SSL for the MySQL driver ...
+            setSystemProperty(""javax.net.ssl.keyStore"", MySqlConnectorConfig.SSL_KEYSTORE, true);
+            setSystemProperty(""javax.net.ssl.keyStorePassword"", MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD, false);
+            setSystemProperty(""javax.net.ssl.trustStore"", MySqlConnectorConfig.SSL_TRUSTSTORE, true);
+            setSystemProperty(""javax.net.ssl.trustStorePassword"", MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD, false);
+        }
     }
 
     public void shutdown() {
         try {
             jdbc.close();
         } catch (SQLException e) {
             logger.error(""Unexpected error shutting down the database connection"", e);
+        } finally {
+            // Reset the system properties to their original value ...
+            originalSystemProperties.forEach((name,value)->{
+                if ( value != null ) {
+                    System.setProperty(name,value);
+                } else {
+                    System.clearProperty(name);
+                }
+            });
         }
     }
 
     @Override
     public void close() {
         shutdown();
     }
-    
+
     protected String connectionString() {
         return jdbc.connectionString(MYSQL_CONNECTION_URL);
     }
+
+    protected void setSystemProperty(String property, Field field, boolean showValueInError) {
+        String value = config.getString(field);
+        if (value != null) {
+            String existingValue = System.getProperty(property);
+            if (existingValue != null && existingValue.equalsIgnoreCase(value)) {
+                String msg = ""System or JVM property '"" + property + ""' is already defined, but the configuration property '"" + field.name()
+                        + ""' defines a different value"";
+                if (showValueInError) {
+                    msg = ""System or JVM property '"" + property + ""' is already defined as "" + existingValue
+                            + "", but the configuration property '"" + field.name() + ""' defines a different value '"" + value + ""'"";
+                }
+                throw new ConnectException(msg);
+            } else {
+                String existing = System.setProperty(property, value);
+                originalSystemProperties.put(property, existing); // the existing value may be null
+            }
+        }
+    }
 }",2016-08-24T18:27:35Z,61
"@@ -18,6 +18,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.data.KeyValueStore;
 import io.debezium.data.KeyValueStore.Collection;
 import io.debezium.data.SchemaChangeHistory;
@@ -99,14 +100,14 @@ protected Configuration.Builder simpleConfig() {
                             .with(MySqlConnectorConfig.PORT, port)
                             .with(MySqlConnectorConfig.USER, ""replicator"")
                             .with(MySqlConnectorConfig.PASSWORD, ""replpass"")
+                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                             .with(MySqlConnectorConfig.SERVER_ID, 18911)
                             .with(MySqlConnectorConfig.SERVER_NAME, LOGICAL_NAME)
                             .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
                             .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, false)
                             .with(MySqlConnectorConfig.DATABASE_WHITELIST, DB_NAME)
                             .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                            .with(""database.useSSL"", false); // eliminates MySQL driver warning about SSL connections
+                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH);
     }
 
     @Test",2016-08-24T18:27:35Z,67
"@@ -26,6 +26,7 @@ public class MySQLConnection extends JdbcConnection {
     public static MySQLConnection forTestDatabase(String databaseName) {
         return new MySQLConnection(JdbcConfiguration.copy(Configuration.fromSystemProperties(""database.""))
                                                     .withDatabase(databaseName)
+                                                    .with(""useSSL"", false)
                                                     .build());
     }
 
@@ -42,6 +43,7 @@ public static MySQLConnection forTestDatabase(String databaseName, String userna
                                                     .withDatabase(databaseName)
                                                     .withUser(username)
                                                     .withPassword(password)
+                                                    .with(""useSSL"", false)
                                                     .build());
     }
 ",2016-08-24T18:27:35Z,36
"@@ -24,6 +24,7 @@
 
 import io.debezium.config.Configuration;
 import io.debezium.config.Field.Recommender;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
 import io.debezium.embedded.AbstractConnectorTest;
@@ -106,26 +107,85 @@ public void shouldFailToValidateInvalidConfiguration() {
         assertNoConfigurationErrors(result, MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES);
         assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MODE);
         assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_MODE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE_PASSWORD);
         assertConfigurationErrors(result, KafkaDatabaseHistory.BOOTSTRAP_SERVERS);
         assertConfigurationErrors(result, KafkaDatabaseHistory.TOPIC);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS);
     }
 
+    @Test
+    public void shouldValidateValidConfigurationWithSSL() {
+        Configuration config = Configuration.create()
+                                            .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                                            .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                                            .with(MySqlConnectorConfig.USER, ""snapper"")
+                                            .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.REQUIRED.name().toLowerCase())
+                                            .with(MySqlConnectorConfig.SSL_KEYSTORE, ""/some/path/to/keystore"")
+                                            .with(MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD, ""keystore1234"")
+                                            .with(MySqlConnectorConfig.SSL_TRUSTSTORE, ""/some/path/to/truststore"")
+                                            .with(MySqlConnectorConfig.SSL_TRUSTSTORE_PASSWORD, ""truststore1234"")
+                                            .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                                            .with(MySqlConnectorConfig.SERVER_NAME, ""myServer"")
+                                            .with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS, ""some.host.com"")
+                                            .with(KafkaDatabaseHistory.TOPIC, ""my.db.history.topic"")
+                                            .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                                            .build();
+        MySqlConnector connector = new MySqlConnector();
+        Config result = connector.validate(config.asMap());
+
+        // Can't connect to MySQL using SSL on a container using the 'mysql/mysql-server' image maintained by MySQL team,
+        // but can actually connect to MySQL using SSL on a container using the 'mysql' image maintained by Docker, Inc.
+        assertConfigurationErrors(result, MySqlConnectorConfig.HOSTNAME, 0, 1);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.PORT);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.USER);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.PASSWORD);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SERVER_NAME);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SERVER_ID);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.TABLES_IGNORE_BUILTIN);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.DATABASE_WHITELIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.DATABASE_BLACKLIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.TABLE_WHITELIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.TABLE_BLACKLIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.COLUMN_BLACKLIST);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.CONNECTION_TIMEOUT_MS);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.KEEP_ALIVE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.MAX_QUEUE_SIZE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.MAX_BATCH_SIZE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.POLL_INTERVAL_MS);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.DATABASE_HISTORY);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MODE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_MODE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE_PASSWORD);
+        assertNoConfigurationErrors(result, KafkaDatabaseHistory.BOOTSTRAP_SERVERS);
+        assertNoConfigurationErrors(result, KafkaDatabaseHistory.TOPIC);
+        assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS);
+        assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_INTERVAL_MS);
+    }
+
     @Test
     public void shouldValidateAcceptableConfiguration() {
         Configuration config = Configuration.create()
                                             .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
                                             .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                                             .with(MySqlConnectorConfig.USER, ""snapper"")
                                             .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                                             .with(MySqlConnectorConfig.SERVER_ID, 18765)
                                             .with(MySqlConnectorConfig.SERVER_NAME, ""myServer"")
                                             .with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS, ""some.host.com"")
                                             .with(KafkaDatabaseHistory.TOPIC, ""my.db.history.topic"")
                                             .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
-                                            .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL
-                                                                            // connections
                                             .build();
         MySqlConnector connector = new MySqlConnector();
         Config result = connector.validate(config.asMap());
@@ -151,6 +211,11 @@ public void shouldValidateAcceptableConfiguration() {
         assertNoConfigurationErrors(result, MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES);
         assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MODE);
         assertNoConfigurationErrors(result, MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_MODE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_KEYSTORE_PASSWORD);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE);
+        assertNoConfigurationErrors(result, MySqlConnectorConfig.SSL_TRUSTSTORE_PASSWORD);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.BOOTSTRAP_SERVERS);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.TOPIC);
         assertNoConfigurationErrors(result, KafkaDatabaseHistory.RECOVERY_POLL_ATTEMPTS);
@@ -206,12 +271,12 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
                               .with(MySqlConnectorConfig.SERVER_ID, 18765)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""myServer"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
                               .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test"")
                               .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
@@ -430,6 +495,7 @@ public void shouldConsumeEventsWithNoSnapshot() throws SQLException, Interrupted
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18780)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""myServer1"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -438,7 +504,6 @@ public void shouldConsumeEventsWithNoSnapshot() throws SQLException, Interrupted
                               .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.name().toLowerCase())
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
 
         // Start the connector ...
@@ -479,6 +544,7 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18780)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""myServer2"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -488,7 +554,6 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
                               .with(MySqlConnectorConfig.MASK_COLUMN(12), ""connector_test_ro.customers.email"")
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, false)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
 
         // Start the connector ...",2016-08-24T18:27:35Z,71
"@@ -25,6 +25,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.connector.mysql.MySqlConnectorConfig.TemporalPrecisionMode;
 import io.debezium.data.Envelope;
@@ -68,6 +69,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18765)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -76,7 +78,6 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
@@ -188,6 +189,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18765)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -197,7 +199,6 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnect
                               .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
                               .with(MySqlConnectorConfig.TIME_PRECISION_MODE, TemporalPrecisionMode.CONNECT.toString())
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
@@ -307,6 +308,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                               .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
                               .with(MySqlConnectorConfig.USER, ""snapper"")
                               .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                               .with(MySqlConnectorConfig.SERVER_ID, 18765)
                               .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
                               .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
@@ -315,7 +317,6 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                               .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.INITIAL.toString())
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);",2016-08-24T18:27:35Z,48
"@@ -16,6 +16,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.util.Testing;
@@ -69,12 +70,12 @@ protected Configuration.Builder simpleConfig() {
                             .with(MySqlConnectorConfig.PORT, port)
                             .with(MySqlConnectorConfig.USER, username)
                             .with(MySqlConnectorConfig.PASSWORD, password)
+                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.name().toLowerCase())
                             .with(MySqlConnectorConfig.SERVER_ID, serverId)
                             .with(MySqlConnectorConfig.SERVER_NAME, serverName)
                             .with(MySqlConnectorConfig.DATABASE_WHITELIST, databaseName)
                             .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                            .with(""database.useSSL"",false); // eliminates MySQL driver warning about SSL connections
+                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH);
     }
 
     @Test",2016-08-24T18:27:35Z,117
"@@ -44,6 +44,7 @@
 import com.github.shyiko.mysql.binlog.event.WriteRowsEventData;
 import com.github.shyiko.mysql.binlog.event.XidEventData;
 import com.github.shyiko.mysql.binlog.event.deserialization.EventDeserializer;
+import com.github.shyiko.mysql.binlog.network.SSLMode;
 import com.github.shyiko.mysql.binlog.network.ServerException;
 
 import static org.fest.assertions.Assertions.assertThat;
@@ -105,6 +106,7 @@ protected void startClient(Consumer<BinaryLogClient> preConnect) throws IOExcept
         client = new BinaryLogClient(config.getHostname(), config.getPort(), ""replicator"", ""replpass"");
         client.setServerId(client.getServerId() - 1); // avoid clashes between BinaryLogClient instances
         client.setKeepAlive(false);
+        client.setSSLMode(SSLMode.DISABLED);
         client.registerEventListener(counters);
         client.registerEventListener(this::recordEvent);
         client.registerLifecycleListener(new TraceLifecycleListener());",2016-08-24T18:27:35Z,118
"@@ -20,6 +20,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SecureConnectionMode;
 import io.debezium.data.KeyValueStore;
 import io.debezium.data.KeyValueStore.Collection;
 import io.debezium.data.SchemaChangeHistory;
@@ -70,14 +71,14 @@ protected Configuration.Builder simpleConfig() {
                             .with(MySqlConnectorConfig.PORT, port)
                             .with(MySqlConnectorConfig.USER, ""snapper"")
                             .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                            .with(MySqlConnectorConfig.SSL_MODE, SecureConnectionMode.DISABLED.toString().toLowerCase())
                             .with(MySqlConnectorConfig.SERVER_ID, 18911)
                             .with(MySqlConnectorConfig.SERVER_NAME, LOGICAL_NAME)
                             .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
                             .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, false)
                             .with(MySqlConnectorConfig.DATABASE_WHITELIST, DB_NAME)
                             .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
-                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
-                            .with(""database.useSSL"",false); // eliminates MySQL driver warning about SSL connections
+                            .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH);
     }
 
     @Test",2016-08-24T18:27:35Z,62
"@@ -639,8 +639,26 @@ public Field withType(Type type) {
      * @return the new field; never null
      */
     public <T extends Enum<T>> Field withEnum(Class<T> enumType) {
+        return withEnum(enumType,null);
+    }
+
+    /**
+     * Create and return a new Field instance that is a copy of this field but has a {@link #withType(Type) type} of
+     * {@link org.apache.kafka.connect.data.Schema.Type#STRING}, a {@link #withRecommender(Recommender) recommender}
+     * that returns a list of {@link Enum#name() Enum names} as valid values, and a validator that verifies values are valid
+     * enumeration names.
+     * 
+     * @param enumType the enumeration type for the field
+     * @param defaultOption the default enumeration value; may be null
+     * @return the new field; never null
+     */
+    public <T extends Enum<T>> Field withEnum(Class<T> enumType, T defaultOption) {
         EnumRecommender<T> recommendator = new EnumRecommender<>(enumType);
-        return withType(Type.STRING).withRecommender(recommendator).withValidation(recommendator);
+        Field result = withType(Type.STRING).withRecommender(recommendator).withValidation(recommendator);
+        if ( defaultOption != null ) {
+            result = result.withDefault(defaultOption.name().toLowerCase());
+        }
+        return result;
     }
 
     /**",2016-08-24T18:27:35Z,119
"@@ -134,15 +134,22 @@ private static Field[] combineVariables(Field[] overriddenVariables,
 
     private static String findAndReplace(String url, Properties props, Field... variables) {
         for (Field field : variables) {
-            String variable = field.name();
-            if (variable != null && url.contains(""${"" + variable + ""}"")) {
-                // Otherwise, we have to remove it from the properties ...
-                String value = props.getProperty(variable);
-                if (value != null) {
-                    props.remove(variable);
-                    // And replace the variable ...
-                    url = url.replaceAll(""\\$\\{"" + variable + ""\\}"", value);
-                }
+            if ( field != null ) url = findAndReplace(url, field.name(), props);
+        }
+        for (Object key : new HashSet<>(props.keySet())) {
+            if (key != null ) url = findAndReplace(url, key.toString(), props);
+        }
+        return url;
+    }
+    
+    private static String findAndReplace(String url, String name, Properties props) {
+        if (name != null && url.contains(""${"" + name + ""}"")) {
+            // Otherwise, we have to remove it from the properties ...
+            String value = props.getProperty(name);
+            if (value != null) {
+                props.remove(name);
+                // And replace the variable ...
+                url = url.replaceAll(""\\$\\{"" + name + ""\\}"", value);
             }
         }
         return url;",2016-08-24T18:27:35Z,9
"@@ -5,6 +5,8 @@
  */
 package io.debezium.embedded;
 
+import static org.junit.Assert.fail;
+
 import java.nio.file.Path;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -510,6 +512,12 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
         assertThat(value.errorMessages().size()).isEqualTo(numErrors);
     }
 
+    protected void assertConfigurationErrors(Config config, io.debezium.config.Field field, int minErrorsInclusive, int maxErrorsInclusive) {
+        ConfigValue value = configValue(config, field.name());
+        assertThat(value.errorMessages().size()).isGreaterThanOrEqualTo(minErrorsInclusive);
+        assertThat(value.errorMessages().size()).isLessThanOrEqualTo(maxErrorsInclusive);
+    }
+
     protected void assertConfigurationErrors(Config config, io.debezium.config.Field field) {
         ConfigValue value = configValue(config, field.name());
         assertThat(value.errorMessages().size()).isGreaterThan(0);
@@ -518,7 +526,11 @@ protected void assertConfigurationErrors(Config config, io.debezium.config.Field
     protected void assertNoConfigurationErrors(Config config, io.debezium.config.Field... fields) {
         for (io.debezium.config.Field field : fields) {
             ConfigValue value = configValue(config, field.name());
-            assertThat(value.errorMessages().size()).isEqualTo(0);
+            if ( value != null ) {
+                if ( !value.errorMessages().isEmpty() ) {
+                    fail(""Error messages on field '"" + field.name() + ""': "" + value.errorMessages());
+                }
+            }
         }
     }
 ",2016-08-24T18:27:35Z,35
"@@ -64,7 +64,7 @@
         <version.postgresql.server>9.4</version.postgresql.server>
         <version.mysql.server>5.7</version.mysql.server>
         <version.mysql.driver>5.1.39</version.mysql.driver>
-        <version.mysql.binlog>0.3.3</version.mysql.binlog>
+        <version.mysql.binlog>0.4.0</version.mysql.binlog>
         <version.mongo.server>3.2.6</version.mongo.server>
         <version.mongo.driver>3.2.2</version.mongo.driver>
 ",2016-08-24T18:27:35Z,60
"@@ -146,6 +146,53 @@
                       </assembly>
                     </build>
                   </image>
+                  <image>
+                    <!-- A Docker image using a partial MySQL installation maintained by MySQL team. -->
+                    <name>debezium/mysql-server-gtids-test-databases</name>
+                    <alias>database</alias>
+                    <run>
+                      <namingStrategy>alias</namingStrategy>
+                      <env>
+                        <MYSQL_ROOT_PASSWORD>debezium-rocks</MYSQL_ROOT_PASSWORD>
+                        <MYSQL_DATABASE>mysql</MYSQL_DATABASE> <!-- database created upon init -->
+                        <MYSQL_USER>${database.user}</MYSQL_USER>
+                        <MYSQL_PASSWORD>${database.password}</MYSQL_PASSWORD>
+                      </env>
+                      <ports>
+                        <port>${database.port}:3306</port>
+                      </ports>
+                      <log>
+                        <prefix>mysql</prefix>
+                        <enabled>true</enabled>
+                        <color>yellow</color>
+                      </log>
+                      <wait>
+                        <log>MySQL init process done. Ready for start up.</log>
+                        <time>30000</time> <!-- 30 seconds max -->
+                      </wait>
+                    </run>
+                    <build>
+                      <from>mysql/mysql-server:${version.mysql.server}</from>
+                      <assembly>
+                        <inline>
+                          <fileSets>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/server-gtids</directory>
+                              <includes>
+                                <include>my.cnf</include>
+                              </includes>
+                              <outputDirectory>etc/mysql</outputDirectory>
+                            </fileSet>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/init</directory>
+                              <outputDirectory>docker-entrypoint-initdb.d</outputDirectory>
+                            </fileSet>
+                          </fileSets>
+                        </inline>
+                        <basedir>/</basedir>
+                      </assembly>
+                    </build>
+                  </image>
                   <image>
                     <!-- A Docker image using a complete MySQL installation maintained by Docker team. -->
                     <name>debezium/mysql-test--databases</name>
@@ -350,5 +397,22 @@
           <docker.skip>false</docker.skip>
         </properties>
       </profile>
+      <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+            Use the Docker image for MySQL configured to use GTIDs.
+            To use, specify ""-Dgtid-mysql"" or -Pgtid-mysql on the Maven command line.
+            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+      <profile>
+        <id>gtid-mysql</id>
+        <activation>
+          <activeByDefault>false</activeByDefault>
+          <property>
+            <name>gtid-mysql</name>
+          </property>
+        </activation>
+        <properties>
+          <docker.image>debezium/mysql-server-gtids-test-databases</docker.image>
+          <docker.skip>false</docker.skip>
+        </properties>
+      </profile>
     </profiles>
 </project>",2016-06-07T17:01:51Z,83
"@@ -88,6 +88,7 @@ protected void doStart() {
         eventHandlers.put(EventType.ROTATE, this::handleRotateLogsEvent);
         eventHandlers.put(EventType.TABLE_MAP, this::handleUpdateTableMetadata);
         eventHandlers.put(EventType.QUERY, this::handleQueryEvent);
+        eventHandlers.put(EventType.GTID, this::handleGtidEvent);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, this::handleInsert);
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
@@ -142,9 +143,10 @@ protected void ignoreEvent(Event event) {
     protected void handleEvent(Event event) {
         if (event == null) return;
 
-        // Update the source offset info ...
+        // Update the source offset info. Note that the client returns the value in *milliseconds*, even though the binlog
+        // contains only *seconds* precision ...
         EventHeader eventHeader = event.getHeader();
-        source.setBinlogTimestamp(eventHeader.getTimestamp());
+        source.setBinlogTimestampSeconds(eventHeader.getTimestamp()/1000L); // client returns milliseconds, we record seconds
         source.setBinlogServerId(eventHeader.getServerId());
         EventType eventType = eventHeader.getEventType();
         if (eventType == EventType.ROTATE) {
@@ -166,16 +168,6 @@ protected void handleEvent(Event event) {
                 source.setRowInEvent(0);
             }
         }
-        if (eventType == EventType.GTID) {
-            EventData eventData = event.getData();
-            GtidEventData gtidEventData;
-            if (eventData instanceof EventDeserializer.EventDataWrapper) {
-                gtidEventData = (GtidEventData) ((EventDeserializer.EventDataWrapper) eventData).getInternal();
-            } else {
-                gtidEventData = (GtidEventData) eventData;
-            }
-            source.setGtids(gtidEventData.getGtid());
-        }
 
         // If there is a handler for this event, forward the event to it ...
         try {
@@ -188,6 +180,15 @@ protected void handleEvent(Event event) {
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    protected <T extends EventData> T unwrapData(Event event) {
+        EventData eventData = event.getData();
+        if (eventData instanceof EventDeserializer.EventDataWrapper) {
+            eventData = ((EventDeserializer.EventDataWrapper) eventData).getInternal();
+        }
+        return (T)eventData;
+    }
+
     /**
      * Handle the supplied event that signals that mysqld has stopped.
      * 
@@ -226,19 +227,31 @@ protected void handleServerIncident(Event event) {
      */
     protected void handleRotateLogsEvent(Event event) {
         logger.debug(""Rotating logs: {}"", event);
-        RotateEventData command = event.getData();
+        RotateEventData command = unwrapData(event);
         assert command != null;
         recordMakers.clear();
     }
 
+    /**
+     * Handle the supplied event with a {@link GtidEventData} that signals the beginning of a GTID transaction.
+     * 
+     * @param event the GTID event to be processed; may not be null
+     */
+    protected void handleGtidEvent(Event event) {
+        logger.debug(""GTID transaction: {}"", event);
+        GtidEventData gtidEvent = unwrapData(event);
+        source.setGtid(gtidEvent.getGtid());
+        source.setGtidSet(client.getGtidSet());
+    }
+
     /**
      * Handle the supplied event with an {@link QueryEventData} by possibly recording the DDL statements as changes in the
      * MySQL schemas.
      * 
      * @param event the database change data event to be processed; may not be null
      */
     protected void handleQueryEvent(Event event) {
-        QueryEventData command = event.getData();
+        QueryEventData command = unwrapData(event);
         logger.debug(""Received update table command: {}"", event);
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords && recordMakers.schemaChanges(dbName, statements, super::enqueueRecord) > 0) {
@@ -262,7 +275,7 @@ protected void handleQueryEvent(Event event) {
      * @param event the update event; never null
      */
     protected void handleUpdateTableMetadata(Event event) {
-        TableMapEventData metadata = event.getData();
+        TableMapEventData metadata = unwrapData(event);
         long tableNumber = metadata.getTableId();
         String databaseName = metadata.getDatabase();
         String tableName = metadata.getTable();
@@ -281,7 +294,7 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
-        WriteRowsEventData write = event.getData();
+        WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);
@@ -302,7 +315,7 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
-        UpdateRowsEventData update = event.getData();
+        UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
         // BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
@@ -330,7 +343,7 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
-        DeleteRowsEventData deleted = event.getData();
+        DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);",2016-06-07T17:01:51Z,67
"@@ -7,6 +7,7 @@
 
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.TreeMap;
@@ -111,11 +112,17 @@ protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
                 intervals.add(new Interval(interval.getStart(), interval.getEnd()));
             });
             Collections.sort(this.intervals);
-        }
-
-        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
-            this.uuid = uuid;
-            this.intervals = intervals;
+            if ( this.intervals.size() > 1 ) {
+                // Collapse adjacent intervals ...
+                for ( int i=intervals.size()-1; i!=0; --i) {
+                    Interval before = this.intervals.get(i-1);
+                    Interval after = this.intervals.get(i);
+                    if ( (before.getEnd() + 1) == after.getStart() ) {
+                        this.intervals.set(i-1,new Interval(before.getStart(),after.getEnd()));
+                        this.intervals.remove(i);
+                    }
+                }
+            }
         }
 
         /**
@@ -218,17 +225,41 @@ public String toString() {
             StringBuilder sb = new StringBuilder();
             if (sb.length() != 0) sb.append(',');
             sb.append(uuid).append(':');
-            sb.append(intervals.getFirst().getStart());
-            sb.append(intervals.getLast().getEnd());
+            Iterator<Interval> iter = intervals.iterator();
+            if ( iter.hasNext() ) sb.append(iter.next());
+            while ( iter.hasNext() ) {
+                sb.append(':');
+                sb.append(iter.next());
+            }
             return sb.toString();
         }
     }
 
     @Immutable
-    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+    public static class Interval implements Comparable<Interval> {
+
+        private final long start;
+        private final long end;
 
         public Interval(long start, long end) {
-            super(start, end);
+            this.start = start;
+            this.end = end;
+        }
+
+        /**
+         * Get the starting transaction number in this interval.
+         * @return this interval's first transaction number
+         */
+        public long getStart() {
+            return start;
+        }
+
+        /**
+         * Get the ending transaction number in this interval.
+         * @return this interval's last transaction number
+         */
+        public long getEnd() {
+            return end;
         }
 
         /**
@@ -244,6 +275,15 @@ public boolean isSubsetOf(Interval other) {
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
         }
+        
+        @Override
+        public int compareTo(Interval that) {
+            if ( that == this ) return 0;
+            long diff = this.start - that.start;
+            if ( diff > Integer.MAX_VALUE ) return Integer.MAX_VALUE;
+            if ( diff < Integer.MIN_VALUE ) return Integer.MIN_VALUE;
+            return (int) diff;
+        }
 
         @Override
         public int hashCode() {",2016-06-07T17:01:51Z,87
"@@ -9,6 +9,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -110,9 +111,16 @@ public void start(Map<String, String> props) {
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;
+                // The snapshot will determine if GTIDs are set
             }
         }
 
+        if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
+            // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
+            // previously used but the MySQL server has them enabled ...
+            source.setGtidSet("""");
+        }
+
         // Set up the readers ...
         this.binlogReader = new BinlogReader(taskContext);
         if (startWithSnapshot) {
@@ -174,13 +182,33 @@ protected void transitionToReadBinlog() {
      * @return {@code true} if the server has the binlog coordinates, or {@code false} otherwise
      */
     protected boolean isBinlogAvailable() {
+        String gtidStr = taskContext.source().gtidSet();
+        if ( gtidStr != null) {
+            if ( gtidStr.trim().isEmpty() ) return true; // start at beginning ...
+            String availableGtidStr = knownGtidSet();
+            if ( availableGtidStr == null || availableGtidStr.trim().isEmpty() ) {
+                // Last offsets had GTIDs but the server does not use them ...
+                logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
+                return false;
+            }
+            // GTIDs are enabled, and we used them previously ...
+            GtidSet gtidSet = new GtidSet(gtidStr);
+            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+                return true;
+            }
+            logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);
+            return false;
+        }
+        
         String binlogFilename = taskContext.source().binlogFilename();
         if (binlogFilename == null) return true; // start at current position
         if (binlogFilename.equals("""")) return true; // start at beginning
 
         // Accumulate the available binlog filenames ...
         List<String> logNames = new ArrayList<>();
         try {
+            logger.info(""Stop 0: Get all known binlogs from MySQL"");
             taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
                 while (rs.next()) {
                     logNames.add(rs.getString(1));
@@ -191,6 +219,50 @@ protected boolean isBinlogAvailable() {
         }
 
         // And compare with the one we're supposed to use ...
-        return logNames.stream().anyMatch(binlogFilename::equals);
+        boolean found = logNames.stream().anyMatch(binlogFilename::equals);
+        if ( !found ) {
+            logger.info(""Connector requires binlog file '{}', but MySQL only has {}"",binlogFilename,String.join("", "",logNames));
+        }
+        return found;
+    }
+
+    /**
+     * Determine whether the MySQL server has GTIDs enabled.
+     * 
+     * @return {@code false} if the server's {@code gtid_mode} is set and is {@code OFF}, or {@code true} otherwise
+     */
+    protected boolean isGtidModeEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>(""off"");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'GTID_MODE'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return !""OFF"".equalsIgnoreCase(mode.get());
+    }
+
+    /**
+     * Determine the available GTID set for MySQL.
+     * 
+     * @return the string representation of MySQL's GTID sets.
+     */
+    protected String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return gtidSetStr.get();
     }
 }",2016-06-07T17:01:51Z,68
"@@ -188,7 +188,7 @@ protected void execute() {
                 if (rs.next()) {
                     source.setBinlogFilename(rs.getString(1));
                     source.setBinlogPosition(rs.getLong(2));
-                    source.setGtids(rs.getString(5));// GTIDs
+                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
                     source.startSnapshot();
                 }
             });",2016-06-07T17:01:51Z,62
"@@ -14,6 +14,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.data.Envelope;
 import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
@@ -26,7 +27,7 @@
  * 
  * <pre>
  * {
- *     ""db"" : ""myDatabase""
+ *     ""server"" : ""production-server""
  * }
  * </pre>
  * 
@@ -36,14 +37,40 @@
  * 
  * <pre>
  * {
- *         ""gtids"" = ""3E11FA47-71CA-11E1-9E33-C80AA9429562:1-5"",
- *         ""file"" = ""mysql-bin.000003"",
- *         ""pos"" = 105586,
- *         ""row"" = 0
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
+ *     ""file"" = ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"" = 0,
+ *     ""snapshot"": true
  * }
  * </pre>
  * 
- * Note that the ""{@code gtid}"" field is present only when GTIDs are enabled.
+ * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
+ * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
+ * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
+ * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
+ * 
+ * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
+ * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
+ * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
+ * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
+ * an event that corresponds to the above partition and offset:
+ * 
+ * <pre>
+ * {
+ *     ""name"": ""production-server"",
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"": 0,
+ *     ""snapshot"": true
+ * }
+ * </pre>
  * 
  * @author Randall Hauch
  */
@@ -56,11 +83,12 @@ final class SourceInfo {
 
     public static final String SERVER_NAME_KEY = ""name"";
     public static final String SERVER_PARTITION_KEY = ""server"";
-    public static final String GTID_KEY = ""gtids"";
+    public static final String GTID_SET_KEY = ""gtids"";
+    public static final String GTID_KEY = ""gtid"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
     public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
-    public static final String TIMESTAMP_KEY = ""ts"";
+    public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
     /**
@@ -78,13 +106,14 @@ final class SourceInfo {
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private GtidSet binlogGtids;
+    private String gtidSet;
+    private String binlogGtid;
     private String binlogFilename;
     private long binlogPosition = 4;
     private int eventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
-    private long binlogTs = 0;
+    private long binlogTimestampSeconds = 0;
     private Map<String, String> sourcePartition;
     private boolean snapshot = false;
 
@@ -122,11 +151,11 @@ public Map<String, String> partition() {
      */
     public Map<String, ?> offset() {
         Map<String, Object> map = new HashMap<>();
-        if (binlogGtids != null) {
-            map.put(GTID_KEY, binlogGtids.toString());
+        if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
+        if (gtidSet != null) {
+            map.put(GTID_SET_KEY, gtidSet);
         }
-        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTs != 0 ) map.put(TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -158,13 +187,14 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        if (binlogGtids != null) {
-            result.put(GTID_KEY, binlogGtids.toString());
+        // Don't put the GTID Set into the struct; only the current GTID is fine ...
+        if (binlogGtid != null) {
+            result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
-        result.put(TIMESTAMP_KEY, binlogTs);
+        result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
@@ -193,12 +223,23 @@ public boolean isSnapshotInEffect() {
     }
 
     /**
-     * Set the GTID range for the MySQL binary log file.
+     * Set the latest GTID from the MySQL binary log file.
+     * 
+     * @param gtid the string representation of a specific GTID; may not be null
+     */
+    public void setGtid(String gtid) {
+        this.binlogGtid = gtid;
+    }
+
+    /**
+     * Set the set of GTIDs known to the MySQL server.
      * 
-     * @param gtids the string representation of the binlog GTIDs; may not be null
+     * @param gtidSet the string representation of GTID set; may not be null
      */
-    public void setGtids(String gtids) {
-        this.binlogGtids = gtids != null && !gtids.trim().isEmpty() ? new GtidSet(gtids) : null;
+    public void setGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            this.gtidSet = gtidSet;
+        }
     }
 
     /**
@@ -239,12 +280,12 @@ public void setBinlogServerId(long serverId) {
     }
 
     /**
-     * Set the timestamp as found within the MySQL binary log file.
+     * Set the number of <em>seconds</em> since Unix epoch (January 1, 1970) as found within the MySQL binary log file.
      * 
-     * @param timestamp the timestamp found within the binary log file
+     * @param timestampInSeconds the timestamp in <em>seconds</em> found within the binary log file
      */
-    public void setBinlogTimestamp(long timestamp) {
-        this.binlogTs = timestamp;
+    public void setBinlogTimestampSeconds(long timestampInSeconds) {
+        this.binlogTimestampSeconds = timestampInSeconds / 1000;
     }
 
     /**
@@ -270,7 +311,7 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtids((String) sourceOffset.get(GTID_KEY)); // may be null
+            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
             binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
@@ -297,7 +338,7 @@ private long longOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.binlogGtids != null ? this.binlogGtids.toString() : null;
+        return this.gtidSet != null ? this.gtidSet.toString() : null;
     }
 
     /**
@@ -340,9 +381,9 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (binlogGtids != null) {
+        if (gtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(binlogGtids);
+            sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
             sb.append("", pos="").append(binlogPosition());
             sb.append("", row="").append(eventRowNumber());
@@ -379,17 +420,17 @@ public String toString() {
      * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
      */
     public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
-        String recordedGtidSetStr = recorded.getString(GTID_KEY);
-        String desiredGtidSetStr = desired.getString(GTID_KEY);
+        String recordedGtidSetStr = recorded.getString(GTID_SET_KEY);
+        String desiredGtidSetStr = desired.getString(GTID_SET_KEY);
         if (desiredGtidSetStr != null) {
             // The desired position uses GTIDs, so we ideally compare using GTIDs ...
             if (recordedGtidSetStr != null) {
                 // Both have GTIDs, so base the comparison entirely on the GTID sets.
                 GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
                 GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
-                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                if (recordedGtidSet.equals(desiredGtidSet)) {
                     // They are exactly the same, which means the recorded position exactly matches the desired ...
-                    if ( !recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
+                    if (!recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
@@ -414,35 +455,35 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         }
 
         // Both positions are missing GTIDs. Look at the servers ...
-        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        if ( recordedServerId != desiredServerId ) {
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        if (recordedServerId != desiredServerId) {
             // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
             // is compare timestamps, and we have to assume that the server timestamps can be compared ...
-            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
-            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
+            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
+            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
             return recordedTimestamp <= desiredTimestamp;
         }
-        
+
         // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
         // comparable ...
         String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
         String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
-        if ( diff > 0 ) return false;
-        
+        if (diff > 0) return false;
+
         // The positions are the same, so compare the row number ...
         int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The binlog coordinates are the same ...
         return true;",2016-06-07T17:01:51Z,70
"@@ -0,0 +1,52 @@
+# For advice on how to change settings please see
+# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
+
+[mysqld]
+#
+# Remove leading # and set to the amount of RAM for the most important data
+# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
+# innodb_buffer_pool_size = 128M
+#
+# Remove leading # to turn on a very important data integrity option: logging
+# changes to the binary log between backups.
+# log_bin
+#
+# Remove leading # to set options mainly useful for reporting servers.
+# The server defaults are faster for transactions and fast SELECTs.
+# Adjust sizes as needed, experiment to find the optimal values.
+# join_buffer_size = 128M
+# sort_buffer_size = 2M
+# read_rnd_buffer_size = 2M
+skip-host-cache
+skip-name-resolve
+datadir=/var/lib/mysql
+socket=/var/lib/mysql/mysql.sock
+secure-file-priv=/var/lib/mysql-files
+user=mysql
+
+# Disabling symbolic-links is recommended to prevent assorted security risks
+symbolic-links=0
+
+log-error=/var/log/mysqld.log
+pid-file=/var/run/mysqld/mysqld.pid
+
+# ----------------------------------------------
+# Enable GTIDs on this master
+# ----------------------------------------------
+gtid_mode                 = on
+enforce_gtid_consistency  = on
+
+# ----------------------------------------------
+# Debezium ingest
+# ----------------------------------------------
+
+# Enable binary replication log and set the prefix, expiration, and log format.
+# The prefix is arbitrary, expiration can be short for integration tests but would
+# be longer on a production system. Row-level info is required for ingest to work.
+# Server ID is required, but this will vary on production systems
+server-id         = 112233
+log_bin           = mysql-bin
+expire_logs_days  = 1
+binlog_format     = row
+
+",2016-06-07T17:01:51Z,104
"@@ -0,0 +1,107 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.connector.mysql.GtidSet.Interval;
+import io.debezium.connector.mysql.GtidSet.UUIDSet;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class GtidSetTest {
+    
+    private static final String UUID1 = ""24bc7850-2c16-11e6-a073-0242ac110002"";
+
+    private GtidSet gtids;
+    
+    @Test
+    public void shouldCreateSetWithSingleInterval() {
+        gtids = new GtidSet(UUID1 + "":1-191"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,191);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,1,191);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191"");
+    }
+    
+    @Test
+    public void shouldCollapseAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,199);
+        asertFirstInterval(UUID1,1,199);
+        asertLastInterval(UUID1,1,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199"");
+    }
+
+    
+    @Test
+    public void shouldNotCollapseNonAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199"");
+        asertIntervalCount(UUID1,2);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,193,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199:1000-1033"");
+        asertIntervalCount(UUID1,3);
+        asertFirstInterval(UUID1,1,191);
+        asertIntervalExists(UUID1,193,199);
+        asertLastInterval(UUID1,1000,1033);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199:1000-1033"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervalsThatMayBeAdjacent() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199:1000-1033:1035-1036:1038-1039"");
+        asertIntervalCount(UUID1, 4);
+        asertFirstInterval(UUID1, 1, 199);
+        asertIntervalExists(UUID1, 1000, 1033);
+        asertIntervalExists(UUID1, 1035, 1036);
+        asertLastInterval(UUID1, 1038, 1039);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199:1000-1033:1035-1036:1038-1039""); // ??
+    }
+    
+    protected void asertIntervalCount( String uuid, int count) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        assertThat(set.getIntervals().size()).isEqualTo(count);
+    }
+    
+    protected void asertIntervalExists( String uuid, int start, int end) {
+        assertThat(hasInterval(uuid,start,end)).isTrue();
+    }
+    
+    protected void asertFirstInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getFirstInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected void asertLastInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getLastInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected boolean hasInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        for ( Interval interval : set.getIntervals() ) {
+            if ( interval.getStart() == start && interval.getEnd() == end ) return true;
+        }
+        return false;
+    }
+
+}",2016-06-07T17:01:51Z,105
"@@ -86,9 +86,9 @@ protected Document positionWithGtids(String gtids) {
 
     protected Document positionWithGtids(String gtids, boolean snapshot) {
         if (snapshot) {
-            return Document.create(SourceInfo.GTID_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
+            return Document.create(SourceInfo.GTID_SET_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
         }
-        return Document.create(SourceInfo.GTID_KEY, gtids);
+        return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
     protected Document positionWithoutGtids(String filename, int position, int row) {",2016-06-07T17:01:51Z,70
"@@ -45,15 +45,18 @@ public static enum Operation {
          */
         DELETE(""d"");
         private final String code;
+
         private Operation(String code) {
             this.code = code;
         }
-        public static Operation forCode( String code ) {
-            for ( Operation op : Operation.values()) {
-                if ( op.code().equalsIgnoreCase(code)) return op;
+
+        public static Operation forCode(String code) {
+            for (Operation op : Operation.values()) {
+                if (op.code().equalsIgnoreCase(code)) return op;
             }
             return null;
         }
+
         public String code() {
             return code;
         }
@@ -81,11 +84,13 @@ public static final class FieldName {
          */
         public static final String SOURCE = ""source"";
         /**
-         * The {@code ts} field is used to store the information about the local time at which the connector processed/generated
-         * the event. Note that the accuracy of the timestamp is not defined, and the values may not always be monotonically
-         * increasing.
+         * The {@code ts_ms} field is used to store the information about the local time at which the connector
+         * processed/generated the event. The timestamp values are the number of milliseconds past epoch (January 1, 1970), and
+         * determined by the {@link System#currentTimeMillis() JVM current time in milliseconds}. Note that the <em>accuracy</em>
+         * of the timestamp value depends on the JVM's system clock and all of its assumptions, limitations, conditions, and
+         * variations.
          */
-        public static final String TIMESTAMP = ""ts"";
+        public static final String TIMESTAMP = ""ts_ms"";
     }
 
     /**
@@ -304,6 +309,7 @@ public Struct delete(Struct before, Struct source, Long timestamp) {
 
     /**
      * Obtain the operation for the given source record.
+     * 
      * @param record the source record; may not be null
      * @return the operation, or null if no valid operation was found in the record
      */",2016-06-07T17:01:51Z,106
"@@ -5,10 +5,12 @@
  */
 package io.debezium.jdbc;
 
+import java.time.Instant;
 import java.time.LocalDate;
 import java.time.LocalTime;
 import java.time.Month;
 import java.time.ZonedDateTime;
+import java.time.temporal.ChronoField;
 import java.util.Calendar;
 
 import org.junit.Before;
@@ -38,12 +40,13 @@ public void beforeEach() {
     public void shouldAdaptSqlDate() {
         // '2014-09-08', '17:51:04.777', '2014-09-08 17:51:04.777', '2014-09-08 17:51:04.777'
         java.sql.Date sqlDate = createSqlDate(2014, Month.SEPTEMBER, 8);
+        ZonedDateTime expectedDateInTargetTZ = ZonedDateTime.ofInstant(Instant.ofEpochMilli(sqlDate.getTime()), adapter.targetZoneId());
         ZonedDateTime zdt = adapter.toZonedDateTime(sqlDate);
         // The date should match ...
         LocalDate date = zdt.toLocalDate();
         assertThat(date.getYear()).isEqualTo(2014);
         assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
-        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        assertThat(date.getDayOfMonth()).isEqualTo(expectedDateInTargetTZ.get(ChronoField.DAY_OF_MONTH));
         // There should be no time component ...
         LocalTime time = zdt.toLocalTime();
         assertThat(time.getHour()).isEqualTo(0);",2016-07-26T11:17:31Z,120
"@@ -41,6 +41,7 @@ public class ZookeeperServer {
     private volatile File dataDir;
     private volatile File snapshotDir;
     private volatile File logDir;
+    private volatile ZooKeeperServer server;
 
     /**
      * Create a new server instance.
@@ -75,7 +76,8 @@ public synchronized ZookeeperServer startup() throws IOException {
         this.logDir.mkdirs();
 
         try {
-            factory.startup(new ZooKeeperServer(snapshotDir, logDir, tickTime));
+            server = new ZooKeeperServer(snapshotDir, logDir, tickTime); 
+            factory.startup(server);
             return this;
         } catch (InterruptedException e) {
             factory = null;
@@ -100,6 +102,12 @@ public synchronized void shutdown(boolean deleteData) {
         if (factory != null) {
             try {
                 factory.shutdown();
+                try {
+                    // Zookeeper 3.4.6 does not close the ZK DB during shutdown, so we must do this here to avoid file locks and open handles...
+                    server.getZKDatabase().close();
+                } catch (IOException e) {
+                    LOGGER.error(""Unable to close zookeeper DB"", e);
+                }
             } finally {
                 factory = null;
                 if (deleteData) {",2016-07-26T11:17:31Z,121
"@@ -146,6 +146,53 @@
                       </assembly>
                     </build>
                   </image>
+                  <image>
+                    <!-- A Docker image using a partial MySQL installation maintained by MySQL team. -->
+                    <name>debezium/mysql-server-gtids-test-databases</name>
+                    <alias>database</alias>
+                    <run>
+                      <namingStrategy>alias</namingStrategy>
+                      <env>
+                        <MYSQL_ROOT_PASSWORD>debezium-rocks</MYSQL_ROOT_PASSWORD>
+                        <MYSQL_DATABASE>mysql</MYSQL_DATABASE> <!-- database created upon init -->
+                        <MYSQL_USER>${database.user}</MYSQL_USER>
+                        <MYSQL_PASSWORD>${database.password}</MYSQL_PASSWORD>
+                      </env>
+                      <ports>
+                        <port>${database.port}:3306</port>
+                      </ports>
+                      <log>
+                        <prefix>mysql</prefix>
+                        <enabled>true</enabled>
+                        <color>yellow</color>
+                      </log>
+                      <wait>
+                        <log>MySQL init process done. Ready for start up.</log>
+                        <time>30000</time> <!-- 30 seconds max -->
+                      </wait>
+                    </run>
+                    <build>
+                      <from>mysql/mysql-server:${version.mysql.server}</from>
+                      <assembly>
+                        <inline>
+                          <fileSets>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/server-gtids</directory>
+                              <includes>
+                                <include>my.cnf</include>
+                              </includes>
+                              <outputDirectory>etc/mysql</outputDirectory>
+                            </fileSet>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/init</directory>
+                              <outputDirectory>docker-entrypoint-initdb.d</outputDirectory>
+                            </fileSet>
+                          </fileSets>
+                        </inline>
+                        <basedir>/</basedir>
+                      </assembly>
+                    </build>
+                  </image>
                   <image>
                     <!-- A Docker image using a complete MySQL installation maintained by Docker team. -->
                     <name>debezium/mysql-test--databases</name>
@@ -350,5 +397,22 @@
           <docker.skip>false</docker.skip>
         </properties>
       </profile>
+      <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+            Use the Docker image for MySQL configured to use GTIDs.
+            To use, specify ""-Dgtid-mysql"" or -Pgtid-mysql on the Maven command line.
+            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+      <profile>
+        <id>gtid-mysql</id>
+        <activation>
+          <activeByDefault>false</activeByDefault>
+          <property>
+            <name>gtid-mysql</name>
+          </property>
+        </activation>
+        <properties>
+          <docker.image>debezium/mysql-server-gtids-test-databases</docker.image>
+          <docker.skip>false</docker.skip>
+        </properties>
+      </profile>
     </profiles>
 </project>",2016-06-07T17:01:51Z,83
"@@ -88,6 +88,7 @@ protected void doStart() {
         eventHandlers.put(EventType.ROTATE, this::handleRotateLogsEvent);
         eventHandlers.put(EventType.TABLE_MAP, this::handleUpdateTableMetadata);
         eventHandlers.put(EventType.QUERY, this::handleQueryEvent);
+        eventHandlers.put(EventType.GTID, this::handleGtidEvent);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, this::handleInsert);
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
@@ -142,9 +143,10 @@ protected void ignoreEvent(Event event) {
     protected void handleEvent(Event event) {
         if (event == null) return;
 
-        // Update the source offset info ...
+        // Update the source offset info. Note that the client returns the value in *milliseconds*, even though the binlog
+        // contains only *seconds* precision ...
         EventHeader eventHeader = event.getHeader();
-        source.setBinlogTimestamp(eventHeader.getTimestamp());
+        source.setBinlogTimestampSeconds(eventHeader.getTimestamp()/1000L); // client returns milliseconds, we record seconds
         source.setBinlogServerId(eventHeader.getServerId());
         EventType eventType = eventHeader.getEventType();
         if (eventType == EventType.ROTATE) {
@@ -166,16 +168,6 @@ protected void handleEvent(Event event) {
                 source.setRowInEvent(0);
             }
         }
-        if (eventType == EventType.GTID) {
-            EventData eventData = event.getData();
-            GtidEventData gtidEventData;
-            if (eventData instanceof EventDeserializer.EventDataWrapper) {
-                gtidEventData = (GtidEventData) ((EventDeserializer.EventDataWrapper) eventData).getInternal();
-            } else {
-                gtidEventData = (GtidEventData) eventData;
-            }
-            source.setGtids(gtidEventData.getGtid());
-        }
 
         // If there is a handler for this event, forward the event to it ...
         try {
@@ -188,6 +180,15 @@ protected void handleEvent(Event event) {
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    protected <T extends EventData> T unwrapData(Event event) {
+        EventData eventData = event.getData();
+        if (eventData instanceof EventDeserializer.EventDataWrapper) {
+            eventData = ((EventDeserializer.EventDataWrapper) eventData).getInternal();
+        }
+        return (T)eventData;
+    }
+
     /**
      * Handle the supplied event that signals that mysqld has stopped.
      * 
@@ -226,19 +227,31 @@ protected void handleServerIncident(Event event) {
      */
     protected void handleRotateLogsEvent(Event event) {
         logger.debug(""Rotating logs: {}"", event);
-        RotateEventData command = event.getData();
+        RotateEventData command = unwrapData(event);
         assert command != null;
         recordMakers.clear();
     }
 
+    /**
+     * Handle the supplied event with a {@link GtidEventData} that signals the beginning of a GTID transaction.
+     * 
+     * @param event the GTID event to be processed; may not be null
+     */
+    protected void handleGtidEvent(Event event) {
+        logger.debug(""GTID transaction: {}"", event);
+        GtidEventData gtidEvent = unwrapData(event);
+        source.setGtid(gtidEvent.getGtid());
+        source.setGtidSet(client.getGtidSet());
+    }
+
     /**
      * Handle the supplied event with an {@link QueryEventData} by possibly recording the DDL statements as changes in the
      * MySQL schemas.
      * 
      * @param event the database change data event to be processed; may not be null
      */
     protected void handleQueryEvent(Event event) {
-        QueryEventData command = event.getData();
+        QueryEventData command = unwrapData(event);
         logger.debug(""Received update table command: {}"", event);
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords && recordMakers.schemaChanges(dbName, statements, super::enqueueRecord) > 0) {
@@ -262,7 +275,7 @@ protected void handleQueryEvent(Event event) {
      * @param event the update event; never null
      */
     protected void handleUpdateTableMetadata(Event event) {
-        TableMapEventData metadata = event.getData();
+        TableMapEventData metadata = unwrapData(event);
         long tableNumber = metadata.getTableId();
         String databaseName = metadata.getDatabase();
         String tableName = metadata.getTable();
@@ -281,7 +294,7 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
-        WriteRowsEventData write = event.getData();
+        WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);
@@ -302,7 +315,7 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
-        UpdateRowsEventData update = event.getData();
+        UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
         // BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
@@ -330,7 +343,7 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
-        DeleteRowsEventData deleted = event.getData();
+        DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);",2016-06-07T17:01:51Z,67
"@@ -7,6 +7,7 @@
 
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.TreeMap;
@@ -111,11 +112,17 @@ protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
                 intervals.add(new Interval(interval.getStart(), interval.getEnd()));
             });
             Collections.sort(this.intervals);
-        }
-
-        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
-            this.uuid = uuid;
-            this.intervals = intervals;
+            if ( this.intervals.size() > 1 ) {
+                // Collapse adjacent intervals ...
+                for ( int i=intervals.size()-1; i!=0; --i) {
+                    Interval before = this.intervals.get(i-1);
+                    Interval after = this.intervals.get(i);
+                    if ( (before.getEnd() + 1) == after.getStart() ) {
+                        this.intervals.set(i-1,new Interval(before.getStart(),after.getEnd()));
+                        this.intervals.remove(i);
+                    }
+                }
+            }
         }
 
         /**
@@ -218,17 +225,41 @@ public String toString() {
             StringBuilder sb = new StringBuilder();
             if (sb.length() != 0) sb.append(',');
             sb.append(uuid).append(':');
-            sb.append(intervals.getFirst().getStart());
-            sb.append(intervals.getLast().getEnd());
+            Iterator<Interval> iter = intervals.iterator();
+            if ( iter.hasNext() ) sb.append(iter.next());
+            while ( iter.hasNext() ) {
+                sb.append(':');
+                sb.append(iter.next());
+            }
             return sb.toString();
         }
     }
 
     @Immutable
-    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+    public static class Interval implements Comparable<Interval> {
+
+        private final long start;
+        private final long end;
 
         public Interval(long start, long end) {
-            super(start, end);
+            this.start = start;
+            this.end = end;
+        }
+
+        /**
+         * Get the starting transaction number in this interval.
+         * @return this interval's first transaction number
+         */
+        public long getStart() {
+            return start;
+        }
+
+        /**
+         * Get the ending transaction number in this interval.
+         * @return this interval's last transaction number
+         */
+        public long getEnd() {
+            return end;
         }
 
         /**
@@ -244,6 +275,15 @@ public boolean isSubsetOf(Interval other) {
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
         }
+        
+        @Override
+        public int compareTo(Interval that) {
+            if ( that == this ) return 0;
+            long diff = this.start - that.start;
+            if ( diff > Integer.MAX_VALUE ) return Integer.MAX_VALUE;
+            if ( diff < Integer.MIN_VALUE ) return Integer.MIN_VALUE;
+            return (int) diff;
+        }
 
         @Override
         public int hashCode() {",2016-06-07T17:01:51Z,87
"@@ -9,6 +9,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -110,9 +111,16 @@ public void start(Map<String, String> props) {
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;
+                // The snapshot will determine if GTIDs are set
             }
         }
 
+        if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
+            // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
+            // previously used but the MySQL server has them enabled ...
+            source.setGtidSet("""");
+        }
+
         // Set up the readers ...
         this.binlogReader = new BinlogReader(taskContext);
         if (startWithSnapshot) {
@@ -174,13 +182,33 @@ protected void transitionToReadBinlog() {
      * @return {@code true} if the server has the binlog coordinates, or {@code false} otherwise
      */
     protected boolean isBinlogAvailable() {
+        String gtidStr = taskContext.source().gtidSet();
+        if ( gtidStr != null) {
+            if ( gtidStr.trim().isEmpty() ) return true; // start at beginning ...
+            String availableGtidStr = knownGtidSet();
+            if ( availableGtidStr == null || availableGtidStr.trim().isEmpty() ) {
+                // Last offsets had GTIDs but the server does not use them ...
+                logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
+                return false;
+            }
+            // GTIDs are enabled, and we used them previously ...
+            GtidSet gtidSet = new GtidSet(gtidStr);
+            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+                return true;
+            }
+            logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);
+            return false;
+        }
+        
         String binlogFilename = taskContext.source().binlogFilename();
         if (binlogFilename == null) return true; // start at current position
         if (binlogFilename.equals("""")) return true; // start at beginning
 
         // Accumulate the available binlog filenames ...
         List<String> logNames = new ArrayList<>();
         try {
+            logger.info(""Stop 0: Get all known binlogs from MySQL"");
             taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
                 while (rs.next()) {
                     logNames.add(rs.getString(1));
@@ -191,6 +219,50 @@ protected boolean isBinlogAvailable() {
         }
 
         // And compare with the one we're supposed to use ...
-        return logNames.stream().anyMatch(binlogFilename::equals);
+        boolean found = logNames.stream().anyMatch(binlogFilename::equals);
+        if ( !found ) {
+            logger.info(""Connector requires binlog file '{}', but MySQL only has {}"",binlogFilename,String.join("", "",logNames));
+        }
+        return found;
+    }
+
+    /**
+     * Determine whether the MySQL server has GTIDs enabled.
+     * 
+     * @return {@code false} if the server's {@code gtid_mode} is set and is {@code OFF}, or {@code true} otherwise
+     */
+    protected boolean isGtidModeEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>(""off"");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'GTID_MODE'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return !""OFF"".equalsIgnoreCase(mode.get());
+    }
+
+    /**
+     * Determine the available GTID set for MySQL.
+     * 
+     * @return the string representation of MySQL's GTID sets.
+     */
+    protected String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return gtidSetStr.get();
     }
 }",2016-06-07T17:01:51Z,68
"@@ -188,7 +188,7 @@ protected void execute() {
                 if (rs.next()) {
                     source.setBinlogFilename(rs.getString(1));
                     source.setBinlogPosition(rs.getLong(2));
-                    source.setGtids(rs.getString(5));// GTIDs
+                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
                     source.startSnapshot();
                 }
             });",2016-06-07T17:01:51Z,62
"@@ -14,6 +14,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.data.Envelope;
 import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
@@ -26,7 +27,7 @@
  * 
  * <pre>
  * {
- *     ""db"" : ""myDatabase""
+ *     ""server"" : ""production-server""
  * }
  * </pre>
  * 
@@ -36,14 +37,40 @@
  * 
  * <pre>
  * {
- *         ""gtids"" = ""3E11FA47-71CA-11E1-9E33-C80AA9429562:1-5"",
- *         ""file"" = ""mysql-bin.000003"",
- *         ""pos"" = 105586,
- *         ""row"" = 0
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
+ *     ""file"" = ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"" = 0,
+ *     ""snapshot"": true
  * }
  * </pre>
  * 
- * Note that the ""{@code gtid}"" field is present only when GTIDs are enabled.
+ * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
+ * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
+ * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
+ * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
+ * 
+ * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
+ * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
+ * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
+ * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
+ * an event that corresponds to the above partition and offset:
+ * 
+ * <pre>
+ * {
+ *     ""name"": ""production-server"",
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"": 0,
+ *     ""snapshot"": true
+ * }
+ * </pre>
  * 
  * @author Randall Hauch
  */
@@ -56,11 +83,12 @@ final class SourceInfo {
 
     public static final String SERVER_NAME_KEY = ""name"";
     public static final String SERVER_PARTITION_KEY = ""server"";
-    public static final String GTID_KEY = ""gtids"";
+    public static final String GTID_SET_KEY = ""gtids"";
+    public static final String GTID_KEY = ""gtid"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
     public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
-    public static final String TIMESTAMP_KEY = ""ts"";
+    public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
     /**
@@ -78,13 +106,14 @@ final class SourceInfo {
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private GtidSet binlogGtids;
+    private String gtidSet;
+    private String binlogGtid;
     private String binlogFilename;
     private long binlogPosition = 4;
     private int eventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
-    private long binlogTs = 0;
+    private long binlogTimestampSeconds = 0;
     private Map<String, String> sourcePartition;
     private boolean snapshot = false;
 
@@ -122,11 +151,11 @@ public Map<String, String> partition() {
      */
     public Map<String, ?> offset() {
         Map<String, Object> map = new HashMap<>();
-        if (binlogGtids != null) {
-            map.put(GTID_KEY, binlogGtids.toString());
+        if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
+        if (gtidSet != null) {
+            map.put(GTID_SET_KEY, gtidSet);
         }
-        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTs != 0 ) map.put(TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -158,13 +187,14 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        if (binlogGtids != null) {
-            result.put(GTID_KEY, binlogGtids.toString());
+        // Don't put the GTID Set into the struct; only the current GTID is fine ...
+        if (binlogGtid != null) {
+            result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
-        result.put(TIMESTAMP_KEY, binlogTs);
+        result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
@@ -193,12 +223,23 @@ public boolean isSnapshotInEffect() {
     }
 
     /**
-     * Set the GTID range for the MySQL binary log file.
+     * Set the latest GTID from the MySQL binary log file.
+     * 
+     * @param gtid the string representation of a specific GTID; may not be null
+     */
+    public void setGtid(String gtid) {
+        this.binlogGtid = gtid;
+    }
+
+    /**
+     * Set the set of GTIDs known to the MySQL server.
      * 
-     * @param gtids the string representation of the binlog GTIDs; may not be null
+     * @param gtidSet the string representation of GTID set; may not be null
      */
-    public void setGtids(String gtids) {
-        this.binlogGtids = gtids != null && !gtids.trim().isEmpty() ? new GtidSet(gtids) : null;
+    public void setGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            this.gtidSet = gtidSet;
+        }
     }
 
     /**
@@ -239,12 +280,12 @@ public void setBinlogServerId(long serverId) {
     }
 
     /**
-     * Set the timestamp as found within the MySQL binary log file.
+     * Set the number of <em>seconds</em> since Unix epoch (January 1, 1970) as found within the MySQL binary log file.
      * 
-     * @param timestamp the timestamp found within the binary log file
+     * @param timestampInSeconds the timestamp in <em>seconds</em> found within the binary log file
      */
-    public void setBinlogTimestamp(long timestamp) {
-        this.binlogTs = timestamp;
+    public void setBinlogTimestampSeconds(long timestampInSeconds) {
+        this.binlogTimestampSeconds = timestampInSeconds / 1000;
     }
 
     /**
@@ -270,7 +311,7 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtids((String) sourceOffset.get(GTID_KEY)); // may be null
+            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
             binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
@@ -297,7 +338,7 @@ private long longOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.binlogGtids != null ? this.binlogGtids.toString() : null;
+        return this.gtidSet != null ? this.gtidSet.toString() : null;
     }
 
     /**
@@ -340,9 +381,9 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (binlogGtids != null) {
+        if (gtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(binlogGtids);
+            sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
             sb.append("", pos="").append(binlogPosition());
             sb.append("", row="").append(eventRowNumber());
@@ -379,17 +420,17 @@ public String toString() {
      * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
      */
     public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
-        String recordedGtidSetStr = recorded.getString(GTID_KEY);
-        String desiredGtidSetStr = desired.getString(GTID_KEY);
+        String recordedGtidSetStr = recorded.getString(GTID_SET_KEY);
+        String desiredGtidSetStr = desired.getString(GTID_SET_KEY);
         if (desiredGtidSetStr != null) {
             // The desired position uses GTIDs, so we ideally compare using GTIDs ...
             if (recordedGtidSetStr != null) {
                 // Both have GTIDs, so base the comparison entirely on the GTID sets.
                 GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
                 GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
-                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                if (recordedGtidSet.equals(desiredGtidSet)) {
                     // They are exactly the same, which means the recorded position exactly matches the desired ...
-                    if ( !recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
+                    if (!recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
@@ -414,35 +455,35 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         }
 
         // Both positions are missing GTIDs. Look at the servers ...
-        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        if ( recordedServerId != desiredServerId ) {
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        if (recordedServerId != desiredServerId) {
             // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
             // is compare timestamps, and we have to assume that the server timestamps can be compared ...
-            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
-            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
+            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
+            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
             return recordedTimestamp <= desiredTimestamp;
         }
-        
+
         // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
         // comparable ...
         String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
         String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
-        if ( diff > 0 ) return false;
-        
+        if (diff > 0) return false;
+
         // The positions are the same, so compare the row number ...
         int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The binlog coordinates are the same ...
         return true;",2016-06-07T17:01:51Z,70
"@@ -0,0 +1,52 @@
+# For advice on how to change settings please see
+# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
+
+[mysqld]
+#
+# Remove leading # and set to the amount of RAM for the most important data
+# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
+# innodb_buffer_pool_size = 128M
+#
+# Remove leading # to turn on a very important data integrity option: logging
+# changes to the binary log between backups.
+# log_bin
+#
+# Remove leading # to set options mainly useful for reporting servers.
+# The server defaults are faster for transactions and fast SELECTs.
+# Adjust sizes as needed, experiment to find the optimal values.
+# join_buffer_size = 128M
+# sort_buffer_size = 2M
+# read_rnd_buffer_size = 2M
+skip-host-cache
+skip-name-resolve
+datadir=/var/lib/mysql
+socket=/var/lib/mysql/mysql.sock
+secure-file-priv=/var/lib/mysql-files
+user=mysql
+
+# Disabling symbolic-links is recommended to prevent assorted security risks
+symbolic-links=0
+
+log-error=/var/log/mysqld.log
+pid-file=/var/run/mysqld/mysqld.pid
+
+# ----------------------------------------------
+# Enable GTIDs on this master
+# ----------------------------------------------
+gtid_mode                 = on
+enforce_gtid_consistency  = on
+
+# ----------------------------------------------
+# Debezium ingest
+# ----------------------------------------------
+
+# Enable binary replication log and set the prefix, expiration, and log format.
+# The prefix is arbitrary, expiration can be short for integration tests but would
+# be longer on a production system. Row-level info is required for ingest to work.
+# Server ID is required, but this will vary on production systems
+server-id         = 112233
+log_bin           = mysql-bin
+expire_logs_days  = 1
+binlog_format     = row
+
+",2016-06-07T17:01:51Z,104
"@@ -0,0 +1,107 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.connector.mysql.GtidSet.Interval;
+import io.debezium.connector.mysql.GtidSet.UUIDSet;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class GtidSetTest {
+    
+    private static final String UUID1 = ""24bc7850-2c16-11e6-a073-0242ac110002"";
+
+    private GtidSet gtids;
+    
+    @Test
+    public void shouldCreateSetWithSingleInterval() {
+        gtids = new GtidSet(UUID1 + "":1-191"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,191);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,1,191);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191"");
+    }
+    
+    @Test
+    public void shouldCollapseAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,199);
+        asertFirstInterval(UUID1,1,199);
+        asertLastInterval(UUID1,1,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199"");
+    }
+
+    
+    @Test
+    public void shouldNotCollapseNonAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199"");
+        asertIntervalCount(UUID1,2);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,193,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199:1000-1033"");
+        asertIntervalCount(UUID1,3);
+        asertFirstInterval(UUID1,1,191);
+        asertIntervalExists(UUID1,193,199);
+        asertLastInterval(UUID1,1000,1033);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199:1000-1033"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervalsThatMayBeAdjacent() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199:1000-1033:1035-1036:1038-1039"");
+        asertIntervalCount(UUID1, 4);
+        asertFirstInterval(UUID1, 1, 199);
+        asertIntervalExists(UUID1, 1000, 1033);
+        asertIntervalExists(UUID1, 1035, 1036);
+        asertLastInterval(UUID1, 1038, 1039);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199:1000-1033:1035-1036:1038-1039""); // ??
+    }
+    
+    protected void asertIntervalCount( String uuid, int count) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        assertThat(set.getIntervals().size()).isEqualTo(count);
+    }
+    
+    protected void asertIntervalExists( String uuid, int start, int end) {
+        assertThat(hasInterval(uuid,start,end)).isTrue();
+    }
+    
+    protected void asertFirstInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getFirstInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected void asertLastInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getLastInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected boolean hasInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        for ( Interval interval : set.getIntervals() ) {
+            if ( interval.getStart() == start && interval.getEnd() == end ) return true;
+        }
+        return false;
+    }
+
+}",2016-06-07T17:01:51Z,105
"@@ -86,9 +86,9 @@ protected Document positionWithGtids(String gtids) {
 
     protected Document positionWithGtids(String gtids, boolean snapshot) {
         if (snapshot) {
-            return Document.create(SourceInfo.GTID_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
+            return Document.create(SourceInfo.GTID_SET_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
         }
-        return Document.create(SourceInfo.GTID_KEY, gtids);
+        return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
     protected Document positionWithoutGtids(String filename, int position, int row) {",2016-06-07T17:01:51Z,70
"@@ -45,15 +45,18 @@ public static enum Operation {
          */
         DELETE(""d"");
         private final String code;
+
         private Operation(String code) {
             this.code = code;
         }
-        public static Operation forCode( String code ) {
-            for ( Operation op : Operation.values()) {
-                if ( op.code().equalsIgnoreCase(code)) return op;
+
+        public static Operation forCode(String code) {
+            for (Operation op : Operation.values()) {
+                if (op.code().equalsIgnoreCase(code)) return op;
             }
             return null;
         }
+
         public String code() {
             return code;
         }
@@ -81,11 +84,13 @@ public static final class FieldName {
          */
         public static final String SOURCE = ""source"";
         /**
-         * The {@code ts} field is used to store the information about the local time at which the connector processed/generated
-         * the event. Note that the accuracy of the timestamp is not defined, and the values may not always be monotonically
-         * increasing.
+         * The {@code ts_ms} field is used to store the information about the local time at which the connector
+         * processed/generated the event. The timestamp values are the number of milliseconds past epoch (January 1, 1970), and
+         * determined by the {@link System#currentTimeMillis() JVM current time in milliseconds}. Note that the <em>accuracy</em>
+         * of the timestamp value depends on the JVM's system clock and all of its assumptions, limitations, conditions, and
+         * variations.
          */
-        public static final String TIMESTAMP = ""ts"";
+        public static final String TIMESTAMP = ""ts_ms"";
     }
 
     /**
@@ -304,6 +309,7 @@ public Struct delete(Struct before, Struct source, Long timestamp) {
 
     /**
      * Obtain the operation for the given source record.
+     * 
      * @param record the source record; may not be null
      * @return the operation, or null if no valid operation was found in the record
      */",2016-06-07T17:01:51Z,106
"@@ -5,10 +5,12 @@
  */
 package io.debezium.connector.mysql;
 
+import java.util.ArrayList;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.Iterator;
 import java.util.LinkedList;
+import java.util.List;
 import java.util.Map;
 import java.util.TreeMap;
 
@@ -23,7 +25,6 @@
 @Immutable
 public final class GtidSet {
 
-    private final String orderedString;
     private final Map<String, UUIDSet> uuidSetsByServerId = new TreeMap<>(); // sorts on keys
 
     /**
@@ -38,7 +39,6 @@ public GtidSet(String gtids) {
             if (sb.length() != 0) sb.append(',');
             sb.append(uuidSet.toString());
         });
-        orderedString = sb.toString();
     }
 
     /**
@@ -67,34 +67,38 @@ public UUIDSet forServerWithId(String uuid) {
      * @return {@code true} if all of the GTIDs in this set are completely contained within the supplied set of GTIDs, or
      *         {@code false} otherwise
      */
-    public boolean isSubsetOf(GtidSet other) {
+    public boolean isContainedWithin(GtidSet other) {
         if (other == null) return false;
         if (this.equals(other)) return true;
         for (UUIDSet uuidSet : uuidSetsByServerId.values()) {
             UUIDSet thatSet = other.forServerWithId(uuidSet.getUUID());
-            if (!uuidSet.isSubsetOf(thatSet)) return false;
+            if (!uuidSet.isContainedWithin(thatSet)) return false;
         }
         return true;
     }
 
     @Override
     public int hashCode() {
-        return orderedString.hashCode();
+        return uuidSetsByServerId.keySet().hashCode();
     }
 
     @Override
     public boolean equals(Object obj) {
         if (obj == this) return true;
         if (obj instanceof GtidSet) {
             GtidSet that = (GtidSet) obj;
-            return this.orderedString.equalsIgnoreCase(that.orderedString);
+            return this.uuidSetsByServerId.equals(that.uuidSetsByServerId);
         }
         return false;
     }
 
     @Override
     public String toString() {
-        return orderedString;
+        List<String> gtids = new ArrayList<String>();
+        for (UUIDSet uuidSet : uuidSetsByServerId.values()) {
+            gtids.add(uuidSet.toString());
+        }
+        return String.join("","",gtids);
     }
 
     /**
@@ -139,37 +143,8 @@ public String getUUID() {
          * 
          * @return the immutable transaction intervals; never null
          */
-        public Collection<Interval> getIntervals() {
-            return Collections.unmodifiableCollection(intervals);
-        }
-
-        /**
-         * Get the first interval of transaction numbers for this server.
-         * 
-         * @return the first interval, or {@code null} if there is none
-         */
-        public Interval getFirstInterval() {
-            return intervals.isEmpty() ? null : intervals.getFirst();
-        }
-
-        /**
-         * Get the last interval of transaction numbers for this server.
-         * 
-         * @return the last interval, or {@code null} if there is none
-         */
-        public Interval getLastInterval() {
-            return intervals.isEmpty() ? null : intervals.getLast();
-        }
-
-        /**
-         * Get the interval that contains the full range (and possibly more) of all of the individual intervals for this server.
-         * 
-         * @return the complete interval comprised of the {@link Interval#getStart() start} of the {@link #getFirstInterval()
-         *         first interval} and the {@link Interval#getEnd() end} of the {@link #getLastInterval()}, or {@code null} if
-         *         this server has no intervals at all
-         */
-        public Interval getCompleteInterval() {
-            return intervals.isEmpty() ? null : new Interval(getFirstInterval().getStart(), getLastInterval().getEnd());
+        public List<Interval> getIntervals() {
+            return Collections.unmodifiableList(intervals);
         }
 
         /**
@@ -180,7 +155,7 @@ public Interval getCompleteInterval() {
          * @return {@code true} if this server's transaction numbers are a subset of the transaction numbers of the supplied set,
          *         or false otherwise
          */
-        public boolean isSubsetOf(UUIDSet other) {
+        public boolean isContainedWithin(UUIDSet other) {
             if (other == null) return false;
             if (!this.getUUID().equalsIgnoreCase(other.getUUID())) {
                 // Not even the same server ...
@@ -195,7 +170,7 @@ public boolean isSubsetOf(UUIDSet other) {
             for (Interval thisInterval : this.intervals) {
                 boolean found = false;
                 for (Interval otherInterval : other.intervals) {
-                    if (thisInterval.isSubsetOf(otherInterval)) {
+                    if (thisInterval.isContainedWithin(otherInterval)) {
                         found = true;
                         break;
                     }
@@ -270,7 +245,7 @@ public long getEnd() {
          *         {@link #getStart() start} and the {@link #getEnd() end} is less than or equal to the supplied interval's
          *         {@link #getEnd() end}, or {@code false} otherwise
          */
-        public boolean isSubsetOf(Interval other) {
+        public boolean isContainedWithin(Interval other) {
             if (other == this) return true;
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
@@ -293,16 +268,16 @@ public int hashCode() {
         @Override
         public boolean equals(Object obj) {
             if (this == obj) return true;
-            if (obj instanceof com.github.shyiko.mysql.binlog.GtidSet.Interval) {
-                com.github.shyiko.mysql.binlog.GtidSet.Interval that = (com.github.shyiko.mysql.binlog.GtidSet.Interval) obj;
+            if (obj instanceof Interval) {
+                Interval that = (Interval) obj;
                 return this.getStart() == that.getStart() && this.getEnd() == that.getEnd();
             }
             return false;
         }
 
         @Override
         public String toString() {
-            return getStart() == getEnd() ? Long.toString(getStart()) : """" + getStart() + ""-"" + getEnd();
+            return """" + getStart() + ""-"" + getEnd();
         }
     }
 }",2016-06-16T15:57:58Z,87
"@@ -194,7 +194,7 @@ protected boolean isBinlogAvailable() {
             // GTIDs are enabled, and we used them previously ...
             GtidSet gtidSet = new GtidSet(gtidStr);
             GtidSet availableGtidSet = new GtidSet(knownGtidSet());
-            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+            if ( gtidSet.isContainedWithin(availableGtidSet)) {
                 return true;
             }
             logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);",2016-06-16T15:57:58Z,68
"@@ -503,7 +503,7 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
                     return true;
                 }
                 // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
-                return recordedGtidSet.isSubsetOf(desiredGtidSet);
+                return recordedGtidSet.isContainedWithin(desiredGtidSet);
             }
             // The desired position did use GTIDs while the recorded did not use GTIDs. So, we assume that the
             // recorded position is older since GTIDs are often enabled but rarely disabled. And if they are disabled,",2016-06-16T15:57:58Z,70
"@@ -5,6 +5,8 @@
  */
 package io.debezium.connector.mysql;
 
+import java.util.LinkedList;
+
 import org.junit.Test;
 
 import static org.fest.assertions.Assertions.assertThat;
@@ -84,14 +86,14 @@ protected void asertIntervalExists( String uuid, int start, int end) {
     
     protected void asertFirstInterval( String uuid, int start, int end) {
         UUIDSet set = gtids.forServerWithId(uuid);
-        Interval interval = set.getFirstInterval();
+        Interval interval = set.getIntervals().iterator().next();
         assertThat(interval.getStart()).isEqualTo(start);
         assertThat(interval.getEnd()).isEqualTo(end);
     }
     
     protected void asertLastInterval( String uuid, int start, int end) {
         UUIDSet set = gtids.forServerWithId(uuid);
-        Interval interval = set.getLastInterval();
+        Interval interval = new LinkedList<>(set.getIntervals()).getLast();
         assertThat(interval.getStart()).isEqualTo(start);
         assertThat(interval.getEnd()).isEqualTo(end);
     }",2016-06-16T15:57:58Z,105
"@@ -268,6 +268,15 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                                       .withDescription(""Frequency in milliseconds to wait for new change events to appear after receiving no events. Defaults to 1 second (1000 ms)."")
                                                       .withDefault(TimeUnit.SECONDS.toMillis(1))
                                                       .withValidation(Field::isPositiveInteger);
+    
+    public static final Field ROW_COUNT_FOR_STREAMING_RESULT_SETS = Field.create(""min.row.count.to.stream.results"")
+                                                                         .withDisplayName(""Stream result set larger than"")
+                                                                         .withType(Type.LONG)
+                                                                         .withWidth(Width.MEDIUM)
+                                                                         .withImportance(Importance.LOW)
+                                                                         .withDescription(""The number of rows a table must contain to stream results rather than pull all into memory during snapshots. Defaults to 1,000."")
+                                                                         .withDefault(1_000)
+                                                                         .withValidation(Field::isPositiveInteger);
 
     /**
      * The database history class is hidden in the {@link #configDef()} since that is designed to work with a user interface,",2016-08-04T21:06:50Z,40
"@@ -102,6 +102,10 @@ public long timeoutInMilliseconds() {
     public long pollIntervalInMillseconds() {
         return config.getLong(MySqlConnectorConfig.POLL_INTERVAL_MS);
     }
+    
+    public long rowCountForLargeTable() {
+        return config.getLong(MySqlConnectorConfig.ROW_COUNT_FOR_STREAMING_RESULT_SETS);
+    }
 
     public boolean includeSchemaChangeRecords() {
         return config.getBoolean(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES);",2016-08-04T21:06:50Z,69
"@@ -80,6 +80,20 @@ public RecordsForTable forTable(TableId tableId, BitSet includedColumns, Blockin
         return tableNumber != null ? forTable(tableNumber, includedColumns, consumer) : null;
     }
 
+    /**
+     * Determine if there is a record maker for the given table.
+     * 
+     * @param tableId the identifier of the table
+     * @return {@code true} if there is a {@link #forTable(TableId, BitSet, BlockingConsumer) record maker}, or {@code false}
+     * if there is none
+     */
+    public boolean hasTable(TableId tableId) {
+        Long tableNumber = tableNumbersByTableId.get(tableId);
+        if ( tableNumber == null ) return false;
+        Converter converter = convertersByTableNumber.get(tableNumber);
+        return converter != null;
+    }
+
     /**
      * Obtain the record maker for the given table, using the specified columns and sending records to the given consumer.
      * ",2016-08-04T21:06:50Z,122
"@@ -5,7 +5,10 @@
  */
 package io.debezium.connector.mysql;
 
+import java.sql.Connection;
+import java.sql.ResultSet;
 import java.sql.SQLException;
+import java.sql.Statement;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -14,11 +17,16 @@
 import java.util.Map;
 import java.util.Set;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.AtomicReference;
 
+import org.apache.kafka.connect.source.SourceRecord;
+
 import io.debezium.connector.mysql.RecordMakers.RecordsForTable;
+import io.debezium.function.BufferedBlockingConsumer;
 import io.debezium.function.Predicates;
 import io.debezium.jdbc.JdbcConnection;
+import io.debezium.jdbc.JdbcConnection.StatementFactory;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.util.Clock;
@@ -140,8 +148,8 @@ protected void execute() {
         final Clock clock = context.clock();
         final long ts = clock.currentTimeInMillis();
         logger.info(""Starting snapshot for {} with user '{}'"", context.connectionString(), mysql.username());
-        logRolesForCurrentUser(sql, mysql);
-        logServerInformation(sql, mysql);
+        logRolesForCurrentUser(mysql);
+        logServerInformation(mysql);
         try {
             // ------
             // STEP 0
@@ -222,15 +230,15 @@ protected void execute() {
                 }
             });
             logger.debug(""\t list of available databases is: {}"", databaseNames);
-           
+
             // ------
             // STEP 5
             // ------
             // Get the list of table IDs for each database. We can't use a prepared statement with MySQL, so we have to
             // build the SQL statement each time. Although in other cases this might lead to SQL injection, in our case
             // we are reading the database names from the database and not taking them from the user ...
             logger.info(""Step 5: read list of available tables in each database"");
-            final List<TableId> tableIds = new ArrayList<>();
+            List<TableId> tableIds = new ArrayList<>();
             final Map<String, List<TableId>> tableIdsByDbName = new HashMap<>();
             for (String dbName : databaseNames) {
                 sql.set(""SHOW TABLES IN "" + dbName);
@@ -301,28 +309,50 @@ protected void execute() {
                 mysql.execute(sql.get());
                 unlocked = true;
                 long lockReleased = clock.currentTimeInMillis();
-                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased - lockAcquired));
+                logger.info(""Step 7: blocked writes to MySQL for a total of {}"", Strings.duration(lockReleased - lockAcquired));
             }
 
             // ------
             // STEP 8
             // ------
+            // Use a buffered blocking consumer to buffer all of the records, so that after we copy all of the tables
+            // and produce events we can update the very last event with the non-snapshot offset ...
+            BufferedBlockingConsumer<SourceRecord> bufferedRecordQueue = BufferedBlockingConsumer.bufferLast(super::enqueueRecord);
+
             // Dump all of the tables and generate source records ...
             logger.info(""Step 8: scanning contents of {} tables"", tableIds.size());
             long startScan = clock.currentTimeInMillis();
             AtomicBoolean interrupted = new AtomicBoolean(false);
+            AtomicLong totalRowCount = new AtomicLong();
             int counter = 0;
+            int completedCounter = 0;
+            long largeTableCount = context.rowCountForLargeTable();
             Iterator<TableId> tableIdIter = tableIds.iterator();
             while (tableIdIter.hasNext()) {
                 TableId tableId = tableIdIter.next();
-                boolean isLastTable = tableIdIter.hasNext() == false;
-                long start = clock.currentTimeInMillis();
-                logger.debug(""Step 8.{}: scanning table '{}'; {} tables remain"", ++counter, tableId, tableIds.size() - counter);
-                sql.set(""SELECT * FROM "" + tableId);
-                mysql.query(sql.get(), rs -> {
-                    RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, super::enqueueRecord);
-                    if (recordMaker != null) {
-                        boolean completed = false;
+
+                // Obtain a record maker for this table, which knows about the schema ...
+                RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, bufferedRecordQueue);
+                if (recordMaker != null) {
+
+                    // Choose how we create statements based on the # of rows ...
+                    sql.set(""SELECT COUNT(*) FROM "" + tableId);
+                    AtomicLong numRows = new AtomicLong();
+                    mysql.query(sql.get(), rs -> {
+                        if (rs.next()) numRows.set(rs.getLong(1));
+                    });
+                    StatementFactory statementFactory = this::createStatement;
+                    if (numRows.get() > largeTableCount) {
+                        statementFactory = this::createStatementWithLargeResultSet;
+                    }
+
+                    // Scan the rows in the table ...
+                    long start = clock.currentTimeInMillis();
+                    logger.debug(""Step 8: - scanning table '{}' ({} of {} tables)"", tableId, ++counter, tableIds.size());
+                    sql.set(""SELECT * FROM "" + tableId);
+                    mysql.query(sql.get(), statementFactory, rs -> {
+                        long rowNum = 0;
+                        long rowCount = numRows.get();
                         try {
                             // The table is included in the connector's filters, so process all of the table records ...
                             final Table table = schema.tableFor(tableId);
@@ -332,37 +362,52 @@ protected void execute() {
                                 for (int i = 0, j = 1; i != numColumns; ++i, ++j) {
                                     row[i] = rs.getObject(j);
                                 }
-                                if (isLastTable && rs.isLast()) {
-                                    // This is the last record, so mark the offset as having completed the snapshot
-                                    // but the SourceInfo.struct() will still be marked as being part of the snapshot ...
-                                    source.markLastSnapshot();
-                                }
                                 recorder.recordRow(recordMaker, row, ts); // has no row number!
+                                ++rowNum;
+                                if (rowNum % 10_000 == 0 || rowNum == rowCount) {
+                                    long stop = clock.currentTimeInMillis();
+                                    logger.info(""Step 8: - {} of {} rows scanned from table '{}' after {}"", rowNum, rowCount, tableId,
+                                                Strings.duration(stop - start));
+                                }
                             }
-                            if (isLastTable) completed = true;
                         } catch (InterruptedException e) {
                             Thread.interrupted();
-                            if (!completed) {
-                                // We were not able to finish all rows in all tables ...
-                                logger.info(""Stopping the snapshot after thread interruption"");
-                                interrupted.set(true);
-                            }
+                            // We were not able to finish all rows in all tables ...
+                            logger.info(""Step 8: Stopping the snapshot due to thread interruption"");
+                            interrupted.set(true);
+                        } finally {
+                            totalRowCount.addAndGet(rowCount);
                         }
-                    }
-                });
-                if (interrupted.get()) break;
-                long stop = clock.currentTimeInMillis();
-                logger.info(""Step 8.{}: scanned table '{}' in {}"", counter, tableId, Strings.duration(stop - start));
+                    });
+
+                    if (interrupted.get()) break;
+                }
+                ++completedCounter;
             }
+
+            // We've copied all of the tables, but our buffer holds onto the very last record.
+            // First mark the snapshot as complete and then apply the updated offset to the buffered record ...
+            source.markLastSnapshot();
             long stop = clock.currentTimeInMillis();
-            logger.info(""Step 8: scanned contents of {} tables in {}"", tableIds.size(), Strings.duration(stop - startScan));
+            try {
+                bufferedRecordQueue.flush(this::replaceOffset);
+                logger.info(""Step 8: scanned {} rows in {} tables in {}"",
+                            totalRowCount, tableIds.size(), Strings.duration(stop - startScan));
+            } catch (InterruptedException e) {
+                Thread.interrupted();
+                // We were not able to finish all rows in all tables ...
+                logger.info(""Step 8: aborting the snapshot after {} rows in {} of {} tables {}"",
+                            totalRowCount, completedCounter, tableIds.size(), Strings.duration(stop - startScan));
+                interrupted.set(true);
+            }
 
             // ------
             // STEP 9
             // ------
             // Release the read lock if we have not yet done so ...
+            int step = 9;
             if (!unlocked) {
-                logger.info(""Step 9: releasing global read lock to enable MySQL writes"");
+                logger.info(""Step {}: releasing global read lock to enable MySQL writes"", step++);
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
@@ -375,13 +420,13 @@ protected void execute() {
             // -------
             if (interrupted.get()) {
                 // We were interrupted while reading the tables, so roll back the transaction and return immediately ...
-                logger.info(""Step 10: rolling back transaction after request to stop"");
+                logger.info(""Step {}: rolling back transaction after abort"", step++);
                 sql.set(""ROLLBACK"");
                 mysql.execute(sql.get());
                 return;
             }
             // Otherwise, commit our transaction
-            logger.info(""Step 10: committing transaction"");
+            logger.info(""Step {}: committing transaction"", step++);
             sql.set(""COMMIT"");
             mysql.execute(sql.get());
 
@@ -399,35 +444,93 @@ protected void execute() {
             failed(e, ""Aborting snapshot after running '"" + sql.get() + ""': "" + e.getMessage());
         }
     }
-    
-    private void logServerInformation(AtomicReference<String> sql, JdbcConnection mysql) {
+
+    /**
+     * Create a JDBC statement that can be used for large result sets.
+     * <p>
+     * By default, the MySQL Connector/J driver retrieves all rows for ResultSets and stores them in memory. In most cases this
+     * is the most efficient way to operate and, due to the design of the MySQL network protocol, is easier to implement.
+     * However, when ResultSets that have a large number of rows or large values, the driver may not be able to allocate
+     * heap space in the JVM and may result in an {@link OutOfMemoryError}. See
+     * <a href=""https://issues.jboss.org/browse/DBZ-94"">DBZ-94</a> for details.
+     * <p>
+     * This method handles such cases using the
+     * <a href=""https://dev.mysql.com/doc/connector-j/5.1/en/connector-j-reference-implementation-notes.html"">recommended
+     * technique</a> for MySQL by creating the JDBC {@link Statement} with {@link ResultSet#TYPE_FORWARD_ONLY forward-only} cursor
+     * and {@link ResultSet#CONCUR_READ_ONLY read-only concurrency} flags, and with a {@link Integer#MIN_VALUE minimum value}
+     * {@link Statement#setFetchSize(int) fetch size hint}.
+     * 
+     * @param connection the JDBC connection; may not be null
+     * @return the statement; never null
+     * @throws SQLException if there is a problem creating the statement
+     */
+    private Statement createStatementWithLargeResultSet(Connection connection) throws SQLException {
+        Statement stmt = connection.createStatement(ResultSet.TYPE_FORWARD_ONLY, ResultSet.CONCUR_READ_ONLY);
+        stmt.setFetchSize(Integer.MIN_VALUE);
+        return stmt;
+    }
+
+    private Statement createStatement(Connection connection) throws SQLException {
+        return connection.createStatement();
+    }
+
+    private void logServerInformation(JdbcConnection mysql) {
         try {
-            sql.set(""SHOW VARIABLES LIKE 'version'"");
-            mysql.query(sql.get(), rs -> {
-                if (rs.next()) {
-                    logger.info(""MySql server version is '{}'"", rs.getString(2));        
+            logger.info(""MySQL server variables related to change data capture:"");
+            mysql.query(""SHOW VARIABLES WHERE Variable_name REGEXP 'version|binlog|tx_|gtid'"", rs -> {
+                while (rs.next()) {
+                    logger.info(""\t{} = {}"",
+                                Strings.pad(rs.getString(1), 45, ' '),
+                                Strings.pad(rs.getString(2), 45, ' '));
                 }
             });
         } catch (SQLException e) {
             logger.info(""Cannot determine MySql server version"", e);
-        }       
+        }
     }
 
-    private void logRolesForCurrentUser(AtomicReference<String> sql, JdbcConnection mysql) {
+    private void logRolesForCurrentUser(JdbcConnection mysql) {
         try {
-            List<String> privileges = new ArrayList<>();
-            sql.set(""SHOW GRANTS"");
-            mysql.query(sql.get(), rs -> {
+            List<String> grants = new ArrayList<>();
+            mysql.query(""SHOW GRANTS FOR CURRENT_USER"", rs -> {
                 while (rs.next()) {
-                    privileges.add(rs.getString(1));
+                    grants.add(rs.getString(1));
                 }
             });
-            logger.info(""User '{}' has '{}'"", mysql.username(), privileges);
+            if (grants.isEmpty()) {
+                logger.warn(""Snapshot is using user '{}' but it likely doesn't have proper privileges. "" +
+                        ""If tables are missing or are empty, ensure connector is configured with the correct MySQL user "" +
+                        ""and/or ensure that the MySQL user has the required privileges."",
+                            mysql.username());
+            } else {
+                logger.info(""Snapshot is using user '{}' with these MySQL grants:"", mysql.username());
+                grants.forEach(grant -> logger.info(""\t{}"", grant));
+            }
         } catch (SQLException e) {
             logger.info(""Cannot determine the privileges for '{}' "", mysql.username(), e);
         }
     }
 
+    /**
+     * Utility method to replace the offset in the given record with the latest. This is used on the last record produced
+     * during the snapshot.
+     * 
+     * @param record the record
+     * @return the updated record
+     */
+    protected SourceRecord replaceOffset(SourceRecord record) {
+        if (record == null) return null;
+        Map<String, ?> newOffset = context.source().offset();
+        return new SourceRecord(record.sourcePartition(),
+                newOffset,
+                record.topic(),
+                record.kafkaPartition(),
+                record.keySchema(),
+                record.key(),
+                record.valueSchema(),
+                record.value());
+    }
+
     protected void enqueueSchemaChanges(String dbName, String ddlStatements) {
         if (context.includeSchemaChangeRecords() &&
                 context.makeRecord().schemaChanges(dbName, ddlStatements, super::enqueueRecord) > 0) {",2016-08-04T21:06:50Z,62
"@@ -0,0 +1,71 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.function;
+
+import java.util.function.Function;
+
+/**
+ * A {@link BlockingConsumer} that retains a maximum number of values in a buffer before sending them to
+ * a delegate consumer. Note that any buffered values may need to be {@link #flush() flushed} periodically.
+ * <p>
+ * This maintains the same order of the values.
+ * 
+ * @param <T> the type of the input to the operation
+ * @author Randall Hauch
+ */
+public interface BufferedBlockingConsumer<T> extends BlockingConsumer<T> {
+
+    /**
+     * Flush all of the buffered values to the delegate.
+     * 
+     * @throws InterruptedException if the thread is interrupted while this consumer is blocked
+     */
+    public default void flush() throws InterruptedException {
+        flush(t -> t);
+    }
+
+    /**
+     * Flush all of the buffered values to the delegate by first running each buffered value through the given function
+     * to generate a new value to be flushed to the delegate consumer.
+     * 
+     * @param function the function to apply to the values that are flushed
+     * @throws InterruptedException if the thread is interrupted while this consumer is blocked
+     */
+    public void flush(Function<T, T> function) throws InterruptedException;
+
+    /**
+     * Get a {@link BufferedBlockingConsumer} that buffers just the last value seen by the consumer.
+     * When another value is then added to the consumer, this buffered consumer will push the prior value into the delegate
+     * and buffer the latest.
+     * <p>
+     * The resulting consumer is not threadsafe.
+     * 
+     * @param delegate the delegate to which values should be flushed; may not be null
+     * @return the blocking consumer that buffers a single value at a time; never null
+     */
+    public static <T> BufferedBlockingConsumer<T> bufferLast(BlockingConsumer<T> delegate) {
+        return new BufferedBlockingConsumer<T>() {
+            private T last;
+
+            @Override
+            public void accept(T t) throws InterruptedException {
+                if (last != null) delegate.accept(last);
+                last = t;
+            }
+
+            @Override
+            public void flush(Function<T, T> function) throws InterruptedException {
+                if (last != null) {
+                    try {
+                        delegate.accept(function.apply(last));
+                    } finally {
+                        last = null;
+                    }
+                }
+            }
+        };
+    }
+}",2016-08-04T21:06:50Z,123
"@@ -27,8 +27,10 @@
 import java.util.concurrent.ConcurrentMap;
 import java.util.function.Consumer;
 import java.util.stream.Stream;
+
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
+
 import io.debezium.annotation.ThreadSafe;
 import io.debezium.config.Configuration;
 import io.debezium.config.Field;
@@ -275,8 +277,22 @@ public static interface StatementPreparer {
      * @see #execute(Operations)
      */
     public JdbcConnection query(String query, ResultSetConsumer resultConsumer) throws SQLException {
+        return query(query,conn->conn.createStatement(),resultConsumer);
+    }
+
+    /**
+     * Execute a SQL query.
+     * 
+     * @param query the SQL query
+     * @param statementFactory the function that should be used to create the statement from the connection; may not be null
+     * @param resultConsumer the consumer of the query results
+     * @return this object for chaining methods together
+     * @throws SQLException if there is an error connecting to the database or executing the statements
+     * @see #execute(Operations)
+     */
+    public JdbcConnection query(String query, StatementFactory statementFactory, ResultSetConsumer resultConsumer) throws SQLException {
         Connection conn = connection();
-        try (Statement statement = conn.createStatement();) {
+        try (Statement statement = statementFactory.createStatement(conn);) {
             if (LOGGER.isTraceEnabled()) {
                 LOGGER.trace(""running '{}'"", query);
             }
@@ -288,6 +304,21 @@ public JdbcConnection query(String query, ResultSetConsumer resultConsumer) thro
         }
         return this;
     }
+    
+    /**
+     * A function to create a statement from a connection.
+     * @author Randall Hauch
+     */
+    @FunctionalInterface
+    public interface StatementFactory {
+        /**
+         * Use the given connection to create a statement.
+         * @param connection the JDBC connection; never null
+         * @return the statement
+         * @throws SQLException if there are problems creating a statement
+         */
+        Statement createStatement(Connection connection) throws SQLException;
+    }
 
     /**
      * Execute a SQL prepared query.
@@ -521,24 +552,24 @@ public Set<TableId> readTableNames(String databaseCatalog, String schemaNamePatt
     }
 
     /**
-     * Returns a JDBC connection string using the current configuration and url. 
+     * Returns a JDBC connection string using the current configuration and url.
      * 
      * @param urlPattern a {@code String} representing a JDBC connection with variables that will be replaced
      * @return a {@code String} where the variables in {@code urlPattern} are replaced with values from the configuration
      */
     public String connectionString(String urlPattern) {
         Properties props = config.asProperties();
         return findAndReplace(urlPattern, props, JdbcConfiguration.DATABASE, JdbcConfiguration.HOSTNAME, JdbcConfiguration.PORT,
-                              JdbcConfiguration.USER, JdbcConfiguration.PASSWORD);   
+                              JdbcConfiguration.USER, JdbcConfiguration.PASSWORD);
     }
 
     /**
-     * Returns the username for this connection 
+     * Returns the username for this connection
      * 
      * @return a {@code String}, never {@code null}
      */
     public String username()  {
-        return config.getString(JdbcConfiguration.USER);    
+        return config.getString(JdbcConfiguration.USER);
     }
 
     /**",2016-08-04T21:06:50Z,9
"@@ -264,6 +264,26 @@ public static String createString(final char charToRepeat,
         return sb.toString();
     }
 
+    /**
+     * Pad the string with the specific character to ensure the string is at least the specified length.
+     * 
+     * @param original the string to be padded; may not be null
+     * @param length the minimum desired length; must be positive
+     * @param padChar the character to use for padding, if the supplied string is not long enough
+     * @return the padded string of the desired length
+     * @see #justifyLeft(String, int, char)
+     */
+    public static String pad(String original,
+                                   int length,
+                                   char padChar) {
+        if ( original.length() >= length ) return original;
+        StringBuilder sb = new StringBuilder(original);
+        while ( sb.length() < length ) {
+            sb.append(padChar);
+        }
+        return sb.toString();
+    }
+
     /**
      * Set the length of the string, padding with the supplied character if the supplied string is shorter than desired, or
      * truncating the string if it is longer than desired. Unlike {@link #justifyLeft(String, int, char)}, this method does not",2016-08-04T21:06:50Z,102
"@@ -0,0 +1,52 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.function;
+
+import java.util.LinkedList;
+import java.util.List;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author Randall Hauch
+ */
+public class BufferedBlockingConsumerTest {
+
+    private List<Integer> history;
+    private BlockingConsumer<Integer> consumer;
+    
+
+    @Before
+    public void beforeEach() {
+        history = new LinkedList<>();
+        consumer = history::add;
+    }
+    
+    @Test
+    public void shouldMaintainSameOrder() throws InterruptedException {
+        BufferedBlockingConsumer<Integer> buffered = BufferedBlockingConsumer.bufferLast(consumer);
+        
+        // Add several values ...
+        buffered.accept(1);
+        buffered.accept(2);
+        buffered.accept(3);
+        buffered.accept(4);
+        buffered.accept(5);
+
+        // And verify the history contains all but the last value ...
+        assertThat(history).containsExactly(1,2,3,4);
+        
+        // Flush the last value...
+        buffered.flush();
+        
+        // And verify the history contains the same values ...
+        assertThat(history).containsExactly(1,2,3,4,5);
+    }
+
+}",2016-08-04T21:06:50Z,124
"@@ -32,6 +32,61 @@
  */
 public class MySqlConnectorConfig {
 
+    /**
+     * The set of predefined TemporalPrecisionMode options or aliases.
+     */
+    public static enum TemporalPrecisionMode {
+        /**
+         * Represent time and date values based upon the resolution in the database, using {@link io.debezium.time} semantic
+         * types.
+         */
+        ADAPTIVE(""adaptive""),
+
+        /**
+         * Represent time and date values using Kafka Connect {@link org.apache.kafka.connect.data} logical types, which always
+         * have millisecond precision.
+         */
+        CONNECT(""connect"");
+
+        private final String value;
+
+        private TemporalPrecisionMode(String value) {
+            this.value = value;
+        }
+
+        public String getValue() {
+            return value;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         * 
+         * @param value the configuration property value; may not be null
+         * @return the matching option, or null if no match is found
+         */
+        public static TemporalPrecisionMode parse(String value) {
+            if (value == null) return null;
+            value = value.trim();
+            for (TemporalPrecisionMode option : TemporalPrecisionMode.values()) {
+                if (option.getValue().equalsIgnoreCase(value)) return option;
+            }
+            return null;
+        }
+
+        /**
+         * Determine if the supplied value is one of the predefined options.
+         * 
+         * @param value the configuration property value; may not be null
+         * @param defaultValue the default value; may be null
+         * @return the matching option, or null if no match is found and the non-null default is invalid
+         */
+        public static TemporalPrecisionMode parse(String value, String defaultValue) {
+            TemporalPrecisionMode mode = parse(value);
+            if (mode == null && defaultValue != null) mode = parse(defaultValue);
+            return mode;
+        }
+    }
+
     /**
      * The set of predefined SnapshotMode options or aliases.
      */
@@ -268,7 +323,7 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                                       .withDescription(""Frequency in milliseconds to wait for new change events to appear after receiving no events. Defaults to 1 second (1000 ms)."")
                                                       .withDefault(TimeUnit.SECONDS.toMillis(1))
                                                       .withValidation(Field::isPositiveInteger);
-    
+
     public static final Field ROW_COUNT_FOR_STREAMING_RESULT_SETS = Field.create(""min.row.count.to.stream.results"")
                                                                          .withDisplayName(""Stream result set larger than"")
                                                                          .withType(Type.LONG)
@@ -330,6 +385,17 @@ public static SnapshotMode parse(String value, String defaultValue) {
                                                                       + ""of the snapshot; in such cases set this property to 'false'."")
                                                               .withDefault(true);
 
+    public static final Field TIME_PRECISION_MODE = Field.create(""time.precision.mode"")
+                                                         .withDisplayName(""Time Precision"")
+                                                         .withEnum(TemporalPrecisionMode.class)
+                                                         .withWidth(Width.SHORT)
+                                                         .withImportance(Importance.MEDIUM)
+                                                         .withDescription(""Time, date, and timestamps can be represented with different kinds of precisions, including:""
+                                                                 + ""'adaptive' (the default) bases the precision of time, date, and timestamp values on the database column's precision; ""
+                                                                 + ""'connect' always represents time, date, and timestamp values using Kafka Connect's built-in representations for Time, Date, and Timestamp, ""
+                                                                 + ""which uses millisecond precision regardless of the database columns' precision ."")
+                                                         .withDefault(TemporalPrecisionMode.ADAPTIVE.getValue());
+
     /**
      * Method that generates a Field for specifying that string columns whose names match a set of regular expressions should
      * have their values truncated to be no longer than the specified number of characters.
@@ -371,7 +437,8 @@ public static final Field MASK_COLUMN(int length) {
                                                      DATABASE_HISTORY, INCLUDE_SCHEMA_CHANGES,
                                                      TABLE_WHITELIST, TABLE_BLACKLIST, TABLES_IGNORE_BUILTIN,
                                                      DATABASE_WHITELIST, DATABASE_BLACKLIST,
-                                                     COLUMN_BLACKLIST, SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING);
+                                                     COLUMN_BLACKLIST, SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING,
+                                                     TIME_PRECISION_MODE);
 
     /**
      * The set of {@link Field}s that are included in the {@link #configDef() configuration definition}. This includes
@@ -393,7 +460,7 @@ protected static ConfigDef configDef() {
         Field.group(config, ""Events"", INCLUDE_SCHEMA_CHANGES, TABLES_IGNORE_BUILTIN, DATABASE_WHITELIST, TABLE_WHITELIST,
                     COLUMN_BLACKLIST, TABLE_BLACKLIST, DATABASE_BLACKLIST);
         Field.group(config, ""Connector"", CONNECTION_TIMEOUT_MS, KEEP_ALIVE, MAX_QUEUE_SIZE, MAX_BATCH_SIZE, POLL_INTERVAL_MS,
-                    SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING);
+                    SNAPSHOT_MODE, SNAPSHOT_MINIMAL_LOCKING, TIME_PRECISION_MODE);
         return config;
     }
 ",2016-08-11T15:48:07Z,40
"@@ -23,6 +23,7 @@
 
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.TemporalPrecisionMode;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.TimeZoneAdapter;
 import io.debezium.relational.Table;
@@ -100,7 +101,10 @@ public MySqlSchema(Configuration config, String serverName) {
         this.ddlParser.addListener(ddlChanges);
 
         // Use MySQL-specific converters and schemas for values ...
-        MySqlValueConverters valueConverters = new MySqlValueConverters();
+        String timePrecisionModeStr = config.getString(MySqlConnectorConfig.TIME_PRECISION_MODE);
+        TemporalPrecisionMode timePrecisionMode = TemporalPrecisionMode.parse(timePrecisionModeStr);
+        boolean adaptiveTimePrecision = TemporalPrecisionMode.ADAPTIVE.equals(timePrecisionMode);
+        MySqlValueConverters valueConverters = new MySqlValueConverters(adaptiveTimePrecision);
         this.schemaBuilder = new TableSchemaBuilder(valueConverters, schemaNameValidator::validate);
         
         // Set up the server name and schema prefix ...",2016-08-11T15:48:07Z,19
"@@ -42,21 +42,28 @@ public class MySqlValueConverters extends JdbcValueConverters {
      * Create a new instance that always uses UTC for the default time zone when converting values without timezone information
      * to values that require timezones.
      * <p>
+     * 
+     * @param adaptiveTimePrecision {@code true} if the time, date, and timestamp values should be based upon the precision of the
+     *            database columns using {@link io.debezium.time} semantic types, or {@code false} if they should be fixed to
+     *            millisecond precision using Kafka Connect {@link org.apache.kafka.connect.data} logical types.
      */
-    public MySqlValueConverters() {
-        super();
+    public MySqlValueConverters(boolean adaptiveTimePrecision) {
+        this(adaptiveTimePrecision, ZoneOffset.UTC);
     }
 
     /**
      * Create a new instance, and specify the time zone offset that should be used only when converting values without timezone
      * information to values that require timezones. This default offset should not be needed when values are highly-correlated
      * with the expected SQL/JDBC types.
      * 
+     * @param adaptiveTimePrecision {@code true} if the time, date, and timestamp values should be based upon the precision of the
+     *            database columns using {@link io.debezium.time} semantic types, or {@code false} if they should be fixed to
+     *            millisecond precision using Kafka Connect {@link org.apache.kafka.connect.data} logical types.
      * @param defaultOffset the zone offset that is to be used when converting non-timezone related values to values that do
      *            have timezones; may be null if UTC is to be used
      */
-    public MySqlValueConverters(ZoneOffset defaultOffset) {
-        super(defaultOffset);
+    public MySqlValueConverters(boolean adaptiveTimePrecision, ZoneOffset defaultOffset) {
+        super(adaptiveTimePrecision, defaultOffset);
     }
 
     @Override
@@ -105,17 +112,17 @@ public ValueConverter converter(Column column, Field fieldDefn) {
     @SuppressWarnings(""deprecation"")
     protected Object convertYear(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
-        if ( data instanceof java.time.Year ) {
+        if (data instanceof java.time.Year) {
             // The MySQL binlog always returns a Year object ...
-            return ((java.time.Year)data).getValue();
+            return ((java.time.Year) data).getValue();
         }
-        if ( data instanceof java.sql.Date ) {
+        if (data instanceof java.sql.Date) {
             // MySQL JDBC driver sometimes returns a Java SQL Date object ...
-            return ((java.sql.Date)data).getYear();
+            return ((java.sql.Date) data).getYear();
         }
         if (data instanceof Number) {
             // MySQL JDBC driver sometimes returns a short ...
-            return ((Number)data).intValue();
+            return ((Number) data).intValue();
         }
         return handleUnknownData(column, fieldDefn, data);
     }",2016-08-11T15:48:07Z,47
"@@ -24,6 +24,7 @@
 
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.connector.mysql.MySqlConnectorConfig.TemporalPrecisionMode;
 import io.debezium.data.Envelope;
 import io.debezium.doc.FixFor;
 import io.debezium.embedded.AbstractConnectorTest;
@@ -111,7 +112,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 // {""c1"" : ""16321"", ""c2"" : ""64264780"", ""c3"" : ""1410198664780"", ""c4"" : ""2014-09-08T17:51:04.78-05:00""}
 
                 // '2014-09-08'
-                Long c1 = after.getInt64(""c1""); // epoch days
+                Integer c1 = after.getInt32(""c1""); // epoch days
                 LocalDate c1Date = LocalDate.ofEpochDay(c1);
                 assertThat(c1Date.getYear()).isEqualTo(2014);
                 assertThat(c1Date.getMonth()).isEqualTo(Month.SEPTEMBER);
@@ -162,6 +163,112 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
         });
     }
 
+    @Test
+    @FixFor( ""DBZ-61"" )
+    public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshotAndConnectTimesTypes() throws SQLException, InterruptedException {
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(MySqlConnectorConfig.TIME_PRECISION_MODE, TemporalPrecisionMode.CONNECT.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"", false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+
+        // ---------------------------------------------------------------------------------------------------------------
+        // Consume all of the events due to startup and initialization of the database
+        // ---------------------------------------------------------------------------------------------------------------
+        // Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(4 + 3); // 4 schema change record, 3 inserts
+        stopConnector();
+        assertThat(records).isNotNull();
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(4);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(4);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+        records.forEach(record -> {
+            Struct value = (Struct) record.value();
+            if (record.topic().endsWith(""dbz_85_fractest"")) {
+                // The microseconds of all three should be exactly 780
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                // c1 DATE,
+                // c2 TIME(2),
+                // c3 DATETIME(2),
+                // c4 TIMESTAMP(2)
+                //
+                // {""c1"" : ""16321"", ""c2"" : ""64264780"", ""c3"" : ""1410198664780"", ""c4"" : ""2014-09-08T17:51:04.78-05:00""}
+
+                // '2014-09-08'
+                java.util.Date c1 = (java.util.Date)after.get(""c1""); // epoch days
+                LocalDate c1Date = LocalDate.ofEpochDay(c1.getTime() / TimeUnit.DAYS.toMillis(1));
+                assertThat(c1Date.getYear()).isEqualTo(2014);
+                assertThat(c1Date.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(c1Date.getDayOfMonth()).isEqualTo(8);
+
+                // '17:51:04.777'
+                java.util.Date c2 = (java.util.Date)after.get(""c2""); // milliseconds past midnight
+                LocalTime c2Time = LocalTime.ofNanoOfDay(TimeUnit.MILLISECONDS.toNanos(c2.getTime()));
+                assertThat(c2Time.getHour()).isEqualTo(17);
+                assertThat(c2Time.getMinute()).isEqualTo(51);
+                assertThat(c2Time.getSecond()).isEqualTo(4);
+                assertThat(c2Time.getNano()).isEqualTo((int) TimeUnit.MILLISECONDS.toNanos(780));
+                assertThat(io.debezium.time.Time.toMilliOfDay(c2Time)).isEqualTo((int)c2.getTime());
+
+                // '2014-09-08 17:51:04.777'
+                java.util.Date c3 = (java.util.Date)after.get(""c3""); // epoch millis
+                long c3Seconds = c3.getTime() / 1000;
+                long c3Millis = c3.getTime() % 1000;
+                LocalDateTime c3DateTime = LocalDateTime.ofEpochSecond(c3Seconds,
+                                                                       (int) TimeUnit.MILLISECONDS.toNanos(c3Millis),
+                                                                       ZoneOffset.UTC);
+                assertThat(c3DateTime.getYear()).isEqualTo(2014);
+                assertThat(c3DateTime.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(c3DateTime.getDayOfMonth()).isEqualTo(8);
+                assertThat(c3DateTime.getHour()).isEqualTo(17);
+                assertThat(c3DateTime.getMinute()).isEqualTo(51);
+                assertThat(c3DateTime.getSecond()).isEqualTo(4);
+                assertThat(c3DateTime.getNano()).isEqualTo((int) TimeUnit.MILLISECONDS.toNanos(780));
+                assertThat(io.debezium.time.Timestamp.toEpochMillis(c3DateTime)).isEqualTo(c3.getTime());
+
+                // '2014-09-08 17:51:04.777'
+                String c4 = after.getString(""c4""); // MySQL timestamp, so always ZonedTimestamp
+                OffsetDateTime c4DateTime = OffsetDateTime.parse(c4, ZonedTimestamp.FORMATTER);
+                // In case the timestamp string not in our timezone, convert to ours so we can compare ...
+                c4DateTime = c4DateTime.withOffsetSameInstant(OffsetDateTime.now().getOffset());
+                assertThat(c4DateTime.getYear()).isEqualTo(2014);
+                assertThat(c4DateTime.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(c4DateTime.getDayOfMonth()).isEqualTo(8);
+                assertThat(c4DateTime.getHour()).isEqualTo(17);
+                assertThat(c4DateTime.getMinute()).isEqualTo(51);
+                assertThat(c4DateTime.getSecond()).isEqualTo(4);
+                assertThat(c4DateTime.getNano()).isEqualTo((int) TimeUnit.MILLISECONDS.toNanos(780));
+                // We're running the connector in the same timezone as the server, so the timezone in the timestamp
+                // should match our current offset ...
+                assertThat(c4DateTime.getOffset()).isEqualTo(OffsetDateTime.now().getOffset());
+            }
+        });
+    }
+
     @Test
     public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
@@ -217,7 +324,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
                 // {""c1"" : ""16321"", ""c2"" : ""64264780"", ""c3"" : ""1410198664780"", ""c4"" : ""2014-09-08T17:51:04.78-05:00""}
 
                 // '2014-09-08'
-                Long c1 = after.getInt64(""c1""); // epoch days
+                Integer c1 = after.getInt32(""c1""); // epoch days
                 LocalDate c1Date = LocalDate.ofEpochDay(c1);
                 assertThat(c1Date.getYear()).isEqualTo(2014);
                 assertThat(c1Date.getMonth()).isEqualTo(Month.SEPTEMBER);",2016-08-11T15:48:07Z,48
"@@ -11,6 +11,7 @@
 import java.time.OffsetDateTime;
 import java.time.OffsetTime;
 import java.time.ZoneOffset;
+import java.util.concurrent.TimeUnit;
 
 import org.apache.kafka.connect.data.Decimal;
 import org.apache.kafka.connect.data.Field;
@@ -62,23 +63,31 @@ public class JdbcValueConverters implements ValueConverterProvider {
 
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final ZoneOffset defaultOffset;
+    private final boolean adaptiveTimePrecision;
 
     /**
-     * Create a new instance.
+     * Create a new instance that always uses UTC for the default time zone when converting values without timezone information
+     * to values that require timezones, and uses adapts time and timestamp values based upon the precision of the database
+     * columns.
      */
     public JdbcValueConverters() {
-        this(ZoneOffset.UTC);
+        this(true, ZoneOffset.UTC);
     }
 
     /**
      * Create a new instance, and specify the time zone offset that should be used only when converting values without timezone
      * information to values that require timezones. This default offset should not be needed when values are highly-correlated
      * with the expected SQL/JDBC types.
+     * 
+     * @param adaptiveTimePrecision {@code true} if the time, date, and timestamp values should be based upon the precision of the
+     *            database columns using {@link io.debezium.time} semantic types, or {@code false} if they should be fixed to
+     *            millisecond precision using Kafka Connect {@link org.apache.kafka.connect.data} logical types.
      * @param defaultOffset the zone offset that is to be used when converting non-timezone related values to values that do
-     * have timezones; may be null if UTC is to be used
+     *            have timezones; may be null if UTC is to be used
      */
-    public JdbcValueConverters(ZoneOffset defaultOffset) {
+    public JdbcValueConverters(boolean adaptiveTimePrecision, ZoneOffset defaultOffset) {
         this.defaultOffset = defaultOffset != null ? defaultOffset : ZoneOffset.UTC;
+        this.adaptiveTimePrecision = adaptiveTimePrecision;
     }
 
     @Override
@@ -153,15 +162,24 @@ public SchemaBuilder schemaBuilder(Column column) {
 
             // Date and time values
             case Types.DATE:
+                if (adaptiveTimePrecision) {
                 return Date.builder();
+                }
+                return org.apache.kafka.connect.data.Date.builder();
             case Types.TIME:
-                if (column.length() <= 3) return Time.builder();
-                if (column.length() <= 6) return MicroTime.builder();
-                return NanoTime.builder();
+                if (adaptiveTimePrecision) {
+                    if (column.length() <= 3) return Time.builder();
+                    if (column.length() <= 6) return MicroTime.builder();
+                    return NanoTime.builder();
+                }
+                return org.apache.kafka.connect.data.Time.builder();
             case Types.TIMESTAMP:
-                if (column.length() <= 3) return Timestamp.builder();
-                if (column.length() <= 6) return MicroTimestamp.builder();
-                return NanoTimestamp.builder();
+                if (adaptiveTimePrecision) {
+                    if (column.length() <= 3 || !adaptiveTimePrecision) return Timestamp.builder();
+                    if (column.length() <= 6) return MicroTimestamp.builder();
+                    return NanoTimestamp.builder();
+                }
+                return org.apache.kafka.connect.data.Timestamp.builder();
             case Types.TIME_WITH_TIMEZONE:
                 return ZonedTime.builder();
             case Types.TIMESTAMP_WITH_TIMEZONE:
@@ -184,6 +202,7 @@ public SchemaBuilder schemaBuilder(Column column) {
                 break;
         }
         return null;
+
     }
 
     @Override
@@ -240,15 +259,24 @@ public ValueConverter converter(Column column, Field fieldDefn) {
 
             // Date and time values
             case Types.DATE:
+                if (adaptiveTimePrecision) {
                 return (data) -> convertDateToEpochDays(column, fieldDefn, data);
+                }
+                return (data) -> convertDateToEpochDaysAsDate(column, fieldDefn, data);
             case Types.TIME:
-                if (column.length() <= 3) return (data) -> convertTimeToMillisPastMidnight(column, fieldDefn, data);
-                if (column.length() <= 6) return (data) -> convertTimeToMicrosPastMidnight(column, fieldDefn, data);
-                return (data) -> convertTimeToNanosPastMidnight(column, fieldDefn, data);
+                if (adaptiveTimePrecision) {
+                    if (column.length() <= 3) return (data) -> convertTimeToMillisPastMidnight(column, fieldDefn, data);
+                    if (column.length() <= 6) return (data) -> convertTimeToMicrosPastMidnight(column, fieldDefn, data);
+                    return (data) -> convertTimeToNanosPastMidnight(column, fieldDefn, data);
+                }
+                return (data) -> convertTimeToMillisPastMidnightAsDate(column, fieldDefn, data);
             case Types.TIMESTAMP:
-                if (column.length() <= 3) return (data) -> convertTimestampToEpochMillis(column, fieldDefn, data);
-                if (column.length() <= 6) return (data) -> convertTimestampToEpochMicros(column, fieldDefn, data);
-                return (data) -> convertTimestampToEpochNanos(column, fieldDefn, data);
+                if (adaptiveTimePrecision) {
+                    if (column.length() <= 3) return (data) -> convertTimestampToEpochMillis(column, fieldDefn, data);
+                    if (column.length() <= 6) return (data) -> convertTimestampToEpochMicros(column, fieldDefn, data);
+                    return (data) -> convertTimestampToEpochNanos(column, fieldDefn, data);
+                }
+                return (data) -> convertTimestampToEpochMillisAsDate(column, fieldDefn, data);
             case Types.TIME_WITH_TIMEZONE:
                 return (data) -> convertTimeWithZone(column, fieldDefn, data);
             case Types.TIMESTAMP_WITH_TIMEZONE:
@@ -379,6 +407,29 @@ protected Object convertTimestampToEpochNanos(Column column, Field fieldDefn, Ob
         }
     }
 
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#TIMESTAMP} to {@link java.util.Date} values representing
+     * milliseconds past epoch.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Timestamp} instances, which have date and time info
+     * but no time zone info. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such
+     * as {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTimestampToEpochMillisAsDate(Column column, Field fieldDefn, Object data) {
+        try {
+            Long epochMillis = Timestamp.toEpochMillis(data);
+            if ( epochMillis == null ) return null;
+            return new java.util.Date(epochMillis.longValue());
+        } catch (IllegalArgumentException e) {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+    }
+
     /**
      * Converts a value object for an expected JDBC type of {@link Types#TIME} to {@link Time} values, or milliseconds past
      * midnight.
@@ -446,7 +497,31 @@ protected Object convertTimeToNanosPastMidnight(Column column, Field fieldDefn,
     }
 
     /**
-     * Converts a value object for an expected JDBC type of {@link Types#DATE}.
+     * Converts a value object for an expected JDBC type of {@link Types#TIME} to {@link java.util.Date} values representing
+     * the milliseconds past midnight on the epoch day.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Time} instances that have no notion of date or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have date components, those date components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTimeToMillisPastMidnightAsDate(Column column, Field fieldDefn, Object data) {
+        try {
+            Integer millisOfDay = Time.toMilliOfDay(data);
+            if ( millisOfDay == null ) return null;
+            return new java.util.Date(millisOfDay.longValue());
+        } catch (IllegalArgumentException e) {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DATE} to the number of days past epoch.
      * <p>
      * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
      * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
@@ -469,6 +544,34 @@ protected Object convertDateToEpochDays(Column column, Field fieldDefn, Object d
         }
     }
 
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DATE} to the number of days past epoch, but represented
+     * as a {@link java.util.Date} value at midnight on the date.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have time components, those time components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDateToEpochDaysAsDate(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        try {
+            Integer epochDay = Date.toEpochDay(data);
+            if ( epochDay == null ) return null;
+            long epochMillis = TimeUnit.DAYS.toMillis(epochDay.longValue());
+            return new java.util.Date(epochMillis);
+        } catch (IllegalArgumentException e) {
+            logger.warn(""Unexpected JDBC DATE value for field {} with schema {}: class={}, value={}"", fieldDefn.name(),
+                        fieldDefn.schema(), data.getClass(), data);
+            return null;
+        }
+    }
+
     /**
      * Converts a value object for an expected JDBC type of {@link Types#BLOB}, {@link Types#BINARY},
      * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.",2016-08-11T15:48:07Z,75
"@@ -23,7 +23,7 @@ public class Date {
 
     /**
      * Returns a {@link SchemaBuilder} for a {@link Date}. The builder will create a schema that describes a field
-     * with the {@value #SCHEMA_NAME} as the {@link Schema#name() name} and {@link SchemaBuilder#int64() INT64} for the literal
+     * with the {@value #SCHEMA_NAME} as the {@link Schema#name() name} and {@link SchemaBuilder#int32() INT32} for the literal
      * type storing the number of <em>days</em> since January 1, 1970, at 00:00:00Z.
      * <p>
      * You can use the resulting SchemaBuilder to set or override additional schema settings such as required/optional, default
@@ -32,14 +32,14 @@ public class Date {
      * @return the schema builder
      */
     public static SchemaBuilder builder() {
-        return SchemaBuilder.int64()
+        return SchemaBuilder.int32()
                             .name(SCHEMA_NAME)
                             .version(1);
     }
 
     /**
      * Returns a Schema for a {@link Date} but with all other default Schema settings. The schema describes a field
-     * with the {@value #SCHEMA_NAME} as the {@link Schema#name() name} and {@link SchemaBuilder#int64() INT64} for the literal
+     * with the {@value #SCHEMA_NAME} as the {@link Schema#name() name} and {@link SchemaBuilder#int32() INT32} for the literal
      * type storing the number of <em>days</em> since January 1, 1970, at 00:00:00Z.
      * 
      * @return the schema
@@ -55,12 +55,12 @@ public static Schema schema() {
      * {@link java.sql.Timestamp}, ignoring any time portions of the supplied value.
      * 
      * @param value the local or SQL date, time, or timestamp value
-     * @return the microseconds past midnight
+     * @return the number of days past epoch
      * @throws IllegalArgumentException if the value is not an instance of the acceptable types
      */
-    public static Long toEpochDay(Object value) {
+    public static Integer toEpochDay(Object value) {
         if ( value == null ) return null;
-        return Conversions.toLocalDate(value).toEpochDay();
+        return (int)Conversions.toLocalDate(value).toEpochDay();
     }
 
     private Date() {",2016-08-11T15:48:07Z,125
"@@ -166,20 +166,17 @@ public void shouldValidateAcceptableConfiguration() {
 
         Recommender tableNameRecommender = MySqlConnectorConfig.TABLE_WHITELIST.recommender();
         List<Object> tableNames = tableNameRecommender.validValues(MySqlConnectorConfig.TABLE_WHITELIST, config);
-        assertThat(tableNames).containsOnly(""connector_test.customers"",
-                                            ""connector_test.orders"",
-                                            ""connector_test.products"",
-                                            ""connector_test.products_on_hand"",
-                                            ""connector_test_ro.customers"",
-                                            ""connector_test_ro.orders"",
-                                            ""connector_test_ro.products"",
-                                            ""connector_test_ro.products_on_hand"",
-                                            ""regression_test.t1464075356413_testtable6"",
-                                            ""regression_test.dbz_85_fractest"",
-                                            ""regression_test.dbz84_integer_types_table"",
-                                            ""readbinlog_test.product"",
-                                            ""readbinlog_test.purchased"",
-                                            ""readbinlog_test.person"");
+        assertThat(tableNames).contains(""connector_test.customers"",
+                                        ""connector_test.orders"",
+                                        ""connector_test.products"",
+                                        ""connector_test.products_on_hand"",
+                                        ""connector_test_ro.customers"",
+                                        ""connector_test_ro.orders"",
+                                        ""connector_test_ro.products"",
+                                        ""connector_test_ro.products_on_hand"",
+                                        ""regression_test.t1464075356413_testtable6"",
+                                        ""regression_test.dbz_85_fractest"",
+                                        ""regression_test.dbz84_integer_types_table"");
         Testing.debug(""List of tableNames: "" + tableNames);
 
         // Now set the whitelist to two databases ...",2016-08-08T11:25:38Z,71
"@@ -24,6 +24,7 @@
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
+import io.debezium.doc.FixFor;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.util.Testing;
@@ -54,6 +55,7 @@ public void afterEach() {
     }
 
     @Test
+    @FixFor( ""DBZ-61"" )
     public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()",2016-08-08T11:25:38Z,48
"@@ -10,4 +10,5 @@ log4j.rootLogger=INFO, stdout
 # Set up the default logging to be INFO level, then override specific units
 log4j.logger.io.debezium=INFO
 log4j.logger.io.debezium.embedded.EmbeddedEngine$EmbeddedConfig=WARN
-#log4j.logger.io.debezium.connector.mysql.BinlogReader=DEBUG
\ No newline at end of file
+#log4j.logger.io.debezium.connector.mysql.BinlogReader=DEBUG
+#log4j.logger.io.debezium.connector.mysql.SnapshotReader=DEBUG
\ No newline at end of file",2016-08-08T11:25:38Z,73
"@@ -0,0 +1,49 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.doc;
+
+import static java.lang.annotation.ElementType.METHOD;
+import static java.lang.annotation.RetentionPolicy.CLASS;
+
+import java.lang.annotation.Documented;
+import java.lang.annotation.Retention;
+import java.lang.annotation.Target;
+
+/**
+ * Annotation that can be used to help track that a test is verifying the fix for one or more specific issues. To use, simply
+ * place this annotation on the test method and reference the JIRA issue number:
+ * 
+ * <pre>
+ *    &#064;FixFor(""DBZ-123"")
+ *    &#064;Test
+ *    public void shouldVerifyBehavior() {
+ *     ...
+ *    }
+ * </pre>
+ * <p>
+ * It is also possible to reference multiple JIRA issues if the test is verifying multiple ones:
+ * 
+ * <pre>
+ *    &#064;FixFor({""DBZ-123"",""DBZ-456""})
+ *    &#064;Test
+ *    public void shouldVerifyBehavior() {
+ *     ...
+ *    }
+ * </pre>
+ * 
+ * </p>
+ */
+@Documented
+@Retention( CLASS )
+@Target( METHOD )
+public @interface FixFor {
+    /**
+     * The JIRA issue for which this is a fix. For example, ""DBZ-123"".
+     * 
+     * @return the issue
+     */
+    String[] value();
+}",2016-08-08T11:25:38Z,126
"@@ -285,6 +285,8 @@ protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordC
                             + (numberOfRecords - recordsConsumed) + "" more)"");
                     print(record);
                 }
+            } else {
+                return recordsConsumed;
             }
         }
         return recordsConsumed;",2016-08-08T11:25:38Z,35
"@@ -6,10 +6,8 @@
 package io.debezium.connector.mysql;
 
 import java.sql.SQLException;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
 import io.debezium.config.Configuration;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
@@ -21,7 +19,8 @@
  */
 public class MySqlJdbcContext implements AutoCloseable {
 
-    protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false"");
+    protected static final String MYSQL_CONNECTION_URL = ""jdbc:mysql://${hostname}:${port}/?useInformationSchema=true&nullCatalogMeansCurrent=false"";
+    protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(MYSQL_CONNECTION_URL);
 
     protected final Logger logger = LoggerFactory.getLogger(getClass());
     protected final Configuration config;
@@ -83,4 +82,8 @@ public void shutdown() {
     public void close() {
         shutdown();
     }
+    
+    protected String connectionString() {
+        return jdbc.connectionString(MYSQL_CONNECTION_URL);
+    }
 }",2016-08-03T13:54:17Z,61
"@@ -5,6 +5,7 @@
  */
 package io.debezium.connector.mysql;
 
+import java.sql.SQLException;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.HashSet;
@@ -131,14 +132,16 @@ protected void doCleanup() {
      */
     protected void execute() {
         context.configureLoggingContext(""snapshot"");
-        logger.info(""Starting snapshot"");
         final AtomicReference<String> sql = new AtomicReference<>();
         final JdbcConnection mysql = context.jdbc();
         final MySqlSchema schema = context.dbSchema();
         final Filters filters = schema.filters();
         final SourceInfo source = context.source();
         final Clock clock = context.clock();
         final long ts = clock.currentTimeInMillis();
+        logger.info(""Starting snapshot for {} with user '{}'"", context.connectionString(), mysql.username());
+        logRolesForCurrentUser(sql, mysql);
+        logServerInformation(sql, mysql);
         try {
             // ------
             // STEP 0
@@ -194,6 +197,10 @@ protected void execute() {
                         // This column exists only in MySQL 5.6.5 or later ...
                         String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
                         source.setGtidSet(gtidSet);
+                        logger.debug(""\t using binlog '{}' at position '{}' and gtid '{}'"", binlogFilename, binlogPosition,
+                                     gtidSet);
+                    } else {
+                        logger.debug(""\t using binlog '{}' at position '{}'"", binlogFilename, binlogPosition);
                     }
                     source.startSnapshot();
                 }
@@ -214,7 +221,8 @@ protected void execute() {
                     databaseNames.add(rs.getString(1));
                 }
             });
-
+            logger.debug(""\t list of available databases is: {}"", databaseNames);
+           
             // ------
             // STEP 5
             // ------
@@ -232,6 +240,9 @@ protected void execute() {
                         if (filters.tableFilter().test(id)) {
                             tableIds.add(id);
                             tableIdsByDbName.computeIfAbsent(dbName, k -> new ArrayList<>()).add(id);
+                            logger.debug(""\t including '{}'"", id);
+                        } else {
+                            logger.debug(""\t '{}' is filtered out, discarding"", id);
                         }
                     }
                 });
@@ -388,6 +399,34 @@ protected void execute() {
             failed(e, ""Aborting snapshot after running '"" + sql.get() + ""': "" + e.getMessage());
         }
     }
+    
+    private void logServerInformation(AtomicReference<String> sql, JdbcConnection mysql) {
+        try {
+            sql.set(""SHOW VARIABLES LIKE 'version'"");
+            mysql.query(sql.get(), rs -> {
+                if (rs.next()) {
+                    logger.info(""MySql server version is '{}'"", rs.getString(2));        
+                }
+            });
+        } catch (SQLException e) {
+            logger.info(""Cannot determine MySql server version"", e);
+        }       
+    }
+
+    private void logRolesForCurrentUser(AtomicReference<String> sql, JdbcConnection mysql) {
+        try {
+            List<String> privileges = new ArrayList<>();
+            sql.set(""SHOW GRANTS"");
+            mysql.query(sql.get(), rs -> {
+                while (rs.next()) {
+                    privileges.add(rs.getString(1));
+                }
+            });
+            logger.info(""User '{}' has '{}'"", mysql.username(), privileges);
+        } catch (SQLException e) {
+            logger.info(""Cannot determine the privileges for '{}' "", mysql.username(), e);
+        }
+    }
 
     protected void enqueueSchemaChanges(String dbName, String ddlStatements) {
         if (context.includeSchemaChangeRecords() &&",2016-08-03T13:54:17Z,62
"@@ -48,7 +48,7 @@ public static MySQLConnection forTestDatabase(String databaseName, String userna
     protected static void addDefaults(Configuration.Builder builder) {
         builder.withDefault(JdbcConfiguration.HOSTNAME, ""localhost"")
                .withDefault(JdbcConfiguration.PORT, 3306)
-               .withDefault(JdbcConfiguration.USER, ""mysql"")
+               .withDefault(JdbcConfiguration.USER, ""mysqluser"")
                .withDefault(JdbcConfiguration.PASSWORD, ""mysqlpw"");
     }
 ",2016-08-03T13:54:17Z,36
"@@ -27,10 +27,8 @@
 import java.util.concurrent.ConcurrentMap;
 import java.util.function.Consumer;
 import java.util.stream.Stream;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
-
 import io.debezium.annotation.ThreadSafe;
 import io.debezium.config.Configuration;
 import io.debezium.config.Field;
@@ -228,7 +226,12 @@ public JdbcConnection connect() throws SQLException {
     public JdbcConnection execute(String... sqlStatements) throws SQLException {
         return execute(statement -> {
             for (String sqlStatement : sqlStatements) {
-                if (sqlStatement != null) statement.execute(sqlStatement);
+                if (sqlStatement != null) {
+                    if (LOGGER.isTraceEnabled()) {
+                        LOGGER.trace(""executing '{}'"", sqlStatement);
+                    }
+                    statement.execute(sqlStatement);
+                }
             }
         });
     }
@@ -274,6 +277,9 @@ public static interface StatementPreparer {
     public JdbcConnection query(String query, ResultSetConsumer resultConsumer) throws SQLException {
         Connection conn = connection();
         try (Statement statement = conn.createStatement();) {
+            if (LOGGER.isTraceEnabled()) {
+                LOGGER.trace(""running '{}'"", query);
+            }
             try (ResultSet resultSet = statement.executeQuery(query);) {
                 if (resultConsumer != null) {
                     resultConsumer.accept(resultSet);
@@ -514,6 +520,27 @@ public Set<TableId> readTableNames(String databaseCatalog, String schemaNamePatt
         return tableIds;
     }
 
+    /**
+     * Returns a JDBC connection string using the current configuration and url. 
+     * 
+     * @param urlPattern a {@code String} representing a JDBC connection with variables that will be replaced
+     * @return a {@code String} where the variables in {@code urlPattern} are replaced with values from the configuration
+     */
+    public String connectionString(String urlPattern) {
+        Properties props = config.asProperties();
+        return findAndReplace(urlPattern, props, JdbcConfiguration.DATABASE, JdbcConfiguration.HOSTNAME, JdbcConfiguration.PORT,
+                              JdbcConfiguration.USER, JdbcConfiguration.PASSWORD);   
+    }
+
+    /**
+     * Returns the username for this connection 
+     * 
+     * @return a {@code String}, never {@code null}
+     */
+    public String username()  {
+        return config.getString(JdbcConfiguration.USER);    
+    }
+
     /**
      * Create definitions for each tables in the database, given the catalog name, schema pattern, table filter, and
      * column filter.",2016-08-03T13:54:17Z,9
"@@ -232,7 +232,7 @@ protected boolean isBinlogAvailable() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking for binary logs: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking for binary logs: "", e);
         }
 
         // And compare with the one we're supposed to use ...
@@ -257,7 +257,7 @@ protected boolean isGtidModeEnabled() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
         }
 
         return !""OFF"".equalsIgnoreCase(mode.get());
@@ -277,7 +277,7 @@ protected String knownGtidSet() {
                 }
             });
         } catch (SQLException e) {
-            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+            throw new ConnectException(""Unexpected error while connecting to MySQL and looking at GTID mode: "", e);
         }
 
         return gtidSetStr.get();",2016-07-29T05:57:47Z,68
"@@ -5,21 +5,22 @@
  */
 package io.debezium.connector.mysql;
 
+import static org.fest.assertions.Assertions.assertThat;
+
 import java.nio.file.Path;
 import java.sql.SQLException;
 import java.time.LocalDate;
 import java.time.LocalDateTime;
 import java.time.LocalTime;
 import java.time.Month;
 import java.time.ZoneId;
-
+import java.time.ZonedDateTime;
+import java.time.temporal.ChronoField;
+import java.time.temporal.ChronoUnit;
 import org.apache.kafka.connect.data.Struct;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
-
-import static org.fest.assertions.Assertions.assertThat;
-
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.data.Envelope;
@@ -110,12 +111,23 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 assertThat(c2.getTime() % 1000).isEqualTo(780);
                 assertThat(c3.getTime() % 1000).isEqualTo(780);
                 assertThat(c4.getTime() % 1000).isEqualTo(780);
-                assertThat(c1.getTime()).isEqualTo(1410134400000L);
-                assertThat(c2.getTime()).isEqualTo(64264780L);
-                assertThat(c3.getTime()).isEqualTo(1410198664780L);
-                assertThat(c4.getTime()).isEqualTo(1410198664780L);
-                // None of these Dates have timezone information, so to convert to locals we have to use our local timezone ...
                 ZoneId utc = ZoneId.of(""UTC"");
+                ZoneId defaultTZ = ZoneId.systemDefault();
+                LocalDate expectedDate = LocalDate.of(2014, 9, 8);
+                // the time is stored as 17:51:04.777 but rounded up to 780 due to the column configs
+                LocalTime expectedTime = LocalTime.of(17, 51, 4).plus(780, ChronoUnit.MILLIS);
+                // c1 '2014-09-08' is stored as a MySQL DATE (without any time) in the local TZ and then converted to 
+                // a truncated UTC by the connector, so we must assert against the same thing....
+                ZonedDateTime expectedC1UTC = ZonedDateTime.of(expectedDate, LocalTime.of(0, 0), defaultTZ)
+                                                           .withZoneSameInstant(utc)
+                                                           .truncatedTo(ChronoUnit.DAYS);
+                assertThat(c1.getTime()).isEqualTo(expectedC1UTC.toInstant().toEpochMilli());
+                ZonedDateTime expectedC2UTC = ZonedDateTime.of(LocalDate.ofEpochDay(0), expectedTime, utc);                         
+                assertThat(c2.getTime()).isEqualTo(expectedC2UTC.toInstant().toEpochMilli());
+                ZonedDateTime expectedC3UTC = ZonedDateTime.of(expectedDate, expectedTime, utc);
+                assertThat(c3.getTime()).isEqualTo(expectedC3UTC.toInstant().toEpochMilli());
+                assertThat(c4.getTime()).isEqualTo(expectedC3UTC.toInstant().toEpochMilli());
+                // None of these Dates have timezone information, so to convert to locals we have to use our local timezone ...
                 LocalDate localC1 = c1.toInstant().atZone(utc).toLocalDate();
                 LocalTime localC2 = c2.toInstant().atZone(utc).toLocalTime();
                 LocalDateTime localC3 = c3.toInstant().atZone(utc).toLocalDateTime();
@@ -124,7 +136,7 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
                 final int expectedNanos = 780 * 1000 * 1000;
                 assertThat(localC1.getYear()).isEqualTo(2014);
                 assertThat(localC1.getMonth()).isEqualTo(Month.SEPTEMBER);
-                assertThat(localC1.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC1.getDayOfMonth()).isEqualTo(expectedC1UTC.get(ChronoField.DAY_OF_MONTH));
                 assertThat(localC2.getHour()).isEqualTo(17);
                 assertThat(localC2.getMinute()).isEqualTo(51);
                 assertThat(localC2.getSecond()).isEqualTo(4);",2016-07-29T05:57:47Z,48
"@@ -298,21 +298,11 @@ protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordC
      * @throws InterruptedException if the thread was interrupted while waiting for a record to be returned
      */
     protected SourceRecords consumeRecordsByTopic(int numRecords) throws InterruptedException {
-        return consumeRecordsByTopic(numRecords, new SourceRecords());
-    }
-
-    /**
-     * Try to consume and capture exactly the specified number of records from the connector.
-     * 
-     * @param numRecords the number of records that should be consumed
-     * @param records the collector into which all consumed messages should be placed
-     * @return the actual number of records that were consumed
-     * @throws InterruptedException if the thread was interrupted while waiting for a record to be returned
-     */
-    protected SourceRecords consumeRecordsByTopic(int numRecords, SourceRecords records) throws InterruptedException {
+        SourceRecords records = new SourceRecords();
         consumeRecords(numRecords, records::add);
         return records;
     }
+    
 
     protected class SourceRecords {
         private final List<SourceRecord> records = new ArrayList<>();",2016-07-29T05:57:47Z,35
"@@ -11,14 +11,20 @@
 import java.util.Set;
 import java.util.concurrent.Callable;
 
+import org.apache.kafka.connect.data.Date;
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.Time;
+import org.apache.kafka.connect.data.Timestamp;
 import org.apache.kafka.connect.errors.ConnectException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import com.github.shyiko.mysql.binlog.event.deserialization.AbstractRowsEventDataDeserializer;
+
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
 import io.debezium.jdbc.JdbcConnection;
+import io.debezium.jdbc.TimeZoneAdapter;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
 import io.debezium.relational.TableSchema;
@@ -70,6 +76,18 @@ public class MySqlSchema {
 
     /**
      * Create a schema component given the supplied {@link MySqlConnectorConfig MySQL connector configuration}.
+     * <p>
+     * This component sets up a {@link TimeZoneAdapter} that is specific to how the MySQL Binary Log client library
+     * works. The {@link AbstractRowsEventDataDeserializer} class has various methods to instantiate the
+     * {@link java.util.Date}, {@link java.sql.Date}, {@link java.sql.Time}, and {@link java.sql.Timestamp} temporal values,
+     * where the values for {@link java.util.Date}, {@link java.sql.Date}, and {@link java.sql.Time} are all in terms of
+     * the <em>local time zone</em> (since it uses {@link java.util.Calendar#getInstance()}), but where the
+     * {@link java.sql.Timestamp} values are created differently using the milliseconds past epoch and therefore in terms of
+     * the <em>UTC time zone</em>.
+     * <p>
+     * And, because Kafka Connect {@link Time}, {@link Date}, and {@link Timestamp} logical
+     * schema types all expect the {@link java.util.Date} to be in terms of the <em>UTC time zone</em>, the
+     * {@link TimeZoneAdapter} also needs to produce {@link java.util.Date} values that will be correct in UTC.
      * 
      * @param config the connector configuration, which is presumed to be valid
      * @param serverName the name of the server
@@ -80,10 +98,18 @@ public MySqlSchema(Configuration config, String serverName) {
         this.tables = new Tables();
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
-        this.schemaBuilder = new TableSchemaBuilder(schemaNameValidator::validate);
-        if ( serverName != null ) serverName = serverName.trim();
+
+        // Specific to how the MySQL Binary Log client library creates temporal values ...
+        TimeZoneAdapter tzAdapter = TimeZoneAdapter.create()
+                                                   .withLocalZoneForUtilDate()
+                                                   .withLocalZoneForSqlDate()
+                                                   .withLocalZoneForSqlTime()
+                                                   .withUtcZoneForSqlTimestamp()
+                                                   .withUtcTargetZone();
+        this.schemaBuilder = new TableSchemaBuilder(tzAdapter, schemaNameValidator::validate);
+        if (serverName != null) serverName = serverName.trim();
         this.serverName = serverName;
-        if ( this.serverName == null || serverName.isEmpty() ) {
+        if (this.serverName == null || serverName.isEmpty()) {
             this.schemaPrefix = """";
         } else {
             this.schemaPrefix = serverName.endsWith(""."") ? serverName : serverName + ""."";
@@ -97,7 +123,7 @@ public MySqlSchema(Configuration config, String serverName) {
         }
         // Do not remove the prefix from the subset of config properties ...
         Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false);
-        this.dbHistory.configure(dbHistoryConfig,HISTORY_COMPARATOR); // validates
+        this.dbHistory.configure(dbHistoryConfig, HISTORY_COMPARATOR); // validates
     }
 
     /**
@@ -212,8 +238,8 @@ protected void changeTablesAndRecordInHistory(SourceInfo source, Callable<Void>
             changeFunction.call();
         } catch (Exception e) {
             this.tables = copy;
-            if ( e instanceof SQLException) throw (SQLException)e;
-            this.logger.error(""Unexpected error whle changing model of MySQL schemas: {}"",e.getMessage(),e);
+            if (e instanceof SQLException) throw (SQLException) e;
+            this.logger.error(""Unexpected error whle changing model of MySQL schemas: {}"", e.getMessage(), e);
         }
 
         // At least one table has changed or was removed, so first refresh the Kafka Connect schemas ...
@@ -309,12 +335,12 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
                     // to the same _affected_ database...
                     ddlChanges.groupStatementStringsByDatabase((dbName, ddl) -> {
                         if (filters.databaseFilter().test(dbName)) {
-                            if ( dbName == null ) dbName = """";
+                            if (dbName == null) dbName = """";
                             statementConsumer.consume(dbName, ddlStatements);
                         }
                     });
                 } else if (filters.databaseFilter().test(databaseName)) {
-                    if ( databaseName == null ) databaseName = """";
+                    if (databaseName == null) databaseName = """";
                     statementConsumer.consume(databaseName, ddlStatements);
                 }
             }",2016-07-20T22:07:56Z,19
"@@ -220,3 +220,12 @@ CREATE TABLE dbz84_integer_types_table (
 );
 INSERT INTO dbz84_integer_types_table
 VALUES(127,-128,128,255, default,201,202,203, default,301,302,303, default,401,402,403, default,501,502,503);
+
+-- DBZ-85 handle fractional part of seconds
+CREATE TABLE dbz_85_fractest (
+  c1 DATE,
+  c2 TIME(2),
+  c3 DATETIME(2),
+  c4 TIMESTAMP(2)
+);
+INSERT INTO dbz_85_fractest VALUES ('2014-09-08', '17:51:04.777', '2014-09-08 17:51:04.777', '2014-09-08 17:51:04.777');
\ No newline at end of file",2016-07-20T22:07:56Z,79
"@@ -7,7 +7,13 @@
 
 import java.nio.file.Path;
 import java.sql.SQLException;
+import java.time.LocalDate;
+import java.time.LocalDateTime;
+import java.time.LocalTime;
+import java.time.Month;
+import java.time.ZoneId;
 
+import org.apache.kafka.connect.data.Struct;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Test;
@@ -16,6 +22,7 @@
 
 import io.debezium.config.Configuration;
 import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.data.Envelope;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.relational.history.FileDatabaseHistory;
 import io.debezium.util.Testing;
@@ -69,22 +76,75 @@ public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws
         // ---------------------------------------------------------------------------------------------------------------
         // Consume all of the events due to startup and initialization of the database
         // ---------------------------------------------------------------------------------------------------------------
-        // Testing.Debug.enable();
-        SourceRecords records = consumeRecordsByTopic(3 + 2); // 3 schema change record, 2 inserts
+        Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(4 + 3); // 4 schema change record, 3 inserts
         stopConnector();
         assertThat(records).isNotNull();
-        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(3);
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(4);
         assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
         assertThat(records.recordsForTopic(""regression.regression_test.dbz84_integer_types_table"").size()).isEqualTo(1);
-        assertThat(records.topics().size()).isEqualTo(3);
+        assertThat(records.recordsForTopic(""regression.regression_test.dbz_85_fractest"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(4);
         assertThat(records.databaseNames().size()).isEqualTo(1);
-        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(3);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(4);
         assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
 
         // Check that all records are valid, can be serialized and deserialized ...
         records.forEach(this::validate);
+        records.forEach(record->{
+            Struct value = (Struct)record.value();
+            if ( record.topic().endsWith(""dbz_85_fractest"")) {
+                // The microseconds of all three should be exactly 780
+                Struct after = value.getStruct(Envelope.FieldName.AFTER);
+                java.util.Date c1 = (java.util.Date)after.get(""c1"");
+                java.util.Date c2 = (java.util.Date)after.get(""c2"");
+                java.util.Date c3 = (java.util.Date)after.get(""c3"");
+                java.util.Date c4 = (java.util.Date)after.get(""c4"");
+                Testing.debug(""c1 = "" + c1.getTime());
+                Testing.debug(""c2 = "" + c2.getTime());
+                Testing.debug(""c3 = "" + c3.getTime());
+                Testing.debug(""c4 = "" + c4.getTime());
+                assertThat(c1.getTime() % 1000).isEqualTo(0);   // date only, no time
+                assertThat(c2.getTime() % 1000).isEqualTo(780);
+                assertThat(c3.getTime() % 1000).isEqualTo(780);
+                assertThat(c4.getTime() % 1000).isEqualTo(780);
+                assertThat(c1.getTime()).isEqualTo(1410134400000L);
+                assertThat(c2.getTime()).isEqualTo(64264780L);
+                assertThat(c3.getTime()).isEqualTo(1410198664780L);
+                assertThat(c4.getTime()).isEqualTo(1410198664780L);
+                // None of these Dates have timezone information, so to convert to locals we have to use our local timezone ...
+                ZoneId utc = ZoneId.of(""UTC"");
+                LocalDate localC1 = c1.toInstant().atZone(utc).toLocalDate();
+                LocalTime localC2 = c2.toInstant().atZone(utc).toLocalTime();
+                LocalDateTime localC3 = c3.toInstant().atZone(utc).toLocalDateTime();
+                LocalDateTime localC4 = c4.toInstant().atZone(utc).toLocalDateTime();
+                // row is ('2014-09-08', '17:51:04.78', '2014-09-08 17:51:04.78', '2014-09-08 17:51:04.78')
+                final int expectedNanos = 780 * 1000 * 1000;
+                assertThat(localC1.getYear()).isEqualTo(2014);
+                assertThat(localC1.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(localC1.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC2.getHour()).isEqualTo(17);
+                assertThat(localC2.getMinute()).isEqualTo(51);
+                assertThat(localC2.getSecond()).isEqualTo(4);
+                assertThat(localC2.getNano()).isEqualTo(expectedNanos);
+                assertThat(localC3.getYear()).isEqualTo(2014);
+                assertThat(localC3.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(localC3.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC3.getHour()).isEqualTo(17);
+                assertThat(localC3.getMinute()).isEqualTo(51);
+                assertThat(localC3.getSecond()).isEqualTo(4);
+                assertThat(localC3.getNano()).isEqualTo(expectedNanos);
+                assertThat(localC4.getYear()).isEqualTo(2014);
+                assertThat(localC4.getMonth()).isEqualTo(Month.SEPTEMBER);
+                assertThat(localC4.getDayOfMonth()).isEqualTo(8);
+                assertThat(localC4.getHour()).isEqualTo(17);
+                assertThat(localC4.getMinute()).isEqualTo(51);
+                assertThat(localC4.getSecond()).isEqualTo(4);
+                assertThat(localC4.getNano()).isEqualTo(expectedNanos);
+            }
+        });
     }
 
 }",2016-07-20T22:07:56Z,48
"@@ -6,6 +6,9 @@
 package io.debezium.data;
 
 import java.nio.ByteBuffer;
+import java.time.Instant;
+import java.time.format.DateTimeFormatter;
+import java.time.temporal.TemporalAccessor;
 import java.util.Base64;
 import java.util.List;
 import java.util.Map;
@@ -22,7 +25,7 @@
  * @author Randall Hauch
  */
 public class SchemaUtil {
-
+    
     private SchemaUtil() {
     }
 
@@ -228,8 +231,24 @@ public RecordWriter append(Object obj) {
                 }
                 appendAdditional(""value"", record.value());
                 sb.append('}');
+            } else if ( obj instanceof java.sql.Time ){
+                java.sql.Time time = (java.sql.Time)obj;
+                append(DateTimeFormatter.ISO_LOCAL_TIME.format(time.toLocalTime()));
+            } else if ( obj instanceof java.sql.Date ){
+                java.sql.Date date = (java.sql.Date)obj;
+                append(DateTimeFormatter.ISO_DATE.format(date.toLocalDate()));
+            } else if ( obj instanceof java.sql.Timestamp ){
+                java.sql.Timestamp ts = (java.sql.Timestamp)obj;
+                Instant instant = ts.toInstant();
+                append(DateTimeFormatter.ISO_INSTANT.format(instant));
+            } else if ( obj instanceof java.util.Date ){
+                java.util.Date date = (java.util.Date)obj;
+                append(DateTimeFormatter.ISO_INSTANT.format(date.toInstant()));
+            } else if ( obj instanceof TemporalAccessor ){
+                TemporalAccessor temporal = (TemporalAccessor)obj;
+                append(DateTimeFormatter.ISO_INSTANT.format(temporal));
             } else {
-                sb.append(obj.toString());
+                append(obj.toString());
             }
             return this;
         }",2016-07-20T22:07:56Z,127
"@@ -0,0 +1,330 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.jdbc;
+
+import java.time.Instant;
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.ZoneId;
+import java.time.ZonedDateTime;
+import java.time.temporal.ChronoUnit;
+
+import io.debezium.annotation.Immutable;
+
+/**
+ * An adapter that can convert {@link java.util.Date}, {@link java.sql.Date}, {@link java.sql.Time}, and
+ * {@link java.sql.Timestamp} objects to {@link ZonedDateTime} instances, where the time zone in which the temporal objects
+ * were created by the database/driver can be adjusted.
+ * 
+ * @author Randall Hauch
+ */
+@Immutable
+public class TimeZoneAdapter {
+
+    private static final LocalDate EPOCH = LocalDate.ofEpochDay(0);
+    public static final ZoneId UTC = ZoneId.of(""UTC"");
+
+    /**
+     * Create a new adapter with UTC as the target zone and for a database that uses UTC for all temporal values.
+     * 
+     * @return the new adapter
+     */
+    public static TimeZoneAdapter create() {
+        return new TimeZoneAdapter(UTC, UTC, UTC, UTC, UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the specified zone for all temporal values.
+     * 
+     * @param zoneId the zone in which all temporal values are created by the database; may not be null
+     * @return the new adapter
+     */
+    public static TimeZoneAdapter originatingIn(ZoneId zoneId) {
+        return new TimeZoneAdapter(ZoneId.systemDefault(), zoneId, zoneId, zoneId, zoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that creates all temporal values in UTC.
+     * 
+     * @return the new adapter
+     */
+    public static TimeZoneAdapter originatingInUtc() {
+        return originatingIn(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that creates all temporal values in the local system time zone,
+     * which is the same time zone used by {@link java.util.Calendar#getInstance()}.
+     * 
+     * @return the new adapter
+     */
+    public static TimeZoneAdapter originatingInLocal() {
+        return originatingIn(ZoneId.systemDefault()); // same as Calendar.getInstance().getTimeZone().toZoneId()
+    }
+
+    private final ZoneId targetZoneId;
+    private final ZoneId utilDateZoneId;
+    private final ZoneId sqlDateZoneId;
+    private final ZoneId sqlTimeZoneId;
+    private final ZoneId sqlTimestampZoneId;
+
+    /**
+     * Create an adapter for temporal values defined in terms of the given zone.
+     * 
+     * @param targetZoneId the zone in which the output temporal values are defined; may not be null
+     * @param utilDateZoneId the zone in which {@link java.util.Date} values are defined; may not be null
+     * @param sqlDateZoneId the zone in which {@link java.sql.Date} values are defined; may not be null
+     * @param sqlTimeZoneId the zone in which {@link java.sql.Time} values are defined; may not be null
+     * @param sqlTimestampZoneId the zone in which {@link java.sql.Timestamp} values are defined; may not be null
+     */
+    protected TimeZoneAdapter(ZoneId targetZoneId, ZoneId utilDateZoneId, ZoneId sqlDateZoneId, ZoneId sqlTimeZoneId,
+            ZoneId sqlTimestampZoneId) {
+        this.targetZoneId = targetZoneId;
+        this.utilDateZoneId = utilDateZoneId;
+        this.sqlDateZoneId = sqlDateZoneId;
+        this.sqlTimeZoneId = sqlTimeZoneId;
+        this.sqlTimestampZoneId = sqlTimestampZoneId;
+    }
+
+    protected ZoneId targetZoneId() {
+        return targetZoneId;
+    }
+
+    /**
+     * Convert the specified database {@link java.util.Date}, {@link java.sql.Date}, {@link java.sql.Time}, or
+     * {@link java.sql.Timestamp} objects to a date and time in the same time zone in which the database created the
+     * value. If only {@link java.sql.Time time} information is provided in the input value, the date information will
+     * be set to the first day of the epoch. If only {@link java.sql.Date date} information is provided in the input
+     * value, the time information will be at midnight on the specified day.
+     * 
+     * @param dbDate the database-generated value; may not be null
+     * @return the date time in the same zone used by the database; never null
+     */
+    public ZonedDateTime toZonedDateTime(java.util.Date dbDate) {
+        if (dbDate instanceof java.sql.Date) {
+            return toZonedDateTime((java.sql.Date) dbDate);
+        }
+        if (dbDate instanceof java.sql.Time) {
+            return toZonedDateTime((java.sql.Time) dbDate);
+        }
+        if (dbDate instanceof java.sql.Timestamp) {
+            return toZonedDateTime((java.sql.Timestamp) dbDate);
+        }
+        return dbDate.toInstant().atZone(UTC) // milliseconds is in terms of UTC
+                     .withZoneSameInstant(sqlTimeZoneId) // correct value in the zone where it was created
+                     .withZoneSameLocal(targetZoneId); // use same value, but in our desired timezone
+    }
+
+    /**
+     * Convert the specified database {@link java.sql.Date} to a date (at midnight) in the same time zone in which the
+     * database created the value.
+     * 
+     * @param dbDate the database-generated value; may not be null
+     * @return the date (at midnight) in the same zone used by the database; never null
+     */
+    public ZonedDateTime toZonedDateTime(java.sql.Date dbDate) {
+        long millis = dbDate.getTime();
+        Instant instant = Instant.ofEpochMilli(millis).truncatedTo(ChronoUnit.DAYS);
+        return instant.atZone(sqlDateZoneId).withZoneSameInstant(targetZoneId);
+    }
+
+    /**
+     * Convert the specified database {@link java.sql.Time} to a time (on the first epoch day) in the same time zone in which
+     * the database created the value.
+     * 
+     * @param dbTime the database-generated value; may not be null
+     * @return the time (on the first epoch day) in the same zone used by the database; never null
+     */
+    public ZonedDateTime toZonedDateTime(java.sql.Time dbTime) {
+        long millis = dbTime.getTime();
+        LocalTime local = LocalTime.ofNanoOfDay(millis * 1000 * 1000);
+        return ZonedDateTime.of(EPOCH, local, UTC) // milliseconds is in terms of UTC
+                            .withZoneSameInstant(sqlTimeZoneId) // correct value in the zone where it was created
+                            .withZoneSameLocal(targetZoneId); // use same value, but in our desired timezone
+    }
+
+    /**
+     * Convert the specified database {@link java.sql.Timestamp} to a timestamp in the same time zone in which
+     * the database created the value.
+     * 
+     * @param dbTimestamp the database-generated value; may not be null
+     * @return the timestamp in the same zone used by the database; never null
+     */
+    public ZonedDateTime toZonedDateTime(java.sql.Timestamp dbTimestamp) {
+        return dbTimestamp.toInstant().atZone(UTC) // milliseconds is in terms of UTC
+                          .withZoneSameInstant(sqlTimestampZoneId) // correct value in the zone where it was created
+                          .withZoneSameLocal(targetZoneId); // use same value, but in our desired timezone
+    }
+
+    /**
+     * Create a new adapter that produces temporal values in the specified time zone.
+     * 
+     * @param zoneId the zone in which all temporal values are to be defined; may not be null
+     * @return the new adapter
+     */
+    public TimeZoneAdapter withTargetZone(ZoneId zoneId) {
+        if (targetZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(zoneId, utilDateZoneId, sqlDateZoneId, sqlTimeZoneId, sqlTimestampZoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the specified zone for all temporal values and this adapter's target zone.
+     * 
+     * @param zoneId the zone in which all temporal values are created by the database; may not be null
+     * @return the new adapter
+     */
+    public TimeZoneAdapter withZoneForAll(ZoneId zoneId) {
+        return new TimeZoneAdapter(targetZoneId, zoneId, zoneId, zoneId, zoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the specified
+     * zone for {@link java.util.Date} temporal values.
+     * 
+     * @param zoneId the zone in which all {@link java.util.Date} values are created by the database; may not be null
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withZoneForUtilDate(ZoneId zoneId) {
+        if (utilDateZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(targetZoneId, zoneId, sqlDateZoneId, sqlTimeZoneId, sqlTimestampZoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the specified
+     * zone for {@link java.sql.Date} temporal values.
+     * 
+     * @param zoneId the zone in which all {@link java.sql.Date} values are created by the database; may not be null
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withZoneForSqlDate(ZoneId zoneId) {
+        if (sqlDateZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(targetZoneId, utilDateZoneId, zoneId, sqlTimeZoneId, sqlTimestampZoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the specified
+     * zone for {@link java.sql.Time} temporal values.
+     * 
+     * @param zoneId the zone in which all {@link java.sql.Time} values are created by the database; may not be null
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withZoneForSqlTime(ZoneId zoneId) {
+        if (sqlTimeZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(targetZoneId, utilDateZoneId, sqlDateZoneId, zoneId, sqlTimestampZoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the specified
+     * zone for {@link java.sql.Timestamp} temporal values.
+     * 
+     * @param zoneId the zone in which all {@link java.sql.Timestamp} values are created by the database; may not be null
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withZoneForSqlTimestamp(ZoneId zoneId) {
+        if (sqlTimestampZoneId.equals(zoneId)) return this;
+        return new TimeZoneAdapter(targetZoneId, utilDateZoneId, sqlDateZoneId, sqlTimeZoneId, zoneId);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for the target.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcTargetZone() {
+        return withTargetZone(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.util.Date} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcZoneForUtilDate() {
+        return withZoneForUtilDate(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Date} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcZoneForSqlDate() {
+        return withZoneForSqlDate(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Time} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcZoneForSqlTime() {
+        return withZoneForSqlTime(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Timestamp} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withUtcZoneForSqlTimestamp() {
+        return withZoneForSqlTimestamp(UTC);
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for the target.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalTargetZone() {
+        return withTargetZone(ZoneId.systemDefault());
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.util.Date} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalZoneForUtilDate() {
+        return withZoneForUtilDate(ZoneId.systemDefault());
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Date} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalZoneForSqlDate() {
+        return withZoneForSqlDate(ZoneId.systemDefault());
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Time} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalZoneForSqlTime() {
+        return withZoneForSqlTime(ZoneId.systemDefault());
+    }
+
+    /**
+     * Create a new adapter for a database that uses the same time zones as this adapter except it uses the UTC
+     * zone for {@link java.sql.Timestamp} temporal values.
+     * 
+     * @return the new adapter; never null
+     */
+    public TimeZoneAdapter withLocalZoneForSqlTimestamp() {
+        return withZoneForSqlTimestamp(ZoneId.systemDefault());
+    }
+}
\ No newline at end of file",2016-07-20T22:07:56Z,128
"@@ -10,11 +10,11 @@
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
-import java.time.Instant;
 import java.time.LocalDate;
 import java.time.OffsetDateTime;
 import java.time.OffsetTime;
 import java.time.ZoneOffset;
+import java.time.ZonedDateTime;
 import java.time.temporal.ChronoField;
 import java.time.temporal.ChronoUnit;
 import java.util.ArrayList;
@@ -43,6 +43,7 @@
 import io.debezium.data.IsoTimestamp;
 import io.debezium.data.SchemaUtil;
 import io.debezium.jdbc.JdbcConnection;
+import io.debezium.jdbc.TimeZoneAdapter;
 import io.debezium.relational.mapping.ColumnMapper;
 import io.debezium.relational.mapping.ColumnMappers;
 
@@ -77,14 +78,26 @@ public class TableSchemaBuilder {
     private static final LocalDate EPOCH_DAY = LocalDate.ofEpochDay(0);
 
     private final Function<String, String> schemaNameValidator;
+    private final TimeZoneAdapter timeZoneAdapter;
 
     /**
-     * Create a new instance of the builder.
+     * Create a new instance of the builder that uses the {@link TimeZoneAdapter#create() default time zone adapter}.
      * 
      * @param schemaNameValidator the validation function for schema names; may not be null
      */
     public TableSchemaBuilder(Function<String, String> schemaNameValidator) {
+        this(TimeZoneAdapter.create(),schemaNameValidator);
+    }
+
+    /**
+     * Create a new instance of the builder.
+     * 
+     * @param timeZoneAdapter the adapter for temporal objects created by the source database; may not be null
+     * @param schemaNameValidator the validation function for schema names; may not be null
+     */
+    public TableSchemaBuilder(TimeZoneAdapter timeZoneAdapter, Function<String, String> schemaNameValidator) {
         this.schemaNameValidator = schemaNameValidator;
+        this.timeZoneAdapter = timeZoneAdapter;
     }
 
     /**
@@ -612,6 +625,7 @@ protected Object handleUnknownData(Column column, Field fieldDefn, Object data)
     protected Object convertTimestampWithZone(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         OffsetDateTime dateTime = null;
+        LoggerFactory.getLogger(getClass()).info(""TimestampWithZone: "" + data + "" , class="" + data.getClass());
         if (data instanceof OffsetDateTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
             dateTime = (OffsetDateTime) data;
@@ -673,6 +687,7 @@ protected OffsetDateTime unexpectedTimestampWithZone(Object value, Field fieldDe
     protected Object convertTimeWithZone(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         OffsetTime time = null;
+        LoggerFactory.getLogger(getClass()).info(""TimeWithZone: "" + data + "" , class="" + data.getClass());
         if (data instanceof OffsetTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
             time = (OffsetTime) data;
@@ -727,15 +742,10 @@ protected OffsetTime unexpectedTimeWithZone(Object value, Field fieldDefn) {
     protected Object convertTimestamp(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         java.util.Date date = null;
-        if (data instanceof java.sql.Timestamp) {
-            // JDBC specification indicates that this will be the canonical object for this JDBC type.
-            date = (java.util.Date) data;
-        } else if (data instanceof java.sql.Date) {
-            // This should still work, even though it should have just date info
-            date = (java.util.Date) data;
-        } else if (data instanceof java.util.Date) {
-            // Possible that some implementations might use this.
-            date = (java.util.Date) data;
+        LoggerFactory.getLogger(getClass()).info(""Timestamp: "" + data + "" , class="" + data.getClass());
+        if (data instanceof java.util.Date) {
+            ZonedDateTime zdt = timeZoneAdapter.toZonedDateTime((java.util.Date)data);
+            date = java.util.Date.from(zdt.toInstant());
         } else if (data instanceof java.time.LocalDate) {
             // If we get a local date (no TZ info), we need to just convert to a util.Date (no TZ info) ...
             java.time.LocalDate local = (java.time.LocalDate) data;
@@ -780,16 +790,10 @@ protected java.util.Date unexpectedTimestamp(Object value, Field fieldDefn) {
     protected Object convertTime(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         java.util.Date date = null;
-        if (data instanceof java.sql.Time) {
-            // JDBC specification indicates that this will be the canonical object for this JDBC type.
-            // Contains only time info, with the date set to the epoch day ...
-            date = (java.sql.Date) data;
-        } else if (data instanceof java.util.Date) {
-            // Possible that some implementations might use this. We ignore any date info by converting to an
-            // instant and changing the date to the epoch date, and finally creating a new java.util.Date ...
-            date = (java.util.Date) data;
-            Instant instant = Instant.ofEpochMilli(date.getTime()).with(ChronoField.EPOCH_DAY, 0);
-            date = new java.util.Date(instant.toEpochMilli());
+        LoggerFactory.getLogger(getClass()).info(""Time: "" + data + "" , class="" + data.getClass());
+        if (data instanceof java.util.Date) {
+            ZonedDateTime zdt = timeZoneAdapter.toZonedDateTime((java.util.Date)data);
+            date = java.util.Date.from(zdt.toInstant());
         } else if (data instanceof java.time.LocalTime) {
             // If we get a local time (no TZ info), we need to just convert to a util.Date (no TZ info) ...
             java.time.LocalTime local = (java.time.LocalTime) data;
@@ -834,21 +838,10 @@ protected java.util.Date unexpectedTime(Object value, Field fieldDefn) {
     protected Object convertDate(Column column, Field fieldDefn, Object data) {
         if (data == null) return null;
         java.util.Date date = null;
-        if (data instanceof java.sql.Date) {
-            // JDBC specification indicates that this will be the nominal object for this JDBC type.
-            // Contains only date info, with all time values set to all zeros (e.g. midnight).
-            // However, the java.sql.Date object *may* contain timezone information for some DBMS+Driver combinations.
-            // Therefore, first convert it to a local LocalDate, then to a LocalDateTime at midnight, and then to an
-            // instant in UTC ...
-            java.sql.Date sqlDate = (java.sql.Date) data;
-            LocalDate localDate = sqlDate.toLocalDate();
-            date = java.util.Date.from(localDate.atStartOfDay().toInstant(ZoneOffset.UTC));
-        } else if (data instanceof java.util.Date) {
-            // Possible that some implementations might use this. We should be prepared to ignore any time,
-            // information by truncating to days and creating a new java.util.Date ...
-            date = (java.util.Date) data;
-            Instant instant = Instant.ofEpochMilli(date.getTime()).truncatedTo(ChronoUnit.DAYS);
-            date = new java.util.Date(instant.toEpochMilli());
+        LoggerFactory.getLogger(getClass()).info(""Date: "" + data + "" , class="" + data.getClass());
+        if (data instanceof java.util.Date) {
+            ZonedDateTime zdt = timeZoneAdapter.toZonedDateTime((java.util.Date)data);
+            date = java.util.Date.from(zdt.toInstant());
         } else if (data instanceof java.time.LocalDate) {
             // If we get a local date (no TZ info), we need to just convert to a util.Date (no TZ info) ...
             java.time.LocalDate local = (java.time.LocalDate) data;",2016-07-20T22:07:56Z,129
"@@ -0,0 +1,208 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.jdbc;
+
+import java.time.LocalDate;
+import java.time.LocalTime;
+import java.time.Month;
+import java.time.ZonedDateTime;
+import java.util.Calendar;
+
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class TimeZoneAdapterTest {
+
+    private TimeZoneAdapter adapter;
+
+    @Before
+    public void beforeEach() {
+        adapter = TimeZoneAdapter.create()
+                                 .withLocalZoneForUtilDate()
+                                 .withLocalZoneForSqlDate()
+                                 .withLocalZoneForSqlTime()
+                                 .withLocalZoneForSqlTimestamp()
+                                 .withUtcTargetZone();
+    }
+
+    @Test
+    public void shouldAdaptSqlDate() {
+        // '2014-09-08', '17:51:04.777', '2014-09-08 17:51:04.777', '2014-09-08 17:51:04.777'
+        java.sql.Date sqlDate = createSqlDate(2014, Month.SEPTEMBER, 8);
+        ZonedDateTime zdt = adapter.toZonedDateTime(sqlDate);
+        // The date should match ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(2014);
+        assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
+        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        // There should be no time component ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(0);
+        assertThat(time.getMinute()).isEqualTo(0);
+        assertThat(time.getSecond()).isEqualTo(0);
+        assertThat(time.getNano()).isEqualTo(0);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    @Test
+    public void shouldAdaptSqlTime() {
+        // '17:51:04.777'
+        java.sql.Time sqlTime = createSqlTime(17, 51, 04, 777);
+        ZonedDateTime zdt = adapter.toZonedDateTime(sqlTime);
+        // The date should be at epoch ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(1970);
+        assertThat(date.getMonth()).isEqualTo(Month.JANUARY);
+        assertThat(date.getDayOfMonth()).isEqualTo(1);
+        // The time should match exactly ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(17);
+        assertThat(time.getMinute()).isEqualTo(51);
+        assertThat(time.getSecond()).isEqualTo(4);
+        assertThat(time.getNano()).isEqualTo(777 * 1000 * 1000);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    @Test
+    public void shouldAdaptSqlTimestamp() {
+        adapter = TimeZoneAdapter.create()
+                                 .withLocalZoneForSqlTimestamp()
+                                 .withUtcTargetZone();
+
+        // '2014-09-08 17:51:04.777'
+        // This technique creates the timestamp using the milliseconds from epoch in terms of the local zone ...
+        java.sql.Timestamp sqlTimestamp = createSqlTimestamp(2014, Month.SEPTEMBER, 8, 17, 51, 04, 777);
+        ZonedDateTime zdt = adapter.toZonedDateTime(sqlTimestamp);
+        // The date should match ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(2014);
+        assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
+        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        // The time should match exactly ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(17);
+        assertThat(time.getMinute()).isEqualTo(51);
+        assertThat(time.getSecond()).isEqualTo(4);
+        assertThat(time.getNano()).isEqualTo(777 * 1000 * 1000);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    @Test
+    public void shouldAdaptSqlTimestampViaSecondsAndMillis() {
+        adapter = TimeZoneAdapter.create()
+                                 .withUtcZoneForSqlTimestamp()
+                                 .withUtcTargetZone();
+
+        // '2014-09-08 17:51:04.777'
+        // This technique creates the timestamp using the milliseconds from epoch in terms of UTC ...
+        java.sql.Timestamp sqlTimestamp = createSqlTimestamp(1410198664L, 777);
+        ZonedDateTime zdt = adapter.toZonedDateTime(sqlTimestamp);
+        // The date should match ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(2014);
+        assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
+        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        // The time should match exactly ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(17);
+        assertThat(time.getMinute()).isEqualTo(51);
+        assertThat(time.getSecond()).isEqualTo(4);
+        assertThat(time.getNano()).isEqualTo(777 * 1000 * 1000);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    @Test
+    public void shouldAdaptUtilDate() {
+        // '2014-09-08 17:51:04.777'
+        java.util.Date utilDate = createUtilDate(2014, Month.SEPTEMBER, 8, 17, 51, 04, 777);
+        ZonedDateTime zdt = adapter.toZonedDateTime(utilDate);
+        // The date should match ...
+        LocalDate date = zdt.toLocalDate();
+        assertThat(date.getYear()).isEqualTo(2014);
+        assertThat(date.getMonth()).isEqualTo(Month.SEPTEMBER);
+        assertThat(date.getDayOfMonth()).isEqualTo(8);
+        // The time should match exactly ...
+        LocalTime time = zdt.toLocalTime();
+        assertThat(time.getHour()).isEqualTo(17);
+        assertThat(time.getMinute()).isEqualTo(51);
+        assertThat(time.getSecond()).isEqualTo(4);
+        assertThat(time.getNano()).isEqualTo(777 * 1000 * 1000);
+        // The zone should be our target ...
+        assertThat(zdt.getZone()).isEqualTo(adapter.targetZoneId());
+    }
+
+    protected java.sql.Date createSqlDate(int year, Month month, int dayOfMonth) {
+        Calendar cal = Calendar.getInstance();
+        cal.clear();
+        cal.set(Calendar.YEAR, year);
+        cal.set(Calendar.MONTH, month.getValue() - 1);
+        cal.set(Calendar.DATE, dayOfMonth);
+        return new java.sql.Date(cal.getTimeInMillis());
+    }
+
+    protected java.sql.Time createSqlTime(int hourOfDay, int minute, int second, int milliseconds) {
+        Calendar c = Calendar.getInstance();
+        c.clear();
+        c.set(Calendar.HOUR_OF_DAY, hourOfDay);
+        c.set(Calendar.MINUTE, minute);
+        c.set(Calendar.SECOND, second);
+        c.set(Calendar.MILLISECOND, milliseconds);
+        return new java.sql.Time(c.getTimeInMillis());
+    }
+
+    /**
+     * This sets the calendar via the milliseconds past epoch, and this behaves differently than actually setting the various
+     * components of the calendar (see {@link #createSqlTimestamp(int, Month, int, int, int, int, int)}). This is how the
+     * MySQL Binary Log client library creates timestamps (v2).
+     * 
+     * @param secondsFromEpoch the number of seconds since epoch
+     * @param millis the number of milliseconds
+     * @return the SQL timestamp
+     */
+    protected java.sql.Timestamp createSqlTimestamp(long secondsFromEpoch, int millis) {
+        Calendar c = Calendar.getInstance();
+        c.setTimeInMillis(secondsFromEpoch * 1000);
+        c.set(Calendar.MILLISECOND, millis);
+        return new java.sql.Timestamp(c.getTimeInMillis());
+    }
+
+    protected java.sql.Timestamp createSqlTimestamp(int year, Month month, int dayOfMonth, int hourOfDay, int minute, int second,
+                                                    int milliseconds) {
+        Calendar c = Calendar.getInstance();
+        c.set(Calendar.YEAR, year);
+        c.set(Calendar.MONTH, month.getValue() - 1);
+        c.set(Calendar.DAY_OF_MONTH, dayOfMonth);
+        c.set(Calendar.HOUR_OF_DAY, hourOfDay);
+        c.set(Calendar.MINUTE, minute);
+        c.set(Calendar.SECOND, second);
+        c.set(Calendar.MILLISECOND, milliseconds);
+        return new java.sql.Timestamp(c.getTimeInMillis());
+    }
+
+    protected java.util.Date createUtilDate(int year, Month month, int dayOfMonth, int hourOfDay, int minute, int second,
+                                            int milliseconds) {
+        Calendar c = Calendar.getInstance();
+        c.set(Calendar.YEAR, year);
+        c.set(Calendar.MONTH, month.getValue() - 1);
+        c.set(Calendar.DAY_OF_MONTH, dayOfMonth);
+        c.set(Calendar.HOUR_OF_DAY, hourOfDay);
+        c.set(Calendar.MINUTE, minute);
+        c.set(Calendar.SECOND, second);
+        c.set(Calendar.MILLISECOND, milliseconds);
+        return c.getTime();
+    }
+
+}",2016-07-20T22:07:56Z,120
"@@ -64,7 +64,7 @@
         <version.postgresql.server>9.4</version.postgresql.server>
         <version.mysql.server>5.7</version.mysql.server>
         <version.mysql.driver>5.1.39</version.mysql.driver>
-        <version.mysql.binlog>0.3.1</version.mysql.binlog>
+        <version.mysql.binlog>0.3.2</version.mysql.binlog>
         <version.mongo.server>3.2.6</version.mongo.server>
         <version.mongo.driver>3.2.2</version.mongo.driver>
 ",2016-07-20T22:07:56Z,60
"@@ -172,3 +172,24 @@ VALUES (default, '2016-01-16', 1001, 1, 102),
        (default, '2016-02-18', 1004, 3, 109),
        (default, '2016-02-19', 1002, 2, 106),
        (default, '2016-02-21', 1003, 1, 107);
+
+
+
+# ----------------------------------------------------------------------------------------------------------------
+# DATABASE:  regression_test
+# ----------------------------------------------------------------------------------------------------------------
+# The integration test for this database expects to scans all of the binlog events associated with this database
+# without error or problems. The integration test does not modify any records in this database, so this script
+# must contain all operations to these tables.
+#
+CREATE DATABASE regression_test;
+USE regression_test;
+
+# DBZ-61 handle binary value recorded as hex string value
+CREATE TABLE t1464075356413_testtable6 (
+  pk_column int auto_increment NOT NULL,
+  varbinary_col varbinary(20) NOT NULL,
+  PRIMARY KEY(pk_column)
+);
+INSERT INTO t1464075356413_testtable6 (pk_column, varbinary_col)
+VALUES(default, 0x4D7953514C)",2016-06-07T22:53:07Z,79
"@@ -0,0 +1,89 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.nio.file.Path;
+import java.sql.SQLException;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.relational.history.FileDatabaseHistory;
+import io.debezium.util.Testing;
+
+/**
+ * @author Randall Hauch
+ */
+public class MySqlConnectorRegressionIT extends AbstractConnectorTest {
+
+    private static final Path DB_HISTORY_PATH = Testing.Files.createTestingPath(""file-db-history-regression.txt"").toAbsolutePath();
+
+    private Configuration config;
+
+    @Before
+    public void beforeEach() {
+        stopConnector();
+        initializeConnectorTestFramework();
+        Testing.Files.delete(DB_HISTORY_PATH);
+    }
+
+    @After
+    public void afterEach() {
+        try {
+            stopConnector();
+        } finally {
+            Testing.Files.delete(DB_HISTORY_PATH);
+        }
+    }
+
+    @Test
+    public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+        
+        // ---------------------------------------------------------------------------------------------------------------
+        // Consume all of the events due to startup and initialization of the database
+        // ---------------------------------------------------------------------------------------------------------------
+        //Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(2+1);   // 2 schema change record, 1 insert
+        stopConnector();
+        assertThat(records).isNotNull();
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(2);
+        assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(2);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(2);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+    }
+
+}",2016-06-07T22:53:07Z,48
"@@ -5,6 +5,8 @@
  */
 package io.debezium.data;
 
+import java.nio.ByteBuffer;
+import java.util.Base64;
 import java.util.List;
 import java.util.Map;
 
@@ -169,6 +171,11 @@ public RecordWriter append(Object obj) {
                     appendFirst(field.name(), s.get(field));
                 }
                 sb.append('}');
+            } else if (obj instanceof ByteBuffer) {
+                ByteBuffer b = (ByteBuffer) obj;
+                sb.append('""').append(Base64.getEncoder().encode(b.array())).append('""');
+            } else if (obj instanceof byte[]) {
+                sb.append('""').append(Base64.getEncoder().encode((byte[])obj)).append('""');
             } else if (obj instanceof Map<?, ?>) {
                 Map<?, ?> map = (Map<?, ?>) obj;
                 sb.append('{');",2016-06-07T22:53:07Z,127
"@@ -6,6 +6,7 @@
 package io.debezium.relational;
 
 import java.math.BigDecimal;
+import java.nio.ByteBuffer;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
@@ -105,7 +106,8 @@ public TableSchema create(ResultSet resultSet, String name) throws SQLException
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
@@ -121,15 +123,16 @@ public TableSchema create(String schemaPrefix, Table table) {
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @param filter the filter that specifies whether columns in the table should be included; may be null if all columns
      *            are to be included
      * @param mappers the mapping functions for columns; may be null if none of the columns are to be mapped to different values
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
     public TableSchema create(String schemaPrefix, Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
-        if ( schemaPrefix == null ) schemaPrefix = """";
+        if (schemaPrefix == null) schemaPrefix = """";
         // Build the schemas ...
         final TableId tableId = table.id();
         final String tableIdStr = tableId.toString();
@@ -285,7 +288,10 @@ protected ValueConverter[] convertersForColumns(Schema schema, TableId tableId,
                 if (mappers != null) {
                     ValueConverter mappingConverter = mappers.mappingConverterFor(tableId, column);
                     if (mappingConverter != null) {
-                        converter = (value) -> mappingConverter.convert(valueConverter.convert(value));
+                        converter = (value) -> {
+                            if (value != null) value = valueConverter.convert(value);
+                            return mappingConverter.convert(value);
+                        };
                     }
                 }
                 if (converter == null) converter = valueConverter;
@@ -376,7 +382,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
             case Types.NCLOB:
                 fieldBuilder = SchemaBuilder.string();
                 break;
-                
+
             // Variable-length string values
             case Types.VARCHAR:
             case Types.LONGVARCHAR:
@@ -424,7 +430,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
         if (fieldBuilder != null) {
             if (mapper != null) {
                 // Let the mapper add properties to the schema ...
-                mapper.alterFieldSchema(column,fieldBuilder);
+                mapper.alterFieldSchema(column, fieldBuilder);
             }
             if (column.isOptional()) fieldBuilder.optional();
             builder.field(column.name(), fieldBuilder.build());
@@ -450,6 +456,14 @@ protected SchemaBuilder addOtherField(Column column, ColumnMapper mapper) {
     /**
      * Create a {@link ValueConverter} that can be used to convert row values for the given column into the Kafka Connect value
      * object described by the {@link Field field definition}.
+     * <p>
+     * Subclasses can override this method to specialize the behavior. The subclass method should do custom checks and
+     * conversions,
+     * and then delegate to this method implementation to handle all other cases.
+     * <p>
+     * Alternatively, subclasses can leave this method as-is and instead override one of the lower-level type-specific methods
+     * that this method calls (e.g., {@link #convertBinary(Column, Field, Object)},
+     * {@link #convertTinyInt(Column, Field, Object)}, etc.).
      * 
      * @param column the column describing the input values; never null
      * @param fieldDefn the definition for the field in a Kafka Connect {@link Schema} describing the output of the function;
@@ -461,94 +475,38 @@ protected ValueConverter createValueConverterFor(Column column, Field fieldDefn)
             case Types.NULL:
                 return (data) -> null;
             case Types.BIT:
+                return (data) -> convertBit(column, fieldDefn, data);
             case Types.BOOLEAN:
-                return (data) -> {
-                    if (data instanceof Boolean) return (Boolean) data;
-                    if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBoolean(column, fieldDefn, data);
 
             // Binary values ...
             case Types.BLOB:
             case Types.BINARY:
             case Types.VARBINARY:
             case Types.LONGVARBINARY:
-                return (data) -> (byte[]) data;
+                return (data) -> convertBinary(column, fieldDefn, data);
 
             // Numeric integers
             case Types.TINYINT:
-                return (data) -> {
-                    if (data instanceof Byte) return (Byte) data;
-                    if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertTinyInt(column, fieldDefn, data);
             case Types.SMALLINT:
-                return (data) -> {
-                    if (data instanceof Short) return (Short) data;
-                    if (data instanceof Integer) return new Short(((Integer) data).shortValue());
-                    if (data instanceof Long) return new Short(((Long) data).shortValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertSmallInt(column, fieldDefn, data);
             case Types.INTEGER:
-                return (data) -> {
-                    if (data instanceof Integer) return (Integer) data;
-                    if (data instanceof Short) return new Integer(((Short) data).intValue());
-                    if (data instanceof Long) return new Integer(((Long) data).intValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertInteger(column, fieldDefn, data);
             case Types.BIGINT:
-                return (data) -> {
-                    if (data instanceof Long) return (Long) data;
-                    if (data instanceof Integer) return new Long(((Integer) data).longValue());
-                    if (data instanceof Short) return new Long(((Short) data).longValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBigInt(column, fieldDefn, data);
 
             // Numeric decimal numbers
             case Types.FLOAT:
+                return (data) -> convertFloat(column, fieldDefn, data);
             case Types.DOUBLE:
-                return (data) -> {
-                    if (data instanceof Double) return (Double) data;
-                    if (data instanceof Float) return new Double(((Float) data).doubleValue());
-                    if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
-                    if (data instanceof Long) return new Double(((Long) data).doubleValue());
-                    if (data instanceof Short) return new Double(((Short) data).doubleValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertDouble(column, fieldDefn, data);
             case Types.REAL:
-                return (data) -> {
-                    if (data instanceof Float) return (Float) data;
-                    if (data instanceof Double) return new Float(((Double) data).floatValue());
-                    if (data instanceof Integer) return new Float(((Integer) data).floatValue());
-                    if (data instanceof Long) return new Float(((Long) data).floatValue());
-                    if (data instanceof Short) return new Float(((Short) data).floatValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertReal(column, fieldDefn, data);
             case Types.NUMERIC:
+                return (data) -> convertNumeric(column, fieldDefn, data);
             case Types.DECIMAL:
-                return (data) -> {
-                    BigDecimal decimal = null;
-                    if (data instanceof BigDecimal)
-                        decimal = (BigDecimal) data;
-                    else if (data instanceof Boolean)
-                        decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
-                    else if (data instanceof Short)
-                        decimal = new BigDecimal(((Short) data).intValue());
-                    else if (data instanceof Integer)
-                        decimal = new BigDecimal(((Integer) data).intValue());
-                    else if (data instanceof Long)
-                        decimal = BigDecimal.valueOf(((Long) data).longValue());
-                    else if (data instanceof Float)
-                        decimal = BigDecimal.valueOf(((Float) data).doubleValue());
-                    else if (data instanceof Double)
-                        decimal = BigDecimal.valueOf(((Double) data).doubleValue());
-                    else {
-                        handleUnknownData(column, fieldDefn, data);
-                    }
-                    return decimal;
-                };
+                return (data) -> convertDecimal(column, fieldDefn, data);
 
             // String values
             case Types.CHAR: // variable-length
@@ -561,26 +519,23 @@ else if (data instanceof Double)
             case Types.NCLOB: // fixed-length
             case Types.DATALINK:
             case Types.SQLXML:
-                return (data) -> data.toString();
+                return (data) -> convertString(column, fieldDefn, data);
 
             // Date and time values
             case Types.DATE:
-                return (data) -> convertDate(fieldDefn, data);
+                return (data) -> convertDate(column, fieldDefn, data);
             case Types.TIME:
-                return (data) -> convertTime(fieldDefn, data);
+                return (data) -> convertTime(column, fieldDefn, data);
             case Types.TIMESTAMP:
-                return (data) -> convertTimestamp(fieldDefn, data);
+                return (data) -> convertTimestamp(column, fieldDefn, data);
             case Types.TIME_WITH_TIMEZONE:
-                return (data) -> convertTimeWithZone(fieldDefn, data);
+                return (data) -> convertTimeWithZone(column, fieldDefn, data);
             case Types.TIMESTAMP_WITH_TIMEZONE:
-                return (data) -> convertTimestampWithZone(fieldDefn, data);
+                return (data) -> convertTimestampWithZone(column, fieldDefn, data);
 
             // Other types ...
             case Types.ROWID:
-                return (data) -> {
-                    java.sql.RowId rowId = (java.sql.RowId) data;
-                    return rowId.getBytes();
-                };
+                return (data) -> convertRowId(column, fieldDefn, data);
 
             // Unhandled types
             case Types.ARRAY:
@@ -620,11 +575,13 @@ protected Object handleUnknownData(Column column, Field fieldDefn, Object data)
      * This method handles several types of objects, including {@link OffsetDateTime}, {@link java.sql.Timestamp},
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestampWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimestampWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetDateTime dateTime = null;
         if (data instanceof OffsetDateTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -679,11 +636,13 @@ protected OffsetDateTime unexpectedTimestampWithZone(Object value, Field fieldDe
      * {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types have date components, those date
      * components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimeWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimeWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetTime time = null;
         if (data instanceof OffsetTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -731,11 +690,13 @@ protected OffsetTime unexpectedTimeWithZone(Object value, Field fieldDefn) {
      * but no time zone info. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such
      * as {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestamp(Field fieldDefn, Object data) {
+    protected Object convertTimestamp(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Timestamp) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -782,11 +743,13 @@ protected java.util.Date unexpectedTimestamp(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types might
      * have date components, those date components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTime(Field fieldDefn, Object data) {
+    protected Object convertTime(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Time) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -834,11 +797,13 @@ protected java.util.Date unexpectedTime(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
      * have time components, those time components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertDate(Field fieldDefn, Object data) {
+    protected Object convertDate(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Date) {
             // JDBC specification indicates that this will be the nominal object for this JDBC type.
@@ -882,4 +847,286 @@ protected java.util.Date unexpectedDate(Object value, Field fieldDefn) {
                     fieldDefn.schema(), value.getClass(), value);
         return null;
     }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have time components, those time components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBinary(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof char[]) {
+            data = new String((char[]) data); // convert to string
+        }
+        if (data instanceof String) {
+            // This was encoded as a hexadecimal string, but we receive it as a normal string ...
+            data = ((String) data).getBytes();
+        }
+        if (data instanceof byte[]) {
+            return ByteBuffer.wrap((byte[])data);
+        }
+        // An unexpected value
+        return unexpectedBinary(data, fieldDefn);
+    }
+
+    /**
+     * Handle the unexpected value from a row with a column type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * 
+     * @param value the binary value for which no conversion was found; never null
+     * @param fieldDefn the field definition in the Kafka Connect schema; never null
+     * @return the converted value, or null
+     * @see #convertBinary(Column, Field, Object)
+     */
+    protected byte[] unexpectedBinary(Object value, Field fieldDefn) {
+        LOGGER.warn(""Unexpected JDBC BINARY value for field {} with schema {}: class={}, value={}"", fieldDefn.name(),
+                    fieldDefn.schema(), value.getClass(), value);
+        return null;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#TINYINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTinyInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Byte) return data;
+        if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#SMALLINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertSmallInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Short) return data;
+        if (data instanceof Integer) return new Short(((Integer) data).shortValue());
+        if (data instanceof Long) return new Short(((Long) data).shortValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertInteger(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Integer) return data;
+        if (data instanceof Short) return new Integer(((Short) data).intValue());
+        if (data instanceof Long) return new Integer(((Long) data).intValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBigInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Long) return data;
+        if (data instanceof Integer) return new Long(((Integer) data).longValue());
+        if (data instanceof Short) return new Long(((Short) data).longValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#FLOAT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertFloat(Column column, Field fieldDefn, Object data) {
+        return convertDouble(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DOUBLE}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDouble(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Double) return data;
+        if (data instanceof Float) return new Double(((Float) data).doubleValue());
+        if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
+        if (data instanceof Long) return new Double(((Long) data).doubleValue());
+        if (data instanceof Short) return new Double(((Short) data).doubleValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#REAL}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertReal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Float) return data;
+        if (data instanceof Double) return new Float(((Double) data).floatValue());
+        if (data instanceof Integer) return new Float(((Integer) data).floatValue());
+        if (data instanceof Long) return new Float(((Long) data).floatValue());
+        if (data instanceof Short) return new Float(((Short) data).floatValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertNumeric(Column column, Field fieldDefn, Object data) {
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDecimal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#CHAR}, {@link Types#VARCHAR},
+     * {@link Types#LONGVARCHAR}, {@link Types#CLOB}, {@link Types#NCHAR}, {@link Types#NVARCHAR}, {@link Types#LONGNVARCHAR},
+     * {@link Types#NCLOB}, {@link Types#DATALINK}, and {@link Types#SQLXML}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertString(Column column, Field fieldDefn, Object data) {
+        return data == null ? null : data.toString();
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#ROWID}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertRowId(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof java.sql.RowId) {
+            java.sql.RowId row = (java.sql.RowId)data;
+            return ByteBuffer.wrap(row.getBytes());
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BIT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBit(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BOOLEAN}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
 }",2016-06-07T22:53:07Z,129
"@@ -7,8 +7,13 @@
 
 import static org.junit.Assert.fail;
 
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.HashMap;
+import java.util.Iterator;
 import java.util.Map;
+import java.util.Objects;
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
@@ -47,18 +52,18 @@ public class VerifyRecord {
     private static final MockSchemaRegistryClient schemaRegistry = new MockSchemaRegistryClient();
     private static final AvroConverter avroKeyConverter = new AvroConverter(schemaRegistry);
     private static final AvroConverter avroValueConverter = new AvroConverter(schemaRegistry);
-    
+
     static {
-        Map<String,Object> config = new HashMap<>();
-        config.put(""schemas.enable"",Boolean.TRUE.toString());
-        config.put(""schemas.cache.size"",100);
+        Map<String, Object> config = new HashMap<>();
+        config.put(""schemas.enable"", Boolean.TRUE.toString());
+        config.put(""schemas.cache.size"", 100);
         keyJsonConverter.configure(config, true);
         keyJsonDeserializer.configure(config, true);
         valueJsonConverter.configure(config, false);
         valueJsonDeserializer.configure(config, false);
 
         config = new HashMap<>();
-        config.put(""schema.registry.url"",""http://fake-url"");
+        config.put(""schema.registry.url"", ""http://fake-url"");
         avroKeyConverter.configure(config, false);
         avroValueConverter.configure(config, false);
     }
@@ -276,73 +281,97 @@ public static void isValid(SourceRecord record) {
         SchemaAndValue valueWithSchema = null;
         SchemaAndValue avroKeyWithSchema = null;
         SchemaAndValue avroValueWithSchema = null;
+        String msg = null;
         try {
             // The key should never be null ...
+            msg = ""checking key is not null"";
             assertThat(record.key()).isNotNull();
             assertThat(record.keySchema()).isNotNull();
 
             // If the value is not null there must be a schema; otherwise, the schema should also be null ...
             if (record.value() == null) {
+                msg = ""checking value schema is null"";
                 assertThat(record.valueSchema()).isNull();
             } else {
+                msg = ""checking value schema is not null"";
                 assertThat(record.valueSchema()).isNotNull();
             }
 
             // First serialize and deserialize the key ...
+            msg = ""serializing key using JSON converter"";
             byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using JSON deserializer"";
             keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            msg = ""deserializing key using JSON converter"";
             keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            msg = ""comparing key schema to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // then the value ...
+            msg = ""serializing value using JSON converter"";
             byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using JSON deserializer"";
             valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            msg = ""deserializing value using JSON converter"";
             valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
             // Serialize and deserialize the key using the Avro converter, and check that we got the same result ...
+            msg = ""serializing key using Avro converter"";
             byte[] avroKeyBytes = avroValueConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using Avro converter"";
             avroKeyWithSchema = avroValueConverter.toConnectData(record.topic(), avroKeyBytes);
-            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
-            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key schema to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.schema(),record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.value(),record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // Serialize and deserialize the value using the Avro converter, and check that we got the same result ...
+            msg = ""serializing value using Avro converter"";
             byte[] avroValueBytes = avroValueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using Avro converter"";
             avroValueWithSchema = avroValueConverter.toConnectData(record.topic(), avroValueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
         } catch (Throwable t) {
             Testing.Print.enable();
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
             Testing.printError(t);
-            if (keyJson == null ){
-                Testing.print(""error deserializing key from JSON: "" + SchemaUtil.asString(record.key()));
-            } else if (keyWithSchema == null ){
-                Testing.print(""error using JSON converter on key: "" + prettyJson(keyJson));
-            } else if (avroKeyWithSchema == null ){
-                Testing.print(""error using Avro converter on key: "" + prettyJson(keyJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            Testing.print(""error "" + msg);
+            Testing.print(""  key: "" + SchemaUtil.asString(record.key()));
+            Testing.print(""  key deserialized from JSON: "" + prettyJson(keyJson));
+            if (keyWithSchema != null) {
+                Testing.print(""  key to/from JSON: "" + SchemaUtil.asString(keyWithSchema.value()));
             }
-
-            if (valueJson == null ){
-                Testing.print(""error deserializing value from JSON: "" + SchemaUtil.asString(record.value()));
-            } else if (valueWithSchema == null ){
-                Testing.print(""error using JSON converter on value: "" + prettyJson(valueJson));
-            } else if (avroValueWithSchema == null ){
-                Testing.print(""error using Avro converter on value: "" + prettyJson(valueJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            if (avroKeyWithSchema != null) {
+                Testing.print(""  key to/from Avro: "" + SchemaUtil.asString(avroKeyWithSchema.value()));
+            }
+            Testing.print(""  value: "" + SchemaUtil.asString(record.value()));
+            Testing.print(""  value deserialized from JSON: "" + prettyJson(valueJson));
+            if (valueWithSchema != null) {
+                Testing.print(""  value to/from JSON: "" + SchemaUtil.asString(valueWithSchema.value()));
+            }
+            if (avroValueWithSchema != null) {
+                Testing.print(""  value to/from Avro: "" + SchemaUtil.asString(avroValueWithSchema.value()));
             }
             if (t instanceof AssertionError) throw t;
-            fail(t.getMessage());
+            fail(""error "" + msg + "": "" + t.getMessage());
         }
     }
 
@@ -390,5 +419,135 @@ protected static String prettyJson(JsonNode json) {
             return null;
         }
     }
+    
+    // The remaining methods are needed simply because of the KAFKA-3803, so our comparisons cannot rely upon Struct.equals
+    
+    protected static void assertEquals( Object o1, Object o2 ) {
+        // assertThat(o1).isEqualTo(o2);
+        if ( !equals(o1,o2) ) {
+            fail(SchemaUtil.asString(o1) + "" was not equal to "" + SchemaUtil.asString(o2));
+        }
+    }
+    
+    @SuppressWarnings(""unchecked"")
+    protected static boolean equals( Object o1, Object o2 ) {
+        if ( o1 == o2 ) return true;
+        if (o1 == null) return o2 == null ? true : false;
+        if (o2 == null ) return false;
+        if ( o1 instanceof ByteBuffer ) {
+            o1 = ((ByteBuffer)o1).array();
+        }
+        if ( o2 instanceof ByteBuffer ) {
+            o2 = ((ByteBuffer)o2).array();
+        }
+        if ( o1 instanceof byte[] && o2 instanceof byte[] ) {
+            boolean result = Arrays.equals((byte[])o1,(byte[])o2);
+            return result;
+        }
+        if ( o1 instanceof Object[] && o2 instanceof Object[] ) {
+            boolean result = deepEquals((Object[])o1,(Object[])o2);
+            return result;
+        }
+        if ( o1 instanceof Map && o2 instanceof Map ) {
+            Map<String,Object> m1 = (Map<String,Object>)o1;
+            Map<String,Object> m2 = (Map<String,Object>)o2;
+            if ( !m1.keySet().equals(m2.keySet())) return false;
+            for ( Map.Entry<String, Object> entry : m1.entrySet()) {
+                Object v1 = entry.getValue();
+                Object v2 = m2.get(entry.getKey());
+                if ( !equals(v1,v2) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Collection && o2 instanceof Collection ) {
+            Collection<Object> m1 = (Collection<Object>)o1;
+            Collection<Object> m2 = (Collection<Object>)o2;
+            if ( m1.size() != m2.size() ) return false;
+            Iterator<?> iter1 = m1.iterator();
+            Iterator<?> iter2 = m2.iterator();
+            while ( iter1.hasNext() && iter2.hasNext() ) {
+                if ( !equals(iter1.next(),iter2.next()) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Struct && o2 instanceof Struct ) {
+            // Unfortunately, the Struct.equals() method has a bug in that it is not using Arrays.deepEquals(...) to
+            // compare values in two Struct objects. The result is that the equals only works if the values of the
+            // first level Struct are non arrays; otherwise, the array values are compared using == and that obviously
+            // does not work for non-primitive values.
+            Struct struct1 = (Struct) o1;
+            Struct struct2 = (Struct) o2;
+            if (! Objects.equals(struct1.schema(),struct2.schema()) ) {
+                return false;
+            }
+            Object[] array1 = valuesFor(struct1);
+            Object[] array2 = valuesFor(struct2);
+            boolean result = deepEquals(array1, array2);
+            return result;
+        }
+        return Objects.equals(o1, o2);
+    }
+    
+    private static Object[] valuesFor( Struct struct ) {
+        Object[] array = new Object[struct.schema().fields().size()];
+        int index = 0;
+        for ( Field field : struct.schema().fields() ) {
+            array[index] = struct.get(field);
+            ++index;
+        }
+        return array;
+    }
 
+    private static boolean deepEquals(Object[] a1, Object[] a2) {
+        if (a1 == a2)
+            return true;
+        if (a1 == null || a2==null)
+            return false;
+        int length = a1.length;
+        if (a2.length != length)
+            return false;
+
+        for (int i = 0; i < length; i++) {
+            Object e1 = a1[i];
+            Object e2 = a2[i];
+
+            if (e1 == e2)
+                continue;
+            if (e1 == null)
+                return false;
+
+            // Figure out whether the two elements are equal
+            boolean eq = deepEquals0(e1, e2);
+
+            if (!eq)
+                return false;
+        }
+        return true;
+    }
+
+    private static boolean deepEquals0(Object e1, Object e2) {
+        assert e1 != null;
+        boolean eq;
+        if (e1 instanceof Object[] && e2 instanceof Object[])
+            eq = deepEquals ((Object[]) e1, (Object[]) e2);
+        else if (e1 instanceof byte[] && e2 instanceof byte[])
+            eq = Arrays.equals((byte[]) e1, (byte[]) e2);
+        else if (e1 instanceof short[] && e2 instanceof short[])
+            eq = Arrays.equals((short[]) e1, (short[]) e2);
+        else if (e1 instanceof int[] && e2 instanceof int[])
+            eq = Arrays.equals((int[]) e1, (int[]) e2);
+        else if (e1 instanceof long[] && e2 instanceof long[])
+            eq = Arrays.equals((long[]) e1, (long[]) e2);
+        else if (e1 instanceof char[] && e2 instanceof char[])
+            eq = Arrays.equals((char[]) e1, (char[]) e2);
+        else if (e1 instanceof float[] && e2 instanceof float[])
+            eq = Arrays.equals((float[]) e1, (float[]) e2);
+        else if (e1 instanceof double[] && e2 instanceof double[])
+            eq = Arrays.equals((double[]) e1, (double[]) e2);
+        else if (e1 instanceof boolean[] && e2 instanceof boolean[])
+            eq = Arrays.equals((boolean[]) e1, (boolean[]) e2);
+        else
+            eq = equals(e1,e2);
+        return eq;
+    }
 }",2016-06-07T22:53:07Z,77
"@@ -63,6 +63,7 @@ public class MySqlSchema {
     private final TableSchemaBuilder schemaBuilder;
     private final DdlChanges ddlChanges;
     private final String serverName;
+    private final String schemaPrefix;
     private Tables tables;
 
     /**
@@ -78,7 +79,13 @@ public MySqlSchema(Configuration config, String serverName) {
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
         this.schemaBuilder = new TableSchemaBuilder();
+        if ( serverName != null ) serverName = serverName.trim();
         this.serverName = serverName;
+        if ( this.serverName == null || serverName.isEmpty() ) {
+            this.schemaPrefix = """";
+        } else {
+            this.schemaPrefix = serverName.endsWith(""."") ? serverName : serverName + ""."";
+        }
 
         // Create and configure the database history ...
         this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
@@ -253,7 +260,7 @@ protected void refreshSchemas() {
         // Create TableSchema instances for any existing table ...
         this.tables.tableIds().forEach(id -> {
             Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(serverName,table, filters.columnFilter(), filters.columnMappers());
+            TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
             tableSchemaByTableId.put(id, schema);
         });
     }
@@ -323,7 +330,7 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
             if (table == null) { // removed
                 tableSchemaByTableId.remove(tableId);
             } else {
-                TableSchema schema = schemaBuilder.create(serverName, table, filters.columnFilter(), filters.columnMappers());
+                TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
                 tableSchemaByTableId.put(tableId, schema);
             }
         });",2016-06-10T02:08:09Z,19
"@@ -30,6 +30,10 @@ public Configurator with(Field field, boolean value) {
         return this;
     }
 
+    public Configurator serverName(String serverName) {
+        return with(MySqlConnectorConfig.SERVER_NAME, serverName);
+    }
+
     public Configurator includeDatabases(String regexList) {
         return with(MySqlConnectorConfig.DATABASE_WHITELIST, regexList);
     }",2016-06-10T02:08:09Z,110
"@@ -29,6 +29,7 @@
 public class MySqlSchemaTest {
 
     private static final Path TEST_FILE_PATH = Testing.Files.createTestingPath(""dbHistory.log"");
+    private static final String SERVER_NAME = ""test-server"";
 
     private Configurator build;
     private MySqlSchema mysql;
@@ -55,7 +56,7 @@ public void afterEach() {
 
     @Test
     public void shouldApplyDdlStatementsAndRecover() {
-        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).serverName(SERVER_NAME).createSchemas();
         mysql.start();
 
         // Testing.Print.enable();
@@ -74,6 +75,7 @@ public void shouldApplyDdlStatementsAndRecover() {
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .excludeBuiltInTables()
                      .createSchemas();
@@ -99,6 +101,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .includeBuiltInTables()
                      .createSchemas();
@@ -124,7 +127,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
     protected void assertTableIncluded(String fullyQualifiedTableName) {
         TableId tableId = TableId.parse(fullyQualifiedTableName);
         assertThat(mysql.tables().forTable(tableId)).isNotNull();
-        assertThat(mysql.schemaFor(tableId)).isNotNull();
+        TableSchema tableSchema = mysql.schemaFor(tableId);
+        assertThat(tableSchema).isNotNull();
+        assertThat(tableSchema.keySchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Key"");
+        assertThat(tableSchema.valueSchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Value"");
     }
 
     protected void assertTableExcluded(String fullyQualifiedTableName) {",2016-06-10T02:08:09Z,19
"@@ -52,6 +52,7 @@ public class BinlogReader extends AbstractReader {
     private final SourceInfo source;
     private final EnumMap<EventType, BlockingConsumer<Event>> eventHandlers = new EnumMap<>(EventType.class);
     private BinaryLogClient client;
+    private int startingRowNumber = 0;
 
     /**
      * Create a binlog reader.
@@ -93,12 +94,15 @@ protected void doStart() {
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
 
-        // And set the client to start from that point ...
+        // The 'source' object holds the starting point in the binlog where we should start reading,
+        // set set the client to start from that point ...
         client.setGtidSet(source.gtidSet()); // may be null
         client.setBinlogFilename(source.binlogFilename());
-        client.setBinlogPosition(source.binlogPosition());
-        // The event row number will be used when processing the first event ...
+        client.setBinlogPosition(source.nextBinlogPosition());
 
+        // Set the starting row number, which is the next row number to be read ...
+        startingRowNumber = source.nextEventRowNumber();
+        
         // Start the log reader, which starts background threads ...
         long timeoutInMilliseconds = context.timeoutInMilliseconds();
         try {
@@ -157,21 +161,19 @@ protected void handleEvent(Event event) {
             } else {
                 rotateEventData = (RotateEventData) eventData;
             }
-            source.setBinlogFilename(rotateEventData.getBinlogFilename());
-            source.setBinlogPosition(rotateEventData.getBinlogPosition());
-            source.setRowInEvent(0);
+            source.setBinlogStartPoint(rotateEventData.getBinlogFilename(), rotateEventData.getBinlogPosition());
         } else if (eventHeader instanceof EventHeaderV4) {
             EventHeaderV4 trackableEventHeader = (EventHeaderV4) eventHeader;
-            long nextBinlogPosition = trackableEventHeader.getNextPosition();
-            if (nextBinlogPosition > 0) {
-                source.setBinlogPosition(nextBinlogPosition);
-                source.setRowInEvent(0);
-            }
+            source.setEventPosition(trackableEventHeader.getPosition(), trackableEventHeader.getEventLength());
         }
 
         // If there is a handler for this event, forward the event to it ...
         try {
+            // Forward the event to the handler ...
             eventHandlers.getOrDefault(eventType, this::ignoreEvent).accept(event);
+            
+            // And after that event has been processed, always set the starting row number to 0 ...
+            startingRowNumber = 0;
         } catch (InterruptedException e) {
             // Most likely because this reader was stopped and our thread was interrupted ...
             Thread.interrupted();
@@ -301,7 +303,11 @@ protected void handleInsert(Event event) throws InterruptedException {
         if (recordMaker != null) {
             List<Serializable[]> rows = write.getRows();
             Long ts = context.clock().currentTimeInMillis();
-            int count = recordMaker.createEach(rows, ts);
+            int count = 0;
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
+                count += recordMaker.create(rows.get(row), ts, row, numRows);
+            }
             logger.debug(""Recorded {} insert records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping insert row event: {}"", event);
@@ -324,11 +330,12 @@ protected void handleUpdate(Event event) throws InterruptedException {
             List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
-            for (int row = 0; row != rows.size(); ++row) {
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
                 Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
                 Serializable[] before = changes.getKey();
                 Serializable[] after = changes.getValue();
-                count += recordMaker.update(before, after, ts, row);
+                count += recordMaker.update(before, after, ts, row, numRows);
             }
             logger.debug(""Recorded {} update records for event: {}"", count, event);
         } else {
@@ -350,7 +357,11 @@ protected void handleDelete(Event event) throws InterruptedException {
         if (recordMaker != null) {
             List<Serializable[]> rows = deleted.getRows();
             Long ts = context.clock().currentTimeInMillis();
-            int count = recordMaker.deleteEach(rows, ts);
+            int count = 0;
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
+                count += recordMaker.delete(rows.get(row), ts, row, numRows);
+            }
             logger.debug(""Recorded {} delete records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping delete row event: {}"", event);",2016-06-14T22:43:58Z,67
"@@ -107,7 +107,7 @@ public void start(Map<String, String> props) {
             if (taskContext.isSnapshotNeverAllowed()) {
                 // We're not allowed to take a snapshot, so instead we have to assume that the binlog contains the
                 // full history of the database.
-                source.setBinlogFilename("""");// start from the beginning of the binlog
+                source.setBinlogStartPoint("""", 0L);// start from the beginning of the binlog
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;",2016-06-14T22:43:58Z,68
"@@ -7,7 +7,6 @@
 
 import java.util.BitSet;
 import java.util.HashMap;
-import java.util.Iterator;
 import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
@@ -152,7 +151,8 @@ public void regenerate() {
      */
     public boolean assign(long tableNumber, TableId id) {
         Long existingTableNumber = tableNumbersByTableId.get(id);
-        if ( existingTableNumber != null && existingTableNumber.longValue() == tableNumber && convertersByTableNumber.containsKey(tableNumber)) {
+        if (existingTableNumber != null && existingTableNumber.longValue() == tableNumber
+                && convertersByTableNumber.containsKey(tableNumber)) {
             // This is the exact same table number for the same table, so do nothing ...
             return true;
         }
@@ -171,15 +171,15 @@ public boolean assign(long tableNumber, TableId id) {
         Converter converter = new Converter() {
 
             @Override
-            public int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int read(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                             BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 Object key = tableSchema.keyFromColumnData(row);
                 Struct value = tableSchema.valueFromColumnData(row);
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
                             keySchema, key, envelope.schema(), envelope.read(value, origin, ts));
@@ -190,15 +190,15 @@ public int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedC
             }
 
             @Override
-            public int insert(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int insert(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 Object key = tableSchema.keyFromColumnData(row);
                 Struct value = tableSchema.valueFromColumnData(row);
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
                             keySchema, key, envelope.schema(), envelope.create(value, origin, ts));
@@ -209,7 +209,7 @@ public int insert(SourceInfo source, Object[] row, int rowNumber, BitSet include
             }
 
             @Override
-            public int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, BitSet includedColumns, long ts,
+            public int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 int count = 0;
@@ -220,7 +220,7 @@ public int update(SourceInfo source, Object[] before, Object[] after, int rowNum
                     Struct valueBefore = tableSchema.valueFromColumnData(before);
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     if (key != null && !Objects.equals(key, oldKey)) {
                         // The key has indeed changed, so first send a create event ...
@@ -251,7 +251,7 @@ record = new SourceRecord(partition, offset, topicName, partitionNum, keySchema,
             }
 
             @Override
-            public int delete(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int delete(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 int count = 0;
@@ -260,7 +260,7 @@ public int delete(SourceInfo source, Object[] row, int rowNumber, BitSet include
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     // Send a delete message ...
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
@@ -275,7 +275,7 @@ record = new SourceRecord(partition, offset, topicName, partitionNum,
                 }
                 return count;
             }
-            
+
             @Override
             public String toString() {
                 return ""RecordMaker.Converter("" + id + "")"";
@@ -307,17 +307,20 @@ protected Struct schemaChangeRecordValue(String databaseName, String ddlStatemen
     }
 
     protected static interface Converter {
-        int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int read(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                 BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int insert(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int insert(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                   BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, BitSet includedColumns, long ts,
+        int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                    BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int delete(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int delete(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                   BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
     }
@@ -346,7 +349,7 @@ protected RecordsForTable(Converter converter, BitSet includedColumns, BlockingC
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int read(Object[] row, long ts) throws InterruptedException {
-            return read(row, ts, 0);
+            return read(row, ts, 0, 1);
         }
 
         /**
@@ -356,29 +359,12 @@ public int read(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int read(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.read(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#READ read} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int readEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += read(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int read(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.read(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -391,7 +377,7 @@ public int readEach(Iterable<? extends Object[]> rows, long ts) throws Interrupt
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int create(Object[] row, long ts) throws InterruptedException {
-            return create(row, ts, 0);
+            return create(row, ts, 0, 1);
         }
 
         /**
@@ -401,29 +387,12 @@ public int create(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int create(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.insert(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#CREATE create} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int createEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += create(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int create(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.insert(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -438,7 +407,7 @@ public int createEach(Iterable<? extends Object[]> rows, long ts) throws Interru
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int update(Object[] before, Object[] after, long ts) throws InterruptedException {
-            return update(before, after, ts, 0);
+            return update(before, after, ts, 0, 1);
         }
 
         /**
@@ -450,11 +419,12 @@ public int update(Object[] before, Object[] after, long ts) throws InterruptedEx
          *            definition in the {@link MySqlSchema}
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int update(Object[] before, Object[] after, long ts, int rowNumber) throws InterruptedException {
-            return converter.update(source, before, after, rowNumber, includedColumns, ts, consumer);
+        public int update(Object[] before, Object[] after, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.update(source, before, after, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -467,7 +437,7 @@ public int update(Object[] before, Object[] after, long ts, int rowNumber) throw
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int delete(Object[] row, long ts) throws InterruptedException {
-            return delete(row, ts, 0);
+            return delete(row, ts, 0, 1);
         }
 
         /**
@@ -477,29 +447,12 @@ public int delete(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int delete(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.delete(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#DELETE delete} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int deleteEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += delete(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int delete(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.delete(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
     }
 }",2016-06-14T22:43:58Z,122
"@@ -186,9 +186,11 @@ protected void execute() {
             sql.set(""SHOW MASTER STATUS"");
             mysql.query(sql.get(), rs -> {
                 if (rs.next()) {
-                    source.setBinlogFilename(rs.getString(1));
-                    source.setBinlogPosition(rs.getLong(2));
-                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                    String binlogFilename = rs.getString(1);
+                    long binlogPosition = rs.getLong(2);
+                    String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                    source.setBinlogStartPoint(binlogFilename, binlogPosition);
+                    source.setGtidSet(gtidSet);
                     source.startSnapshot();
                 }
             });",2016-06-14T22:43:58Z,62
"@@ -33,16 +33,18 @@
  * 
  * <p>
  * The {@link #offset() source offset} information describes how much of the database's binary log the source the change detector
- * has processed. Here's a JSON-like representation of an example:
+ * has already processed, and it includes the {@link #binlogFilename() binlog filename}, the {@link #nextBinlogPosition() next
+ * position} in the binlog to start reading, and the {@link #nextEventRowNumber() next event row number}. Here's a JSON-like
+ * representation of an example:
  * 
  * <pre>
  * {
  *     ""server_id"": 112233,
- *     ""ts_sec"": 1465236179,
- *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
- *     ""file"" = ""mysql-bin.000003"",
- *     ""pos"" = 105586,
- *     ""row"" = 0,
+ *     ""ts_sec"": 1465937,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 990,
+ *     ""row"": 0,
  *     ""snapshot"": true
  * }
  * </pre>
@@ -53,20 +55,21 @@
  * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
  * 
  * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
- * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
- * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
- * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
- * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
- * an event that corresponds to the above partition and offset:
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), the
+ * {@link #lastBinlogPosition() position} of the event (and {@link #lastEventRowNumber() row number} within the event) inside
+ * the {@link #binlogFilename() binlog file}. When GTIDs are enabled, it also includes the GTID of the transaction in which the
+ * event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced when the connector is in the
+ * middle of a snapshot. Here's a JSON-like representation of the source for an event that corresponds to the above partition and
+ * offset:
  * 
  * <pre>
  * {
  *     ""name"": ""production-server"",
  *     ""server_id"": 112233,
- *     ""ts_sec"": 1465236179,
+ *     ""ts_sec"": 1465937,
  *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
  *     ""file"": ""mysql-bin.000003"",
- *     ""pos"" = 105586,
+ *     ""pos"" = 1081,
  *     ""row"": 0,
  *     ""snapshot"": true
  * }
@@ -109,8 +112,10 @@ final class SourceInfo {
     private String gtidSet;
     private String binlogGtid;
     private String binlogFilename;
-    private long binlogPosition = 4;
-    private int eventRowNumber = 0;
+    private long lastBinlogPosition = 0;
+    private int lastEventRowNumber = 0;
+    private long nextBinlogPosition = 4;
+    private int nextEventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
     private long binlogTimestampSeconds = 0;
@@ -150,6 +155,32 @@ public Map<String, String> partition() {
      * @return a copy of the current offset; never null
      */
     public Map<String, ?> offset() {
+        return offsetUsingPosition(nextBinlogPosition);
+    }
+
+    /**
+     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
+     * describes the position within the source where we have last read.
+     * 
+     * @param eventRowNumber the 0-based row number within the event being processed
+     * @param totalNumberOfRows the total number of rows within the event being processed
+     * @return a copy of the current offset; never null
+     */
+    public Map<String, ?> offset(int eventRowNumber, int totalNumberOfRows) {
+        if (eventRowNumber < (totalNumberOfRows - 1)) {
+            // This is not the last row, so our offset should record the next row to be used ...
+            this.lastEventRowNumber = eventRowNumber;
+            this.nextEventRowNumber = eventRowNumber + 1;
+            // so write out the offset with the position of this event
+            return offsetUsingPosition(lastBinlogPosition);
+        }
+        // This is the last row, so write out the offset that has the position of the next event ...
+        this.lastEventRowNumber = this.nextEventRowNumber;
+        this.nextEventRowNumber = 0;
+        return offsetUsingPosition(nextBinlogPosition);
+    }
+
+    private Map<String, ?> offsetUsingPosition( long binlogPosition ) {
         Map<String, Object> map = new HashMap<>();
         if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
         if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
@@ -158,7 +189,7 @@ public Map<String, String> partition() {
         }
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
+        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, nextEventRowNumber);
         if (isSnapshotInEffect()) {
             map.put(SNAPSHOT_KEY, true);
         }
@@ -192,27 +223,15 @@ public Struct struct() {
             result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
+        result.put(BINLOG_POSITION_OFFSET_KEY, lastBinlogPosition);
+        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, lastEventRowNumber);
         result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
         return result;
     }
 
-    /**
-     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
-     * describes the position within the source where we have last read.
-     * 
-     * @param eventRowNumber the 0-based row number within the last event that was successfully processed
-     * @return a copy of the current offset; never null
-     */
-    public Map<String, ?> offset(int eventRowNumber) {
-        setRowInEvent(eventRowNumber);
-        return offset();
-    }
-
     /**
      * Determine whether a snapshot is currently in effect.
      * 
@@ -246,28 +265,27 @@ public void setGtidSet(String gtidSet) {
      * Set the name of the MySQL binary log file.
      * 
      * @param binlogFilename the name of the binary log file; may not be null
+     * @param positionOfFirstEvent the position in the binary log file to begin processing
      */
-    public void setBinlogFilename(String binlogFilename) {
+    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
         this.binlogFilename = binlogFilename;
+        this.nextBinlogPosition = positionOfFirstEvent;
+        this.lastBinlogPosition = this.nextBinlogPosition;
+        this.nextEventRowNumber = 0;
+        this.lastEventRowNumber = 0;
     }
 
     /**
-     * Set the position within the MySQL binary log file.
-     * 
-     * @param binlogPosition the position within the binary log file
-     */
-    public void setBinlogPosition(long binlogPosition) {
-        this.binlogPosition = binlogPosition;
-    }
-
-    /**
-     * Set the index of the row within the event appearing at the {@link #binlogPosition() position} within the
-     * {@link #binlogFilename() binary log file}.
+     * Set the position within the MySQL binary log file of the <em>current event</em>.
      * 
-     * @param rowNumber the 0-based row number
+     * @param positionOfCurrentEvent the position within the binary log file of the current event
+     * @param eventSizeInBytes the size in bytes of this event
      */
-    public void setRowInEvent(int rowNumber) {
-        this.eventRowNumber = rowNumber;
+    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
+        this.lastBinlogPosition = positionOfCurrentEvent;
+        this.nextBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
+        this.nextEventRowNumber = 0;
+        this.lastEventRowNumber = 0;
     }
 
     /**
@@ -316,8 +334,10 @@ public void setOffset(Map<String, ?> sourceOffset) {
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
             }
-            binlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
-            eventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
+            nextBinlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
+            nextEventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
+            lastBinlogPosition = nextBinlogPosition;
+            lastEventRowNumber = nextEventRowNumber;
         }
     }
 
@@ -344,29 +364,50 @@ public String gtidSet() {
     /**
      * Get the name of the MySQL binary log file that has been processed.
      * 
-     * @return the name of the binary log file; null if it has not been {@link #setBinlogFilename(String) set}
+     * @return the name of the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
     public String binlogFilename() {
         return binlogFilename;
     }
 
     /**
-     * Get the position within the MySQL binary log file that has been processed.
+     * Get the position within the MySQL binary log file of the next event to be processed.
      * 
-     * @return the position within the binary log file; null if it has not been {@link #setBinlogPosition(long) set}
+     * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
+     */
+    public long nextBinlogPosition() {
+        return nextBinlogPosition;
+    }
+
+    /**
+     * Get the position within the MySQL binary log file of the most recently processed event.
+     * 
+     * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
+     */
+    public long lastBinlogPosition() {
+        return lastBinlogPosition;
+    }
+
+    /**
+     * Get the next row within the event at the {@link #nextBinlogPosition() position} within the {@link #binlogFilename() binary
+     * log file}
+     * .
+     * 
+     * @return the 0-based row number
      */
-    public long binlogPosition() {
-        return binlogPosition;
+    public int nextEventRowNumber() {
+        return nextEventRowNumber;
     }
 
     /**
-     * Get the row within the event at the {@link #binlogPosition() position} within the {@link #binlogFilename() binary log file}
+     * Get the previous row within the event at the {@link #lastBinlogPosition() position} within the {@link #binlogFilename()
+     * binary log file}
      * .
      * 
      * @return the 0-based row number
      */
-    public int eventRowNumber() {
-        return eventRowNumber;
+    public int lastEventRowNumber() {
+        return lastEventRowNumber;
     }
 
     /**
@@ -385,8 +426,8 @@ public String toString() {
             sb.append(""GTIDs "");
             sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
-            sb.append("", pos="").append(binlogPosition());
-            sb.append("", row="").append(eventRowNumber());
+            sb.append("", pos="").append(nextBinlogPosition());
+            sb.append("", row="").append(nextEventRowNumber());
         } else {
             if (binlogFilename == null) {
                 sb.append(""<latest>"");
@@ -395,8 +436,8 @@ public String toString() {
                     sb.append(""earliest binlog file and position"");
                 } else {
                     sb.append(""binlog file '"").append(binlogFilename).append(""'"");
-                    sb.append("", pos="").append(binlogPosition());
-                    sb.append("", row="").append(eventRowNumber());
+                    sb.append("", pos="").append(nextBinlogPosition());
+                    sb.append("", row="").append(nextEventRowNumber());
                 }
             }
         }",2016-06-14T22:43:58Z,70
"@@ -114,7 +114,7 @@ public void shouldCreateSnapshotOfSingleDatabase() throws Exception {
         config = simpleConfig().build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setBinlogFilename(""""); // start from beginning
+        context.source().setBinlogStartPoint("""",0L); // start from beginning
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...
@@ -173,7 +173,7 @@ public void shouldCreateSnapshotOfSingleDatabaseWithSchemaChanges() throws Excep
         config = simpleConfig().with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true).build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setBinlogFilename(""""); // start from beginning
+        context.source().setBinlogStartPoint("""",0L); // start from beginning
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...",2016-06-14T22:43:58Z,67
"@@ -22,6 +22,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.history.FileDatabaseHistory;
@@ -284,6 +285,56 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         stopConnector();
     }
 
+    @Test
+    public void shouldConsumeEventsWithNoSnapshot() throws SQLException, InterruptedException {
+        Testing.Files.delete(DB_HISTORY_PATH);
+        
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18780)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""kafka-connect-2"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test_ro"")
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.name().toLowerCase())
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+
+        // Consume the first records due to startup and initialization of the database ...
+        // Testing.Print.enable();
+        SourceRecords records = consumeRecordsByTopic(9+9+4+5+6);   // 6 DDL changes
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.products"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.products_on_hand"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.customers"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.orders"").size()).isEqualTo(5);
+        assertThat(records.topics().size()).isEqualTo(4+1);
+        assertThat(records.ddlRecordsForDatabase(""connector_test_ro"").size()).isEqualTo(6);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+        
+        // More records may have been written (if this method were run after the others), but we don't care ...
+        stopConnector();
+
+        records.recordsForTopic(""kafka-connect-2.connector_test_ro.orders"").forEach(record->{
+            print(record);
+        });
+        
+        records.recordsForTopic(""kafka-connect-2.connector_test_ro.customers"").forEach(record->{
+            print(record);
+        });
+    }
+
+
     @Test
     public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLException, InterruptedException {
         Testing.Files.delete(DB_HISTORY_PATH);",2016-06-14T22:43:58Z,71
"@@ -60,8 +60,7 @@ public void shouldApplyDdlStatementsAndRecover() {
         mysql.start();
 
         // Testing.Print.enable();
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -81,11 +80,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
                      .createSchemas();
         mysql.start();
 
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
-        source.setBinlogPosition(1000);
+        source.setBinlogStartPoint(""binlog-001"",1000);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -107,11 +105,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
                      .createSchemas();
         mysql.start();
 
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
-        source.setBinlogPosition(1000);
+        source.setBinlogStartPoint(""binlog-001"",1000);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...",2016-06-14T22:43:58Z,19
"@@ -189,9 +189,12 @@ protected void execute() {
                 if (rs.next()) {
                     String binlogFilename = rs.getString(1);
                     long binlogPosition = rs.getLong(2);
-                    String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
                     source.setBinlogStartPoint(binlogFilename, binlogPosition);
-                    source.setGtidSet(gtidSet);
+                    if ( rs.getMetaData().getColumnCount() > 4 ) {
+                        // This column exists only in MySQL 5.6.5 or later ...
+                        String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                        source.setGtidSet(gtidSet);
+                    }
                     source.startSnapshot();
                 }
             });",2016-06-27T14:23:12Z,62
"@@ -172,3 +172,24 @@ VALUES (default, '2016-01-16', 1001, 1, 102),
        (default, '2016-02-18', 1004, 3, 109),
        (default, '2016-02-19', 1002, 2, 106),
        (default, '2016-02-21', 1003, 1, 107);
+
+
+
+# ----------------------------------------------------------------------------------------------------------------
+# DATABASE:  regression_test
+# ----------------------------------------------------------------------------------------------------------------
+# The integration test for this database expects to scans all of the binlog events associated with this database
+# without error or problems. The integration test does not modify any records in this database, so this script
+# must contain all operations to these tables.
+#
+CREATE DATABASE regression_test;
+USE regression_test;
+
+# DBZ-61 handle binary value recorded as hex string value
+CREATE TABLE t1464075356413_testtable6 (
+  pk_column int auto_increment NOT NULL,
+  varbinary_col varbinary(20) NOT NULL,
+  PRIMARY KEY(pk_column)
+);
+INSERT INTO t1464075356413_testtable6 (pk_column, varbinary_col)
+VALUES(default, 0x4D7953514C)",2016-06-07T22:53:07Z,79
"@@ -0,0 +1,89 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.nio.file.Path;
+import java.sql.SQLException;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.relational.history.FileDatabaseHistory;
+import io.debezium.util.Testing;
+
+/**
+ * @author Randall Hauch
+ */
+public class MySqlConnectorRegressionIT extends AbstractConnectorTest {
+
+    private static final Path DB_HISTORY_PATH = Testing.Files.createTestingPath(""file-db-history-regression.txt"").toAbsolutePath();
+
+    private Configuration config;
+
+    @Before
+    public void beforeEach() {
+        stopConnector();
+        initializeConnectorTestFramework();
+        Testing.Files.delete(DB_HISTORY_PATH);
+    }
+
+    @After
+    public void afterEach() {
+        try {
+            stopConnector();
+        } finally {
+            Testing.Files.delete(DB_HISTORY_PATH);
+        }
+    }
+
+    @Test
+    public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+        
+        // ---------------------------------------------------------------------------------------------------------------
+        // Consume all of the events due to startup and initialization of the database
+        // ---------------------------------------------------------------------------------------------------------------
+        //Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(2+1);   // 2 schema change record, 1 insert
+        stopConnector();
+        assertThat(records).isNotNull();
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(2);
+        assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(2);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(2);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+    }
+
+}",2016-06-07T22:53:07Z,48
"@@ -5,6 +5,8 @@
  */
 package io.debezium.data;
 
+import java.nio.ByteBuffer;
+import java.util.Base64;
 import java.util.List;
 import java.util.Map;
 
@@ -169,6 +171,11 @@ public RecordWriter append(Object obj) {
                     appendFirst(field.name(), s.get(field));
                 }
                 sb.append('}');
+            } else if (obj instanceof ByteBuffer) {
+                ByteBuffer b = (ByteBuffer) obj;
+                sb.append('""').append(Base64.getEncoder().encode(b.array())).append('""');
+            } else if (obj instanceof byte[]) {
+                sb.append('""').append(Base64.getEncoder().encode((byte[])obj)).append('""');
             } else if (obj instanceof Map<?, ?>) {
                 Map<?, ?> map = (Map<?, ?>) obj;
                 sb.append('{');",2016-06-07T22:53:07Z,127
"@@ -6,6 +6,7 @@
 package io.debezium.relational;
 
 import java.math.BigDecimal;
+import java.nio.ByteBuffer;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
@@ -105,7 +106,8 @@ public TableSchema create(ResultSet resultSet, String name) throws SQLException
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
@@ -121,15 +123,16 @@ public TableSchema create(String schemaPrefix, Table table) {
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @param filter the filter that specifies whether columns in the table should be included; may be null if all columns
      *            are to be included
      * @param mappers the mapping functions for columns; may be null if none of the columns are to be mapped to different values
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
     public TableSchema create(String schemaPrefix, Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
-        if ( schemaPrefix == null ) schemaPrefix = """";
+        if (schemaPrefix == null) schemaPrefix = """";
         // Build the schemas ...
         final TableId tableId = table.id();
         final String tableIdStr = tableId.toString();
@@ -285,7 +288,10 @@ protected ValueConverter[] convertersForColumns(Schema schema, TableId tableId,
                 if (mappers != null) {
                     ValueConverter mappingConverter = mappers.mappingConverterFor(tableId, column);
                     if (mappingConverter != null) {
-                        converter = (value) -> mappingConverter.convert(valueConverter.convert(value));
+                        converter = (value) -> {
+                            if (value != null) value = valueConverter.convert(value);
+                            return mappingConverter.convert(value);
+                        };
                     }
                 }
                 if (converter == null) converter = valueConverter;
@@ -376,7 +382,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
             case Types.NCLOB:
                 fieldBuilder = SchemaBuilder.string();
                 break;
-                
+
             // Variable-length string values
             case Types.VARCHAR:
             case Types.LONGVARCHAR:
@@ -424,7 +430,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
         if (fieldBuilder != null) {
             if (mapper != null) {
                 // Let the mapper add properties to the schema ...
-                mapper.alterFieldSchema(column,fieldBuilder);
+                mapper.alterFieldSchema(column, fieldBuilder);
             }
             if (column.isOptional()) fieldBuilder.optional();
             builder.field(column.name(), fieldBuilder.build());
@@ -450,6 +456,14 @@ protected SchemaBuilder addOtherField(Column column, ColumnMapper mapper) {
     /**
      * Create a {@link ValueConverter} that can be used to convert row values for the given column into the Kafka Connect value
      * object described by the {@link Field field definition}.
+     * <p>
+     * Subclasses can override this method to specialize the behavior. The subclass method should do custom checks and
+     * conversions,
+     * and then delegate to this method implementation to handle all other cases.
+     * <p>
+     * Alternatively, subclasses can leave this method as-is and instead override one of the lower-level type-specific methods
+     * that this method calls (e.g., {@link #convertBinary(Column, Field, Object)},
+     * {@link #convertTinyInt(Column, Field, Object)}, etc.).
      * 
      * @param column the column describing the input values; never null
      * @param fieldDefn the definition for the field in a Kafka Connect {@link Schema} describing the output of the function;
@@ -461,94 +475,38 @@ protected ValueConverter createValueConverterFor(Column column, Field fieldDefn)
             case Types.NULL:
                 return (data) -> null;
             case Types.BIT:
+                return (data) -> convertBit(column, fieldDefn, data);
             case Types.BOOLEAN:
-                return (data) -> {
-                    if (data instanceof Boolean) return (Boolean) data;
-                    if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBoolean(column, fieldDefn, data);
 
             // Binary values ...
             case Types.BLOB:
             case Types.BINARY:
             case Types.VARBINARY:
             case Types.LONGVARBINARY:
-                return (data) -> (byte[]) data;
+                return (data) -> convertBinary(column, fieldDefn, data);
 
             // Numeric integers
             case Types.TINYINT:
-                return (data) -> {
-                    if (data instanceof Byte) return (Byte) data;
-                    if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertTinyInt(column, fieldDefn, data);
             case Types.SMALLINT:
-                return (data) -> {
-                    if (data instanceof Short) return (Short) data;
-                    if (data instanceof Integer) return new Short(((Integer) data).shortValue());
-                    if (data instanceof Long) return new Short(((Long) data).shortValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertSmallInt(column, fieldDefn, data);
             case Types.INTEGER:
-                return (data) -> {
-                    if (data instanceof Integer) return (Integer) data;
-                    if (data instanceof Short) return new Integer(((Short) data).intValue());
-                    if (data instanceof Long) return new Integer(((Long) data).intValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertInteger(column, fieldDefn, data);
             case Types.BIGINT:
-                return (data) -> {
-                    if (data instanceof Long) return (Long) data;
-                    if (data instanceof Integer) return new Long(((Integer) data).longValue());
-                    if (data instanceof Short) return new Long(((Short) data).longValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBigInt(column, fieldDefn, data);
 
             // Numeric decimal numbers
             case Types.FLOAT:
+                return (data) -> convertFloat(column, fieldDefn, data);
             case Types.DOUBLE:
-                return (data) -> {
-                    if (data instanceof Double) return (Double) data;
-                    if (data instanceof Float) return new Double(((Float) data).doubleValue());
-                    if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
-                    if (data instanceof Long) return new Double(((Long) data).doubleValue());
-                    if (data instanceof Short) return new Double(((Short) data).doubleValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertDouble(column, fieldDefn, data);
             case Types.REAL:
-                return (data) -> {
-                    if (data instanceof Float) return (Float) data;
-                    if (data instanceof Double) return new Float(((Double) data).floatValue());
-                    if (data instanceof Integer) return new Float(((Integer) data).floatValue());
-                    if (data instanceof Long) return new Float(((Long) data).floatValue());
-                    if (data instanceof Short) return new Float(((Short) data).floatValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertReal(column, fieldDefn, data);
             case Types.NUMERIC:
+                return (data) -> convertNumeric(column, fieldDefn, data);
             case Types.DECIMAL:
-                return (data) -> {
-                    BigDecimal decimal = null;
-                    if (data instanceof BigDecimal)
-                        decimal = (BigDecimal) data;
-                    else if (data instanceof Boolean)
-                        decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
-                    else if (data instanceof Short)
-                        decimal = new BigDecimal(((Short) data).intValue());
-                    else if (data instanceof Integer)
-                        decimal = new BigDecimal(((Integer) data).intValue());
-                    else if (data instanceof Long)
-                        decimal = BigDecimal.valueOf(((Long) data).longValue());
-                    else if (data instanceof Float)
-                        decimal = BigDecimal.valueOf(((Float) data).doubleValue());
-                    else if (data instanceof Double)
-                        decimal = BigDecimal.valueOf(((Double) data).doubleValue());
-                    else {
-                        handleUnknownData(column, fieldDefn, data);
-                    }
-                    return decimal;
-                };
+                return (data) -> convertDecimal(column, fieldDefn, data);
 
             // String values
             case Types.CHAR: // variable-length
@@ -561,26 +519,23 @@ else if (data instanceof Double)
             case Types.NCLOB: // fixed-length
             case Types.DATALINK:
             case Types.SQLXML:
-                return (data) -> data.toString();
+                return (data) -> convertString(column, fieldDefn, data);
 
             // Date and time values
             case Types.DATE:
-                return (data) -> convertDate(fieldDefn, data);
+                return (data) -> convertDate(column, fieldDefn, data);
             case Types.TIME:
-                return (data) -> convertTime(fieldDefn, data);
+                return (data) -> convertTime(column, fieldDefn, data);
             case Types.TIMESTAMP:
-                return (data) -> convertTimestamp(fieldDefn, data);
+                return (data) -> convertTimestamp(column, fieldDefn, data);
             case Types.TIME_WITH_TIMEZONE:
-                return (data) -> convertTimeWithZone(fieldDefn, data);
+                return (data) -> convertTimeWithZone(column, fieldDefn, data);
             case Types.TIMESTAMP_WITH_TIMEZONE:
-                return (data) -> convertTimestampWithZone(fieldDefn, data);
+                return (data) -> convertTimestampWithZone(column, fieldDefn, data);
 
             // Other types ...
             case Types.ROWID:
-                return (data) -> {
-                    java.sql.RowId rowId = (java.sql.RowId) data;
-                    return rowId.getBytes();
-                };
+                return (data) -> convertRowId(column, fieldDefn, data);
 
             // Unhandled types
             case Types.ARRAY:
@@ -620,11 +575,13 @@ protected Object handleUnknownData(Column column, Field fieldDefn, Object data)
      * This method handles several types of objects, including {@link OffsetDateTime}, {@link java.sql.Timestamp},
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestampWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimestampWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetDateTime dateTime = null;
         if (data instanceof OffsetDateTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -679,11 +636,13 @@ protected OffsetDateTime unexpectedTimestampWithZone(Object value, Field fieldDe
      * {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types have date components, those date
      * components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimeWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimeWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetTime time = null;
         if (data instanceof OffsetTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -731,11 +690,13 @@ protected OffsetTime unexpectedTimeWithZone(Object value, Field fieldDefn) {
      * but no time zone info. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such
      * as {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestamp(Field fieldDefn, Object data) {
+    protected Object convertTimestamp(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Timestamp) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -782,11 +743,13 @@ protected java.util.Date unexpectedTimestamp(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types might
      * have date components, those date components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTime(Field fieldDefn, Object data) {
+    protected Object convertTime(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Time) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -834,11 +797,13 @@ protected java.util.Date unexpectedTime(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
      * have time components, those time components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertDate(Field fieldDefn, Object data) {
+    protected Object convertDate(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Date) {
             // JDBC specification indicates that this will be the nominal object for this JDBC type.
@@ -882,4 +847,286 @@ protected java.util.Date unexpectedDate(Object value, Field fieldDefn) {
                     fieldDefn.schema(), value.getClass(), value);
         return null;
     }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have time components, those time components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBinary(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof char[]) {
+            data = new String((char[]) data); // convert to string
+        }
+        if (data instanceof String) {
+            // This was encoded as a hexadecimal string, but we receive it as a normal string ...
+            data = ((String) data).getBytes();
+        }
+        if (data instanceof byte[]) {
+            return ByteBuffer.wrap((byte[])data);
+        }
+        // An unexpected value
+        return unexpectedBinary(data, fieldDefn);
+    }
+
+    /**
+     * Handle the unexpected value from a row with a column type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * 
+     * @param value the binary value for which no conversion was found; never null
+     * @param fieldDefn the field definition in the Kafka Connect schema; never null
+     * @return the converted value, or null
+     * @see #convertBinary(Column, Field, Object)
+     */
+    protected byte[] unexpectedBinary(Object value, Field fieldDefn) {
+        LOGGER.warn(""Unexpected JDBC BINARY value for field {} with schema {}: class={}, value={}"", fieldDefn.name(),
+                    fieldDefn.schema(), value.getClass(), value);
+        return null;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#TINYINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTinyInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Byte) return data;
+        if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#SMALLINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertSmallInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Short) return data;
+        if (data instanceof Integer) return new Short(((Integer) data).shortValue());
+        if (data instanceof Long) return new Short(((Long) data).shortValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertInteger(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Integer) return data;
+        if (data instanceof Short) return new Integer(((Short) data).intValue());
+        if (data instanceof Long) return new Integer(((Long) data).intValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBigInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Long) return data;
+        if (data instanceof Integer) return new Long(((Integer) data).longValue());
+        if (data instanceof Short) return new Long(((Short) data).longValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#FLOAT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertFloat(Column column, Field fieldDefn, Object data) {
+        return convertDouble(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DOUBLE}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDouble(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Double) return data;
+        if (data instanceof Float) return new Double(((Float) data).doubleValue());
+        if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
+        if (data instanceof Long) return new Double(((Long) data).doubleValue());
+        if (data instanceof Short) return new Double(((Short) data).doubleValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#REAL}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertReal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Float) return data;
+        if (data instanceof Double) return new Float(((Double) data).floatValue());
+        if (data instanceof Integer) return new Float(((Integer) data).floatValue());
+        if (data instanceof Long) return new Float(((Long) data).floatValue());
+        if (data instanceof Short) return new Float(((Short) data).floatValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertNumeric(Column column, Field fieldDefn, Object data) {
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDecimal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#CHAR}, {@link Types#VARCHAR},
+     * {@link Types#LONGVARCHAR}, {@link Types#CLOB}, {@link Types#NCHAR}, {@link Types#NVARCHAR}, {@link Types#LONGNVARCHAR},
+     * {@link Types#NCLOB}, {@link Types#DATALINK}, and {@link Types#SQLXML}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertString(Column column, Field fieldDefn, Object data) {
+        return data == null ? null : data.toString();
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#ROWID}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertRowId(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof java.sql.RowId) {
+            java.sql.RowId row = (java.sql.RowId)data;
+            return ByteBuffer.wrap(row.getBytes());
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BIT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBit(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BOOLEAN}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
 }",2016-06-07T22:53:07Z,129
"@@ -7,8 +7,13 @@
 
 import static org.junit.Assert.fail;
 
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.HashMap;
+import java.util.Iterator;
 import java.util.Map;
+import java.util.Objects;
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
@@ -47,18 +52,18 @@ public class VerifyRecord {
     private static final MockSchemaRegistryClient schemaRegistry = new MockSchemaRegistryClient();
     private static final AvroConverter avroKeyConverter = new AvroConverter(schemaRegistry);
     private static final AvroConverter avroValueConverter = new AvroConverter(schemaRegistry);
-    
+
     static {
-        Map<String,Object> config = new HashMap<>();
-        config.put(""schemas.enable"",Boolean.TRUE.toString());
-        config.put(""schemas.cache.size"",100);
+        Map<String, Object> config = new HashMap<>();
+        config.put(""schemas.enable"", Boolean.TRUE.toString());
+        config.put(""schemas.cache.size"", 100);
         keyJsonConverter.configure(config, true);
         keyJsonDeserializer.configure(config, true);
         valueJsonConverter.configure(config, false);
         valueJsonDeserializer.configure(config, false);
 
         config = new HashMap<>();
-        config.put(""schema.registry.url"",""http://fake-url"");
+        config.put(""schema.registry.url"", ""http://fake-url"");
         avroKeyConverter.configure(config, false);
         avroValueConverter.configure(config, false);
     }
@@ -276,73 +281,97 @@ public static void isValid(SourceRecord record) {
         SchemaAndValue valueWithSchema = null;
         SchemaAndValue avroKeyWithSchema = null;
         SchemaAndValue avroValueWithSchema = null;
+        String msg = null;
         try {
             // The key should never be null ...
+            msg = ""checking key is not null"";
             assertThat(record.key()).isNotNull();
             assertThat(record.keySchema()).isNotNull();
 
             // If the value is not null there must be a schema; otherwise, the schema should also be null ...
             if (record.value() == null) {
+                msg = ""checking value schema is null"";
                 assertThat(record.valueSchema()).isNull();
             } else {
+                msg = ""checking value schema is not null"";
                 assertThat(record.valueSchema()).isNotNull();
             }
 
             // First serialize and deserialize the key ...
+            msg = ""serializing key using JSON converter"";
             byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using JSON deserializer"";
             keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            msg = ""deserializing key using JSON converter"";
             keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            msg = ""comparing key schema to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // then the value ...
+            msg = ""serializing value using JSON converter"";
             byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using JSON deserializer"";
             valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            msg = ""deserializing value using JSON converter"";
             valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
             // Serialize and deserialize the key using the Avro converter, and check that we got the same result ...
+            msg = ""serializing key using Avro converter"";
             byte[] avroKeyBytes = avroValueConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using Avro converter"";
             avroKeyWithSchema = avroValueConverter.toConnectData(record.topic(), avroKeyBytes);
-            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
-            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key schema to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.schema(),record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.value(),record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // Serialize and deserialize the value using the Avro converter, and check that we got the same result ...
+            msg = ""serializing value using Avro converter"";
             byte[] avroValueBytes = avroValueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using Avro converter"";
             avroValueWithSchema = avroValueConverter.toConnectData(record.topic(), avroValueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
         } catch (Throwable t) {
             Testing.Print.enable();
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
             Testing.printError(t);
-            if (keyJson == null ){
-                Testing.print(""error deserializing key from JSON: "" + SchemaUtil.asString(record.key()));
-            } else if (keyWithSchema == null ){
-                Testing.print(""error using JSON converter on key: "" + prettyJson(keyJson));
-            } else if (avroKeyWithSchema == null ){
-                Testing.print(""error using Avro converter on key: "" + prettyJson(keyJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            Testing.print(""error "" + msg);
+            Testing.print(""  key: "" + SchemaUtil.asString(record.key()));
+            Testing.print(""  key deserialized from JSON: "" + prettyJson(keyJson));
+            if (keyWithSchema != null) {
+                Testing.print(""  key to/from JSON: "" + SchemaUtil.asString(keyWithSchema.value()));
             }
-
-            if (valueJson == null ){
-                Testing.print(""error deserializing value from JSON: "" + SchemaUtil.asString(record.value()));
-            } else if (valueWithSchema == null ){
-                Testing.print(""error using JSON converter on value: "" + prettyJson(valueJson));
-            } else if (avroValueWithSchema == null ){
-                Testing.print(""error using Avro converter on value: "" + prettyJson(valueJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            if (avroKeyWithSchema != null) {
+                Testing.print(""  key to/from Avro: "" + SchemaUtil.asString(avroKeyWithSchema.value()));
+            }
+            Testing.print(""  value: "" + SchemaUtil.asString(record.value()));
+            Testing.print(""  value deserialized from JSON: "" + prettyJson(valueJson));
+            if (valueWithSchema != null) {
+                Testing.print(""  value to/from JSON: "" + SchemaUtil.asString(valueWithSchema.value()));
+            }
+            if (avroValueWithSchema != null) {
+                Testing.print(""  value to/from Avro: "" + SchemaUtil.asString(avroValueWithSchema.value()));
             }
             if (t instanceof AssertionError) throw t;
-            fail(t.getMessage());
+            fail(""error "" + msg + "": "" + t.getMessage());
         }
     }
 
@@ -390,5 +419,135 @@ protected static String prettyJson(JsonNode json) {
             return null;
         }
     }
+    
+    // The remaining methods are needed simply because of the KAFKA-3803, so our comparisons cannot rely upon Struct.equals
+    
+    protected static void assertEquals( Object o1, Object o2 ) {
+        // assertThat(o1).isEqualTo(o2);
+        if ( !equals(o1,o2) ) {
+            fail(SchemaUtil.asString(o1) + "" was not equal to "" + SchemaUtil.asString(o2));
+        }
+    }
+    
+    @SuppressWarnings(""unchecked"")
+    protected static boolean equals( Object o1, Object o2 ) {
+        if ( o1 == o2 ) return true;
+        if (o1 == null) return o2 == null ? true : false;
+        if (o2 == null ) return false;
+        if ( o1 instanceof ByteBuffer ) {
+            o1 = ((ByteBuffer)o1).array();
+        }
+        if ( o2 instanceof ByteBuffer ) {
+            o2 = ((ByteBuffer)o2).array();
+        }
+        if ( o1 instanceof byte[] && o2 instanceof byte[] ) {
+            boolean result = Arrays.equals((byte[])o1,(byte[])o2);
+            return result;
+        }
+        if ( o1 instanceof Object[] && o2 instanceof Object[] ) {
+            boolean result = deepEquals((Object[])o1,(Object[])o2);
+            return result;
+        }
+        if ( o1 instanceof Map && o2 instanceof Map ) {
+            Map<String,Object> m1 = (Map<String,Object>)o1;
+            Map<String,Object> m2 = (Map<String,Object>)o2;
+            if ( !m1.keySet().equals(m2.keySet())) return false;
+            for ( Map.Entry<String, Object> entry : m1.entrySet()) {
+                Object v1 = entry.getValue();
+                Object v2 = m2.get(entry.getKey());
+                if ( !equals(v1,v2) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Collection && o2 instanceof Collection ) {
+            Collection<Object> m1 = (Collection<Object>)o1;
+            Collection<Object> m2 = (Collection<Object>)o2;
+            if ( m1.size() != m2.size() ) return false;
+            Iterator<?> iter1 = m1.iterator();
+            Iterator<?> iter2 = m2.iterator();
+            while ( iter1.hasNext() && iter2.hasNext() ) {
+                if ( !equals(iter1.next(),iter2.next()) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Struct && o2 instanceof Struct ) {
+            // Unfortunately, the Struct.equals() method has a bug in that it is not using Arrays.deepEquals(...) to
+            // compare values in two Struct objects. The result is that the equals only works if the values of the
+            // first level Struct are non arrays; otherwise, the array values are compared using == and that obviously
+            // does not work for non-primitive values.
+            Struct struct1 = (Struct) o1;
+            Struct struct2 = (Struct) o2;
+            if (! Objects.equals(struct1.schema(),struct2.schema()) ) {
+                return false;
+            }
+            Object[] array1 = valuesFor(struct1);
+            Object[] array2 = valuesFor(struct2);
+            boolean result = deepEquals(array1, array2);
+            return result;
+        }
+        return Objects.equals(o1, o2);
+    }
+    
+    private static Object[] valuesFor( Struct struct ) {
+        Object[] array = new Object[struct.schema().fields().size()];
+        int index = 0;
+        for ( Field field : struct.schema().fields() ) {
+            array[index] = struct.get(field);
+            ++index;
+        }
+        return array;
+    }
 
+    private static boolean deepEquals(Object[] a1, Object[] a2) {
+        if (a1 == a2)
+            return true;
+        if (a1 == null || a2==null)
+            return false;
+        int length = a1.length;
+        if (a2.length != length)
+            return false;
+
+        for (int i = 0; i < length; i++) {
+            Object e1 = a1[i];
+            Object e2 = a2[i];
+
+            if (e1 == e2)
+                continue;
+            if (e1 == null)
+                return false;
+
+            // Figure out whether the two elements are equal
+            boolean eq = deepEquals0(e1, e2);
+
+            if (!eq)
+                return false;
+        }
+        return true;
+    }
+
+    private static boolean deepEquals0(Object e1, Object e2) {
+        assert e1 != null;
+        boolean eq;
+        if (e1 instanceof Object[] && e2 instanceof Object[])
+            eq = deepEquals ((Object[]) e1, (Object[]) e2);
+        else if (e1 instanceof byte[] && e2 instanceof byte[])
+            eq = Arrays.equals((byte[]) e1, (byte[]) e2);
+        else if (e1 instanceof short[] && e2 instanceof short[])
+            eq = Arrays.equals((short[]) e1, (short[]) e2);
+        else if (e1 instanceof int[] && e2 instanceof int[])
+            eq = Arrays.equals((int[]) e1, (int[]) e2);
+        else if (e1 instanceof long[] && e2 instanceof long[])
+            eq = Arrays.equals((long[]) e1, (long[]) e2);
+        else if (e1 instanceof char[] && e2 instanceof char[])
+            eq = Arrays.equals((char[]) e1, (char[]) e2);
+        else if (e1 instanceof float[] && e2 instanceof float[])
+            eq = Arrays.equals((float[]) e1, (float[]) e2);
+        else if (e1 instanceof double[] && e2 instanceof double[])
+            eq = Arrays.equals((double[]) e1, (double[]) e2);
+        else if (e1 instanceof boolean[] && e2 instanceof boolean[])
+            eq = Arrays.equals((boolean[]) e1, (boolean[]) e2);
+        else
+            eq = equals(e1,e2);
+        return eq;
+    }
 }",2016-06-07T22:53:07Z,77
"@@ -63,6 +63,7 @@ public class MySqlSchema {
     private final TableSchemaBuilder schemaBuilder;
     private final DdlChanges ddlChanges;
     private final String serverName;
+    private final String schemaPrefix;
     private Tables tables;
 
     /**
@@ -78,7 +79,13 @@ public MySqlSchema(Configuration config, String serverName) {
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
         this.schemaBuilder = new TableSchemaBuilder();
+        if ( serverName != null ) serverName = serverName.trim();
         this.serverName = serverName;
+        if ( this.serverName == null || serverName.isEmpty() ) {
+            this.schemaPrefix = """";
+        } else {
+            this.schemaPrefix = serverName.endsWith(""."") ? serverName : serverName + ""."";
+        }
 
         // Create and configure the database history ...
         this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
@@ -253,7 +260,7 @@ protected void refreshSchemas() {
         // Create TableSchema instances for any existing table ...
         this.tables.tableIds().forEach(id -> {
             Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(serverName,table, filters.columnFilter(), filters.columnMappers());
+            TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
             tableSchemaByTableId.put(id, schema);
         });
     }
@@ -323,7 +330,7 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
             if (table == null) { // removed
                 tableSchemaByTableId.remove(tableId);
             } else {
-                TableSchema schema = schemaBuilder.create(serverName, table, filters.columnFilter(), filters.columnMappers());
+                TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
                 tableSchemaByTableId.put(tableId, schema);
             }
         });",2016-06-10T02:08:09Z,19
"@@ -30,6 +30,10 @@ public Configurator with(Field field, boolean value) {
         return this;
     }
 
+    public Configurator serverName(String serverName) {
+        return with(MySqlConnectorConfig.SERVER_NAME, serverName);
+    }
+
     public Configurator includeDatabases(String regexList) {
         return with(MySqlConnectorConfig.DATABASE_WHITELIST, regexList);
     }",2016-06-10T02:08:09Z,110
"@@ -29,6 +29,7 @@
 public class MySqlSchemaTest {
 
     private static final Path TEST_FILE_PATH = Testing.Files.createTestingPath(""dbHistory.log"");
+    private static final String SERVER_NAME = ""test-server"";
 
     private Configurator build;
     private MySqlSchema mysql;
@@ -55,7 +56,7 @@ public void afterEach() {
 
     @Test
     public void shouldApplyDdlStatementsAndRecover() {
-        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).serverName(SERVER_NAME).createSchemas();
         mysql.start();
 
         // Testing.Print.enable();
@@ -74,6 +75,7 @@ public void shouldApplyDdlStatementsAndRecover() {
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .excludeBuiltInTables()
                      .createSchemas();
@@ -99,6 +101,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .includeBuiltInTables()
                      .createSchemas();
@@ -124,7 +127,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
     protected void assertTableIncluded(String fullyQualifiedTableName) {
         TableId tableId = TableId.parse(fullyQualifiedTableName);
         assertThat(mysql.tables().forTable(tableId)).isNotNull();
-        assertThat(mysql.schemaFor(tableId)).isNotNull();
+        TableSchema tableSchema = mysql.schemaFor(tableId);
+        assertThat(tableSchema).isNotNull();
+        assertThat(tableSchema.keySchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Key"");
+        assertThat(tableSchema.valueSchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Value"");
     }
 
     protected void assertTableExcluded(String fullyQualifiedTableName) {",2016-06-10T02:08:09Z,19
"@@ -172,3 +172,24 @@ VALUES (default, '2016-01-16', 1001, 1, 102),
        (default, '2016-02-18', 1004, 3, 109),
        (default, '2016-02-19', 1002, 2, 106),
        (default, '2016-02-21', 1003, 1, 107);
+
+
+
+# ----------------------------------------------------------------------------------------------------------------
+# DATABASE:  regression_test
+# ----------------------------------------------------------------------------------------------------------------
+# The integration test for this database expects to scans all of the binlog events associated with this database
+# without error or problems. The integration test does not modify any records in this database, so this script
+# must contain all operations to these tables.
+#
+CREATE DATABASE regression_test;
+USE regression_test;
+
+# DBZ-61 handle binary value recorded as hex string value
+CREATE TABLE t1464075356413_testtable6 (
+  pk_column int auto_increment NOT NULL,
+  varbinary_col varbinary(20) NOT NULL,
+  PRIMARY KEY(pk_column)
+);
+INSERT INTO t1464075356413_testtable6 (pk_column, varbinary_col)
+VALUES(default, 0x4D7953514C)",2016-06-07T22:53:07Z,79
"@@ -0,0 +1,89 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.nio.file.Path;
+import java.sql.SQLException;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
+import io.debezium.embedded.AbstractConnectorTest;
+import io.debezium.relational.history.FileDatabaseHistory;
+import io.debezium.util.Testing;
+
+/**
+ * @author Randall Hauch
+ */
+public class MySqlConnectorRegressionIT extends AbstractConnectorTest {
+
+    private static final Path DB_HISTORY_PATH = Testing.Files.createTestingPath(""file-db-history-regression.txt"").toAbsolutePath();
+
+    private Configuration config;
+
+    @Before
+    public void beforeEach() {
+        stopConnector();
+        initializeConnectorTestFramework();
+        Testing.Files.delete(DB_HISTORY_PATH);
+    }
+
+    @After
+    public void afterEach() {
+        try {
+            stopConnector();
+        } finally {
+            Testing.Files.delete(DB_HISTORY_PATH);
+        }
+    }
+
+    @Test
+    public void shouldConsumeAllEventsFromDatabaseUsingBinlogAndNoSnapshot() throws SQLException, InterruptedException {
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18765)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""regression"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""regression_test"")
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.toString())
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+        
+        // ---------------------------------------------------------------------------------------------------------------
+        // Consume all of the events due to startup and initialization of the database
+        // ---------------------------------------------------------------------------------------------------------------
+        //Testing.Debug.enable();
+        SourceRecords records = consumeRecordsByTopic(2+1);   // 2 schema change record, 1 insert
+        stopConnector();
+        assertThat(records).isNotNull();
+        assertThat(records.recordsForTopic(""regression"").size()).isEqualTo(2);
+        assertThat(records.recordsForTopic(""regression.regression_test.t1464075356413_testtable6"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(2);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""regression_test"").size()).isEqualTo(2);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"")).isNull();
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        records.ddlRecordsForDatabase(""regression_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+    }
+
+}",2016-06-07T22:53:07Z,48
"@@ -5,6 +5,8 @@
  */
 package io.debezium.data;
 
+import java.nio.ByteBuffer;
+import java.util.Base64;
 import java.util.List;
 import java.util.Map;
 
@@ -169,6 +171,11 @@ public RecordWriter append(Object obj) {
                     appendFirst(field.name(), s.get(field));
                 }
                 sb.append('}');
+            } else if (obj instanceof ByteBuffer) {
+                ByteBuffer b = (ByteBuffer) obj;
+                sb.append('""').append(Base64.getEncoder().encode(b.array())).append('""');
+            } else if (obj instanceof byte[]) {
+                sb.append('""').append(Base64.getEncoder().encode((byte[])obj)).append('""');
             } else if (obj instanceof Map<?, ?>) {
                 Map<?, ?> map = (Map<?, ?>) obj;
                 sb.append('{');",2016-06-07T22:53:07Z,127
"@@ -6,6 +6,7 @@
 package io.debezium.relational;
 
 import java.math.BigDecimal;
+import java.nio.ByteBuffer;
 import java.sql.ResultSet;
 import java.sql.SQLException;
 import java.sql.Types;
@@ -105,7 +106,8 @@ public TableSchema create(ResultSet resultSet, String name) throws SQLException
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
@@ -121,15 +123,16 @@ public TableSchema create(String schemaPrefix, Table table) {
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
-     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no
+     *            prefix
      * @param table the table definition; may not be null
      * @param filter the filter that specifies whether columns in the table should be included; may be null if all columns
      *            are to be included
      * @param mappers the mapping functions for columns; may be null if none of the columns are to be mapped to different values
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
     public TableSchema create(String schemaPrefix, Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
-        if ( schemaPrefix == null ) schemaPrefix = """";
+        if (schemaPrefix == null) schemaPrefix = """";
         // Build the schemas ...
         final TableId tableId = table.id();
         final String tableIdStr = tableId.toString();
@@ -285,7 +288,10 @@ protected ValueConverter[] convertersForColumns(Schema schema, TableId tableId,
                 if (mappers != null) {
                     ValueConverter mappingConverter = mappers.mappingConverterFor(tableId, column);
                     if (mappingConverter != null) {
-                        converter = (value) -> mappingConverter.convert(valueConverter.convert(value));
+                        converter = (value) -> {
+                            if (value != null) value = valueConverter.convert(value);
+                            return mappingConverter.convert(value);
+                        };
                     }
                 }
                 if (converter == null) converter = valueConverter;
@@ -376,7 +382,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
             case Types.NCLOB:
                 fieldBuilder = SchemaBuilder.string();
                 break;
-                
+
             // Variable-length string values
             case Types.VARCHAR:
             case Types.LONGVARCHAR:
@@ -424,7 +430,7 @@ protected void addField(SchemaBuilder builder, Column column, ColumnMapper mappe
         if (fieldBuilder != null) {
             if (mapper != null) {
                 // Let the mapper add properties to the schema ...
-                mapper.alterFieldSchema(column,fieldBuilder);
+                mapper.alterFieldSchema(column, fieldBuilder);
             }
             if (column.isOptional()) fieldBuilder.optional();
             builder.field(column.name(), fieldBuilder.build());
@@ -450,6 +456,14 @@ protected SchemaBuilder addOtherField(Column column, ColumnMapper mapper) {
     /**
      * Create a {@link ValueConverter} that can be used to convert row values for the given column into the Kafka Connect value
      * object described by the {@link Field field definition}.
+     * <p>
+     * Subclasses can override this method to specialize the behavior. The subclass method should do custom checks and
+     * conversions,
+     * and then delegate to this method implementation to handle all other cases.
+     * <p>
+     * Alternatively, subclasses can leave this method as-is and instead override one of the lower-level type-specific methods
+     * that this method calls (e.g., {@link #convertBinary(Column, Field, Object)},
+     * {@link #convertTinyInt(Column, Field, Object)}, etc.).
      * 
      * @param column the column describing the input values; never null
      * @param fieldDefn the definition for the field in a Kafka Connect {@link Schema} describing the output of the function;
@@ -461,94 +475,38 @@ protected ValueConverter createValueConverterFor(Column column, Field fieldDefn)
             case Types.NULL:
                 return (data) -> null;
             case Types.BIT:
+                return (data) -> convertBit(column, fieldDefn, data);
             case Types.BOOLEAN:
-                return (data) -> {
-                    if (data instanceof Boolean) return (Boolean) data;
-                    if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBoolean(column, fieldDefn, data);
 
             // Binary values ...
             case Types.BLOB:
             case Types.BINARY:
             case Types.VARBINARY:
             case Types.LONGVARBINARY:
-                return (data) -> (byte[]) data;
+                return (data) -> convertBinary(column, fieldDefn, data);
 
             // Numeric integers
             case Types.TINYINT:
-                return (data) -> {
-                    if (data instanceof Byte) return (Byte) data;
-                    if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertTinyInt(column, fieldDefn, data);
             case Types.SMALLINT:
-                return (data) -> {
-                    if (data instanceof Short) return (Short) data;
-                    if (data instanceof Integer) return new Short(((Integer) data).shortValue());
-                    if (data instanceof Long) return new Short(((Long) data).shortValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertSmallInt(column, fieldDefn, data);
             case Types.INTEGER:
-                return (data) -> {
-                    if (data instanceof Integer) return (Integer) data;
-                    if (data instanceof Short) return new Integer(((Short) data).intValue());
-                    if (data instanceof Long) return new Integer(((Long) data).intValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertInteger(column, fieldDefn, data);
             case Types.BIGINT:
-                return (data) -> {
-                    if (data instanceof Long) return (Long) data;
-                    if (data instanceof Integer) return new Long(((Integer) data).longValue());
-                    if (data instanceof Short) return new Long(((Short) data).longValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertBigInt(column, fieldDefn, data);
 
             // Numeric decimal numbers
             case Types.FLOAT:
+                return (data) -> convertFloat(column, fieldDefn, data);
             case Types.DOUBLE:
-                return (data) -> {
-                    if (data instanceof Double) return (Double) data;
-                    if (data instanceof Float) return new Double(((Float) data).doubleValue());
-                    if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
-                    if (data instanceof Long) return new Double(((Long) data).doubleValue());
-                    if (data instanceof Short) return new Double(((Short) data).doubleValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertDouble(column, fieldDefn, data);
             case Types.REAL:
-                return (data) -> {
-                    if (data instanceof Float) return (Float) data;
-                    if (data instanceof Double) return new Float(((Double) data).floatValue());
-                    if (data instanceof Integer) return new Float(((Integer) data).floatValue());
-                    if (data instanceof Long) return new Float(((Long) data).floatValue());
-                    if (data instanceof Short) return new Float(((Short) data).floatValue());
-                    return handleUnknownData(column, fieldDefn, data);
-                };
+                return (data) -> convertReal(column, fieldDefn, data);
             case Types.NUMERIC:
+                return (data) -> convertNumeric(column, fieldDefn, data);
             case Types.DECIMAL:
-                return (data) -> {
-                    BigDecimal decimal = null;
-                    if (data instanceof BigDecimal)
-                        decimal = (BigDecimal) data;
-                    else if (data instanceof Boolean)
-                        decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
-                    else if (data instanceof Short)
-                        decimal = new BigDecimal(((Short) data).intValue());
-                    else if (data instanceof Integer)
-                        decimal = new BigDecimal(((Integer) data).intValue());
-                    else if (data instanceof Long)
-                        decimal = BigDecimal.valueOf(((Long) data).longValue());
-                    else if (data instanceof Float)
-                        decimal = BigDecimal.valueOf(((Float) data).doubleValue());
-                    else if (data instanceof Double)
-                        decimal = BigDecimal.valueOf(((Double) data).doubleValue());
-                    else {
-                        handleUnknownData(column, fieldDefn, data);
-                    }
-                    return decimal;
-                };
+                return (data) -> convertDecimal(column, fieldDefn, data);
 
             // String values
             case Types.CHAR: // variable-length
@@ -561,26 +519,23 @@ else if (data instanceof Double)
             case Types.NCLOB: // fixed-length
             case Types.DATALINK:
             case Types.SQLXML:
-                return (data) -> data.toString();
+                return (data) -> convertString(column, fieldDefn, data);
 
             // Date and time values
             case Types.DATE:
-                return (data) -> convertDate(fieldDefn, data);
+                return (data) -> convertDate(column, fieldDefn, data);
             case Types.TIME:
-                return (data) -> convertTime(fieldDefn, data);
+                return (data) -> convertTime(column, fieldDefn, data);
             case Types.TIMESTAMP:
-                return (data) -> convertTimestamp(fieldDefn, data);
+                return (data) -> convertTimestamp(column, fieldDefn, data);
             case Types.TIME_WITH_TIMEZONE:
-                return (data) -> convertTimeWithZone(fieldDefn, data);
+                return (data) -> convertTimeWithZone(column, fieldDefn, data);
             case Types.TIMESTAMP_WITH_TIMEZONE:
-                return (data) -> convertTimestampWithZone(fieldDefn, data);
+                return (data) -> convertTimestampWithZone(column, fieldDefn, data);
 
             // Other types ...
             case Types.ROWID:
-                return (data) -> {
-                    java.sql.RowId rowId = (java.sql.RowId) data;
-                    return rowId.getBytes();
-                };
+                return (data) -> convertRowId(column, fieldDefn, data);
 
             // Unhandled types
             case Types.ARRAY:
@@ -620,11 +575,13 @@ protected Object handleUnknownData(Column column, Field fieldDefn, Object data)
      * This method handles several types of objects, including {@link OffsetDateTime}, {@link java.sql.Timestamp},
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestampWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimestampWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetDateTime dateTime = null;
         if (data instanceof OffsetDateTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -679,11 +636,13 @@ protected OffsetDateTime unexpectedTimestampWithZone(Object value, Field fieldDe
      * {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types have date components, those date
      * components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimeWithZone(Field fieldDefn, Object data) {
+    protected Object convertTimeWithZone(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         OffsetTime time = null;
         if (data instanceof OffsetTime) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -731,11 +690,13 @@ protected OffsetTime unexpectedTimeWithZone(Object value, Field fieldDefn) {
      * but no time zone info. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such
      * as {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTimestamp(Field fieldDefn, Object data) {
+    protected Object convertTimestamp(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Timestamp) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -782,11 +743,13 @@ protected java.util.Date unexpectedTimestamp(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalTime}, and {@link java.time.LocalDateTime}. If any of the types might
      * have date components, those date components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertTime(Field fieldDefn, Object data) {
+    protected Object convertTime(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Time) {
             // JDBC specification indicates that this will be the canonical object for this JDBC type.
@@ -834,11 +797,13 @@ protected java.util.Date unexpectedTime(Object value, Field fieldDefn) {
      * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
      * have time components, those time components are ignored.
      * 
+     * @param column the column definition describing the {@code data} value; never null
      * @param fieldDefn the field definition; never null
      * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
      * @return the converted value, or null if the conversion could not be made
      */
-    protected Object convertDate(Field fieldDefn, Object data) {
+    protected Object convertDate(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
         java.util.Date date = null;
         if (data instanceof java.sql.Date) {
             // JDBC specification indicates that this will be the nominal object for this JDBC type.
@@ -882,4 +847,286 @@ protected java.util.Date unexpectedDate(Object value, Field fieldDefn) {
                     fieldDefn.schema(), value.getClass(), value);
         return null;
     }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * <p>
+     * Per the JDBC specification, databases should return {@link java.sql.Date} instances that have no notion of time or
+     * time zones. This method handles {@link java.sql.Date} objects plus any other standard date-related objects such as
+     * {@link java.util.Date}, {@link java.time.LocalDate}, and {@link java.time.LocalDateTime}. If any of the types might
+     * have time components, those time components are ignored.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBinary(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof char[]) {
+            data = new String((char[]) data); // convert to string
+        }
+        if (data instanceof String) {
+            // This was encoded as a hexadecimal string, but we receive it as a normal string ...
+            data = ((String) data).getBytes();
+        }
+        if (data instanceof byte[]) {
+            return ByteBuffer.wrap((byte[])data);
+        }
+        // An unexpected value
+        return unexpectedBinary(data, fieldDefn);
+    }
+
+    /**
+     * Handle the unexpected value from a row with a column type of {@link Types#BLOB}, {@link Types#BINARY},
+     * {@link Types#VARBINARY}, {@link Types#LONGVARBINARY}.
+     * 
+     * @param value the binary value for which no conversion was found; never null
+     * @param fieldDefn the field definition in the Kafka Connect schema; never null
+     * @return the converted value, or null
+     * @see #convertBinary(Column, Field, Object)
+     */
+    protected byte[] unexpectedBinary(Object value, Field fieldDefn) {
+        LOGGER.warn(""Unexpected JDBC BINARY value for field {} with schema {}: class={}, value={}"", fieldDefn.name(),
+                    fieldDefn.schema(), value.getClass(), value);
+        return null;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#TINYINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertTinyInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Byte) return data;
+        if (data instanceof Boolean) return ((Boolean) data).booleanValue() ? (byte) 1 : (byte) 0;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#SMALLINT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertSmallInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Short) return data;
+        if (data instanceof Integer) return new Short(((Integer) data).shortValue());
+        if (data instanceof Long) return new Short(((Long) data).shortValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertInteger(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Integer) return data;
+        if (data instanceof Short) return new Integer(((Short) data).intValue());
+        if (data instanceof Long) return new Integer(((Long) data).intValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#INTEGER}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBigInt(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Long) return data;
+        if (data instanceof Integer) return new Long(((Integer) data).longValue());
+        if (data instanceof Short) return new Long(((Short) data).longValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#FLOAT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertFloat(Column column, Field fieldDefn, Object data) {
+        return convertDouble(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#DOUBLE}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDouble(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Double) return data;
+        if (data instanceof Float) return new Double(((Float) data).doubleValue());
+        if (data instanceof Integer) return new Double(((Integer) data).doubleValue());
+        if (data instanceof Long) return new Double(((Long) data).doubleValue());
+        if (data instanceof Short) return new Double(((Short) data).doubleValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#REAL}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertReal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Float) return data;
+        if (data instanceof Double) return new Float(((Double) data).floatValue());
+        if (data instanceof Integer) return new Float(((Integer) data).floatValue());
+        if (data instanceof Long) return new Float(((Long) data).floatValue());
+        if (data instanceof Short) return new Float(((Short) data).floatValue());
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertNumeric(Column column, Field fieldDefn, Object data) {
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#NUMERIC}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertDecimal(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        BigDecimal decimal = null;
+        if (data instanceof BigDecimal)
+            decimal = (BigDecimal) data;
+        else if (data instanceof Boolean)
+            decimal = new BigDecimal(((Boolean) data).booleanValue() ? 1 : 0);
+        else if (data instanceof Short)
+            decimal = new BigDecimal(((Short) data).intValue());
+        else if (data instanceof Integer)
+            decimal = new BigDecimal(((Integer) data).intValue());
+        else if (data instanceof Long)
+            decimal = BigDecimal.valueOf(((Long) data).longValue());
+        else if (data instanceof Float)
+            decimal = BigDecimal.valueOf(((Float) data).doubleValue());
+        else if (data instanceof Double)
+            decimal = BigDecimal.valueOf(((Double) data).doubleValue());
+        else {
+            return handleUnknownData(column, fieldDefn, data);
+        }
+        return decimal;
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#CHAR}, {@link Types#VARCHAR},
+     * {@link Types#LONGVARCHAR}, {@link Types#CLOB}, {@link Types#NCHAR}, {@link Types#NVARCHAR}, {@link Types#LONGNVARCHAR},
+     * {@link Types#NCLOB}, {@link Types#DATALINK}, and {@link Types#SQLXML}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertString(Column column, Field fieldDefn, Object data) {
+        return data == null ? null : data.toString();
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#ROWID}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertRowId(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof java.sql.RowId) {
+            java.sql.RowId row = (java.sql.RowId)data;
+            return ByteBuffer.wrap(row.getBytes());
+        }
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BIT}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBit(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
+
+    /**
+     * Converts a value object for an expected JDBC type of {@link Types#BOOLEAN}.
+     * 
+     * @param column the column definition describing the {@code data} value; never null
+     * @param fieldDefn the field definition; never null
+     * @param data the data object to be converted into a {@link Date Kafka Connect date} type; never null
+     * @return the converted value, or null if the conversion could not be made
+     */
+    protected Object convertBoolean(Column column, Field fieldDefn, Object data) {
+        if (data == null) return null;
+        if (data instanceof Boolean) return data;
+        if (data instanceof Short) return ((Short) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Integer) return ((Integer) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        if (data instanceof Long) return ((Long) data).intValue() == 0 ? Boolean.FALSE : Boolean.TRUE;
+        return handleUnknownData(column, fieldDefn, data);
+    }
 }",2016-06-07T22:53:07Z,129
"@@ -7,8 +7,13 @@
 
 import static org.junit.Assert.fail;
 
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.HashMap;
+import java.util.Iterator;
 import java.util.Map;
+import java.util.Objects;
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
@@ -47,18 +52,18 @@ public class VerifyRecord {
     private static final MockSchemaRegistryClient schemaRegistry = new MockSchemaRegistryClient();
     private static final AvroConverter avroKeyConverter = new AvroConverter(schemaRegistry);
     private static final AvroConverter avroValueConverter = new AvroConverter(schemaRegistry);
-    
+
     static {
-        Map<String,Object> config = new HashMap<>();
-        config.put(""schemas.enable"",Boolean.TRUE.toString());
-        config.put(""schemas.cache.size"",100);
+        Map<String, Object> config = new HashMap<>();
+        config.put(""schemas.enable"", Boolean.TRUE.toString());
+        config.put(""schemas.cache.size"", 100);
         keyJsonConverter.configure(config, true);
         keyJsonDeserializer.configure(config, true);
         valueJsonConverter.configure(config, false);
         valueJsonDeserializer.configure(config, false);
 
         config = new HashMap<>();
-        config.put(""schema.registry.url"",""http://fake-url"");
+        config.put(""schema.registry.url"", ""http://fake-url"");
         avroKeyConverter.configure(config, false);
         avroValueConverter.configure(config, false);
     }
@@ -276,73 +281,97 @@ public static void isValid(SourceRecord record) {
         SchemaAndValue valueWithSchema = null;
         SchemaAndValue avroKeyWithSchema = null;
         SchemaAndValue avroValueWithSchema = null;
+        String msg = null;
         try {
             // The key should never be null ...
+            msg = ""checking key is not null"";
             assertThat(record.key()).isNotNull();
             assertThat(record.keySchema()).isNotNull();
 
             // If the value is not null there must be a schema; otherwise, the schema should also be null ...
             if (record.value() == null) {
+                msg = ""checking value schema is null"";
                 assertThat(record.valueSchema()).isNull();
             } else {
+                msg = ""checking value schema is not null"";
                 assertThat(record.valueSchema()).isNotNull();
             }
 
             // First serialize and deserialize the key ...
+            msg = ""serializing key using JSON converter"";
             byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using JSON deserializer"";
             keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            msg = ""deserializing key using JSON converter"";
             keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            msg = ""comparing key schema to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with JSON converter"";
             assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // then the value ...
+            msg = ""serializing value using JSON converter"";
             byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using JSON deserializer"";
             valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            msg = ""deserializing value using JSON converter"";
             valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with JSON converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
             // Serialize and deserialize the key using the Avro converter, and check that we got the same result ...
+            msg = ""serializing key using Avro converter"";
             byte[] avroKeyBytes = avroValueConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            msg = ""deserializing key using Avro converter"";
             avroKeyWithSchema = avroValueConverter.toConnectData(record.topic(), avroKeyBytes);
-            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
-            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            msg = ""comparing key schema to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.schema(),record.keySchema());
+            msg = ""comparing key to that serialized/deserialized with Avro converter"";
+            assertEquals(keyWithSchema.value(),record.key());
+            msg = ""comparing key to its schema"";
             schemaMatchesStruct(keyWithSchema);
 
             // Serialize and deserialize the value using the Avro converter, and check that we got the same result ...
+            msg = ""serializing value using Avro converter"";
             byte[] avroValueBytes = avroValueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            msg = ""deserializing value using Avro converter"";
             avroValueWithSchema = avroValueConverter.toConnectData(record.topic(), avroValueBytes);
-            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
-            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            msg = ""comparing value schema to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.schema(),record.valueSchema());
+            msg = ""comparing value to that serialized/deserialized with Avro converter"";
+            assertEquals(valueWithSchema.value(),record.value());
+            msg = ""comparing value to its schema"";
             schemaMatchesStruct(valueWithSchema);
-            
+
         } catch (Throwable t) {
             Testing.Print.enable();
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
             Testing.printError(t);
-            if (keyJson == null ){
-                Testing.print(""error deserializing key from JSON: "" + SchemaUtil.asString(record.key()));
-            } else if (keyWithSchema == null ){
-                Testing.print(""error using JSON converter on key: "" + prettyJson(keyJson));
-            } else if (avroKeyWithSchema == null ){
-                Testing.print(""error using Avro converter on key: "" + prettyJson(keyJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            Testing.print(""error "" + msg);
+            Testing.print(""  key: "" + SchemaUtil.asString(record.key()));
+            Testing.print(""  key deserialized from JSON: "" + prettyJson(keyJson));
+            if (keyWithSchema != null) {
+                Testing.print(""  key to/from JSON: "" + SchemaUtil.asString(keyWithSchema.value()));
             }
-
-            if (valueJson == null ){
-                Testing.print(""error deserializing value from JSON: "" + SchemaUtil.asString(record.value()));
-            } else if (valueWithSchema == null ){
-                Testing.print(""error using JSON converter on value: "" + prettyJson(valueJson));
-            } else if (avroValueWithSchema == null ){
-                Testing.print(""error using Avro converter on value: "" + prettyJson(valueJson));
-            } else {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
+            if (avroKeyWithSchema != null) {
+                Testing.print(""  key to/from Avro: "" + SchemaUtil.asString(avroKeyWithSchema.value()));
+            }
+            Testing.print(""  value: "" + SchemaUtil.asString(record.value()));
+            Testing.print(""  value deserialized from JSON: "" + prettyJson(valueJson));
+            if (valueWithSchema != null) {
+                Testing.print(""  value to/from JSON: "" + SchemaUtil.asString(valueWithSchema.value()));
+            }
+            if (avroValueWithSchema != null) {
+                Testing.print(""  value to/from Avro: "" + SchemaUtil.asString(avroValueWithSchema.value()));
             }
             if (t instanceof AssertionError) throw t;
-            fail(t.getMessage());
+            fail(""error "" + msg + "": "" + t.getMessage());
         }
     }
 
@@ -390,5 +419,135 @@ protected static String prettyJson(JsonNode json) {
             return null;
         }
     }
+    
+    // The remaining methods are needed simply because of the KAFKA-3803, so our comparisons cannot rely upon Struct.equals
+    
+    protected static void assertEquals( Object o1, Object o2 ) {
+        // assertThat(o1).isEqualTo(o2);
+        if ( !equals(o1,o2) ) {
+            fail(SchemaUtil.asString(o1) + "" was not equal to "" + SchemaUtil.asString(o2));
+        }
+    }
+    
+    @SuppressWarnings(""unchecked"")
+    protected static boolean equals( Object o1, Object o2 ) {
+        if ( o1 == o2 ) return true;
+        if (o1 == null) return o2 == null ? true : false;
+        if (o2 == null ) return false;
+        if ( o1 instanceof ByteBuffer ) {
+            o1 = ((ByteBuffer)o1).array();
+        }
+        if ( o2 instanceof ByteBuffer ) {
+            o2 = ((ByteBuffer)o2).array();
+        }
+        if ( o1 instanceof byte[] && o2 instanceof byte[] ) {
+            boolean result = Arrays.equals((byte[])o1,(byte[])o2);
+            return result;
+        }
+        if ( o1 instanceof Object[] && o2 instanceof Object[] ) {
+            boolean result = deepEquals((Object[])o1,(Object[])o2);
+            return result;
+        }
+        if ( o1 instanceof Map && o2 instanceof Map ) {
+            Map<String,Object> m1 = (Map<String,Object>)o1;
+            Map<String,Object> m2 = (Map<String,Object>)o2;
+            if ( !m1.keySet().equals(m2.keySet())) return false;
+            for ( Map.Entry<String, Object> entry : m1.entrySet()) {
+                Object v1 = entry.getValue();
+                Object v2 = m2.get(entry.getKey());
+                if ( !equals(v1,v2) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Collection && o2 instanceof Collection ) {
+            Collection<Object> m1 = (Collection<Object>)o1;
+            Collection<Object> m2 = (Collection<Object>)o2;
+            if ( m1.size() != m2.size() ) return false;
+            Iterator<?> iter1 = m1.iterator();
+            Iterator<?> iter2 = m2.iterator();
+            while ( iter1.hasNext() && iter2.hasNext() ) {
+                if ( !equals(iter1.next(),iter2.next()) ) return false;
+            }
+            return true;
+        }
+        if ( o1 instanceof Struct && o2 instanceof Struct ) {
+            // Unfortunately, the Struct.equals() method has a bug in that it is not using Arrays.deepEquals(...) to
+            // compare values in two Struct objects. The result is that the equals only works if the values of the
+            // first level Struct are non arrays; otherwise, the array values are compared using == and that obviously
+            // does not work for non-primitive values.
+            Struct struct1 = (Struct) o1;
+            Struct struct2 = (Struct) o2;
+            if (! Objects.equals(struct1.schema(),struct2.schema()) ) {
+                return false;
+            }
+            Object[] array1 = valuesFor(struct1);
+            Object[] array2 = valuesFor(struct2);
+            boolean result = deepEquals(array1, array2);
+            return result;
+        }
+        return Objects.equals(o1, o2);
+    }
+    
+    private static Object[] valuesFor( Struct struct ) {
+        Object[] array = new Object[struct.schema().fields().size()];
+        int index = 0;
+        for ( Field field : struct.schema().fields() ) {
+            array[index] = struct.get(field);
+            ++index;
+        }
+        return array;
+    }
 
+    private static boolean deepEquals(Object[] a1, Object[] a2) {
+        if (a1 == a2)
+            return true;
+        if (a1 == null || a2==null)
+            return false;
+        int length = a1.length;
+        if (a2.length != length)
+            return false;
+
+        for (int i = 0; i < length; i++) {
+            Object e1 = a1[i];
+            Object e2 = a2[i];
+
+            if (e1 == e2)
+                continue;
+            if (e1 == null)
+                return false;
+
+            // Figure out whether the two elements are equal
+            boolean eq = deepEquals0(e1, e2);
+
+            if (!eq)
+                return false;
+        }
+        return true;
+    }
+
+    private static boolean deepEquals0(Object e1, Object e2) {
+        assert e1 != null;
+        boolean eq;
+        if (e1 instanceof Object[] && e2 instanceof Object[])
+            eq = deepEquals ((Object[]) e1, (Object[]) e2);
+        else if (e1 instanceof byte[] && e2 instanceof byte[])
+            eq = Arrays.equals((byte[]) e1, (byte[]) e2);
+        else if (e1 instanceof short[] && e2 instanceof short[])
+            eq = Arrays.equals((short[]) e1, (short[]) e2);
+        else if (e1 instanceof int[] && e2 instanceof int[])
+            eq = Arrays.equals((int[]) e1, (int[]) e2);
+        else if (e1 instanceof long[] && e2 instanceof long[])
+            eq = Arrays.equals((long[]) e1, (long[]) e2);
+        else if (e1 instanceof char[] && e2 instanceof char[])
+            eq = Arrays.equals((char[]) e1, (char[]) e2);
+        else if (e1 instanceof float[] && e2 instanceof float[])
+            eq = Arrays.equals((float[]) e1, (float[]) e2);
+        else if (e1 instanceof double[] && e2 instanceof double[])
+            eq = Arrays.equals((double[]) e1, (double[]) e2);
+        else if (e1 instanceof boolean[] && e2 instanceof boolean[])
+            eq = Arrays.equals((boolean[]) e1, (boolean[]) e2);
+        else
+            eq = equals(e1,e2);
+        return eq;
+    }
 }",2016-06-07T22:53:07Z,77
"@@ -63,6 +63,7 @@ public class MySqlSchema {
     private final TableSchemaBuilder schemaBuilder;
     private final DdlChanges ddlChanges;
     private final String serverName;
+    private final String schemaPrefix;
     private Tables tables;
 
     /**
@@ -78,7 +79,13 @@ public MySqlSchema(Configuration config, String serverName) {
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
         this.schemaBuilder = new TableSchemaBuilder();
+        if ( serverName != null ) serverName = serverName.trim();
         this.serverName = serverName;
+        if ( this.serverName == null || serverName.isEmpty() ) {
+            this.schemaPrefix = """";
+        } else {
+            this.schemaPrefix = serverName.endsWith(""."") ? serverName : serverName + ""."";
+        }
 
         // Create and configure the database history ...
         this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
@@ -253,7 +260,7 @@ protected void refreshSchemas() {
         // Create TableSchema instances for any existing table ...
         this.tables.tableIds().forEach(id -> {
             Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(serverName,table, filters.columnFilter(), filters.columnMappers());
+            TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
             tableSchemaByTableId.put(id, schema);
         });
     }
@@ -323,7 +330,7 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
             if (table == null) { // removed
                 tableSchemaByTableId.remove(tableId);
             } else {
-                TableSchema schema = schemaBuilder.create(serverName, table, filters.columnFilter(), filters.columnMappers());
+                TableSchema schema = schemaBuilder.create(schemaPrefix, table, filters.columnFilter(), filters.columnMappers());
                 tableSchemaByTableId.put(tableId, schema);
             }
         });",2016-06-10T02:08:09Z,19
"@@ -30,6 +30,10 @@ public Configurator with(Field field, boolean value) {
         return this;
     }
 
+    public Configurator serverName(String serverName) {
+        return with(MySqlConnectorConfig.SERVER_NAME, serverName);
+    }
+
     public Configurator includeDatabases(String regexList) {
         return with(MySqlConnectorConfig.DATABASE_WHITELIST, regexList);
     }",2016-06-10T02:08:09Z,110
"@@ -29,6 +29,7 @@
 public class MySqlSchemaTest {
 
     private static final Path TEST_FILE_PATH = Testing.Files.createTestingPath(""dbHistory.log"");
+    private static final String SERVER_NAME = ""test-server"";
 
     private Configurator build;
     private MySqlSchema mysql;
@@ -55,7 +56,7 @@ public void afterEach() {
 
     @Test
     public void shouldApplyDdlStatementsAndRecover() {
-        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).createSchemas();
+        mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH).serverName(SERVER_NAME).createSchemas();
         mysql.start();
 
         // Testing.Print.enable();
@@ -74,6 +75,7 @@ public void shouldApplyDdlStatementsAndRecover() {
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .excludeBuiltInTables()
                      .createSchemas();
@@ -99,6 +101,7 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
     @Test
     public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
         mysql = build.storeDatabaseHistoryInFile(TEST_FILE_PATH)
+                     .serverName(SERVER_NAME)
                      .includeDatabases(""connector_test"")
                      .includeBuiltInTables()
                      .createSchemas();
@@ -124,7 +127,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
     protected void assertTableIncluded(String fullyQualifiedTableName) {
         TableId tableId = TableId.parse(fullyQualifiedTableName);
         assertThat(mysql.tables().forTable(tableId)).isNotNull();
-        assertThat(mysql.schemaFor(tableId)).isNotNull();
+        TableSchema tableSchema = mysql.schemaFor(tableId);
+        assertThat(tableSchema).isNotNull();
+        assertThat(tableSchema.keySchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Key"");
+        assertThat(tableSchema.valueSchema().name()).isEqualTo(SERVER_NAME + ""."" + fullyQualifiedTableName + "".Value"");
     }
 
     protected void assertTableExcluded(String fullyQualifiedTableName) {",2016-06-10T02:08:09Z,19
"@@ -52,6 +52,7 @@ public class BinlogReader extends AbstractReader {
     private final SourceInfo source;
     private final EnumMap<EventType, BlockingConsumer<Event>> eventHandlers = new EnumMap<>(EventType.class);
     private BinaryLogClient client;
+    private int startingRowNumber = 0;
 
     /**
      * Create a binlog reader.
@@ -93,12 +94,15 @@ protected void doStart() {
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
 
-        // And set the client to start from that point ...
+        // The 'source' object holds the starting point in the binlog where we should start reading,
+        // set set the client to start from that point ...
         client.setGtidSet(source.gtidSet()); // may be null
         client.setBinlogFilename(source.binlogFilename());
-        client.setBinlogPosition(source.binlogPosition());
-        // The event row number will be used when processing the first event ...
+        client.setBinlogPosition(source.nextBinlogPosition());
 
+        // Set the starting row number, which is the next row number to be read ...
+        startingRowNumber = source.nextEventRowNumber();
+        
         // Start the log reader, which starts background threads ...
         long timeoutInMilliseconds = context.timeoutInMilliseconds();
         try {
@@ -157,21 +161,19 @@ protected void handleEvent(Event event) {
             } else {
                 rotateEventData = (RotateEventData) eventData;
             }
-            source.setBinlogFilename(rotateEventData.getBinlogFilename());
-            source.setBinlogPosition(rotateEventData.getBinlogPosition());
-            source.setRowInEvent(0);
+            source.setBinlogStartPoint(rotateEventData.getBinlogFilename(), rotateEventData.getBinlogPosition());
         } else if (eventHeader instanceof EventHeaderV4) {
             EventHeaderV4 trackableEventHeader = (EventHeaderV4) eventHeader;
-            long nextBinlogPosition = trackableEventHeader.getNextPosition();
-            if (nextBinlogPosition > 0) {
-                source.setBinlogPosition(nextBinlogPosition);
-                source.setRowInEvent(0);
-            }
+            source.setEventPosition(trackableEventHeader.getPosition(), trackableEventHeader.getEventLength());
         }
 
         // If there is a handler for this event, forward the event to it ...
         try {
+            // Forward the event to the handler ...
             eventHandlers.getOrDefault(eventType, this::ignoreEvent).accept(event);
+            
+            // And after that event has been processed, always set the starting row number to 0 ...
+            startingRowNumber = 0;
         } catch (InterruptedException e) {
             // Most likely because this reader was stopped and our thread was interrupted ...
             Thread.interrupted();
@@ -301,7 +303,11 @@ protected void handleInsert(Event event) throws InterruptedException {
         if (recordMaker != null) {
             List<Serializable[]> rows = write.getRows();
             Long ts = context.clock().currentTimeInMillis();
-            int count = recordMaker.createEach(rows, ts);
+            int count = 0;
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
+                count += recordMaker.create(rows.get(row), ts, row, numRows);
+            }
             logger.debug(""Recorded {} insert records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping insert row event: {}"", event);
@@ -324,11 +330,12 @@ protected void handleUpdate(Event event) throws InterruptedException {
             List<Entry<Serializable[], Serializable[]>> rows = update.getRows();
             Long ts = context.clock().currentTimeInMillis();
             int count = 0;
-            for (int row = 0; row != rows.size(); ++row) {
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
                 Map.Entry<Serializable[], Serializable[]> changes = rows.get(row);
                 Serializable[] before = changes.getKey();
                 Serializable[] after = changes.getValue();
-                count += recordMaker.update(before, after, ts, row);
+                count += recordMaker.update(before, after, ts, row, numRows);
             }
             logger.debug(""Recorded {} update records for event: {}"", count, event);
         } else {
@@ -350,7 +357,11 @@ protected void handleDelete(Event event) throws InterruptedException {
         if (recordMaker != null) {
             List<Serializable[]> rows = deleted.getRows();
             Long ts = context.clock().currentTimeInMillis();
-            int count = recordMaker.deleteEach(rows, ts);
+            int count = 0;
+            int numRows = rows.size();
+            for (int row = startingRowNumber; row != numRows; ++row) {
+                count += recordMaker.delete(rows.get(row), ts, row, numRows);
+            }
             logger.debug(""Recorded {} delete records for event: {}"", count, event);
         } else {
             logger.debug(""Skipping delete row event: {}"", event);",2016-06-14T22:43:58Z,67
"@@ -107,7 +107,7 @@ public void start(Map<String, String> props) {
             if (taskContext.isSnapshotNeverAllowed()) {
                 // We're not allowed to take a snapshot, so instead we have to assume that the binlog contains the
                 // full history of the database.
-                source.setBinlogFilename("""");// start from the beginning of the binlog
+                source.setBinlogStartPoint("""", 0L);// start from the beginning of the binlog
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;",2016-06-14T22:43:58Z,68
"@@ -7,7 +7,6 @@
 
 import java.util.BitSet;
 import java.util.HashMap;
-import java.util.Iterator;
 import java.util.Map;
 import java.util.Objects;
 import java.util.Set;
@@ -152,7 +151,8 @@ public void regenerate() {
      */
     public boolean assign(long tableNumber, TableId id) {
         Long existingTableNumber = tableNumbersByTableId.get(id);
-        if ( existingTableNumber != null && existingTableNumber.longValue() == tableNumber && convertersByTableNumber.containsKey(tableNumber)) {
+        if (existingTableNumber != null && existingTableNumber.longValue() == tableNumber
+                && convertersByTableNumber.containsKey(tableNumber)) {
             // This is the exact same table number for the same table, so do nothing ...
             return true;
         }
@@ -171,15 +171,15 @@ public boolean assign(long tableNumber, TableId id) {
         Converter converter = new Converter() {
 
             @Override
-            public int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int read(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                             BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 Object key = tableSchema.keyFromColumnData(row);
                 Struct value = tableSchema.valueFromColumnData(row);
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
                             keySchema, key, envelope.schema(), envelope.read(value, origin, ts));
@@ -190,15 +190,15 @@ public int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedC
             }
 
             @Override
-            public int insert(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int insert(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 Object key = tableSchema.keyFromColumnData(row);
                 Struct value = tableSchema.valueFromColumnData(row);
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
                             keySchema, key, envelope.schema(), envelope.create(value, origin, ts));
@@ -209,7 +209,7 @@ public int insert(SourceInfo source, Object[] row, int rowNumber, BitSet include
             }
 
             @Override
-            public int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, BitSet includedColumns, long ts,
+            public int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 int count = 0;
@@ -220,7 +220,7 @@ public int update(SourceInfo source, Object[] before, Object[] after, int rowNum
                     Struct valueBefore = tableSchema.valueFromColumnData(before);
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     if (key != null && !Objects.equals(key, oldKey)) {
                         // The key has indeed changed, so first send a create event ...
@@ -251,7 +251,7 @@ record = new SourceRecord(partition, offset, topicName, partitionNum, keySchema,
             }
 
             @Override
-            public int delete(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts,
+            public int delete(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                               BlockingConsumer<SourceRecord> consumer)
                     throws InterruptedException {
                 int count = 0;
@@ -260,7 +260,7 @@ public int delete(SourceInfo source, Object[] row, int rowNumber, BitSet include
                 if (value != null || key != null) {
                     Schema keySchema = tableSchema.keySchema();
                     Map<String, ?> partition = source.partition();
-                    Map<String, ?> offset = source.offset(rowNumber);
+                    Map<String, ?> offset = source.offset(rowNumber, numberOfRows);
                     Struct origin = source.struct();
                     // Send a delete message ...
                     SourceRecord record = new SourceRecord(partition, offset, topicName, partitionNum,
@@ -275,7 +275,7 @@ record = new SourceRecord(partition, offset, topicName, partitionNum,
                 }
                 return count;
             }
-            
+
             @Override
             public String toString() {
                 return ""RecordMaker.Converter("" + id + "")"";
@@ -307,17 +307,20 @@ protected Struct schemaChangeRecordValue(String databaseName, String ddlStatemen
     }
 
     protected static interface Converter {
-        int read(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int read(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                 BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int insert(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int insert(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                   BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, BitSet includedColumns, long ts,
+        int update(SourceInfo source, Object[] before, Object[] after, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
                    BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
-        int delete(SourceInfo source, Object[] row, int rowNumber, BitSet includedColumns, long ts, BlockingConsumer<SourceRecord> consumer)
+        int delete(SourceInfo source, Object[] row, int rowNumber, int numberOfRows, BitSet includedColumns, long ts,
+                   BlockingConsumer<SourceRecord> consumer)
                 throws InterruptedException;
 
     }
@@ -346,7 +349,7 @@ protected RecordsForTable(Converter converter, BitSet includedColumns, BlockingC
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int read(Object[] row, long ts) throws InterruptedException {
-            return read(row, ts, 0);
+            return read(row, ts, 0, 1);
         }
 
         /**
@@ -356,29 +359,12 @@ public int read(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int read(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.read(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#READ read} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int readEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += read(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int read(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.read(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -391,7 +377,7 @@ public int readEach(Iterable<? extends Object[]> rows, long ts) throws Interrupt
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int create(Object[] row, long ts) throws InterruptedException {
-            return create(row, ts, 0);
+            return create(row, ts, 0, 1);
         }
 
         /**
@@ -401,29 +387,12 @@ public int create(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int create(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.insert(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#CREATE create} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int createEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += create(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int create(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.insert(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -438,7 +407,7 @@ public int createEach(Iterable<? extends Object[]> rows, long ts) throws Interru
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int update(Object[] before, Object[] after, long ts) throws InterruptedException {
-            return update(before, after, ts, 0);
+            return update(before, after, ts, 0, 1);
         }
 
         /**
@@ -450,11 +419,12 @@ public int update(Object[] before, Object[] after, long ts) throws InterruptedEx
          *            definition in the {@link MySqlSchema}
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int update(Object[] before, Object[] after, long ts, int rowNumber) throws InterruptedException {
-            return converter.update(source, before, after, rowNumber, includedColumns, ts, consumer);
+        public int update(Object[] before, Object[] after, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.update(source, before, after, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
 
         /**
@@ -467,7 +437,7 @@ public int update(Object[] before, Object[] after, long ts, int rowNumber) throw
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
         public int delete(Object[] row, long ts) throws InterruptedException {
-            return delete(row, ts, 0);
+            return delete(row, ts, 0, 1);
         }
 
         /**
@@ -477,29 +447,12 @@ public int delete(Object[] row, long ts) throws InterruptedException {
          *            {@link MySqlSchema}.
          * @param ts the timestamp for this row
          * @param rowNumber the number of this row; must be 0 or more
+         * @param numberOfRows the total number of rows to be read; must be 1 or more
          * @return the number of records produced; will be 0 or more
          * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
          */
-        public int delete(Object[] row, long ts, int rowNumber) throws InterruptedException {
-            return converter.delete(source, row, rowNumber, includedColumns, ts, consumer);
-        }
-
-        /**
-         * Produce a {@link io.debezium.data.Envelope.Operation#DELETE delete} record for each of the rows.
-         * 
-         * @param rows the rows, with values in the same order as the columns in the {@link Table} definition in the
-         *            {@link MySqlSchema}.
-         * @param ts the timestamp for this row
-         * @return the number of records produced; will be 0 or more
-         * @throws InterruptedException if this thread is interrupted while waiting to give a source record to the consumer
-         */
-        public int deleteEach(Iterable<? extends Object[]> rows, long ts) throws InterruptedException {
-            int result = 0;
-            int rowNumber = -1;
-            for (Iterator<? extends Object[]> iterator = rows.iterator(); iterator.hasNext();) {
-                result += delete(iterator.next(), ts, ++rowNumber);
-            }
-            return result;
+        public int delete(Object[] row, long ts, int rowNumber, int numberOfRows) throws InterruptedException {
+            return converter.delete(source, row, rowNumber, numberOfRows, includedColumns, ts, consumer);
         }
     }
 }",2016-06-14T22:43:58Z,122
"@@ -186,9 +186,11 @@ protected void execute() {
             sql.set(""SHOW MASTER STATUS"");
             mysql.query(sql.get(), rs -> {
                 if (rs.next()) {
-                    source.setBinlogFilename(rs.getString(1));
-                    source.setBinlogPosition(rs.getLong(2));
-                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                    String binlogFilename = rs.getString(1);
+                    long binlogPosition = rs.getLong(2);
+                    String gtidSet = rs.getString(5);// GTID set, may be null, blank, or contain a GTID set
+                    source.setBinlogStartPoint(binlogFilename, binlogPosition);
+                    source.setGtidSet(gtidSet);
                     source.startSnapshot();
                 }
             });",2016-06-14T22:43:58Z,62
"@@ -33,16 +33,18 @@
  * 
  * <p>
  * The {@link #offset() source offset} information describes how much of the database's binary log the source the change detector
- * has processed. Here's a JSON-like representation of an example:
+ * has already processed, and it includes the {@link #binlogFilename() binlog filename}, the {@link #nextBinlogPosition() next
+ * position} in the binlog to start reading, and the {@link #nextEventRowNumber() next event row number}. Here's a JSON-like
+ * representation of an example:
  * 
  * <pre>
  * {
  *     ""server_id"": 112233,
- *     ""ts_sec"": 1465236179,
- *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
- *     ""file"" = ""mysql-bin.000003"",
- *     ""pos"" = 105586,
- *     ""row"" = 0,
+ *     ""ts_sec"": 1465937,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 990,
+ *     ""row"": 0,
  *     ""snapshot"": true
  * }
  * </pre>
@@ -53,20 +55,21 @@
  * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
  * 
  * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
- * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
- * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
- * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
- * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
- * an event that corresponds to the above partition and offset:
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), the
+ * {@link #lastBinlogPosition() position} of the event (and {@link #lastEventRowNumber() row number} within the event) inside
+ * the {@link #binlogFilename() binlog file}. When GTIDs are enabled, it also includes the GTID of the transaction in which the
+ * event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for events produced when the connector is in the
+ * middle of a snapshot. Here's a JSON-like representation of the source for an event that corresponds to the above partition and
+ * offset:
  * 
  * <pre>
  * {
  *     ""name"": ""production-server"",
  *     ""server_id"": 112233,
- *     ""ts_sec"": 1465236179,
+ *     ""ts_sec"": 1465937,
  *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
  *     ""file"": ""mysql-bin.000003"",
- *     ""pos"" = 105586,
+ *     ""pos"" = 1081,
  *     ""row"": 0,
  *     ""snapshot"": true
  * }
@@ -109,8 +112,10 @@ final class SourceInfo {
     private String gtidSet;
     private String binlogGtid;
     private String binlogFilename;
-    private long binlogPosition = 4;
-    private int eventRowNumber = 0;
+    private long lastBinlogPosition = 0;
+    private int lastEventRowNumber = 0;
+    private long nextBinlogPosition = 4;
+    private int nextEventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
     private long binlogTimestampSeconds = 0;
@@ -150,6 +155,32 @@ public Map<String, String> partition() {
      * @return a copy of the current offset; never null
      */
     public Map<String, ?> offset() {
+        return offsetUsingPosition(nextBinlogPosition);
+    }
+
+    /**
+     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
+     * describes the position within the source where we have last read.
+     * 
+     * @param eventRowNumber the 0-based row number within the event being processed
+     * @param totalNumberOfRows the total number of rows within the event being processed
+     * @return a copy of the current offset; never null
+     */
+    public Map<String, ?> offset(int eventRowNumber, int totalNumberOfRows) {
+        if (eventRowNumber < (totalNumberOfRows - 1)) {
+            // This is not the last row, so our offset should record the next row to be used ...
+            this.lastEventRowNumber = eventRowNumber;
+            this.nextEventRowNumber = eventRowNumber + 1;
+            // so write out the offset with the position of this event
+            return offsetUsingPosition(lastBinlogPosition);
+        }
+        // This is the last row, so write out the offset that has the position of the next event ...
+        this.lastEventRowNumber = this.nextEventRowNumber;
+        this.nextEventRowNumber = 0;
+        return offsetUsingPosition(nextBinlogPosition);
+    }
+
+    private Map<String, ?> offsetUsingPosition( long binlogPosition ) {
         Map<String, Object> map = new HashMap<>();
         if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
         if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
@@ -158,7 +189,7 @@ public Map<String, String> partition() {
         }
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
+        map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, nextEventRowNumber);
         if (isSnapshotInEffect()) {
             map.put(SNAPSHOT_KEY, true);
         }
@@ -192,27 +223,15 @@ public Struct struct() {
             result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
-        result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
-        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
+        result.put(BINLOG_POSITION_OFFSET_KEY, lastBinlogPosition);
+        result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, lastEventRowNumber);
         result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
         return result;
     }
 
-    /**
-     * Set the current row number within a given event, and then get the Kafka Connect detail about the source ""offset"", which
-     * describes the position within the source where we have last read.
-     * 
-     * @param eventRowNumber the 0-based row number within the last event that was successfully processed
-     * @return a copy of the current offset; never null
-     */
-    public Map<String, ?> offset(int eventRowNumber) {
-        setRowInEvent(eventRowNumber);
-        return offset();
-    }
-
     /**
      * Determine whether a snapshot is currently in effect.
      * 
@@ -246,28 +265,27 @@ public void setGtidSet(String gtidSet) {
      * Set the name of the MySQL binary log file.
      * 
      * @param binlogFilename the name of the binary log file; may not be null
+     * @param positionOfFirstEvent the position in the binary log file to begin processing
      */
-    public void setBinlogFilename(String binlogFilename) {
+    public void setBinlogStartPoint(String binlogFilename, long positionOfFirstEvent) {
         this.binlogFilename = binlogFilename;
+        this.nextBinlogPosition = positionOfFirstEvent;
+        this.lastBinlogPosition = this.nextBinlogPosition;
+        this.nextEventRowNumber = 0;
+        this.lastEventRowNumber = 0;
     }
 
     /**
-     * Set the position within the MySQL binary log file.
-     * 
-     * @param binlogPosition the position within the binary log file
-     */
-    public void setBinlogPosition(long binlogPosition) {
-        this.binlogPosition = binlogPosition;
-    }
-
-    /**
-     * Set the index of the row within the event appearing at the {@link #binlogPosition() position} within the
-     * {@link #binlogFilename() binary log file}.
+     * Set the position within the MySQL binary log file of the <em>current event</em>.
      * 
-     * @param rowNumber the 0-based row number
+     * @param positionOfCurrentEvent the position within the binary log file of the current event
+     * @param eventSizeInBytes the size in bytes of this event
      */
-    public void setRowInEvent(int rowNumber) {
-        this.eventRowNumber = rowNumber;
+    public void setEventPosition(long positionOfCurrentEvent, long eventSizeInBytes) {
+        this.lastBinlogPosition = positionOfCurrentEvent;
+        this.nextBinlogPosition = positionOfCurrentEvent + eventSizeInBytes;
+        this.nextEventRowNumber = 0;
+        this.lastEventRowNumber = 0;
     }
 
     /**
@@ -316,8 +334,10 @@ public void setOffset(Map<String, ?> sourceOffset) {
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
             }
-            binlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
-            eventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
+            nextBinlogPosition = longOffsetValue(sourceOffset, BINLOG_POSITION_OFFSET_KEY);
+            nextEventRowNumber = (int) longOffsetValue(sourceOffset, BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY);
+            lastBinlogPosition = nextBinlogPosition;
+            lastEventRowNumber = nextEventRowNumber;
         }
     }
 
@@ -344,29 +364,50 @@ public String gtidSet() {
     /**
      * Get the name of the MySQL binary log file that has been processed.
      * 
-     * @return the name of the binary log file; null if it has not been {@link #setBinlogFilename(String) set}
+     * @return the name of the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
      */
     public String binlogFilename() {
         return binlogFilename;
     }
 
     /**
-     * Get the position within the MySQL binary log file that has been processed.
+     * Get the position within the MySQL binary log file of the next event to be processed.
      * 
-     * @return the position within the binary log file; null if it has not been {@link #setBinlogPosition(long) set}
+     * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
+     */
+    public long nextBinlogPosition() {
+        return nextBinlogPosition;
+    }
+
+    /**
+     * Get the position within the MySQL binary log file of the most recently processed event.
+     * 
+     * @return the position within the binary log file; null if it has not been {@link #setBinlogStartPoint(String, long) set}
+     */
+    public long lastBinlogPosition() {
+        return lastBinlogPosition;
+    }
+
+    /**
+     * Get the next row within the event at the {@link #nextBinlogPosition() position} within the {@link #binlogFilename() binary
+     * log file}
+     * .
+     * 
+     * @return the 0-based row number
      */
-    public long binlogPosition() {
-        return binlogPosition;
+    public int nextEventRowNumber() {
+        return nextEventRowNumber;
     }
 
     /**
-     * Get the row within the event at the {@link #binlogPosition() position} within the {@link #binlogFilename() binary log file}
+     * Get the previous row within the event at the {@link #lastBinlogPosition() position} within the {@link #binlogFilename()
+     * binary log file}
      * .
      * 
      * @return the 0-based row number
      */
-    public int eventRowNumber() {
-        return eventRowNumber;
+    public int lastEventRowNumber() {
+        return lastEventRowNumber;
     }
 
     /**
@@ -385,8 +426,8 @@ public String toString() {
             sb.append(""GTIDs "");
             sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
-            sb.append("", pos="").append(binlogPosition());
-            sb.append("", row="").append(eventRowNumber());
+            sb.append("", pos="").append(nextBinlogPosition());
+            sb.append("", row="").append(nextEventRowNumber());
         } else {
             if (binlogFilename == null) {
                 sb.append(""<latest>"");
@@ -395,8 +436,8 @@ public String toString() {
                     sb.append(""earliest binlog file and position"");
                 } else {
                     sb.append(""binlog file '"").append(binlogFilename).append(""'"");
-                    sb.append("", pos="").append(binlogPosition());
-                    sb.append("", row="").append(eventRowNumber());
+                    sb.append("", pos="").append(nextBinlogPosition());
+                    sb.append("", row="").append(nextEventRowNumber());
                 }
             }
         }",2016-06-14T22:43:58Z,70
"@@ -114,7 +114,7 @@ public void shouldCreateSnapshotOfSingleDatabase() throws Exception {
         config = simpleConfig().build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setBinlogFilename(""""); // start from beginning
+        context.source().setBinlogStartPoint("""",0L); // start from beginning
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...
@@ -173,7 +173,7 @@ public void shouldCreateSnapshotOfSingleDatabaseWithSchemaChanges() throws Excep
         config = simpleConfig().with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true).build();
         context = new MySqlTaskContext(config);
         context.start();
-        context.source().setBinlogFilename(""""); // start from beginning
+        context.source().setBinlogStartPoint("""",0L); // start from beginning
         reader = new BinlogReader(context);
 
         // Start reading the binlog ...",2016-06-14T22:43:58Z,67
"@@ -22,6 +22,7 @@
 import static org.fest.assertions.Assertions.assertThat;
 
 import io.debezium.config.Configuration;
+import io.debezium.connector.mysql.MySqlConnectorConfig.SnapshotMode;
 import io.debezium.embedded.AbstractConnectorTest;
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.history.FileDatabaseHistory;
@@ -284,6 +285,56 @@ public void shouldConsumeAllEventsFromDatabaseUsingSnapshot() throws SQLExceptio
         stopConnector();
     }
 
+    @Test
+    public void shouldConsumeEventsWithNoSnapshot() throws SQLException, InterruptedException {
+        Testing.Files.delete(DB_HISTORY_PATH);
+        
+        // Use the DB configuration to define the connector's configuration ...
+        config = Configuration.create()
+                              .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
+                              .with(MySqlConnectorConfig.PORT, System.getProperty(""database.port""))
+                              .with(MySqlConnectorConfig.USER, ""snapper"")
+                              .with(MySqlConnectorConfig.PASSWORD, ""snapperpass"")
+                              .with(MySqlConnectorConfig.SERVER_ID, 18780)
+                              .with(MySqlConnectorConfig.SERVER_NAME, ""kafka-connect-2"")
+                              .with(MySqlConnectorConfig.POLL_INTERVAL_MS, 10)
+                              .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test_ro"")
+                              .with(MySqlConnectorConfig.SNAPSHOT_MODE, SnapshotMode.NEVER.name().toLowerCase())
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
+                              .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
+                              .with(""database.useSSL"",false) // eliminates MySQL driver warning about SSL connections
+                              .build();
+
+        // Start the connector ...
+        start(MySqlConnector.class, config);
+
+        // Consume the first records due to startup and initialization of the database ...
+        // Testing.Print.enable();
+        SourceRecords records = consumeRecordsByTopic(9+9+4+5+6);   // 6 DDL changes
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.products"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.products_on_hand"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.customers"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test_ro.orders"").size()).isEqualTo(5);
+        assertThat(records.topics().size()).isEqualTo(4+1);
+        assertThat(records.ddlRecordsForDatabase(""connector_test_ro"").size()).isEqualTo(6);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
+        
+        // More records may have been written (if this method were run after the others), but we don't care ...
+        stopConnector();
+
+        records.recordsForTopic(""kafka-connect-2.connector_test_ro.orders"").forEach(record->{
+            print(record);
+        });
+        
+        records.recordsForTopic(""kafka-connect-2.connector_test_ro.customers"").forEach(record->{
+            print(record);
+        });
+    }
+
+
     @Test
     public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLException, InterruptedException {
         Testing.Files.delete(DB_HISTORY_PATH);",2016-06-14T22:43:58Z,71
"@@ -60,8 +60,7 @@ public void shouldApplyDdlStatementsAndRecover() {
         mysql.start();
 
         // Testing.Print.enable();
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -81,11 +80,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeOnlyFilteredDatabases()
                      .createSchemas();
         mysql.start();
 
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
-        source.setBinlogPosition(1000);
+        source.setBinlogStartPoint(""binlog-001"",1000);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...
@@ -107,11 +105,10 @@ public void shouldLoadSystemAndNonSystemTablesAndConsumeAllDatabases() {
                      .createSchemas();
         mysql.start();
 
-        source.setBinlogFilename(""binlog-001"");
-        source.setBinlogPosition(400);
+        source.setBinlogStartPoint(""binlog-001"",400);
         mysql.applyDdl(source, ""mysql"", readFile(""ddl/mysql-test-init-5.7.ddl""), this::printStatements);
 
-        source.setBinlogPosition(1000);
+        source.setBinlogStartPoint(""binlog-001"",1000);
         mysql.applyDdl(source, ""db1"", readFile(""ddl/mysql-products.ddl""), this::printStatements);
 
         // Check that we have tables ...",2016-06-14T22:43:58Z,19
"@@ -0,0 +1,268 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.TreeMap;
+
+import io.debezium.annotation.Immutable;
+
+/**
+ * A set of MySQL GTIDs. This is an improvement of {@link com.github.shyiko.mysql.binlog.GtidSet} that is immutable,
+ * and more properly supports comparisons.
+ * 
+ * @author Randall Hauch
+ */
+@Immutable
+public final class GtidSet {
+
+    private final String orderedString;
+    private final Map<String, UUIDSet> uuidSetsByServerId = new TreeMap<>(); // sorts on keys
+
+    /**
+     * @param gtids the string representation of the GTIDs.
+     */
+    public GtidSet(String gtids) {
+        new com.github.shyiko.mysql.binlog.GtidSet(gtids).getUUIDSets().forEach(uuidSet -> {
+            uuidSetsByServerId.put(uuidSet.getUUID(), new UUIDSet(uuidSet));
+        });
+        StringBuilder sb = new StringBuilder();
+        uuidSetsByServerId.values().forEach(uuidSet -> {
+            if (sb.length() != 0) sb.append(',');
+            sb.append(uuidSet.toString());
+        });
+        orderedString = sb.toString();
+    }
+
+    /**
+     * Get an immutable collection of the {@link UUIDSet range of GTIDs for a single server}.
+     * 
+     * @return the {@link UUIDSet GTID ranges for each server}; never null
+     */
+    public Collection<UUIDSet> getUUIDSets() {
+        return Collections.unmodifiableCollection(uuidSetsByServerId.values());
+    }
+
+    /**
+     * Find the {@link UUIDSet} for the server with the specified UUID.
+     * 
+     * @param uuid the UUID of the server
+     * @return the {@link UUIDSet} for the identified server, or {@code null} if there are no GTIDs from that server.
+     */
+    public UUIDSet forServerWithId(String uuid) {
+        return uuidSetsByServerId.get(uuid);
+    }
+
+    /**
+     * Determine if the GTIDs represented by this object are contained completely within the supplied set of GTIDs.
+     * 
+     * @param other the other set of GTIDs; may be null
+     * @return {@code true} if all of the GTIDs in this set are completely contained within the supplied set of GTIDs, or
+     *         {@code false} otherwise
+     */
+    public boolean isSubsetOf(GtidSet other) {
+        if (other == null) return false;
+        if (this.equals(other)) return true;
+        for (UUIDSet uuidSet : uuidSetsByServerId.values()) {
+            UUIDSet thatSet = other.forServerWithId(uuidSet.getUUID());
+            if (!uuidSet.isSubsetOf(thatSet)) return false;
+        }
+        return true;
+    }
+
+    @Override
+    public int hashCode() {
+        return orderedString.hashCode();
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (obj == this) return true;
+        if (obj instanceof GtidSet) {
+            GtidSet that = (GtidSet) obj;
+            return this.orderedString.equalsIgnoreCase(that.orderedString);
+        }
+        return false;
+    }
+
+    @Override
+    public String toString() {
+        return orderedString;
+    }
+
+    /**
+     * A range of GTIDs for a single server with a specific UUID.
+     */
+    @Immutable
+    public static class UUIDSet {
+
+        private String uuid;
+        private LinkedList<Interval> intervals = new LinkedList<>();
+
+        protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
+            this.uuid = uuidSet.getUUID();
+            uuidSet.getIntervals().forEach(interval -> {
+                intervals.add(new Interval(interval.getStart(), interval.getEnd()));
+            });
+            Collections.sort(this.intervals);
+        }
+
+        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
+            this.uuid = uuid;
+            this.intervals = intervals;
+        }
+
+        /**
+         * Get the UUID for the server that generated the GTIDs.
+         * 
+         * @return the server's UUID; never null
+         */
+        public String getUUID() {
+            return uuid;
+        }
+
+        /**
+         * Get the intervals of transaction numbers.
+         * 
+         * @return the immutable transaction intervals; never null
+         */
+        public Collection<Interval> getIntervals() {
+            return Collections.unmodifiableCollection(intervals);
+        }
+
+        /**
+         * Get the first interval of transaction numbers for this server.
+         * 
+         * @return the first interval, or {@code null} if there is none
+         */
+        public Interval getFirstInterval() {
+            return intervals.isEmpty() ? null : intervals.getFirst();
+        }
+
+        /**
+         * Get the last interval of transaction numbers for this server.
+         * 
+         * @return the last interval, or {@code null} if there is none
+         */
+        public Interval getLastInterval() {
+            return intervals.isEmpty() ? null : intervals.getLast();
+        }
+
+        /**
+         * Get the interval that contains the full range (and possibly more) of all of the individual intervals for this server.
+         * 
+         * @return the complete interval comprised of the {@link Interval#getStart() start} of the {@link #getFirstInterval()
+         *         first interval} and the {@link Interval#getEnd() end} of the {@link #getLastInterval()}, or {@code null} if
+         *         this server has no intervals at all
+         */
+        public Interval getCompleteInterval() {
+            return intervals.isEmpty() ? null : new Interval(getFirstInterval().getStart(), getLastInterval().getEnd());
+        }
+
+        /**
+         * Determine if the set of transaction numbers from this server is completely within the set of transaction numbers from
+         * the set of transaction numbers in the supplied set.
+         * 
+         * @param other the set to compare with this set
+         * @return {@code true} if this server's transaction numbers are a subset of the transaction numbers of the supplied set,
+         *         or false otherwise
+         */
+        public boolean isSubsetOf(UUIDSet other) {
+            if (other == null) return false;
+            if (!this.getUUID().equalsIgnoreCase(other.getUUID())) {
+                // Not even the same server ...
+                return false;
+            }
+            if (this.intervals.isEmpty()) return true;
+            if (other.intervals.isEmpty()) return false;
+            assert this.intervals.size() > 0;
+            assert other.intervals.size() > 0;
+
+            // Every interval in this must be within an interval of the other ...
+            for (Interval thisInterval : this.intervals) {
+                boolean found = false;
+                for (Interval otherInterval : other.intervals) {
+                    if (thisInterval.isSubsetOf(otherInterval)) {
+                        found = true;
+                        break;
+                    }
+                }
+                if (!found) return false; // didn't find a match
+            }
+            return true;
+        }
+
+        @Override
+        public int hashCode() {
+            return uuid.hashCode();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (obj == this) return true;
+            if (obj instanceof UUIDSet) {
+                UUIDSet that = (UUIDSet) obj;
+                return this.getUUID().equalsIgnoreCase(that.getUUID()) && this.getIntervals().equals(that.getIntervals());
+            }
+            return super.equals(obj);
+        }
+
+        @Override
+        public String toString() {
+            StringBuilder sb = new StringBuilder();
+            if (sb.length() != 0) sb.append(',');
+            sb.append(uuid).append(':');
+            sb.append(intervals.getFirst().getStart());
+            sb.append(intervals.getLast().getEnd());
+            return sb.toString();
+        }
+    }
+
+    @Immutable
+    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+
+        public Interval(long start, long end) {
+            super(start, end);
+        }
+
+        /**
+         * Determine if this interval is completely within the supplied interval.
+         * 
+         * @param other the interval to compare with
+         * @return {@code true} if the {@link #getStart() start} is greater than or equal to the supplied interval's
+         *         {@link #getStart() start} and the {@link #getEnd() end} is less than or equal to the supplied interval's
+         *         {@link #getEnd() end}, or {@code false} otherwise
+         */
+        public boolean isSubsetOf(Interval other) {
+            if (other == this) return true;
+            if (other == null) return false;
+            return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
+        }
+
+        @Override
+        public int hashCode() {
+            return (int) getStart();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (this == obj) return true;
+            if (obj instanceof com.github.shyiko.mysql.binlog.GtidSet.Interval) {
+                com.github.shyiko.mysql.binlog.GtidSet.Interval that = (com.github.shyiko.mysql.binlog.GtidSet.Interval) obj;
+                return this.getStart() == that.getStart() && this.getEnd() == that.getEnd();
+            }
+            return false;
+        }
+
+        @Override
+        public String toString() {
+            return getStart() == getEnd() ? Long.toString(getStart()) : """" + getStart() + ""-"" + getEnd();
+        }
+    }
+}",2016-06-04T21:20:26Z,87
"@@ -76,7 +76,8 @@ public class MySqlConnectorConfig {
                                                       .withDescription(""The name of the DatabaseHistory class that should be used to store and recover database schema changes. ""
                                                               + ""The configuration properties for the history are prefixed with the '""
                                                               + DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING + ""' string."")
-                                                      .withDefault(KafkaDatabaseHistory.class.getName());
+                                                      .withDefault(KafkaDatabaseHistory.class.getName())
+                                                      .withValidation(Field::isClassName);
 
     public static final Field INCLUDE_SCHEMA_CHANGES = Field.create(""include.schema.changes"")
                                                             .withDescription(""Whether the connector should publish changes in the database schema to a Kafka topic with """,2016-06-04T21:20:26Z,40
"@@ -27,6 +27,7 @@
 import io.debezium.relational.ddl.DdlChanges;
 import io.debezium.relational.ddl.DdlChanges.DatabaseStatementStringConsumer;
 import io.debezium.relational.history.DatabaseHistory;
+import io.debezium.relational.history.HistoryRecordComparator;
 import io.debezium.text.ParsingException;
 import io.debezium.util.Collect;
 
@@ -51,6 +52,8 @@
 @NotThreadSafe
 public class MySqlSchema {
 
+    private static final HistoryRecordComparator HISTORY_COMPARATOR = HistoryRecordComparator.usingPositions(SourceInfo::isPositionAtOrBefore);
+
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final Set<String> ignoredQueryStatements = Collect.unmodifiableSet(""BEGIN"", ""END"", ""FLUSH PRIVILEGES"");
     private final MySqlDdlParser ddlParser;
@@ -85,7 +88,7 @@ public MySqlSchema(Configuration config, String serverName) {
         }
         // Do not remove the prefix from the subset of config properties ...
         Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false);
-        this.dbHistory.configure(dbHistoryConfig); // validates
+        this.dbHistory.configure(dbHistoryConfig,HISTORY_COMPARATOR); // validates
     }
 
     /**",2016-06-04T21:20:26Z,19
"@@ -13,9 +13,8 @@
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.errors.ConnectException;
 
-import com.github.shyiko.mysql.binlog.GtidSet;
-
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
 /**
@@ -126,6 +125,8 @@ public Map<String, String> partition() {
         if (binlogGtids != null) {
             map.put(BINLOG_GTID_KEY, binlogGtids.toString());
         }
+        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTs != 0 ) map.put(BINLOG_EVENT_TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -360,4 +361,90 @@ public String toString() {
         }
         return sb.toString();
     }
+
+    /**
+     * Determine whether the first {@link #offset() offset} is at or before the point in time of the second
+     * offset, where the offsets are given in JSON representation of the maps returned by {@link #offset()}.
+     * <p>
+     * This logic makes a significant assumption: once a MySQL server/cluster has GTIDs enabled, they will
+     * never be disabled. This is the only way to compare a position with a GTID to a position without a GTID,
+     * and we conclude that any position with a GTID is *after* the position without.
+     * <p>
+     * When both positions have GTIDs, then we compare the positions by using only the GTIDs. Of course, if the
+     * GTIDs are the same, then we also look at whether they have snapshots enabled.
+     * 
+     * @param recorded the position obtained from recorded history; never null
+     * @param desired the desired position that we want to obtain, which should be after some recorded positions,
+     *            at some recorded positions, and before other recorded positions; never null
+     * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
+     */
+    public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
+        String recordedGtidSetStr = recorded.getString(BINLOG_GTID_KEY);
+        String desiredGtidSetStr = desired.getString(BINLOG_GTID_KEY);
+        if (desiredGtidSetStr != null) {
+            // The desired position uses GTIDs, so we ideally compare using GTIDs ...
+            if (recordedGtidSetStr != null) {
+                // Both have GTIDs, so base the comparison entirely on the GTID sets.
+                GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
+                GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
+                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                    // They are exactly the same, which means the recorded position exactly matches the desired ...
+                    if ( !recorded.has(BINLOG_SNAPSHOT_KEY) && desired.has(BINLOG_SNAPSHOT_KEY)) {
+                        // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
+                        return false;
+                    }
+                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired ...
+                    return true;
+                }
+                // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
+                return recordedGtidSet.isSubsetOf(desiredGtidSet);
+            }
+            // The desired position did use GTIDs while the recorded did not use GTIDs. So, we assume that the
+            // recorded position is older since GTIDs are often enabled but rarely disabled. And if they are disabled,
+            // it is likely that the desired position would not include GTIDs as we would be trying to read the binlog of a
+            // server that no longer has GTIDs. And if they are enabled, disabled, and re-enabled, per
+            // https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-failover.html all properly configured slaves that
+            // use GTIDs should always have the complete set of GTIDs copied from the master, in which case
+            // again we know that recorded not having GTIDs is before the desired position ...
+            return true;
+        } else if (recordedGtidSetStr != null) {
+            // The recorded has a GTID but the desired does not, so per the previous paragraph we assume that previous
+            // is not at or before ...
+            return false;
+        }
+
+        // Both positions are missing GTIDs. Look at the servers ...
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
+        if ( recordedServerId != desiredServerId ) {
+            // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
+            // is compare timestamps, and we have to assume that the server timestamps can be compared ...
+            long recordedTimestamp = recorded.getLong(BINLOG_EVENT_TIMESTAMP_KEY,0);
+            long desiredTimestamp = recorded.getLong(BINLOG_EVENT_TIMESTAMP_KEY,0);
+            return recordedTimestamp <= desiredTimestamp;
+        }
+        
+        // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
+        // comparable ...
+        String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
+        String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
+        assert recordedFilename != null;
+        int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
+        if ( diff > 0 ) return false;
+
+        // The filenames are the same, so compare the positions ...
+        int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
+        int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
+        diff = recordedPosition - desiredPosition;
+        if ( diff > 0 ) return false;
+        
+        // The positions are the same, so compare the row number ...
+        int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        diff = recordedRow - desiredRow;
+        if ( diff > 0 ) return false;
+
+        // The binlog coordinates are the same ...
+        return true;
+    }
 }",2016-06-04T21:20:26Z,70
"@@ -5,33 +5,153 @@
  */
 package io.debezium.connector.mysql;
 
-import io.confluent.connect.avro.AvroData;
+import static org.junit.Assert.assertTrue;
 
 import org.apache.avro.Schema;
-
-
+import org.fest.assertions.GenericAssert;
 import org.junit.Test;
 
-import static org.junit.Assert.assertTrue;
+import io.confluent.connect.avro.AvroData;
+import io.debezium.document.Document;
 
 public class SourceInfoTest {
-    private static final AvroData avroData;
-    private static int avroSchemaCacheSize = 1000;
 
-    static {
-        avroData = new AvroData(avroSchemaCacheSize);
-    }
+    private static int avroSchemaCacheSize = 1000;
+    private static final AvroData avroData = new AvroData(avroSchemaCacheSize);
 
     /**
      * When we want to consume SinkRecord which generated by debezium-connector-mysql, it should not
      * throw error ""org.apache.avro.SchemaParseException: Illegal character in: server-id""
      */
     @Test
-    public void testValidateSourceInfoSchema() {
+    public void shouldValidateSourceInfoSchema() {
         org.apache.kafka.connect.data.Schema kafkaSchema = SourceInfo.SCHEMA;
         Schema avroSchema = avroData.fromConnectSchema(kafkaSchema);
         assertTrue(avroSchema != null);
     }
 
-}
+    @Test
+    public void shouldConsiderPositionsWithSameGtidSetsAsSame() {
+        assertPositionWithGtids(""IdA:1-5"").isAtOrBefore(positionWithGtids(""IdA:1-5""));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"").isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"")); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"").isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"")); // equivalent
+    }
+
+    @Test
+    public void shouldConsiderPositionsWithSameGtidSetsAndSnapshotAsSame() {
+        assertPositionWithGtids(""IdA:1-5"",true).isAtOrBefore(positionWithGtids(""IdA:1-5"",true));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"",true)); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"",true)); // equivalent
+    }
 
+    @Test
+    public void shouldOrderPositionWithGtidAndSnapshotBeforePositionWithSameGtidButNoSnapshot() {
+        assertPositionWithGtids(""IdA:1-5"",true).isAtOrBefore(positionWithGtids(""IdA:1-5""));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"")); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"")); // equivalent
+    }
+
+    @Test
+    public void shouldOrderPositionWithoutGtidAndSnapshotAfterPositionWithSameGtidAndSnapshot() {
+        assertPositionWithGtids(""IdA:1-5"",false).isAfter(positionWithGtids(""IdA:1-5"",true));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",false).isAfter(positionWithGtids(""IdA:1-5,IdB:1-20"",true)); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",false).isAfter(positionWithGtids(""IdB:1-20,IdA:1-5"",true)); // equivalent
+    }
+
+    @Test
+    public void shouldOrderPositionWithGtidsAsBeforePositionWithExtraServerUuidInGtids() {
+        assertPositionWithGtids(""IdA:1-5"").isBefore(positionWithGtids(""IdA:1-5,IdB:1-20""));
+    }
+
+    @Test
+    public void shouldOrderPositionsWithSameServerButLowerUpperLimitAsBeforePositionWithSameServerUuidInGtids() {
+        assertPositionWithGtids(""IdA:1-5"").isBefore(positionWithGtids(""IdA:1-6""));
+        assertPositionWithGtids(""IdA:1-5:7-9"").isBefore(positionWithGtids(""IdA:1-10""));
+        assertPositionWithGtids(""IdA:2-5:8-9"").isBefore(positionWithGtids(""IdA:1-10""));
+    }
+
+    @Test
+    public void shouldOrderPositionWithoutGtidAsBeforePositionWithGtid() {
+        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0).isBefore(positionWithGtids(""IdA:1-5""));
+    }
+
+    @Test
+    public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
+        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
+    }
+
+    protected Document positionWithGtids(String gtids) {
+        return positionWithGtids(gtids, false);
+    }
+
+    protected Document positionWithGtids(String gtids, boolean snapshot) {
+        if (snapshot) {
+            return Document.create(SourceInfo.BINLOG_GTID_KEY, gtids, SourceInfo.BINLOG_SNAPSHOT_KEY, true);
+        }
+        return Document.create(SourceInfo.BINLOG_GTID_KEY, gtids);
+    }
+
+    protected Document positionWithoutGtids(String filename, int position, int row) {
+        return positionWithoutGtids(filename, position, row, false);
+    }
+
+    protected Document positionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+        if (snapshot) {
+            return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
+                                   SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
+                                   SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row,
+                                   SourceInfo.BINLOG_SNAPSHOT_KEY, true);
+        }
+        return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
+                               SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
+                               SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row);
+    }
+
+    protected PositionAssert assertThat(Document position) {
+        return new PositionAssert(position);
+    }
+
+    protected PositionAssert assertPositionWithGtids(String gtids) {
+        return assertThat(positionWithGtids(gtids));
+    }
+
+    protected PositionAssert assertPositionWithGtids(String gtids, boolean snapshot) {
+        return assertThat(positionWithGtids(gtids, snapshot));
+    }
+
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row) {
+        return assertPositionWithoutGtids(filename, position, row, false);
+    }
+
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+        return assertThat(positionWithoutGtids(filename, position, row, snapshot));
+    }
+
+    protected static class PositionAssert extends GenericAssert<PositionAssert, Document> {
+        public PositionAssert(Document position) {
+            super(PositionAssert.class, position);
+        }
+
+        public PositionAssert isAt(Document otherPosition) {
+            if (SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider same position as "" + otherPosition);
+        }
+
+        public PositionAssert isBefore(Document otherPosition) {
+            return isAtOrBefore(otherPosition);
+        }
+
+        public PositionAssert isAtOrBefore(Document otherPosition) {
+            if (SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider same position as or before "" + otherPosition);
+        }
+
+        public PositionAssert isAfter(Document otherPosition) {
+            if (!SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider after "" + otherPosition);
+        }
+    }
+}",2016-06-04T21:20:26Z,70
"@@ -14,6 +14,8 @@
 import java.util.function.Predicate;
 import java.util.function.Supplier;
 
+import javax.lang.model.SourceVersion;
+
 import io.debezium.annotation.Immutable;
 
 /**
@@ -395,6 +397,13 @@ public String toString() {
         return name();
     }
 
+    public static int isClassName(Configuration config, Field field, Consumer<String> problems) {
+        String value = config.getString(field);
+        if (value == null || SourceVersion.isName(value)) return 0;
+        problems.accept(""The '"" + field.name() + ""' field must contain a valid name of a Java class."");
+        return 1;
+    }
+
     public static int isRequired(Configuration config, Field field, Consumer<String> problems) {
         String value = config.getString(field);
         if (value != null && value.trim().length() > 0) return 0;",2016-06-04T21:20:26Z,119
"@@ -21,15 +21,17 @@
  */
 public abstract class AbstractDatabaseHistory implements DatabaseHistory {
 
-    protected Configuration config;
     protected final Logger logger = LoggerFactory.getLogger(getClass());
+    protected Configuration config;
+    private HistoryRecordComparator comparator = HistoryRecordComparator.INSTANCE;
 
     protected AbstractDatabaseHistory() {
     }
-
+    
     @Override
-    public void configure(Configuration config) {
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
         this.config = config;
+        this.comparator = comparator != null ? comparator : HistoryRecordComparator.INSTANCE;
     }
     
     @Override
@@ -46,7 +48,7 @@ public final void record(Map<String, ?> source, Map<String, ?> position, String
     public final void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser) {
         HistoryRecord stopPoint = new HistoryRecord(source, position, null, null);
         recoverRecords(schema,ddlParser,recovered->{
-            if (recovered.isAtOrBefore(stopPoint)) {
+            if (comparator.isAtOrBefore(recovered,stopPoint)) {
                 String ddl = recovered.ddl();
                 if (ddl != null) {
                     ddlParser.setCurrentSchema(recovered.databaseName()); // may be null",2016-06-04T21:20:26Z,76
"@@ -26,8 +26,11 @@ public interface DatabaseHistory {
      * Configure this instance.
      * 
      * @param config the configuration for this history store
+     * @param comparator the function that should be used to compare history records during
+     *            {@link #recover(Map, Map, Tables, DdlParser) recovery}; may be null if the
+     *            {@link HistoryRecordComparator#INSTANCE default comparator} is to be used
      */
-    void configure(Configuration config);
+    void configure(Configuration config, HistoryRecordComparator comparator);
 
     /**
      * Start the history.
@@ -62,7 +65,7 @@ public interface DatabaseHistory {
     void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser);
 
     /**
-     * Stop recording history and release any resources acquired since {@link #configure(Configuration)}.
+     * Stop recording history and release any resources acquired since {@link #configure(Configuration, HistoryRecordComparator)}.
      */
     void stop();
 }",2016-06-04T21:20:26Z,41
"@@ -49,15 +49,14 @@ public final class FileDatabaseHistory extends AbstractDatabaseHistory {
     private Path path;
 
     @Override
-    public void configure(Configuration config) {
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
         lock.write(() -> {
-            super.configure(config);
             if (!config.validate(ALL_FIELDS, logger::error)) {
                 throw new ConnectException(
                         ""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
             }
             config.validate(ALL_FIELDS, logger::error);
-            super.configure(config);
+            super.configure(config,comparator);
             path = Paths.get(config.getString(FILE_PATH));
         });
     }",2016-06-04T21:20:26Z,130
"@@ -0,0 +1,62 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.relational.history;
+
+import java.util.function.BiFunction;
+
+import io.debezium.document.Document;
+
+/**
+ * Compares HistoryRecord instances to determine which came first.
+ * 
+ * @author Randall Hauch
+ * @since 0.2
+ */
+public class HistoryRecordComparator {
+
+    /**
+     * A comparator instance that requires the {@link HistoryRecord#source() records' sources} to be the same and considers only
+     * those fields that are in both records' {@link HistoryRecord#position() positions}.
+     */
+    public static final HistoryRecordComparator INSTANCE = new HistoryRecordComparator();
+
+    /**
+     * Create a {@link HistoryRecordComparator} that requires identical sources but will use the supplied function to compare
+     * positions.
+     * 
+     * @param positionComparator the non-null function that returns {@code true} if the first position is at or before
+     *            the second position or {@code false} otherwise
+     * @return the comparator instance; never null
+     */
+    public static HistoryRecordComparator usingPositions(BiFunction<Document, Document, Boolean> positionComparator) {
+        return new HistoryRecordComparator() {
+            @Override
+            protected boolean isPositionAtOrBefore(Document position1, Document position2) {
+                return positionComparator.apply(position1, position2);
+            }
+        };
+    }
+
+    /**
+     * Determine if the first {@link HistoryRecord} is at the same or earlier point in time than the second {@link HistoryRecord}.
+     * 
+     * @param record1 the first record; never null
+     * @param record2 the second record; never null
+     * @return {@code true} if the first record is at the same or earlier point in time than the second record, or {@code false}
+     *         otherwise
+     */
+    public boolean isAtOrBefore(HistoryRecord record1, HistoryRecord record2) {
+        return isSameSource(record1.source(), record2.source()) && isPositionAtOrBefore(record1.position(), record2.position());
+    }
+
+    protected boolean isPositionAtOrBefore(Document position1, Document position2) {
+        return position1.compareToUsingSimilarFields(position2) <= 0;
+    }
+
+    protected boolean isSameSource(Document source1, Document source2) {
+        return source1.equals(source2);
+    }
+}
\ No newline at end of file",2016-06-04T21:20:26Z,131
"@@ -83,8 +83,8 @@ public class KafkaDatabaseHistory extends AbstractDatabaseHistory {
     private int pollIntervalMs = -1;
 
     @Override
-    public void configure(Configuration config) {
-        super.configure(config);
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
+        super.configure(config,comparator);
         if (!config.validate(ALL_FIELDS, logger::error)) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }",2016-06-04T21:20:26Z,115
"@@ -31,7 +31,7 @@ protected DatabaseHistory createHistory() {
         DatabaseHistory history = new FileDatabaseHistory();
         history.configure(Configuration.create()
                                        .with(FileDatabaseHistory.FILE_PATH, TEST_FILE_PATH.toAbsolutePath().toString())
-                                       .build());
+                                       .build(),null);
         return history;
     }
 }",2016-06-04T21:20:26Z,132
"@@ -76,7 +76,7 @@ public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exc
                               .with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS, kafka.brokerList())
                               .with(KafkaDatabaseHistory.TOPIC, topicName)
                               .build();
-        history.configure(config);
+        history.configure(config,null);
         history.start();
 
         DdlParser recoveryParser = new DdlParserSql2003();
@@ -123,7 +123,7 @@ public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exc
         // Stop the history (which should stop the producer) ...
         history.stop();
         history = new KafkaDatabaseHistory();
-        history.configure(config);
+        history.configure(config, null);
         // no need to start
 
         // Recover from the very beginning to just past the first change ...",2016-06-04T21:20:26Z,133
"@@ -18,7 +18,6 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import com.github.shyiko.mysql.binlog.BinaryLogClient;
-import com.github.shyiko.mysql.binlog.BinaryLogClient.AbstractLifecycleListener;
 import com.github.shyiko.mysql.binlog.BinaryLogClient.LifecycleListener;
 import com.github.shyiko.mysql.binlog.event.DeleteRowsEventData;
 import com.github.shyiko.mysql.binlog.event.Event;
@@ -68,17 +67,7 @@ public BinlogReader(MySqlTaskContext context) {
         client.setServerId(context.serverId());
         client.setKeepAlive(context.config().getBoolean(MySqlConnectorConfig.KEEP_ALIVE));
         client.registerEventListener(this::handleEvent);
-        client.registerLifecycleListener(new AbstractLifecycleListener(){
-            @Override
-            public void onCommunicationFailure(BinaryLogClient client, Exception ex) {
-                failed(ex,""Stopped reading binlog due to error: "" + ex.getMessage());
-            }
-            @Override
-            public void onEventDeserializationFailure(BinaryLogClient client, Exception ex) {
-                failed(ex,""Stopped reading binlog due to error: "" + ex.getMessage());
-            }
-        });
-        client.registerLifecycleListener(new TraceLifecycleListener());
+        client.registerLifecycleListener(new ReaderThreadLifecycleListener());
         if (logger.isDebugEnabled()) client.registerEventListener(this::logEvent);
 
         // Set up the event deserializer with additional type(s) ...
@@ -104,34 +93,31 @@ protected void doStart() {
         client.setBinlogFilename(source.binlogFilename());
         client.setBinlogPosition(source.binlogPosition());
         // The event row number will be used when processing the first event ...
-        logger.info(""Reading from MySQL {} starting at {}"",context.serverName(), source);
 
         // Start the log reader, which starts background threads ...
         long timeoutInMilliseconds = context.timeoutInMilliseconds();
         try {
-            logger.debug(""Binlog reader connecting to MySQL server '{}'"", context.serverName());
+            logger.debug(""Attempting to establish binlog reader connection with timeout of {} ms"", timeoutInMilliseconds);
             client.connect(context.timeoutInMilliseconds());
-            logger.info(""Successfully started reading MySQL binlog"");
         } catch (TimeoutException e) {
             double seconds = TimeUnit.MILLISECONDS.toSeconds(timeoutInMilliseconds);
-            throw new ConnectException(""Timed out after "" + seconds + "" seconds while waiting to connect to the MySQL database at "" +
-                    context.username() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
+            throw new ConnectException(""Timed out after "" + seconds + "" seconds while waiting to connect to MySQL at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
         } catch (AuthenticationException e) {
-            throw new ConnectException(""Failed to authenticate to the MySQL database at "" + context.hostname() + "":"" +
-                    context.port() + "" with user '"" + context.username() + ""'"", e);
+            throw new ConnectException(""Failed to authenticate to the MySQL database at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
         } catch (Throwable e) {
-            throw new ConnectException(""Unable to connect to the MySQL database at "" + context.hostname() + "":"" + context.port() +
-                    "" with user '"" + context.username() + ""': "" + e.getMessage(), e);
+            throw new ConnectException(""Unable to connect to the MySQL database at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""': "" + e.getMessage(), e);
         }
 
     }
 
     @Override
     protected void doStop() {
         try {
-            logger.debug(""Binlog reader disconnecting from MySQL server '{}'"", context.serverName());
+            logger.debug(""Stopping binlog reader"");
             client.disconnect();
-            logger.info(""Stopped connector to MySQL server '{}'"", context.serverName());
         } catch (IOException e) {
             logger.error(""Unexpected error when disconnecting from the MySQL binary log reader"", e);
         }
@@ -142,7 +128,7 @@ protected void doCleanup() {
     }
 
     protected void logEvent(Event event) {
-        //logger.debug(""Received event: {}"", event);
+        logger.trace(""Received event: {}"", event);
     }
 
     protected void ignoreEvent(Event event) {
@@ -344,25 +330,31 @@ protected void handleDelete(Event event) throws InterruptedException {
         }
     }
 
-    protected final class TraceLifecycleListener implements LifecycleListener {
+    protected final class ReaderThreadLifecycleListener implements LifecycleListener {
         @Override
         public void onDisconnect(BinaryLogClient client) {
-            logger.debug(""MySQL Connector disconnected"");
+            context.temporaryLoggingContext(""binlog"", () -> {
+                logger.info(""Stopped reading binlog and closed connection"");
+            });
         }
 
         @Override
         public void onConnect(BinaryLogClient client) {
-            logger.info(""MySQL Connector connected"");
+            // Set up the MDC logging context for this thread ...
+            context.configureLoggingContext(""binlog"");
+
+            // The event row number will be used when processing the first event ...
+            logger.info(""Connected to MySQL binlog at {}:{}, starting at {}"", context.hostname(), context.port(), source);
         }
 
         @Override
         public void onCommunicationFailure(BinaryLogClient client, Exception ex) {
-            logger.error(""MySQL Connector communication failure"", ex);
+            BinlogReader.this.failed(ex);
         }
 
         @Override
         public void onEventDeserializationFailure(BinaryLogClient client, Exception ex) {
-            logger.error(""MySQL Connector received event deserialization failure"", ex);
+            BinlogReader.this.failed(ex);
         }
     }
 }",2016-06-02T19:05:06Z,67
"@@ -60,7 +60,7 @@ public void start(Map<String, String> props) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }
 
-        // Create the task and set our running flag ...
+        // Create and start the task context ...
         this.taskContext = new MySqlTaskContext(config);
         this.taskContext.start();
 
@@ -133,13 +133,13 @@ public void start(Map<String, String> props) {
 
     @Override
     public List<SourceRecord> poll() throws InterruptedException {
-        logger.trace(""Polling for events from MySQL connector"");
+        logger.trace(""Polling for events"");
         return currentReader.poll();
     }
 
     @Override
     public void stop() {
-        logger.info(""Stopping MySQL Connector"");
+        logger.info(""Stopping MySQL connector task"");
         // We need to explicitly stop both readers, in this order. If we were to instead call 'currentReader.stop()', there
         // is a chance without synchronization that we'd miss the transition and stop only the snapshot reader. And stopping both
         // is far simpler and more efficient than synchronizing ...
@@ -155,7 +155,7 @@ public void stop() {
                 } catch (Throwable e) {
                     logger.error(""Unexpected error shutting down the database history and/or closing JDBC connections"", e);
                 } finally {
-                    logger.info(""Stopped connector to MySQL server '{}'"", taskContext.serverName());
+                    logger.info(""Connector task successfully stopped"");
                 }
             }
         }",2016-06-02T19:05:06Z,68
"@@ -15,6 +15,8 @@
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
 import io.debezium.util.Clock;
+import io.debezium.util.LoggingContext;
+import io.debezium.util.LoggingContext.PreviousContext;
 
 /**
  * A Kafka Connect source task reads the MySQL binary log and generate the corresponding data change events.
@@ -96,7 +98,7 @@ public void loadHistory(SourceInfo startingPoint) {
         dbSchema.loadHistory(startingPoint);
         recordProcessor.regenerate();
     }
-    
+
     public Clock clock() {
         return clock;
     }
@@ -157,21 +159,23 @@ protected SnapshotMode snapshotMode() {
         String value = config.getString(MySqlConnectorConfig.SNAPSHOT_MODE);
         return SnapshotMode.parse(value, MySqlConnectorConfig.SNAPSHOT_MODE.defaultValue());
     }
-    
+
     public boolean useMinimalSnapshotLocking() {
         return config.getBoolean(MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
     }
 
     public void start() {
-        // Start the MySQL database history, which simply starts up resources but does not recover the history to a specific
-        // point.
+        // First, configure the logging context for the thread that created this context object ...
+        this.configureLoggingContext(""task"");
+
+        // Start the MySQL database history, which simply starts up resources but does not recover the history to a specific point
         dbSchema().start();
     }
 
     public void shutdown() {
         try {
             // Flush and stop the database history ...
-            logger.debug(""Stopping database history for MySQL server '{}'"", serverName());
+            logger.debug(""Stopping database history"");
             dbSchema.shutdown();
         } catch (Throwable e) {
             logger.error(""Unexpected error shutting down the database history"", e);
@@ -184,4 +188,26 @@ public void shutdown() {
         }
     }
 
+    /**
+     * Configure the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * @param contextName the name of the context; may not be null
+     * @return the previous MDC context; never null
+     * @throws IllegalArgumentException if {@code contextName} is null
+     */
+    public PreviousContext configureLoggingContext(String contextName) {
+        return LoggingContext.forConnector(""MySQL"", serverName(), contextName);
+    }
+    
+    /**
+     * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
+     * state before this method was called.
+     * 
+     * @param contextName the name of the context; may not be null
+     * @param operation the function to run in the new MDC context; may not be null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public void temporaryLoggingContext(String contextName, Runnable operation) {
+        LoggingContext.temporarilyForConnector(""MySQL"", serverName(), contextName, operation);
+    }
+
 }",2016-06-02T19:05:06Z,69
"@@ -19,6 +19,8 @@
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
+import io.debezium.util.Clock;
+import io.debezium.util.Strings;
 
 /**
  * A component that performs a snapshot of a MySQL server, and records the schema changes in {@link MySqlSchema}.
@@ -127,13 +129,15 @@ protected void doCleanup() {
      * Perform the snapshot using the same logic as the ""mysqldump"" utility.
      */
     protected void execute() {
-        logger.info(""Starting snapshot for MySQL server {}"", context.serverName());
+        context.configureLoggingContext(""snapshot"");
+        logger.info(""Starting snapshot"");
         final AtomicReference<String> sql = new AtomicReference<>();
         final JdbcConnection mysql = context.jdbc();
         final MySqlSchema schema = context.dbSchema();
         final Filters filters = schema.filters();
         final SourceInfo source = context.source();
-        final long ts = context.clock().currentTimeInMillis();
+        final Clock clock = context.clock();
+        final long ts = clock.currentTimeInMillis();
         try {
             // ------
             // STEP 0
@@ -148,6 +152,7 @@ protected void execute() {
             // See: https://dev.mysql.com/doc/refman/5.7/en/set-transaction.html
             // See: https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html
             // See: https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html
+            logger.info(""Step 0: disabling autocommit and enabling repeatable read transactions"");
             mysql.setAutoCommit(false);
             sql.set(""SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"");
             mysql.execute(sql.get());
@@ -157,6 +162,7 @@ protected void execute() {
             // ------
             // First, start a transaction and request that a consistent MVCC snapshot is obtained immediately.
             // See http://dev.mysql.com/doc/refman/5.7/en/commit.html
+            logger.info(""Step 1: start transaction with consistent snapshot"");
             sql.set(""START TRANSACTION WITH CONSISTENT SNAPSHOT"");
             mysql.execute(sql.get());
 
@@ -166,6 +172,8 @@ protected void execute() {
             // Obtain read lock on all tables. This statement closes all open tables and locks all tables
             // for all databases with a global read lock, and it prevents ALL updates while we have this lock.
             // It also ensures that everything we do while we have this lock will be consistent.
+            long lockAcquired = clock.currentTimeInMillis();
+            logger.info(""Step 2: flush and obtain global read lock (preventing writes to database)"");
             sql.set(""FLUSH TABLES WITH READ LOCK"");
             mysql.execute(sql.get());
 
@@ -174,6 +182,7 @@ protected void execute() {
             // ------
             // Obtain the binlog position and update the SourceInfo in the context. This means that all source records generated
             // as part of the snapshot will contain the binlog position of the snapshot.
+            logger.info(""Step 3: read binlog position of MySQL master"");
             sql.set(""SHOW MASTER STATUS"");
             mysql.query(sql.get(), rs -> {
                 if (rs.next()) {
@@ -191,6 +200,7 @@ protected void execute() {
             // STEP 4
             // ------
             // Get the list of databases ...
+            logger.info(""Step 4: read list of available databases"");
             final List<String> databaseNames = new ArrayList<>();
             sql.set(""SHOW DATABASES"");
             mysql.query(sql.get(), rs -> {
@@ -205,6 +215,7 @@ protected void execute() {
             // Get the list of table IDs for each database. We can't use a prepared statement with MySQL, so we have to
             // build the SQL statement each time. Although in other cases this might lead to SQL injection, in our case
             // we are reading the database names from the database and not taking them from the user ...
+            logger.info(""Step 5: read list of available tables in each database"");
             final List<TableId> tableIds = new ArrayList<>();
             final Map<String,List<TableId>> tableIdsByDbName = new HashMap<>();
             for (String dbName : databaseNames) {
@@ -225,6 +236,7 @@ protected void execute() {
             // ------
             // Transform the current schema so that it reflects the *current* state of the MySQL server's contents.
             // First, get the DROP TABLE and CREATE TABLE statement (with keys and constraint definitions) for our tables ...
+            logger.info(""Step 6: generating DROP and CREATE statements to reflect current database schemas"");
             final List<String> ddlStatements = new ArrayList<>();
             // Add DROP TABLE statements for all tables that we knew about AND those tables found in the databases ...
             Set<TableId> allTableIds = new HashSet<>(schema.tables().tableIds());
@@ -253,6 +265,7 @@ protected void execute() {
                 }
             }
             // Finally, apply the DDL statements to the schema and then update the record maker...
+            logger.debug(""Step 6b: applying DROP and CREATE statements to connector's table model"");
             String ddlStatementsStr = String.join("";"" + System.lineSeparator(), ddlStatements);
             schema.applyDdl(source, null, ddlStatementsStr, this::enqueueSchemaChanges);
             context.makeRecord().regenerate();
@@ -266,17 +279,25 @@ protected void execute() {
                 // should still use the MVCC snapshot obtained when we started our transaction (since we started it
                 // ""...with consistent snapshot""). So, since we're only doing very simple SELECT without WHERE predicates,
                 // we can release the lock now ...
+                logger.info(""Step 7: releasing global read lock to enable MySQL writes"");
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
+                long lockReleased = clock.currentTimeInMillis();
+                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased-lockAcquired));
             }
 
             // ------
             // STEP 8
             // ------
             // Dump all of the tables and generate source records ...
+            logger.info(""Step 8: scanning contents of {} tables"",tableIds.size());
+            long startScan = clock.currentTimeInMillis();
             AtomicBoolean interrupted = new AtomicBoolean(false);
+            int counter = 0;
             for (TableId tableId : tableIds) {
+                long start = clock.currentTimeInMillis();
+                logger.debug(""Step 8.{}: scanning table '{}'; {} tables remain"",++counter,tableId,tableIds.size()-counter);
                 sql.set(""SELECT * FROM "" + tableId);
                 mysql.query(sql.get(), rs -> {
                     RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, super::enqueueRecord);
@@ -300,32 +321,42 @@ protected void execute() {
                     }
                 });
                 if ( interrupted.get() ) break;
+                long stop = clock.currentTimeInMillis();
+                logger.info(""Step 8.{}: scanned table '{}' in {}"",counter,tableId,Strings.duration(stop-start));
             }
+            long stop = clock.currentTimeInMillis();
+            logger.info(""Step 8: scanned contents of {} tables in {}"",tableIds.size(),Strings.duration(stop-startScan));
 
             // ------
             // STEP 9
             // ------
             // Release the read lock if we have not yet done so ...
             if (!unlocked) {
+                logger.info(""Step 9: releasing global read lock to enable MySQL writes"");
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
+                long lockReleased = clock.currentTimeInMillis();
+                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased-lockAcquired));
             }
 
             // -------
             // STEP 10
             // -------
             if (interrupted.get()) {
                 // We were interrupted while reading the tables, so roll back the transaction and return immediately ...
+                logger.info(""Step 10: rolling back transaction after request to stop"");
                 sql.set(""ROLLBACK"");
                 mysql.execute(sql.get());
                 return;
             }
             // Otherwise, commit our transaction
+            logger.info(""Step 10: committing transaction"");
             sql.set(""COMMIT"");
             mysql.execute(sql.get());
 
             try {
+                logger.info(""Step 11: recording completion of snapshot"");
                 // Mark the source as having completed the snapshot. Because of this, **subsequent** source records
                 // produced by the connector (to any topic) will have a normal (not snapshot) offset ...
                 source.completeSnapshot();
@@ -338,7 +369,8 @@ protected void execute() {
             } finally {
                 // Set the completion flag ...
                 super.completeSuccessfully();
-                logger.info(""Completed snapshot for MySQL server {}"", context.serverName());
+                stop = clock.currentTimeInMillis();
+                logger.info(""Completed snapshot in {}"", Strings.duration(stop-ts));
             }
         } catch (Throwable e) {
             failed(e, ""Aborting snapshot after running '"" + sql.get() + ""': "" + e.getMessage());",2016-06-02T19:05:06Z,62
"@@ -2,7 +2,7 @@
 log4j.appender.stdout=org.apache.log4j.ConsoleAppender
 log4j.appender.stdout.Target=System.out
 log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
-log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p     %m (%c)%n
+log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n
 
 # Root logger option
 log4j.rootLogger=INFO, stdout",2016-06-02T19:05:06Z,73
"@@ -0,0 +1,100 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.util;
+
+import java.util.Map;
+
+import org.slf4j.MDC;
+
+/**
+ * A utility that provides a consistent set of properties for the Mapped Diagnostic Context (MDC) properties used by Debezium
+ * components.
+ * 
+ * @author Randall Hauch
+ * @since 0.2
+ */
+public class LoggingContext {
+
+    /**
+     * The key for the connector type MDC property.
+     */
+    public static final String CONNECTOR_TYPE = ""dbz.connectorType"";
+    /**
+     * The key for the connector logical name MDC property.
+     */
+    public static final String CONNECTOR_NAME = ""dbz.connectorName"";
+    /**
+     * The key for the connector context name MDC property.
+     */
+    public static final String CONNECTOR_CONTEXT = ""dbz.connectorContext"";
+
+    private LoggingContext() {
+    }
+    
+    /**
+     * A snapshot of an MDC context that can be {@link #restore()}.
+     */
+    public static final class PreviousContext {
+        private final Map<String,String> context;
+        @SuppressWarnings(""unchecked"")
+        protected PreviousContext() {
+            context = MDC.getCopyOfContextMap();
+        }
+        /**
+         * Restore this logging context.
+         */
+        public void restore() {
+            for ( Map.Entry<String, String> entry : context.entrySet() ) {
+                MDC.put(entry.getKey(), entry.getValue());
+            }
+        }
+    }
+
+    /**
+     * Configure for a connector the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * 
+     * @param connectorType the type of connector; may not be null
+     * @param connectorName the name of the connector; may not be null
+     * @param contextName the name of the context; may not be null
+     * @return the previous MDC context; never null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public static PreviousContext forConnector(String connectorType, String connectorName, String contextName) {
+        if (connectorType == null) throw new IllegalArgumentException(""The MDC value for the connector type may not be null"");
+        if (connectorName == null) throw new IllegalArgumentException(""The MDC value for the connector name may not be null"");
+        if (contextName == null) throw new IllegalArgumentException(""The MDC value for the connector context may not be null"");
+        PreviousContext previous = new PreviousContext();
+        MDC.put(CONNECTOR_TYPE, connectorType);
+        MDC.put(CONNECTOR_NAME, connectorName);
+        MDC.put(CONNECTOR_CONTEXT, contextName);
+        return previous;
+    }
+    
+    /**
+     * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
+     * state before this method was called.
+     * 
+     * @param connectorType the type of connector; may not be null
+     * @param connectorName the logical name of the connector; may not be null
+     * @param contextName the name of the context; may not be null
+     * @param operation the function to run in the new MDC context; may not be null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public static void temporarilyForConnector(String connectorType, String connectorName, String contextName, Runnable operation) {
+        if (connectorType == null) throw new IllegalArgumentException(""The MDC value for the connector type may not be null"");
+        if (connectorName == null) throw new IllegalArgumentException(""The MDC value for the connector name may not be null"");
+        if (contextName == null) throw new IllegalArgumentException(""The MDC value for the connector context may not be null"");
+        if (operation == null) throw new IllegalArgumentException(""The operation may not be null"");
+        PreviousContext previous = new PreviousContext();
+        try {
+            forConnector(connectorType,connectorName,contextName);
+            operation.run();
+        } finally {
+            previous.restore();
+        }
+    }
+    
+}",2016-06-02T19:05:06Z,134
"@@ -7,6 +7,8 @@
 
 import java.io.ByteArrayOutputStream;
 import java.io.PrintWriter;
+import java.math.BigDecimal;
+import java.text.DecimalFormat;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashSet;
@@ -36,11 +38,11 @@ public final class Strings {
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, Function<String, String[]> splitter, Function<String, T> factory) {
-        if ( input == null ) return Collections.emptySet();
+        if (input == null) return Collections.emptySet();
         Set<T> matches = new HashSet<>();
         for (String item : splitter.apply(input)) {
             T obj = factory.apply(item);
-            if ( obj != null ) matches.add(obj);
+            if (obj != null) matches.add(obj);
         }
         return matches;
     }
@@ -54,7 +56,7 @@ public static <T> Set<T> listOf(String input, Function<String, String[]> splitte
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, char delimiter, Function<String, T> factory) {
-        return listOf(input,(str) -> str.split(""["" + delimiter + ""]""),factory);
+        return listOf(input, (str) -> str.split(""["" + delimiter + ""]""), factory);
     }
 
     /**
@@ -65,7 +67,7 @@ public static <T> Set<T> listOf(String input, char delimiter, Function<String, T
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, Function<String, T> factory) {
-        return listOf(input,',',factory);
+        return listOf(input, ',', factory);
     }
 
     /**
@@ -76,7 +78,7 @@ public static <T> Set<T> listOf(String input, Function<String, T> factory) {
      * @return the list of regular expression {@link Pattern}s included in the list; never null
      */
     public static Set<Pattern> listOfRegex(String input) {
-        return listOf(input,',',Pattern::compile);
+        return listOf(input, ',', Pattern::compile);
     }
 
     /**
@@ -88,7 +90,7 @@ public static Set<Pattern> listOfRegex(String input) {
      * @return the list of regular expression {@link Pattern}s included in the list; never null
      */
     public static Set<Pattern> listOfRegex(String input, int regexFlags) {
-        return listOf(input,',',(str)->Pattern.compile(str,regexFlags));
+        return listOf(input, ',', (str) -> Pattern.compile(str, regexFlags));
     }
 
     /**
@@ -111,7 +113,7 @@ public static interface CharacterPredicate {
      * @param content the string content that is to be split
      * @return the list of lines; never null but may be an empty (unmodifiable) list if the supplied content is null or empty
      */
-    public static List<String> splitLines( final String content ) {
+    public static List<String> splitLines(final String content) {
         if (content == null || content.length() == 0) return Collections.emptyList();
         String[] lines = content.split(""[\\r]?\\n"");
         return Arrays.asList(lines);
@@ -410,8 +412,7 @@ public static int asInt(String value, int defaultValue) {
         if (value != null) {
             try {
                 return Integer.parseInt(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -427,8 +428,7 @@ public static long asLong(String value, long defaultValue) {
         if (value != null) {
             try {
                 return Long.parseLong(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -444,8 +444,7 @@ public static double asDouble(String value, double defaultValue) {
         if (value != null) {
             try {
                 return Double.parseDouble(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -461,12 +460,46 @@ public static boolean asBoolean(String value, boolean defaultValue) {
         if (value != null) {
             try {
                 return Boolean.parseBoolean(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
 
+    /**
+     * For the given duration in milliseconds, obtain a readable representation of the form {@code HHH:MM:SS.mmm}, where
+     * <dl>
+     * <dt>HHH</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""03"")</dd>
+     * <dt>MM</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""05"")</dd>
+     * <dt>SS</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""09"")</dd>
+     * <dt>mmm</dt>
+     * <dd>is the fractional part of seconds, written with 1-3 digits (any trailing zeros are dropped)</dd>
+     * </dl>
+     * 
+     * @param durationInMillis the duration in milliseconds
+     * @return the readable duration.
+     */
+    public static String duration(long durationInMillis) {
+        // Calculate how many seconds, and don't lose any information ...
+        BigDecimal bigSeconds = new BigDecimal(Math.abs(durationInMillis)).divide(new BigDecimal(1000));
+        // Calculate the minutes, and round to lose the seconds
+        int minutes = bigSeconds.intValue() / 60;
+        // Remove the minutes from the seconds, to just have the remainder of seconds
+        double dMinutes = minutes;
+        double seconds = bigSeconds.doubleValue() - dMinutes * 60;
+        // Now compute the number of full hours, and change 'minutes' to hold the remaining minutes
+        int hours = minutes / 60;
+        minutes = minutes - (hours * 60);
+
+        // Format the string, and have at least 2 digits for the hours, minutes and whole seconds,
+        // and between 3 and 6 digits for the fractional part of the seconds...
+        String result = new DecimalFormat(""######00"").format(hours) + ':' + new DecimalFormat(""00"").format(minutes) + ':'
+                + new DecimalFormat(""00.0##"").format(seconds);
+        return result;
+    }
+
     private Strings() {
     }
 }",2016-06-02T19:05:06Z,102
"@@ -146,6 +146,53 @@
                       </assembly>
                     </build>
                   </image>
+                  <image>
+                    <!-- A Docker image using a partial MySQL installation maintained by MySQL team. -->
+                    <name>debezium/mysql-server-gtids-test-databases</name>
+                    <alias>database</alias>
+                    <run>
+                      <namingStrategy>alias</namingStrategy>
+                      <env>
+                        <MYSQL_ROOT_PASSWORD>debezium-rocks</MYSQL_ROOT_PASSWORD>
+                        <MYSQL_DATABASE>mysql</MYSQL_DATABASE> <!-- database created upon init -->
+                        <MYSQL_USER>${database.user}</MYSQL_USER>
+                        <MYSQL_PASSWORD>${database.password}</MYSQL_PASSWORD>
+                      </env>
+                      <ports>
+                        <port>${database.port}:3306</port>
+                      </ports>
+                      <log>
+                        <prefix>mysql</prefix>
+                        <enabled>true</enabled>
+                        <color>yellow</color>
+                      </log>
+                      <wait>
+                        <log>MySQL init process done. Ready for start up.</log>
+                        <time>30000</time> <!-- 30 seconds max -->
+                      </wait>
+                    </run>
+                    <build>
+                      <from>mysql/mysql-server:${version.mysql.server}</from>
+                      <assembly>
+                        <inline>
+                          <fileSets>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/server-gtids</directory>
+                              <includes>
+                                <include>my.cnf</include>
+                              </includes>
+                              <outputDirectory>etc/mysql</outputDirectory>
+                            </fileSet>
+                            <fileSet>
+                              <directory>${project.basedir}/src/test/docker/init</directory>
+                              <outputDirectory>docker-entrypoint-initdb.d</outputDirectory>
+                            </fileSet>
+                          </fileSets>
+                        </inline>
+                        <basedir>/</basedir>
+                      </assembly>
+                    </build>
+                  </image>
                   <image>
                     <!-- A Docker image using a complete MySQL installation maintained by Docker team. -->
                     <name>debezium/mysql-test--databases</name>
@@ -350,5 +397,22 @@
           <docker.skip>false</docker.skip>
         </properties>
       </profile>
+      <!--  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+            Use the Docker image for MySQL configured to use GTIDs.
+            To use, specify ""-Dgtid-mysql"" or -Pgtid-mysql on the Maven command line.
+            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -->
+      <profile>
+        <id>gtid-mysql</id>
+        <activation>
+          <activeByDefault>false</activeByDefault>
+          <property>
+            <name>gtid-mysql</name>
+          </property>
+        </activation>
+        <properties>
+          <docker.image>debezium/mysql-server-gtids-test-databases</docker.image>
+          <docker.skip>false</docker.skip>
+        </properties>
+      </profile>
     </profiles>
 </project>",2016-06-07T17:01:51Z,83
"@@ -88,6 +88,7 @@ protected void doStart() {
         eventHandlers.put(EventType.ROTATE, this::handleRotateLogsEvent);
         eventHandlers.put(EventType.TABLE_MAP, this::handleUpdateTableMetadata);
         eventHandlers.put(EventType.QUERY, this::handleQueryEvent);
+        eventHandlers.put(EventType.GTID, this::handleGtidEvent);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, this::handleInsert);
         eventHandlers.put(EventType.EXT_UPDATE_ROWS, this::handleUpdate);
         eventHandlers.put(EventType.EXT_DELETE_ROWS, this::handleDelete);
@@ -142,9 +143,10 @@ protected void ignoreEvent(Event event) {
     protected void handleEvent(Event event) {
         if (event == null) return;
 
-        // Update the source offset info ...
+        // Update the source offset info. Note that the client returns the value in *milliseconds*, even though the binlog
+        // contains only *seconds* precision ...
         EventHeader eventHeader = event.getHeader();
-        source.setBinlogTimestamp(eventHeader.getTimestamp());
+        source.setBinlogTimestampSeconds(eventHeader.getTimestamp()/1000L); // client returns milliseconds, we record seconds
         source.setBinlogServerId(eventHeader.getServerId());
         EventType eventType = eventHeader.getEventType();
         if (eventType == EventType.ROTATE) {
@@ -166,16 +168,6 @@ protected void handleEvent(Event event) {
                 source.setRowInEvent(0);
             }
         }
-        if (eventType == EventType.GTID) {
-            EventData eventData = event.getData();
-            GtidEventData gtidEventData;
-            if (eventData instanceof EventDeserializer.EventDataWrapper) {
-                gtidEventData = (GtidEventData) ((EventDeserializer.EventDataWrapper) eventData).getInternal();
-            } else {
-                gtidEventData = (GtidEventData) eventData;
-            }
-            source.setGtids(gtidEventData.getGtid());
-        }
 
         // If there is a handler for this event, forward the event to it ...
         try {
@@ -188,6 +180,15 @@ protected void handleEvent(Event event) {
         }
     }
 
+    @SuppressWarnings(""unchecked"")
+    protected <T extends EventData> T unwrapData(Event event) {
+        EventData eventData = event.getData();
+        if (eventData instanceof EventDeserializer.EventDataWrapper) {
+            eventData = ((EventDeserializer.EventDataWrapper) eventData).getInternal();
+        }
+        return (T)eventData;
+    }
+
     /**
      * Handle the supplied event that signals that mysqld has stopped.
      * 
@@ -226,19 +227,31 @@ protected void handleServerIncident(Event event) {
      */
     protected void handleRotateLogsEvent(Event event) {
         logger.debug(""Rotating logs: {}"", event);
-        RotateEventData command = event.getData();
+        RotateEventData command = unwrapData(event);
         assert command != null;
         recordMakers.clear();
     }
 
+    /**
+     * Handle the supplied event with a {@link GtidEventData} that signals the beginning of a GTID transaction.
+     * 
+     * @param event the GTID event to be processed; may not be null
+     */
+    protected void handleGtidEvent(Event event) {
+        logger.debug(""GTID transaction: {}"", event);
+        GtidEventData gtidEvent = unwrapData(event);
+        source.setGtid(gtidEvent.getGtid());
+        source.setGtidSet(client.getGtidSet());
+    }
+
     /**
      * Handle the supplied event with an {@link QueryEventData} by possibly recording the DDL statements as changes in the
      * MySQL schemas.
      * 
      * @param event the database change data event to be processed; may not be null
      */
     protected void handleQueryEvent(Event event) {
-        QueryEventData command = event.getData();
+        QueryEventData command = unwrapData(event);
         logger.debug(""Received update table command: {}"", event);
         context.dbSchema().applyDdl(context.source(), command.getDatabase(), command.getSql(), (dbName, statements) -> {
             if (recordSchemaChangesInSourceRecords && recordMakers.schemaChanges(dbName, statements, super::enqueueRecord) > 0) {
@@ -262,7 +275,7 @@ protected void handleQueryEvent(Event event) {
      * @param event the update event; never null
      */
     protected void handleUpdateTableMetadata(Event event) {
-        TableMapEventData metadata = event.getData();
+        TableMapEventData metadata = unwrapData(event);
         long tableNumber = metadata.getTableId();
         String databaseName = metadata.getDatabase();
         String tableName = metadata.getTable();
@@ -281,7 +294,7 @@ protected void handleUpdateTableMetadata(Event event) {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleInsert(Event event) throws InterruptedException {
-        WriteRowsEventData write = event.getData();
+        WriteRowsEventData write = unwrapData(event);
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);
@@ -302,7 +315,7 @@ protected void handleInsert(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleUpdate(Event event) throws InterruptedException {
-        UpdateRowsEventData update = event.getData();
+        UpdateRowsEventData update = unwrapData(event);
         long tableNumber = update.getTableId();
         BitSet includedColumns = update.getIncludedColumns();
         // BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
@@ -330,7 +343,7 @@ protected void handleUpdate(Event event) throws InterruptedException {
      * @throws InterruptedException if this thread is interrupted while blocking
      */
     protected void handleDelete(Event event) throws InterruptedException {
-        DeleteRowsEventData deleted = event.getData();
+        DeleteRowsEventData deleted = unwrapData(event);
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         RecordsForTable recordMaker = recordMakers.forTable(tableNumber, includedColumns, super::enqueueRecord);",2016-06-07T17:01:51Z,67
"@@ -7,6 +7,7 @@
 
 import java.util.Collection;
 import java.util.Collections;
+import java.util.Iterator;
 import java.util.LinkedList;
 import java.util.Map;
 import java.util.TreeMap;
@@ -111,11 +112,17 @@ protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
                 intervals.add(new Interval(interval.getStart(), interval.getEnd()));
             });
             Collections.sort(this.intervals);
-        }
-
-        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
-            this.uuid = uuid;
-            this.intervals = intervals;
+            if ( this.intervals.size() > 1 ) {
+                // Collapse adjacent intervals ...
+                for ( int i=intervals.size()-1; i!=0; --i) {
+                    Interval before = this.intervals.get(i-1);
+                    Interval after = this.intervals.get(i);
+                    if ( (before.getEnd() + 1) == after.getStart() ) {
+                        this.intervals.set(i-1,new Interval(before.getStart(),after.getEnd()));
+                        this.intervals.remove(i);
+                    }
+                }
+            }
         }
 
         /**
@@ -218,17 +225,41 @@ public String toString() {
             StringBuilder sb = new StringBuilder();
             if (sb.length() != 0) sb.append(',');
             sb.append(uuid).append(':');
-            sb.append(intervals.getFirst().getStart());
-            sb.append(intervals.getLast().getEnd());
+            Iterator<Interval> iter = intervals.iterator();
+            if ( iter.hasNext() ) sb.append(iter.next());
+            while ( iter.hasNext() ) {
+                sb.append(':');
+                sb.append(iter.next());
+            }
             return sb.toString();
         }
     }
 
     @Immutable
-    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+    public static class Interval implements Comparable<Interval> {
+
+        private final long start;
+        private final long end;
 
         public Interval(long start, long end) {
-            super(start, end);
+            this.start = start;
+            this.end = end;
+        }
+
+        /**
+         * Get the starting transaction number in this interval.
+         * @return this interval's first transaction number
+         */
+        public long getStart() {
+            return start;
+        }
+
+        /**
+         * Get the ending transaction number in this interval.
+         * @return this interval's last transaction number
+         */
+        public long getEnd() {
+            return end;
         }
 
         /**
@@ -244,6 +275,15 @@ public boolean isSubsetOf(Interval other) {
             if (other == null) return false;
             return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
         }
+        
+        @Override
+        public int compareTo(Interval that) {
+            if ( that == this ) return 0;
+            long diff = this.start - that.start;
+            if ( diff > Integer.MAX_VALUE ) return Integer.MAX_VALUE;
+            if ( diff < Integer.MIN_VALUE ) return Integer.MIN_VALUE;
+            return (int) diff;
+        }
 
         @Override
         public int hashCode() {",2016-06-07T17:01:51Z,87
"@@ -9,6 +9,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
+import java.util.concurrent.atomic.AtomicReference;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -110,9 +111,16 @@ public void start(Map<String, String> props) {
             } else {
                 // We are allowed to use snapshots, and that is the best way to start ...
                 startWithSnapshot = true;
+                // The snapshot will determine if GTIDs are set
             }
         }
 
+        if (!startWithSnapshot && source.gtidSet() == null && isGtidModeEnabled()) {
+            // The snapshot will properly determine the GTID set, but we're not starting with a snapshot and GTIDs were not
+            // previously used but the MySQL server has them enabled ...
+            source.setGtidSet("""");
+        }
+
         // Set up the readers ...
         this.binlogReader = new BinlogReader(taskContext);
         if (startWithSnapshot) {
@@ -174,13 +182,33 @@ protected void transitionToReadBinlog() {
      * @return {@code true} if the server has the binlog coordinates, or {@code false} otherwise
      */
     protected boolean isBinlogAvailable() {
+        String gtidStr = taskContext.source().gtidSet();
+        if ( gtidStr != null) {
+            if ( gtidStr.trim().isEmpty() ) return true; // start at beginning ...
+            String availableGtidStr = knownGtidSet();
+            if ( availableGtidStr == null || availableGtidStr.trim().isEmpty() ) {
+                // Last offsets had GTIDs but the server does not use them ...
+                logger.info(""Connector used GTIDs previously, but MySQL does not know of any GTIDs or they are not enabled"");
+                return false;
+            }
+            // GTIDs are enabled, and we used them previously ...
+            GtidSet gtidSet = new GtidSet(gtidStr);
+            GtidSet availableGtidSet = new GtidSet(knownGtidSet());
+            if ( gtidSet.isSubsetOf(availableGtidSet)) {
+                return true;
+            }
+            logger.info(""Connector last known GTIDs are {}, but MySQL has {}"",gtidSet,availableGtidSet);
+            return false;
+        }
+        
         String binlogFilename = taskContext.source().binlogFilename();
         if (binlogFilename == null) return true; // start at current position
         if (binlogFilename.equals("""")) return true; // start at beginning
 
         // Accumulate the available binlog filenames ...
         List<String> logNames = new ArrayList<>();
         try {
+            logger.info(""Stop 0: Get all known binlogs from MySQL"");
             taskContext.jdbc().query(""SHOW BINARY LOGS"", rs -> {
                 while (rs.next()) {
                     logNames.add(rs.getString(1));
@@ -191,6 +219,50 @@ protected boolean isBinlogAvailable() {
         }
 
         // And compare with the one we're supposed to use ...
-        return logNames.stream().anyMatch(binlogFilename::equals);
+        boolean found = logNames.stream().anyMatch(binlogFilename::equals);
+        if ( !found ) {
+            logger.info(""Connector requires binlog file '{}', but MySQL only has {}"",binlogFilename,String.join("", "",logNames));
+        }
+        return found;
+    }
+
+    /**
+     * Determine whether the MySQL server has GTIDs enabled.
+     * 
+     * @return {@code false} if the server's {@code gtid_mode} is set and is {@code OFF}, or {@code true} otherwise
+     */
+    protected boolean isGtidModeEnabled() {
+        AtomicReference<String> mode = new AtomicReference<String>(""off"");
+        try {
+            taskContext.jdbc().query(""SHOW GLOBAL VARIABLES LIKE 'GTID_MODE'"", rs -> {
+                if (rs.next()) {
+                    mode.set(rs.getString(1));
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return !""OFF"".equalsIgnoreCase(mode.get());
+    }
+
+    /**
+     * Determine the available GTID set for MySQL.
+     * 
+     * @return the string representation of MySQL's GTID sets.
+     */
+    protected String knownGtidSet() {
+        AtomicReference<String> gtidSetStr = new AtomicReference<String>();
+        try {
+            taskContext.jdbc().query(""SHOW MASTER STATUS"", rs -> {
+                if (rs.next()) {
+                    gtidSetStr.set(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
+                }
+            });
+        } catch (SQLException e) {
+            throw new ConnectException(""Unexpected error while connnecting to MySQL and looking at GTID mode: "" + e.getMessage());
+        }
+
+        return gtidSetStr.get();
     }
 }",2016-06-07T17:01:51Z,68
"@@ -188,7 +188,7 @@ protected void execute() {
                 if (rs.next()) {
                     source.setBinlogFilename(rs.getString(1));
                     source.setBinlogPosition(rs.getLong(2));
-                    source.setGtids(rs.getString(5));// GTIDs
+                    source.setGtidSet(rs.getString(5));// GTID set, may be null, blank, or contain a GTID set
                     source.startSnapshot();
                 }
             });",2016-06-07T17:01:51Z,62
"@@ -14,6 +14,7 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.data.Envelope;
 import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
@@ -26,7 +27,7 @@
  * 
  * <pre>
  * {
- *     ""db"" : ""myDatabase""
+ *     ""server"" : ""production-server""
  * }
  * </pre>
  * 
@@ -36,14 +37,40 @@
  * 
  * <pre>
  * {
- *         ""gtids"" = ""3E11FA47-71CA-11E1-9E33-C80AA9429562:1-5"",
- *         ""file"" = ""mysql-bin.000003"",
- *         ""pos"" = 105586,
- *         ""row"" = 0
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtids"" = ""db58b0ae-2c10-11e6-b284-0242ac110002:1-199"",
+ *     ""file"" = ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"" = 0,
+ *     ""snapshot"": true
  * }
  * </pre>
  * 
- * Note that the ""{@code gtid}"" field is present only when GTIDs are enabled.
+ * The ""{@code gtids}"" field only appears in offsets produced when GTIDs are enabled. The ""{@code snapshot}"" field only appears in
+ * offsets produced when the connector is in the middle of a snapshot. And finally, the ""{@code ts}"" field contains the
+ * <em>seconds</em> since Unix epoch (since Jan 1, 1970) of the MySQL event; the message {@link Envelope envelopes} also have a
+ * timestamp, but that timestamp is the <em>milliseconds</em> since since Jan 1, 1970.
+ * 
+ * The {@link #struct() source} struct appears in each message envelope and contains MySQL information about the event. It is
+ * a mixture the field from the {@link #partition() partition} (which is renamed in the source to make more sense), most of
+ * the fields from the {@link #offset() offset} (with the exception of {@code gtids}), and, when GTIDs are enabled, the
+ * GTID of the transaction in which the event occurs. Like with the offset, the ""{@code snapshot}"" field only appears for
+ * events produced when the connector is in the middle of a snapshot. Here's a JSON-like representation of the source for
+ * an event that corresponds to the above partition and offset:
+ * 
+ * <pre>
+ * {
+ *     ""name"": ""production-server"",
+ *     ""server_id"": 112233,
+ *     ""ts_sec"": 1465236179,
+ *     ""gtid"": ""db58b0ae-2c10-11e6-b284-0242ac110002:199"",
+ *     ""file"": ""mysql-bin.000003"",
+ *     ""pos"" = 105586,
+ *     ""row"": 0,
+ *     ""snapshot"": true
+ * }
+ * </pre>
  * 
  * @author Randall Hauch
  */
@@ -56,11 +83,12 @@ final class SourceInfo {
 
     public static final String SERVER_NAME_KEY = ""name"";
     public static final String SERVER_PARTITION_KEY = ""server"";
-    public static final String GTID_KEY = ""gtids"";
+    public static final String GTID_SET_KEY = ""gtids"";
+    public static final String GTID_KEY = ""gtid"";
     public static final String BINLOG_FILENAME_OFFSET_KEY = ""file"";
     public static final String BINLOG_POSITION_OFFSET_KEY = ""pos"";
     public static final String BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY = ""row"";
-    public static final String TIMESTAMP_KEY = ""ts"";
+    public static final String TIMESTAMP_KEY = ""ts_sec"";
     public static final String SNAPSHOT_KEY = ""snapshot"";
 
     /**
@@ -78,13 +106,14 @@ final class SourceInfo {
                                                      .field(SNAPSHOT_KEY, Schema.OPTIONAL_BOOLEAN_SCHEMA)
                                                      .build();
 
-    private GtidSet binlogGtids;
+    private String gtidSet;
+    private String binlogGtid;
     private String binlogFilename;
     private long binlogPosition = 4;
     private int eventRowNumber = 0;
     private String serverName;
     private long serverId = 0;
-    private long binlogTs = 0;
+    private long binlogTimestampSeconds = 0;
     private Map<String, String> sourcePartition;
     private boolean snapshot = false;
 
@@ -122,11 +151,11 @@ public Map<String, String> partition() {
      */
     public Map<String, ?> offset() {
         Map<String, Object> map = new HashMap<>();
-        if (binlogGtids != null) {
-            map.put(GTID_KEY, binlogGtids.toString());
+        if (serverId != 0) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTimestampSeconds != 0) map.put(TIMESTAMP_KEY, binlogTimestampSeconds);
+        if (gtidSet != null) {
+            map.put(GTID_SET_KEY, gtidSet);
         }
-        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
-        if (binlogTs != 0 ) map.put(TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -158,13 +187,14 @@ public Struct struct() {
         Struct result = new Struct(SCHEMA);
         result.put(SERVER_NAME_KEY, serverName);
         result.put(SERVER_ID_KEY, serverId);
-        if (binlogGtids != null) {
-            result.put(GTID_KEY, binlogGtids.toString());
+        // Don't put the GTID Set into the struct; only the current GTID is fine ...
+        if (binlogGtid != null) {
+            result.put(GTID_KEY, binlogGtid);
         }
         result.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         result.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         result.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
-        result.put(TIMESTAMP_KEY, binlogTs);
+        result.put(TIMESTAMP_KEY, binlogTimestampSeconds);
         if (isSnapshotInEffect()) {
             result.put(SNAPSHOT_KEY, true);
         }
@@ -193,12 +223,23 @@ public boolean isSnapshotInEffect() {
     }
 
     /**
-     * Set the GTID range for the MySQL binary log file.
+     * Set the latest GTID from the MySQL binary log file.
+     * 
+     * @param gtid the string representation of a specific GTID; may not be null
+     */
+    public void setGtid(String gtid) {
+        this.binlogGtid = gtid;
+    }
+
+    /**
+     * Set the set of GTIDs known to the MySQL server.
      * 
-     * @param gtids the string representation of the binlog GTIDs; may not be null
+     * @param gtidSet the string representation of GTID set; may not be null
      */
-    public void setGtids(String gtids) {
-        this.binlogGtids = gtids != null && !gtids.trim().isEmpty() ? new GtidSet(gtids) : null;
+    public void setGtidSet(String gtidSet) {
+        if (gtidSet != null && !gtidSet.trim().isEmpty()) {
+            this.gtidSet = gtidSet;
+        }
     }
 
     /**
@@ -239,12 +280,12 @@ public void setBinlogServerId(long serverId) {
     }
 
     /**
-     * Set the timestamp as found within the MySQL binary log file.
+     * Set the number of <em>seconds</em> since Unix epoch (January 1, 1970) as found within the MySQL binary log file.
      * 
-     * @param timestamp the timestamp found within the binary log file
+     * @param timestampInSeconds the timestamp in <em>seconds</em> found within the binary log file
      */
-    public void setBinlogTimestamp(long timestamp) {
-        this.binlogTs = timestamp;
+    public void setBinlogTimestampSeconds(long timestampInSeconds) {
+        this.binlogTimestampSeconds = timestampInSeconds / 1000;
     }
 
     /**
@@ -270,7 +311,7 @@ public void completeSnapshot() {
     public void setOffset(Map<String, ?> sourceOffset) {
         if (sourceOffset != null) {
             // We have previously recorded an offset ...
-            setGtids((String) sourceOffset.get(GTID_KEY)); // may be null
+            setGtidSet((String) sourceOffset.get(GTID_SET_KEY)); // may be null
             binlogFilename = (String) sourceOffset.get(BINLOG_FILENAME_OFFSET_KEY);
             if (binlogFilename == null) {
                 throw new ConnectException(""Source offset '"" + BINLOG_FILENAME_OFFSET_KEY + ""' parameter is missing"");
@@ -297,7 +338,7 @@ private long longOffsetValue(Map<String, ?> values, String key) {
      * @return the string representation of the binlog GTID ranges; may be null
      */
     public String gtidSet() {
-        return this.binlogGtids != null ? this.binlogGtids.toString() : null;
+        return this.gtidSet != null ? this.gtidSet.toString() : null;
     }
 
     /**
@@ -340,9 +381,9 @@ public String serverName() {
     @Override
     public String toString() {
         StringBuilder sb = new StringBuilder();
-        if (binlogGtids != null) {
+        if (gtidSet != null) {
             sb.append(""GTIDs "");
-            sb.append(binlogGtids);
+            sb.append(gtidSet);
             sb.append("" and binlog file '"").append(binlogFilename).append(""'"");
             sb.append("", pos="").append(binlogPosition());
             sb.append("", row="").append(eventRowNumber());
@@ -379,17 +420,17 @@ public String toString() {
      * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
      */
     public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
-        String recordedGtidSetStr = recorded.getString(GTID_KEY);
-        String desiredGtidSetStr = desired.getString(GTID_KEY);
+        String recordedGtidSetStr = recorded.getString(GTID_SET_KEY);
+        String desiredGtidSetStr = desired.getString(GTID_SET_KEY);
         if (desiredGtidSetStr != null) {
             // The desired position uses GTIDs, so we ideally compare using GTIDs ...
             if (recordedGtidSetStr != null) {
                 // Both have GTIDs, so base the comparison entirely on the GTID sets.
                 GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
                 GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
-                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                if (recordedGtidSet.equals(desiredGtidSet)) {
                     // They are exactly the same, which means the recorded position exactly matches the desired ...
-                    if ( !recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
+                    if (!recorded.has(SNAPSHOT_KEY) && desired.has(SNAPSHOT_KEY)) {
                         // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
                         return false;
                     }
@@ -414,35 +455,35 @@ public static boolean isPositionAtOrBefore(Document recorded, Document desired)
         }
 
         // Both positions are missing GTIDs. Look at the servers ...
-        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
-        if ( recordedServerId != desiredServerId ) {
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY, 0);
+        if (recordedServerId != desiredServerId) {
             // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
             // is compare timestamps, and we have to assume that the server timestamps can be compared ...
-            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
-            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY,0);
+            long recordedTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
+            long desiredTimestamp = recorded.getLong(TIMESTAMP_KEY, 0);
             return recordedTimestamp <= desiredTimestamp;
         }
-        
+
         // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
         // comparable ...
         String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
         String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
         assert recordedFilename != null;
         int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The filenames are the same, so compare the positions ...
         int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
         diff = recordedPosition - desiredPosition;
-        if ( diff > 0 ) return false;
-        
+        if (diff > 0) return false;
+
         // The positions are the same, so compare the row number ...
         int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
         diff = recordedRow - desiredRow;
-        if ( diff > 0 ) return false;
+        if (diff > 0) return false;
 
         // The binlog coordinates are the same ...
         return true;",2016-06-07T17:01:51Z,70
"@@ -0,0 +1,52 @@
+# For advice on how to change settings please see
+# http://dev.mysql.com/doc/refman/5.7/en/server-configuration-defaults.html
+
+[mysqld]
+#
+# Remove leading # and set to the amount of RAM for the most important data
+# cache in MySQL. Start at 70% of total RAM for dedicated server, else 10%.
+# innodb_buffer_pool_size = 128M
+#
+# Remove leading # to turn on a very important data integrity option: logging
+# changes to the binary log between backups.
+# log_bin
+#
+# Remove leading # to set options mainly useful for reporting servers.
+# The server defaults are faster for transactions and fast SELECTs.
+# Adjust sizes as needed, experiment to find the optimal values.
+# join_buffer_size = 128M
+# sort_buffer_size = 2M
+# read_rnd_buffer_size = 2M
+skip-host-cache
+skip-name-resolve
+datadir=/var/lib/mysql
+socket=/var/lib/mysql/mysql.sock
+secure-file-priv=/var/lib/mysql-files
+user=mysql
+
+# Disabling symbolic-links is recommended to prevent assorted security risks
+symbolic-links=0
+
+log-error=/var/log/mysqld.log
+pid-file=/var/run/mysqld/mysqld.pid
+
+# ----------------------------------------------
+# Enable GTIDs on this master
+# ----------------------------------------------
+gtid_mode                 = on
+enforce_gtid_consistency  = on
+
+# ----------------------------------------------
+# Debezium ingest
+# ----------------------------------------------
+
+# Enable binary replication log and set the prefix, expiration, and log format.
+# The prefix is arbitrary, expiration can be short for integration tests but would
+# be longer on a production system. Row-level info is required for ingest to work.
+# Server ID is required, but this will vary on production systems
+server-id         = 112233
+log_bin           = mysql-bin
+expire_logs_days  = 1
+binlog_format     = row
+
+",2016-06-07T17:01:51Z,104
"@@ -0,0 +1,107 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+import io.debezium.connector.mysql.GtidSet.Interval;
+import io.debezium.connector.mysql.GtidSet.UUIDSet;
+
+/**
+ * @author Randall Hauch
+ *
+ */
+public class GtidSetTest {
+    
+    private static final String UUID1 = ""24bc7850-2c16-11e6-a073-0242ac110002"";
+
+    private GtidSet gtids;
+    
+    @Test
+    public void shouldCreateSetWithSingleInterval() {
+        gtids = new GtidSet(UUID1 + "":1-191"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,191);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,1,191);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191"");
+    }
+    
+    @Test
+    public void shouldCollapseAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199"");
+        asertIntervalCount(UUID1,1);
+        asertIntervalExists(UUID1,1,199);
+        asertFirstInterval(UUID1,1,199);
+        asertLastInterval(UUID1,1,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199"");
+    }
+
+    
+    @Test
+    public void shouldNotCollapseNonAdjacentIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199"");
+        asertIntervalCount(UUID1,2);
+        asertFirstInterval(UUID1,1,191);
+        asertLastInterval(UUID1,193,199);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervals() {
+        gtids = new GtidSet(UUID1 + "":1-191:193-199:1000-1033"");
+        asertIntervalCount(UUID1,3);
+        asertFirstInterval(UUID1,1,191);
+        asertIntervalExists(UUID1,193,199);
+        asertLastInterval(UUID1,1000,1033);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-191:193-199:1000-1033"");
+    }
+    
+    @Test
+    public void shouldCreateWithMultipleIntervalsThatMayBeAdjacent() {
+        gtids = new GtidSet(UUID1 + "":1-191:192-199:1000-1033:1035-1036:1038-1039"");
+        asertIntervalCount(UUID1, 4);
+        asertFirstInterval(UUID1, 1, 199);
+        asertIntervalExists(UUID1, 1000, 1033);
+        asertIntervalExists(UUID1, 1035, 1036);
+        asertLastInterval(UUID1, 1038, 1039);
+        assertThat(gtids.toString()).isEqualTo(UUID1 + "":1-199:1000-1033:1035-1036:1038-1039""); // ??
+    }
+    
+    protected void asertIntervalCount( String uuid, int count) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        assertThat(set.getIntervals().size()).isEqualTo(count);
+    }
+    
+    protected void asertIntervalExists( String uuid, int start, int end) {
+        assertThat(hasInterval(uuid,start,end)).isTrue();
+    }
+    
+    protected void asertFirstInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getFirstInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected void asertLastInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        Interval interval = set.getLastInterval();
+        assertThat(interval.getStart()).isEqualTo(start);
+        assertThat(interval.getEnd()).isEqualTo(end);
+    }
+    
+    protected boolean hasInterval( String uuid, int start, int end) {
+        UUIDSet set = gtids.forServerWithId(uuid);
+        for ( Interval interval : set.getIntervals() ) {
+            if ( interval.getStart() == start && interval.getEnd() == end ) return true;
+        }
+        return false;
+    }
+
+}",2016-06-07T17:01:51Z,105
"@@ -86,9 +86,9 @@ protected Document positionWithGtids(String gtids) {
 
     protected Document positionWithGtids(String gtids, boolean snapshot) {
         if (snapshot) {
-            return Document.create(SourceInfo.GTID_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
+            return Document.create(SourceInfo.GTID_SET_KEY, gtids, SourceInfo.SNAPSHOT_KEY, true);
         }
-        return Document.create(SourceInfo.GTID_KEY, gtids);
+        return Document.create(SourceInfo.GTID_SET_KEY, gtids);
     }
 
     protected Document positionWithoutGtids(String filename, int position, int row) {",2016-06-07T17:01:51Z,70
"@@ -45,15 +45,18 @@ public static enum Operation {
          */
         DELETE(""d"");
         private final String code;
+
         private Operation(String code) {
             this.code = code;
         }
-        public static Operation forCode( String code ) {
-            for ( Operation op : Operation.values()) {
-                if ( op.code().equalsIgnoreCase(code)) return op;
+
+        public static Operation forCode(String code) {
+            for (Operation op : Operation.values()) {
+                if (op.code().equalsIgnoreCase(code)) return op;
             }
             return null;
         }
+
         public String code() {
             return code;
         }
@@ -81,11 +84,13 @@ public static final class FieldName {
          */
         public static final String SOURCE = ""source"";
         /**
-         * The {@code ts} field is used to store the information about the local time at which the connector processed/generated
-         * the event. Note that the accuracy of the timestamp is not defined, and the values may not always be monotonically
-         * increasing.
+         * The {@code ts_ms} field is used to store the information about the local time at which the connector
+         * processed/generated the event. The timestamp values are the number of milliseconds past epoch (January 1, 1970), and
+         * determined by the {@link System#currentTimeMillis() JVM current time in milliseconds}. Note that the <em>accuracy</em>
+         * of the timestamp value depends on the JVM's system clock and all of its assumptions, limitations, conditions, and
+         * variations.
          */
-        public static final String TIMESTAMP = ""ts"";
+        public static final String TIMESTAMP = ""ts_ms"";
     }
 
     /**
@@ -304,6 +309,7 @@ public Struct delete(Struct before, Struct source, Long timestamp) {
 
     /**
      * Obtain the operation for the given source record.
+     * 
      * @param record the source record; may not be null
      * @return the operation, or null if no valid operation was found in the record
      */",2016-06-07T17:01:51Z,106
"@@ -0,0 +1,268 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.connector.mysql;
+
+import java.util.Collection;
+import java.util.Collections;
+import java.util.LinkedList;
+import java.util.Map;
+import java.util.TreeMap;
+
+import io.debezium.annotation.Immutable;
+
+/**
+ * A set of MySQL GTIDs. This is an improvement of {@link com.github.shyiko.mysql.binlog.GtidSet} that is immutable,
+ * and more properly supports comparisons.
+ * 
+ * @author Randall Hauch
+ */
+@Immutable
+public final class GtidSet {
+
+    private final String orderedString;
+    private final Map<String, UUIDSet> uuidSetsByServerId = new TreeMap<>(); // sorts on keys
+
+    /**
+     * @param gtids the string representation of the GTIDs.
+     */
+    public GtidSet(String gtids) {
+        new com.github.shyiko.mysql.binlog.GtidSet(gtids).getUUIDSets().forEach(uuidSet -> {
+            uuidSetsByServerId.put(uuidSet.getUUID(), new UUIDSet(uuidSet));
+        });
+        StringBuilder sb = new StringBuilder();
+        uuidSetsByServerId.values().forEach(uuidSet -> {
+            if (sb.length() != 0) sb.append(',');
+            sb.append(uuidSet.toString());
+        });
+        orderedString = sb.toString();
+    }
+
+    /**
+     * Get an immutable collection of the {@link UUIDSet range of GTIDs for a single server}.
+     * 
+     * @return the {@link UUIDSet GTID ranges for each server}; never null
+     */
+    public Collection<UUIDSet> getUUIDSets() {
+        return Collections.unmodifiableCollection(uuidSetsByServerId.values());
+    }
+
+    /**
+     * Find the {@link UUIDSet} for the server with the specified UUID.
+     * 
+     * @param uuid the UUID of the server
+     * @return the {@link UUIDSet} for the identified server, or {@code null} if there are no GTIDs from that server.
+     */
+    public UUIDSet forServerWithId(String uuid) {
+        return uuidSetsByServerId.get(uuid);
+    }
+
+    /**
+     * Determine if the GTIDs represented by this object are contained completely within the supplied set of GTIDs.
+     * 
+     * @param other the other set of GTIDs; may be null
+     * @return {@code true} if all of the GTIDs in this set are completely contained within the supplied set of GTIDs, or
+     *         {@code false} otherwise
+     */
+    public boolean isSubsetOf(GtidSet other) {
+        if (other == null) return false;
+        if (this.equals(other)) return true;
+        for (UUIDSet uuidSet : uuidSetsByServerId.values()) {
+            UUIDSet thatSet = other.forServerWithId(uuidSet.getUUID());
+            if (!uuidSet.isSubsetOf(thatSet)) return false;
+        }
+        return true;
+    }
+
+    @Override
+    public int hashCode() {
+        return orderedString.hashCode();
+    }
+
+    @Override
+    public boolean equals(Object obj) {
+        if (obj == this) return true;
+        if (obj instanceof GtidSet) {
+            GtidSet that = (GtidSet) obj;
+            return this.orderedString.equalsIgnoreCase(that.orderedString);
+        }
+        return false;
+    }
+
+    @Override
+    public String toString() {
+        return orderedString;
+    }
+
+    /**
+     * A range of GTIDs for a single server with a specific UUID.
+     */
+    @Immutable
+    public static class UUIDSet {
+
+        private String uuid;
+        private LinkedList<Interval> intervals = new LinkedList<>();
+
+        protected UUIDSet(com.github.shyiko.mysql.binlog.GtidSet.UUIDSet uuidSet) {
+            this.uuid = uuidSet.getUUID();
+            uuidSet.getIntervals().forEach(interval -> {
+                intervals.add(new Interval(interval.getStart(), interval.getEnd()));
+            });
+            Collections.sort(this.intervals);
+        }
+
+        protected UUIDSet(String uuid, LinkedList<Interval> intervals) {
+            this.uuid = uuid;
+            this.intervals = intervals;
+        }
+
+        /**
+         * Get the UUID for the server that generated the GTIDs.
+         * 
+         * @return the server's UUID; never null
+         */
+        public String getUUID() {
+            return uuid;
+        }
+
+        /**
+         * Get the intervals of transaction numbers.
+         * 
+         * @return the immutable transaction intervals; never null
+         */
+        public Collection<Interval> getIntervals() {
+            return Collections.unmodifiableCollection(intervals);
+        }
+
+        /**
+         * Get the first interval of transaction numbers for this server.
+         * 
+         * @return the first interval, or {@code null} if there is none
+         */
+        public Interval getFirstInterval() {
+            return intervals.isEmpty() ? null : intervals.getFirst();
+        }
+
+        /**
+         * Get the last interval of transaction numbers for this server.
+         * 
+         * @return the last interval, or {@code null} if there is none
+         */
+        public Interval getLastInterval() {
+            return intervals.isEmpty() ? null : intervals.getLast();
+        }
+
+        /**
+         * Get the interval that contains the full range (and possibly more) of all of the individual intervals for this server.
+         * 
+         * @return the complete interval comprised of the {@link Interval#getStart() start} of the {@link #getFirstInterval()
+         *         first interval} and the {@link Interval#getEnd() end} of the {@link #getLastInterval()}, or {@code null} if
+         *         this server has no intervals at all
+         */
+        public Interval getCompleteInterval() {
+            return intervals.isEmpty() ? null : new Interval(getFirstInterval().getStart(), getLastInterval().getEnd());
+        }
+
+        /**
+         * Determine if the set of transaction numbers from this server is completely within the set of transaction numbers from
+         * the set of transaction numbers in the supplied set.
+         * 
+         * @param other the set to compare with this set
+         * @return {@code true} if this server's transaction numbers are a subset of the transaction numbers of the supplied set,
+         *         or false otherwise
+         */
+        public boolean isSubsetOf(UUIDSet other) {
+            if (other == null) return false;
+            if (!this.getUUID().equalsIgnoreCase(other.getUUID())) {
+                // Not even the same server ...
+                return false;
+            }
+            if (this.intervals.isEmpty()) return true;
+            if (other.intervals.isEmpty()) return false;
+            assert this.intervals.size() > 0;
+            assert other.intervals.size() > 0;
+
+            // Every interval in this must be within an interval of the other ...
+            for (Interval thisInterval : this.intervals) {
+                boolean found = false;
+                for (Interval otherInterval : other.intervals) {
+                    if (thisInterval.isSubsetOf(otherInterval)) {
+                        found = true;
+                        break;
+                    }
+                }
+                if (!found) return false; // didn't find a match
+            }
+            return true;
+        }
+
+        @Override
+        public int hashCode() {
+            return uuid.hashCode();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (obj == this) return true;
+            if (obj instanceof UUIDSet) {
+                UUIDSet that = (UUIDSet) obj;
+                return this.getUUID().equalsIgnoreCase(that.getUUID()) && this.getIntervals().equals(that.getIntervals());
+            }
+            return super.equals(obj);
+        }
+
+        @Override
+        public String toString() {
+            StringBuilder sb = new StringBuilder();
+            if (sb.length() != 0) sb.append(',');
+            sb.append(uuid).append(':');
+            sb.append(intervals.getFirst().getStart());
+            sb.append(intervals.getLast().getEnd());
+            return sb.toString();
+        }
+    }
+
+    @Immutable
+    public static class Interval extends com.github.shyiko.mysql.binlog.GtidSet.Interval {
+
+        public Interval(long start, long end) {
+            super(start, end);
+        }
+
+        /**
+         * Determine if this interval is completely within the supplied interval.
+         * 
+         * @param other the interval to compare with
+         * @return {@code true} if the {@link #getStart() start} is greater than or equal to the supplied interval's
+         *         {@link #getStart() start} and the {@link #getEnd() end} is less than or equal to the supplied interval's
+         *         {@link #getEnd() end}, or {@code false} otherwise
+         */
+        public boolean isSubsetOf(Interval other) {
+            if (other == this) return true;
+            if (other == null) return false;
+            return this.getStart() >= other.getStart() && this.getEnd() <= other.getEnd();
+        }
+
+        @Override
+        public int hashCode() {
+            return (int) getStart();
+        }
+
+        @Override
+        public boolean equals(Object obj) {
+            if (this == obj) return true;
+            if (obj instanceof com.github.shyiko.mysql.binlog.GtidSet.Interval) {
+                com.github.shyiko.mysql.binlog.GtidSet.Interval that = (com.github.shyiko.mysql.binlog.GtidSet.Interval) obj;
+                return this.getStart() == that.getStart() && this.getEnd() == that.getEnd();
+            }
+            return false;
+        }
+
+        @Override
+        public String toString() {
+            return getStart() == getEnd() ? Long.toString(getStart()) : """" + getStart() + ""-"" + getEnd();
+        }
+    }
+}",2016-06-04T21:20:26Z,87
"@@ -76,7 +76,8 @@ public class MySqlConnectorConfig {
                                                       .withDescription(""The name of the DatabaseHistory class that should be used to store and recover database schema changes. ""
                                                               + ""The configuration properties for the history are prefixed with the '""
                                                               + DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING + ""' string."")
-                                                      .withDefault(KafkaDatabaseHistory.class.getName());
+                                                      .withDefault(KafkaDatabaseHistory.class.getName())
+                                                      .withValidation(Field::isClassName);
 
     public static final Field INCLUDE_SCHEMA_CHANGES = Field.create(""include.schema.changes"")
                                                             .withDescription(""Whether the connector should publish changes in the database schema to a Kafka topic with """,2016-06-04T21:20:26Z,40
"@@ -27,6 +27,7 @@
 import io.debezium.relational.ddl.DdlChanges;
 import io.debezium.relational.ddl.DdlChanges.DatabaseStatementStringConsumer;
 import io.debezium.relational.history.DatabaseHistory;
+import io.debezium.relational.history.HistoryRecordComparator;
 import io.debezium.text.ParsingException;
 import io.debezium.util.Collect;
 
@@ -51,6 +52,8 @@
 @NotThreadSafe
 public class MySqlSchema {
 
+    private static final HistoryRecordComparator HISTORY_COMPARATOR = HistoryRecordComparator.usingPositions(SourceInfo::isPositionAtOrBefore);
+
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final Set<String> ignoredQueryStatements = Collect.unmodifiableSet(""BEGIN"", ""END"", ""FLUSH PRIVILEGES"");
     private final MySqlDdlParser ddlParser;
@@ -85,7 +88,7 @@ public MySqlSchema(Configuration config, String serverName) {
         }
         // Do not remove the prefix from the subset of config properties ...
         Configuration dbHistoryConfig = config.subset(DatabaseHistory.CONFIGURATION_FIELD_PREFIX_STRING, false);
-        this.dbHistory.configure(dbHistoryConfig); // validates
+        this.dbHistory.configure(dbHistoryConfig,HISTORY_COMPARATOR); // validates
     }
 
     /**",2016-06-04T21:20:26Z,19
"@@ -13,9 +13,8 @@
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.errors.ConnectException;
 
-import com.github.shyiko.mysql.binlog.GtidSet;
-
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.document.Document;
 import io.debezium.util.Collect;
 
 /**
@@ -126,6 +125,8 @@ public Map<String, String> partition() {
         if (binlogGtids != null) {
             map.put(BINLOG_GTID_KEY, binlogGtids.toString());
         }
+        if (serverId != 0 ) map.put(SERVER_ID_KEY, serverId);
+        if (binlogTs != 0 ) map.put(BINLOG_EVENT_TIMESTAMP_KEY, binlogTs);
         map.put(BINLOG_FILENAME_OFFSET_KEY, binlogFilename);
         map.put(BINLOG_POSITION_OFFSET_KEY, binlogPosition);
         map.put(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, eventRowNumber);
@@ -360,4 +361,90 @@ public String toString() {
         }
         return sb.toString();
     }
+
+    /**
+     * Determine whether the first {@link #offset() offset} is at or before the point in time of the second
+     * offset, where the offsets are given in JSON representation of the maps returned by {@link #offset()}.
+     * <p>
+     * This logic makes a significant assumption: once a MySQL server/cluster has GTIDs enabled, they will
+     * never be disabled. This is the only way to compare a position with a GTID to a position without a GTID,
+     * and we conclude that any position with a GTID is *after* the position without.
+     * <p>
+     * When both positions have GTIDs, then we compare the positions by using only the GTIDs. Of course, if the
+     * GTIDs are the same, then we also look at whether they have snapshots enabled.
+     * 
+     * @param recorded the position obtained from recorded history; never null
+     * @param desired the desired position that we want to obtain, which should be after some recorded positions,
+     *            at some recorded positions, and before other recorded positions; never null
+     * @return {@code true} if the recorded position is at or before the desired position; or {@code false} otherwise
+     */
+    public static boolean isPositionAtOrBefore(Document recorded, Document desired) {
+        String recordedGtidSetStr = recorded.getString(BINLOG_GTID_KEY);
+        String desiredGtidSetStr = desired.getString(BINLOG_GTID_KEY);
+        if (desiredGtidSetStr != null) {
+            // The desired position uses GTIDs, so we ideally compare using GTIDs ...
+            if (recordedGtidSetStr != null) {
+                // Both have GTIDs, so base the comparison entirely on the GTID sets.
+                GtidSet recordedGtidSet = new GtidSet(recordedGtidSetStr);
+                GtidSet desiredGtidSet = new GtidSet(desiredGtidSetStr);
+                if ( recordedGtidSet.equals(desiredGtidSet)) {
+                    // They are exactly the same, which means the recorded position exactly matches the desired ...
+                    if ( !recorded.has(BINLOG_SNAPSHOT_KEY) && desired.has(BINLOG_SNAPSHOT_KEY)) {
+                        // the desired is in snapshot mode, but the recorded is not. So the recorded is *after* the desired ...
+                        return false;
+                    }
+                    // In all other cases (even when recorded is in snapshot mode), recorded is before or at desired ...
+                    return true;
+                }
+                // The GTIDs are not an exact match, so figure out if recorded is a subset of the desired ...
+                return recordedGtidSet.isSubsetOf(desiredGtidSet);
+            }
+            // The desired position did use GTIDs while the recorded did not use GTIDs. So, we assume that the
+            // recorded position is older since GTIDs are often enabled but rarely disabled. And if they are disabled,
+            // it is likely that the desired position would not include GTIDs as we would be trying to read the binlog of a
+            // server that no longer has GTIDs. And if they are enabled, disabled, and re-enabled, per
+            // https://dev.mysql.com/doc/refman/5.7/en/replication-gtids-failover.html all properly configured slaves that
+            // use GTIDs should always have the complete set of GTIDs copied from the master, in which case
+            // again we know that recorded not having GTIDs is before the desired position ...
+            return true;
+        } else if (recordedGtidSetStr != null) {
+            // The recorded has a GTID but the desired does not, so per the previous paragraph we assume that previous
+            // is not at or before ...
+            return false;
+        }
+
+        // Both positions are missing GTIDs. Look at the servers ...
+        int recordedServerId = recorded.getInteger(SERVER_ID_KEY,0);
+        int desiredServerId = recorded.getInteger(SERVER_ID_KEY,0);
+        if ( recordedServerId != desiredServerId ) {
+            // These are from different servers, and their binlog coordinates are not related. So the only thing we can do
+            // is compare timestamps, and we have to assume that the server timestamps can be compared ...
+            long recordedTimestamp = recorded.getLong(BINLOG_EVENT_TIMESTAMP_KEY,0);
+            long desiredTimestamp = recorded.getLong(BINLOG_EVENT_TIMESTAMP_KEY,0);
+            return recordedTimestamp <= desiredTimestamp;
+        }
+        
+        // First compare the MySQL binlog filenames that include the numeric suffix and therefore are lexicographically
+        // comparable ...
+        String recordedFilename = recorded.getString(BINLOG_FILENAME_OFFSET_KEY);
+        String desiredFilename = desired.getString(BINLOG_FILENAME_OFFSET_KEY);
+        assert recordedFilename != null;
+        int diff = recordedFilename.compareToIgnoreCase(desiredFilename);
+        if ( diff > 0 ) return false;
+
+        // The filenames are the same, so compare the positions ...
+        int recordedPosition = recorded.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
+        int desiredPosition = desired.getInteger(BINLOG_POSITION_OFFSET_KEY, -1);
+        diff = recordedPosition - desiredPosition;
+        if ( diff > 0 ) return false;
+        
+        // The positions are the same, so compare the row number ...
+        int recordedRow = recorded.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        int desiredRow = desired.getInteger(BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, -1);
+        diff = recordedRow - desiredRow;
+        if ( diff > 0 ) return false;
+
+        // The binlog coordinates are the same ...
+        return true;
+    }
 }",2016-06-04T21:20:26Z,70
"@@ -5,33 +5,153 @@
  */
 package io.debezium.connector.mysql;
 
-import io.confluent.connect.avro.AvroData;
+import static org.junit.Assert.assertTrue;
 
 import org.apache.avro.Schema;
-
-
+import org.fest.assertions.GenericAssert;
 import org.junit.Test;
 
-import static org.junit.Assert.assertTrue;
+import io.confluent.connect.avro.AvroData;
+import io.debezium.document.Document;
 
 public class SourceInfoTest {
-    private static final AvroData avroData;
-    private static int avroSchemaCacheSize = 1000;
 
-    static {
-        avroData = new AvroData(avroSchemaCacheSize);
-    }
+    private static int avroSchemaCacheSize = 1000;
+    private static final AvroData avroData = new AvroData(avroSchemaCacheSize);
 
     /**
      * When we want to consume SinkRecord which generated by debezium-connector-mysql, it should not
      * throw error ""org.apache.avro.SchemaParseException: Illegal character in: server-id""
      */
     @Test
-    public void testValidateSourceInfoSchema() {
+    public void shouldValidateSourceInfoSchema() {
         org.apache.kafka.connect.data.Schema kafkaSchema = SourceInfo.SCHEMA;
         Schema avroSchema = avroData.fromConnectSchema(kafkaSchema);
         assertTrue(avroSchema != null);
     }
 
-}
+    @Test
+    public void shouldConsiderPositionsWithSameGtidSetsAsSame() {
+        assertPositionWithGtids(""IdA:1-5"").isAtOrBefore(positionWithGtids(""IdA:1-5""));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"").isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"")); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"").isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"")); // equivalent
+    }
+
+    @Test
+    public void shouldConsiderPositionsWithSameGtidSetsAndSnapshotAsSame() {
+        assertPositionWithGtids(""IdA:1-5"",true).isAtOrBefore(positionWithGtids(""IdA:1-5"",true));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"",true)); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"",true)); // equivalent
+    }
 
+    @Test
+    public void shouldOrderPositionWithGtidAndSnapshotBeforePositionWithSameGtidButNoSnapshot() {
+        assertPositionWithGtids(""IdA:1-5"",true).isAtOrBefore(positionWithGtids(""IdA:1-5""));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdA:1-5,IdB:1-20"")); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",true).isAtOrBefore(positionWithGtids(""IdB:1-20,IdA:1-5"")); // equivalent
+    }
+
+    @Test
+    public void shouldOrderPositionWithoutGtidAndSnapshotAfterPositionWithSameGtidAndSnapshot() {
+        assertPositionWithGtids(""IdA:1-5"",false).isAfter(positionWithGtids(""IdA:1-5"",true));  // same, single
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",false).isAfter(positionWithGtids(""IdA:1-5,IdB:1-20"",true)); // same, multiple
+        assertPositionWithGtids(""IdA:1-5,IdB:1-20"",false).isAfter(positionWithGtids(""IdB:1-20,IdA:1-5"",true)); // equivalent
+    }
+
+    @Test
+    public void shouldOrderPositionWithGtidsAsBeforePositionWithExtraServerUuidInGtids() {
+        assertPositionWithGtids(""IdA:1-5"").isBefore(positionWithGtids(""IdA:1-5,IdB:1-20""));
+    }
+
+    @Test
+    public void shouldOrderPositionsWithSameServerButLowerUpperLimitAsBeforePositionWithSameServerUuidInGtids() {
+        assertPositionWithGtids(""IdA:1-5"").isBefore(positionWithGtids(""IdA:1-6""));
+        assertPositionWithGtids(""IdA:1-5:7-9"").isBefore(positionWithGtids(""IdA:1-10""));
+        assertPositionWithGtids(""IdA:2-5:8-9"").isBefore(positionWithGtids(""IdA:1-10""));
+    }
+
+    @Test
+    public void shouldOrderPositionWithoutGtidAsBeforePositionWithGtid() {
+        assertPositionWithoutGtids(""filename.01"", Integer.MAX_VALUE, 0).isBefore(positionWithGtids(""IdA:1-5""));
+    }
+
+    @Test
+    public void shouldOrderPositionWithGtidAsAfterPositionWithoutGtid() {
+        assertPositionWithGtids(""IdA:1-5"").isAfter(positionWithoutGtids(""filename.01"", 0, 0));
+    }
+
+    protected Document positionWithGtids(String gtids) {
+        return positionWithGtids(gtids, false);
+    }
+
+    protected Document positionWithGtids(String gtids, boolean snapshot) {
+        if (snapshot) {
+            return Document.create(SourceInfo.BINLOG_GTID_KEY, gtids, SourceInfo.BINLOG_SNAPSHOT_KEY, true);
+        }
+        return Document.create(SourceInfo.BINLOG_GTID_KEY, gtids);
+    }
+
+    protected Document positionWithoutGtids(String filename, int position, int row) {
+        return positionWithoutGtids(filename, position, row, false);
+    }
+
+    protected Document positionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+        if (snapshot) {
+            return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
+                                   SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
+                                   SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row,
+                                   SourceInfo.BINLOG_SNAPSHOT_KEY, true);
+        }
+        return Document.create(SourceInfo.BINLOG_FILENAME_OFFSET_KEY, filename,
+                               SourceInfo.BINLOG_POSITION_OFFSET_KEY, position,
+                               SourceInfo.BINLOG_EVENT_ROW_NUMBER_OFFSET_KEY, row);
+    }
+
+    protected PositionAssert assertThat(Document position) {
+        return new PositionAssert(position);
+    }
+
+    protected PositionAssert assertPositionWithGtids(String gtids) {
+        return assertThat(positionWithGtids(gtids));
+    }
+
+    protected PositionAssert assertPositionWithGtids(String gtids, boolean snapshot) {
+        return assertThat(positionWithGtids(gtids, snapshot));
+    }
+
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row) {
+        return assertPositionWithoutGtids(filename, position, row, false);
+    }
+
+    protected PositionAssert assertPositionWithoutGtids(String filename, int position, int row, boolean snapshot) {
+        return assertThat(positionWithoutGtids(filename, position, row, snapshot));
+    }
+
+    protected static class PositionAssert extends GenericAssert<PositionAssert, Document> {
+        public PositionAssert(Document position) {
+            super(PositionAssert.class, position);
+        }
+
+        public PositionAssert isAt(Document otherPosition) {
+            if (SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider same position as "" + otherPosition);
+        }
+
+        public PositionAssert isBefore(Document otherPosition) {
+            return isAtOrBefore(otherPosition);
+        }
+
+        public PositionAssert isAtOrBefore(Document otherPosition) {
+            if (SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider same position as or before "" + otherPosition);
+        }
+
+        public PositionAssert isAfter(Document otherPosition) {
+            if (!SourceInfo.isPositionAtOrBefore(actual, otherPosition)) return this;
+            failIfCustomMessageIsSet();
+            throw failure(actual + "" should be consider after "" + otherPosition);
+        }
+    }
+}",2016-06-04T21:20:26Z,70
"@@ -14,6 +14,8 @@
 import java.util.function.Predicate;
 import java.util.function.Supplier;
 
+import javax.lang.model.SourceVersion;
+
 import io.debezium.annotation.Immutable;
 
 /**
@@ -395,6 +397,13 @@ public String toString() {
         return name();
     }
 
+    public static int isClassName(Configuration config, Field field, Consumer<String> problems) {
+        String value = config.getString(field);
+        if (value == null || SourceVersion.isName(value)) return 0;
+        problems.accept(""The '"" + field.name() + ""' field must contain a valid name of a Java class."");
+        return 1;
+    }
+
     public static int isRequired(Configuration config, Field field, Consumer<String> problems) {
         String value = config.getString(field);
         if (value != null && value.trim().length() > 0) return 0;",2016-06-04T21:20:26Z,119
"@@ -21,15 +21,17 @@
  */
 public abstract class AbstractDatabaseHistory implements DatabaseHistory {
 
-    protected Configuration config;
     protected final Logger logger = LoggerFactory.getLogger(getClass());
+    protected Configuration config;
+    private HistoryRecordComparator comparator = HistoryRecordComparator.INSTANCE;
 
     protected AbstractDatabaseHistory() {
     }
-
+    
     @Override
-    public void configure(Configuration config) {
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
         this.config = config;
+        this.comparator = comparator != null ? comparator : HistoryRecordComparator.INSTANCE;
     }
     
     @Override
@@ -46,7 +48,7 @@ public final void record(Map<String, ?> source, Map<String, ?> position, String
     public final void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser) {
         HistoryRecord stopPoint = new HistoryRecord(source, position, null, null);
         recoverRecords(schema,ddlParser,recovered->{
-            if (recovered.isAtOrBefore(stopPoint)) {
+            if (comparator.isAtOrBefore(recovered,stopPoint)) {
                 String ddl = recovered.ddl();
                 if (ddl != null) {
                     ddlParser.setCurrentSchema(recovered.databaseName()); // may be null",2016-06-04T21:20:26Z,76
"@@ -26,8 +26,11 @@ public interface DatabaseHistory {
      * Configure this instance.
      * 
      * @param config the configuration for this history store
+     * @param comparator the function that should be used to compare history records during
+     *            {@link #recover(Map, Map, Tables, DdlParser) recovery}; may be null if the
+     *            {@link HistoryRecordComparator#INSTANCE default comparator} is to be used
      */
-    void configure(Configuration config);
+    void configure(Configuration config, HistoryRecordComparator comparator);
 
     /**
      * Start the history.
@@ -62,7 +65,7 @@ public interface DatabaseHistory {
     void recover(Map<String, ?> source, Map<String, ?> position, Tables schema, DdlParser ddlParser);
 
     /**
-     * Stop recording history and release any resources acquired since {@link #configure(Configuration)}.
+     * Stop recording history and release any resources acquired since {@link #configure(Configuration, HistoryRecordComparator)}.
      */
     void stop();
 }",2016-06-04T21:20:26Z,41
"@@ -49,15 +49,14 @@ public final class FileDatabaseHistory extends AbstractDatabaseHistory {
     private Path path;
 
     @Override
-    public void configure(Configuration config) {
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
         lock.write(() -> {
-            super.configure(config);
             if (!config.validate(ALL_FIELDS, logger::error)) {
                 throw new ConnectException(
                         ""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
             }
             config.validate(ALL_FIELDS, logger::error);
-            super.configure(config);
+            super.configure(config,comparator);
             path = Paths.get(config.getString(FILE_PATH));
         });
     }",2016-06-04T21:20:26Z,130
"@@ -0,0 +1,62 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.relational.history;
+
+import java.util.function.BiFunction;
+
+import io.debezium.document.Document;
+
+/**
+ * Compares HistoryRecord instances to determine which came first.
+ * 
+ * @author Randall Hauch
+ * @since 0.2
+ */
+public class HistoryRecordComparator {
+
+    /**
+     * A comparator instance that requires the {@link HistoryRecord#source() records' sources} to be the same and considers only
+     * those fields that are in both records' {@link HistoryRecord#position() positions}.
+     */
+    public static final HistoryRecordComparator INSTANCE = new HistoryRecordComparator();
+
+    /**
+     * Create a {@link HistoryRecordComparator} that requires identical sources but will use the supplied function to compare
+     * positions.
+     * 
+     * @param positionComparator the non-null function that returns {@code true} if the first position is at or before
+     *            the second position or {@code false} otherwise
+     * @return the comparator instance; never null
+     */
+    public static HistoryRecordComparator usingPositions(BiFunction<Document, Document, Boolean> positionComparator) {
+        return new HistoryRecordComparator() {
+            @Override
+            protected boolean isPositionAtOrBefore(Document position1, Document position2) {
+                return positionComparator.apply(position1, position2);
+            }
+        };
+    }
+
+    /**
+     * Determine if the first {@link HistoryRecord} is at the same or earlier point in time than the second {@link HistoryRecord}.
+     * 
+     * @param record1 the first record; never null
+     * @param record2 the second record; never null
+     * @return {@code true} if the first record is at the same or earlier point in time than the second record, or {@code false}
+     *         otherwise
+     */
+    public boolean isAtOrBefore(HistoryRecord record1, HistoryRecord record2) {
+        return isSameSource(record1.source(), record2.source()) && isPositionAtOrBefore(record1.position(), record2.position());
+    }
+
+    protected boolean isPositionAtOrBefore(Document position1, Document position2) {
+        return position1.compareToUsingSimilarFields(position2) <= 0;
+    }
+
+    protected boolean isSameSource(Document source1, Document source2) {
+        return source1.equals(source2);
+    }
+}
\ No newline at end of file",2016-06-04T21:20:26Z,131
"@@ -83,8 +83,8 @@ public class KafkaDatabaseHistory extends AbstractDatabaseHistory {
     private int pollIntervalMs = -1;
 
     @Override
-    public void configure(Configuration config) {
-        super.configure(config);
+    public void configure(Configuration config, HistoryRecordComparator comparator) {
+        super.configure(config,comparator);
         if (!config.validate(ALL_FIELDS, logger::error)) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }",2016-06-04T21:20:26Z,115
"@@ -31,7 +31,7 @@ protected DatabaseHistory createHistory() {
         DatabaseHistory history = new FileDatabaseHistory();
         history.configure(Configuration.create()
                                        .with(FileDatabaseHistory.FILE_PATH, TEST_FILE_PATH.toAbsolutePath().toString())
-                                       .build());
+                                       .build(),null);
         return history;
     }
 }",2016-06-04T21:20:26Z,132
"@@ -76,7 +76,7 @@ public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exc
                               .with(KafkaDatabaseHistory.BOOTSTRAP_SERVERS, kafka.brokerList())
                               .with(KafkaDatabaseHistory.TOPIC, topicName)
                               .build();
-        history.configure(config);
+        history.configure(config,null);
         history.start();
 
         DdlParser recoveryParser = new DdlParserSql2003();
@@ -123,7 +123,7 @@ public void shouldStartWithEmptyTopicAndStoreDataAndRecoverAllState() throws Exc
         // Stop the history (which should stop the producer) ...
         history.stop();
         history = new KafkaDatabaseHistory();
-        history.configure(config);
+        history.configure(config, null);
         // no need to start
 
         // Recover from the very beginning to just past the first change ...",2016-06-04T21:20:26Z,133
"@@ -18,7 +18,6 @@
 import org.apache.kafka.connect.errors.ConnectException;
 
 import com.github.shyiko.mysql.binlog.BinaryLogClient;
-import com.github.shyiko.mysql.binlog.BinaryLogClient.AbstractLifecycleListener;
 import com.github.shyiko.mysql.binlog.BinaryLogClient.LifecycleListener;
 import com.github.shyiko.mysql.binlog.event.DeleteRowsEventData;
 import com.github.shyiko.mysql.binlog.event.Event;
@@ -68,17 +67,7 @@ public BinlogReader(MySqlTaskContext context) {
         client.setServerId(context.serverId());
         client.setKeepAlive(context.config().getBoolean(MySqlConnectorConfig.KEEP_ALIVE));
         client.registerEventListener(this::handleEvent);
-        client.registerLifecycleListener(new AbstractLifecycleListener(){
-            @Override
-            public void onCommunicationFailure(BinaryLogClient client, Exception ex) {
-                failed(ex,""Stopped reading binlog due to error: "" + ex.getMessage());
-            }
-            @Override
-            public void onEventDeserializationFailure(BinaryLogClient client, Exception ex) {
-                failed(ex,""Stopped reading binlog due to error: "" + ex.getMessage());
-            }
-        });
-        client.registerLifecycleListener(new TraceLifecycleListener());
+        client.registerLifecycleListener(new ReaderThreadLifecycleListener());
         if (logger.isDebugEnabled()) client.registerEventListener(this::logEvent);
 
         // Set up the event deserializer with additional type(s) ...
@@ -104,34 +93,31 @@ protected void doStart() {
         client.setBinlogFilename(source.binlogFilename());
         client.setBinlogPosition(source.binlogPosition());
         // The event row number will be used when processing the first event ...
-        logger.info(""Reading from MySQL {} starting at {}"",context.serverName(), source);
 
         // Start the log reader, which starts background threads ...
         long timeoutInMilliseconds = context.timeoutInMilliseconds();
         try {
-            logger.debug(""Binlog reader connecting to MySQL server '{}'"", context.serverName());
+            logger.debug(""Attempting to establish binlog reader connection with timeout of {} ms"", timeoutInMilliseconds);
             client.connect(context.timeoutInMilliseconds());
-            logger.info(""Successfully started reading MySQL binlog"");
         } catch (TimeoutException e) {
             double seconds = TimeUnit.MILLISECONDS.toSeconds(timeoutInMilliseconds);
-            throw new ConnectException(""Timed out after "" + seconds + "" seconds while waiting to connect to the MySQL database at "" +
-                    context.username() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
+            throw new ConnectException(""Timed out after "" + seconds + "" seconds while waiting to connect to MySQL at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
         } catch (AuthenticationException e) {
-            throw new ConnectException(""Failed to authenticate to the MySQL database at "" + context.hostname() + "":"" +
-                    context.port() + "" with user '"" + context.username() + ""'"", e);
+            throw new ConnectException(""Failed to authenticate to the MySQL database at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""'"", e);
         } catch (Throwable e) {
-            throw new ConnectException(""Unable to connect to the MySQL database at "" + context.hostname() + "":"" + context.port() +
-                    "" with user '"" + context.username() + ""': "" + e.getMessage(), e);
+            throw new ConnectException(""Unable to connect to the MySQL database at "" +
+                    context.hostname() + "":"" + context.port() + "" with user '"" + context.username() + ""': "" + e.getMessage(), e);
         }
 
     }
 
     @Override
     protected void doStop() {
         try {
-            logger.debug(""Binlog reader disconnecting from MySQL server '{}'"", context.serverName());
+            logger.debug(""Stopping binlog reader"");
             client.disconnect();
-            logger.info(""Stopped connector to MySQL server '{}'"", context.serverName());
         } catch (IOException e) {
             logger.error(""Unexpected error when disconnecting from the MySQL binary log reader"", e);
         }
@@ -142,7 +128,7 @@ protected void doCleanup() {
     }
 
     protected void logEvent(Event event) {
-        //logger.debug(""Received event: {}"", event);
+        logger.trace(""Received event: {}"", event);
     }
 
     protected void ignoreEvent(Event event) {
@@ -344,25 +330,31 @@ protected void handleDelete(Event event) throws InterruptedException {
         }
     }
 
-    protected final class TraceLifecycleListener implements LifecycleListener {
+    protected final class ReaderThreadLifecycleListener implements LifecycleListener {
         @Override
         public void onDisconnect(BinaryLogClient client) {
-            logger.debug(""MySQL Connector disconnected"");
+            context.temporaryLoggingContext(""binlog"", () -> {
+                logger.info(""Stopped reading binlog and closed connection"");
+            });
         }
 
         @Override
         public void onConnect(BinaryLogClient client) {
-            logger.info(""MySQL Connector connected"");
+            // Set up the MDC logging context for this thread ...
+            context.configureLoggingContext(""binlog"");
+
+            // The event row number will be used when processing the first event ...
+            logger.info(""Connected to MySQL binlog at {}:{}, starting at {}"", context.hostname(), context.port(), source);
         }
 
         @Override
         public void onCommunicationFailure(BinaryLogClient client, Exception ex) {
-            logger.error(""MySQL Connector communication failure"", ex);
+            BinlogReader.this.failed(ex);
         }
 
         @Override
         public void onEventDeserializationFailure(BinaryLogClient client, Exception ex) {
-            logger.error(""MySQL Connector received event deserialization failure"", ex);
+            BinlogReader.this.failed(ex);
         }
     }
 }",2016-06-02T19:05:06Z,67
"@@ -60,7 +60,7 @@ public void start(Map<String, String> props) {
             throw new ConnectException(""Error configuring an instance of "" + getClass().getSimpleName() + ""; check the logs for details"");
         }
 
-        // Create the task and set our running flag ...
+        // Create and start the task context ...
         this.taskContext = new MySqlTaskContext(config);
         this.taskContext.start();
 
@@ -133,13 +133,13 @@ public void start(Map<String, String> props) {
 
     @Override
     public List<SourceRecord> poll() throws InterruptedException {
-        logger.trace(""Polling for events from MySQL connector"");
+        logger.trace(""Polling for events"");
         return currentReader.poll();
     }
 
     @Override
     public void stop() {
-        logger.info(""Stopping MySQL Connector"");
+        logger.info(""Stopping MySQL connector task"");
         // We need to explicitly stop both readers, in this order. If we were to instead call 'currentReader.stop()', there
         // is a chance without synchronization that we'd miss the transition and stop only the snapshot reader. And stopping both
         // is far simpler and more efficient than synchronizing ...
@@ -155,7 +155,7 @@ public void stop() {
                 } catch (Throwable e) {
                     logger.error(""Unexpected error shutting down the database history and/or closing JDBC connections"", e);
                 } finally {
-                    logger.info(""Stopped connector to MySQL server '{}'"", taskContext.serverName());
+                    logger.info(""Connector task successfully stopped"");
                 }
             }
         }",2016-06-02T19:05:06Z,68
"@@ -15,6 +15,8 @@
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.jdbc.JdbcConnection.ConnectionFactory;
 import io.debezium.util.Clock;
+import io.debezium.util.LoggingContext;
+import io.debezium.util.LoggingContext.PreviousContext;
 
 /**
  * A Kafka Connect source task reads the MySQL binary log and generate the corresponding data change events.
@@ -96,7 +98,7 @@ public void loadHistory(SourceInfo startingPoint) {
         dbSchema.loadHistory(startingPoint);
         recordProcessor.regenerate();
     }
-    
+
     public Clock clock() {
         return clock;
     }
@@ -157,21 +159,23 @@ protected SnapshotMode snapshotMode() {
         String value = config.getString(MySqlConnectorConfig.SNAPSHOT_MODE);
         return SnapshotMode.parse(value, MySqlConnectorConfig.SNAPSHOT_MODE.defaultValue());
     }
-    
+
     public boolean useMinimalSnapshotLocking() {
         return config.getBoolean(MySqlConnectorConfig.SNAPSHOT_MINIMAL_LOCKING);
     }
 
     public void start() {
-        // Start the MySQL database history, which simply starts up resources but does not recover the history to a specific
-        // point.
+        // First, configure the logging context for the thread that created this context object ...
+        this.configureLoggingContext(""task"");
+
+        // Start the MySQL database history, which simply starts up resources but does not recover the history to a specific point
         dbSchema().start();
     }
 
     public void shutdown() {
         try {
             // Flush and stop the database history ...
-            logger.debug(""Stopping database history for MySQL server '{}'"", serverName());
+            logger.debug(""Stopping database history"");
             dbSchema.shutdown();
         } catch (Throwable e) {
             logger.error(""Unexpected error shutting down the database history"", e);
@@ -184,4 +188,26 @@ public void shutdown() {
         }
     }
 
+    /**
+     * Configure the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * @param contextName the name of the context; may not be null
+     * @return the previous MDC context; never null
+     * @throws IllegalArgumentException if {@code contextName} is null
+     */
+    public PreviousContext configureLoggingContext(String contextName) {
+        return LoggingContext.forConnector(""MySQL"", serverName(), contextName);
+    }
+    
+    /**
+     * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
+     * state before this method was called.
+     * 
+     * @param contextName the name of the context; may not be null
+     * @param operation the function to run in the new MDC context; may not be null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public void temporaryLoggingContext(String contextName, Runnable operation) {
+        LoggingContext.temporarilyForConnector(""MySQL"", serverName(), contextName, operation);
+    }
+
 }",2016-06-02T19:05:06Z,69
"@@ -19,6 +19,8 @@
 import io.debezium.jdbc.JdbcConnection;
 import io.debezium.relational.Table;
 import io.debezium.relational.TableId;
+import io.debezium.util.Clock;
+import io.debezium.util.Strings;
 
 /**
  * A component that performs a snapshot of a MySQL server, and records the schema changes in {@link MySqlSchema}.
@@ -127,13 +129,15 @@ protected void doCleanup() {
      * Perform the snapshot using the same logic as the ""mysqldump"" utility.
      */
     protected void execute() {
-        logger.info(""Starting snapshot for MySQL server {}"", context.serverName());
+        context.configureLoggingContext(""snapshot"");
+        logger.info(""Starting snapshot"");
         final AtomicReference<String> sql = new AtomicReference<>();
         final JdbcConnection mysql = context.jdbc();
         final MySqlSchema schema = context.dbSchema();
         final Filters filters = schema.filters();
         final SourceInfo source = context.source();
-        final long ts = context.clock().currentTimeInMillis();
+        final Clock clock = context.clock();
+        final long ts = clock.currentTimeInMillis();
         try {
             // ------
             // STEP 0
@@ -148,6 +152,7 @@ protected void execute() {
             // See: https://dev.mysql.com/doc/refman/5.7/en/set-transaction.html
             // See: https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-isolation-levels.html
             // See: https://dev.mysql.com/doc/refman/5.7/en/innodb-consistent-read.html
+            logger.info(""Step 0: disabling autocommit and enabling repeatable read transactions"");
             mysql.setAutoCommit(false);
             sql.set(""SET TRANSACTION ISOLATION LEVEL REPEATABLE READ"");
             mysql.execute(sql.get());
@@ -157,6 +162,7 @@ protected void execute() {
             // ------
             // First, start a transaction and request that a consistent MVCC snapshot is obtained immediately.
             // See http://dev.mysql.com/doc/refman/5.7/en/commit.html
+            logger.info(""Step 1: start transaction with consistent snapshot"");
             sql.set(""START TRANSACTION WITH CONSISTENT SNAPSHOT"");
             mysql.execute(sql.get());
 
@@ -166,6 +172,8 @@ protected void execute() {
             // Obtain read lock on all tables. This statement closes all open tables and locks all tables
             // for all databases with a global read lock, and it prevents ALL updates while we have this lock.
             // It also ensures that everything we do while we have this lock will be consistent.
+            long lockAcquired = clock.currentTimeInMillis();
+            logger.info(""Step 2: flush and obtain global read lock (preventing writes to database)"");
             sql.set(""FLUSH TABLES WITH READ LOCK"");
             mysql.execute(sql.get());
 
@@ -174,6 +182,7 @@ protected void execute() {
             // ------
             // Obtain the binlog position and update the SourceInfo in the context. This means that all source records generated
             // as part of the snapshot will contain the binlog position of the snapshot.
+            logger.info(""Step 3: read binlog position of MySQL master"");
             sql.set(""SHOW MASTER STATUS"");
             mysql.query(sql.get(), rs -> {
                 if (rs.next()) {
@@ -191,6 +200,7 @@ protected void execute() {
             // STEP 4
             // ------
             // Get the list of databases ...
+            logger.info(""Step 4: read list of available databases"");
             final List<String> databaseNames = new ArrayList<>();
             sql.set(""SHOW DATABASES"");
             mysql.query(sql.get(), rs -> {
@@ -205,6 +215,7 @@ protected void execute() {
             // Get the list of table IDs for each database. We can't use a prepared statement with MySQL, so we have to
             // build the SQL statement each time. Although in other cases this might lead to SQL injection, in our case
             // we are reading the database names from the database and not taking them from the user ...
+            logger.info(""Step 5: read list of available tables in each database"");
             final List<TableId> tableIds = new ArrayList<>();
             final Map<String,List<TableId>> tableIdsByDbName = new HashMap<>();
             for (String dbName : databaseNames) {
@@ -225,6 +236,7 @@ protected void execute() {
             // ------
             // Transform the current schema so that it reflects the *current* state of the MySQL server's contents.
             // First, get the DROP TABLE and CREATE TABLE statement (with keys and constraint definitions) for our tables ...
+            logger.info(""Step 6: generating DROP and CREATE statements to reflect current database schemas"");
             final List<String> ddlStatements = new ArrayList<>();
             // Add DROP TABLE statements for all tables that we knew about AND those tables found in the databases ...
             Set<TableId> allTableIds = new HashSet<>(schema.tables().tableIds());
@@ -253,6 +265,7 @@ protected void execute() {
                 }
             }
             // Finally, apply the DDL statements to the schema and then update the record maker...
+            logger.debug(""Step 6b: applying DROP and CREATE statements to connector's table model"");
             String ddlStatementsStr = String.join("";"" + System.lineSeparator(), ddlStatements);
             schema.applyDdl(source, null, ddlStatementsStr, this::enqueueSchemaChanges);
             context.makeRecord().regenerate();
@@ -266,17 +279,25 @@ protected void execute() {
                 // should still use the MVCC snapshot obtained when we started our transaction (since we started it
                 // ""...with consistent snapshot""). So, since we're only doing very simple SELECT without WHERE predicates,
                 // we can release the lock now ...
+                logger.info(""Step 7: releasing global read lock to enable MySQL writes"");
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
+                long lockReleased = clock.currentTimeInMillis();
+                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased-lockAcquired));
             }
 
             // ------
             // STEP 8
             // ------
             // Dump all of the tables and generate source records ...
+            logger.info(""Step 8: scanning contents of {} tables"",tableIds.size());
+            long startScan = clock.currentTimeInMillis();
             AtomicBoolean interrupted = new AtomicBoolean(false);
+            int counter = 0;
             for (TableId tableId : tableIds) {
+                long start = clock.currentTimeInMillis();
+                logger.debug(""Step 8.{}: scanning table '{}'; {} tables remain"",++counter,tableId,tableIds.size()-counter);
                 sql.set(""SELECT * FROM "" + tableId);
                 mysql.query(sql.get(), rs -> {
                     RecordsForTable recordMaker = context.makeRecord().forTable(tableId, null, super::enqueueRecord);
@@ -300,32 +321,42 @@ protected void execute() {
                     }
                 });
                 if ( interrupted.get() ) break;
+                long stop = clock.currentTimeInMillis();
+                logger.info(""Step 8.{}: scanned table '{}' in {}"",counter,tableId,Strings.duration(stop-start));
             }
+            long stop = clock.currentTimeInMillis();
+            logger.info(""Step 8: scanned contents of {} tables in {}"",tableIds.size(),Strings.duration(stop-startScan));
 
             // ------
             // STEP 9
             // ------
             // Release the read lock if we have not yet done so ...
             if (!unlocked) {
+                logger.info(""Step 9: releasing global read lock to enable MySQL writes"");
                 sql.set(""UNLOCK TABLES"");
                 mysql.execute(sql.get());
                 unlocked = true;
+                long lockReleased = clock.currentTimeInMillis();
+                logger.info(""Writes to MySQL prevented for a total of {}"", Strings.duration(lockReleased-lockAcquired));
             }
 
             // -------
             // STEP 10
             // -------
             if (interrupted.get()) {
                 // We were interrupted while reading the tables, so roll back the transaction and return immediately ...
+                logger.info(""Step 10: rolling back transaction after request to stop"");
                 sql.set(""ROLLBACK"");
                 mysql.execute(sql.get());
                 return;
             }
             // Otherwise, commit our transaction
+            logger.info(""Step 10: committing transaction"");
             sql.set(""COMMIT"");
             mysql.execute(sql.get());
 
             try {
+                logger.info(""Step 11: recording completion of snapshot"");
                 // Mark the source as having completed the snapshot. Because of this, **subsequent** source records
                 // produced by the connector (to any topic) will have a normal (not snapshot) offset ...
                 source.completeSnapshot();
@@ -338,7 +369,8 @@ protected void execute() {
             } finally {
                 // Set the completion flag ...
                 super.completeSuccessfully();
-                logger.info(""Completed snapshot for MySQL server {}"", context.serverName());
+                stop = clock.currentTimeInMillis();
+                logger.info(""Completed snapshot in {}"", Strings.duration(stop-ts));
             }
         } catch (Throwable e) {
             failed(e, ""Aborting snapshot after running '"" + sql.get() + ""': "" + e.getMessage());",2016-06-02T19:05:06Z,62
"@@ -2,7 +2,7 @@
 log4j.appender.stdout=org.apache.log4j.ConsoleAppender
 log4j.appender.stdout.Target=System.out
 log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
-log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p     %m (%c)%n
+log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p  %X{dbz.connectorType}|%X{dbz.connectorName}|%X{dbz.connectorContext}  %m   [%c]%n
 
 # Root logger option
 log4j.rootLogger=INFO, stdout",2016-06-02T19:05:06Z,73
"@@ -0,0 +1,100 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.util;
+
+import java.util.Map;
+
+import org.slf4j.MDC;
+
+/**
+ * A utility that provides a consistent set of properties for the Mapped Diagnostic Context (MDC) properties used by Debezium
+ * components.
+ * 
+ * @author Randall Hauch
+ * @since 0.2
+ */
+public class LoggingContext {
+
+    /**
+     * The key for the connector type MDC property.
+     */
+    public static final String CONNECTOR_TYPE = ""dbz.connectorType"";
+    /**
+     * The key for the connector logical name MDC property.
+     */
+    public static final String CONNECTOR_NAME = ""dbz.connectorName"";
+    /**
+     * The key for the connector context name MDC property.
+     */
+    public static final String CONNECTOR_CONTEXT = ""dbz.connectorContext"";
+
+    private LoggingContext() {
+    }
+    
+    /**
+     * A snapshot of an MDC context that can be {@link #restore()}.
+     */
+    public static final class PreviousContext {
+        private final Map<String,String> context;
+        @SuppressWarnings(""unchecked"")
+        protected PreviousContext() {
+            context = MDC.getCopyOfContextMap();
+        }
+        /**
+         * Restore this logging context.
+         */
+        public void restore() {
+            for ( Map.Entry<String, String> entry : context.entrySet() ) {
+                MDC.put(entry.getKey(), entry.getValue());
+            }
+        }
+    }
+
+    /**
+     * Configure for a connector the logger's Mapped Diagnostic Context (MDC) properties for the thread making this call.
+     * 
+     * @param connectorType the type of connector; may not be null
+     * @param connectorName the name of the connector; may not be null
+     * @param contextName the name of the context; may not be null
+     * @return the previous MDC context; never null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public static PreviousContext forConnector(String connectorType, String connectorName, String contextName) {
+        if (connectorType == null) throw new IllegalArgumentException(""The MDC value for the connector type may not be null"");
+        if (connectorName == null) throw new IllegalArgumentException(""The MDC value for the connector name may not be null"");
+        if (contextName == null) throw new IllegalArgumentException(""The MDC value for the connector context may not be null"");
+        PreviousContext previous = new PreviousContext();
+        MDC.put(CONNECTOR_TYPE, connectorType);
+        MDC.put(CONNECTOR_NAME, connectorName);
+        MDC.put(CONNECTOR_CONTEXT, contextName);
+        return previous;
+    }
+    
+    /**
+     * Run the supplied function in the temporary connector MDC context, and when complete always return the MDC context to its
+     * state before this method was called.
+     * 
+     * @param connectorType the type of connector; may not be null
+     * @param connectorName the logical name of the connector; may not be null
+     * @param contextName the name of the context; may not be null
+     * @param operation the function to run in the new MDC context; may not be null
+     * @throws IllegalArgumentException if any of the parameters are null
+     */
+    public static void temporarilyForConnector(String connectorType, String connectorName, String contextName, Runnable operation) {
+        if (connectorType == null) throw new IllegalArgumentException(""The MDC value for the connector type may not be null"");
+        if (connectorName == null) throw new IllegalArgumentException(""The MDC value for the connector name may not be null"");
+        if (contextName == null) throw new IllegalArgumentException(""The MDC value for the connector context may not be null"");
+        if (operation == null) throw new IllegalArgumentException(""The operation may not be null"");
+        PreviousContext previous = new PreviousContext();
+        try {
+            forConnector(connectorType,connectorName,contextName);
+            operation.run();
+        } finally {
+            previous.restore();
+        }
+    }
+    
+}",2016-06-02T19:05:06Z,134
"@@ -7,6 +7,8 @@
 
 import java.io.ByteArrayOutputStream;
 import java.io.PrintWriter;
+import java.math.BigDecimal;
+import java.text.DecimalFormat;
 import java.util.Arrays;
 import java.util.Collections;
 import java.util.HashSet;
@@ -36,11 +38,11 @@ public final class Strings {
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, Function<String, String[]> splitter, Function<String, T> factory) {
-        if ( input == null ) return Collections.emptySet();
+        if (input == null) return Collections.emptySet();
         Set<T> matches = new HashSet<>();
         for (String item : splitter.apply(input)) {
             T obj = factory.apply(item);
-            if ( obj != null ) matches.add(obj);
+            if (obj != null) matches.add(obj);
         }
         return matches;
     }
@@ -54,7 +56,7 @@ public static <T> Set<T> listOf(String input, Function<String, String[]> splitte
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, char delimiter, Function<String, T> factory) {
-        return listOf(input,(str) -> str.split(""["" + delimiter + ""]""),factory);
+        return listOf(input, (str) -> str.split(""["" + delimiter + ""]""), factory);
     }
 
     /**
@@ -65,7 +67,7 @@ public static <T> Set<T> listOf(String input, char delimiter, Function<String, T
      * @return the list of objects included in the list; never null
      */
     public static <T> Set<T> listOf(String input, Function<String, T> factory) {
-        return listOf(input,',',factory);
+        return listOf(input, ',', factory);
     }
 
     /**
@@ -76,7 +78,7 @@ public static <T> Set<T> listOf(String input, Function<String, T> factory) {
      * @return the list of regular expression {@link Pattern}s included in the list; never null
      */
     public static Set<Pattern> listOfRegex(String input) {
-        return listOf(input,',',Pattern::compile);
+        return listOf(input, ',', Pattern::compile);
     }
 
     /**
@@ -88,7 +90,7 @@ public static Set<Pattern> listOfRegex(String input) {
      * @return the list of regular expression {@link Pattern}s included in the list; never null
      */
     public static Set<Pattern> listOfRegex(String input, int regexFlags) {
-        return listOf(input,',',(str)->Pattern.compile(str,regexFlags));
+        return listOf(input, ',', (str) -> Pattern.compile(str, regexFlags));
     }
 
     /**
@@ -111,7 +113,7 @@ public static interface CharacterPredicate {
      * @param content the string content that is to be split
      * @return the list of lines; never null but may be an empty (unmodifiable) list if the supplied content is null or empty
      */
-    public static List<String> splitLines( final String content ) {
+    public static List<String> splitLines(final String content) {
         if (content == null || content.length() == 0) return Collections.emptyList();
         String[] lines = content.split(""[\\r]?\\n"");
         return Arrays.asList(lines);
@@ -410,8 +412,7 @@ public static int asInt(String value, int defaultValue) {
         if (value != null) {
             try {
                 return Integer.parseInt(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -427,8 +428,7 @@ public static long asLong(String value, long defaultValue) {
         if (value != null) {
             try {
                 return Long.parseLong(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -444,8 +444,7 @@ public static double asDouble(String value, double defaultValue) {
         if (value != null) {
             try {
                 return Double.parseDouble(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
@@ -461,12 +460,46 @@ public static boolean asBoolean(String value, boolean defaultValue) {
         if (value != null) {
             try {
                 return Boolean.parseBoolean(value);
-            } catch (NumberFormatException e) {
-            }
+            } catch (NumberFormatException e) {}
         }
         return defaultValue;
     }
 
+    /**
+     * For the given duration in milliseconds, obtain a readable representation of the form {@code HHH:MM:SS.mmm}, where
+     * <dl>
+     * <dt>HHH</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""03"")</dd>
+     * <dt>MM</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""05"")</dd>
+     * <dt>SS</dt>
+     * <dd>is the number of hours written in at least 2 digits (e.g., ""09"")</dd>
+     * <dt>mmm</dt>
+     * <dd>is the fractional part of seconds, written with 1-3 digits (any trailing zeros are dropped)</dd>
+     * </dl>
+     * 
+     * @param durationInMillis the duration in milliseconds
+     * @return the readable duration.
+     */
+    public static String duration(long durationInMillis) {
+        // Calculate how many seconds, and don't lose any information ...
+        BigDecimal bigSeconds = new BigDecimal(Math.abs(durationInMillis)).divide(new BigDecimal(1000));
+        // Calculate the minutes, and round to lose the seconds
+        int minutes = bigSeconds.intValue() / 60;
+        // Remove the minutes from the seconds, to just have the remainder of seconds
+        double dMinutes = minutes;
+        double seconds = bigSeconds.doubleValue() - dMinutes * 60;
+        // Now compute the number of full hours, and change 'minutes' to hold the remaining minutes
+        int hours = minutes / 60;
+        minutes = minutes - (hours * 60);
+
+        // Format the string, and have at least 2 digits for the hours, minutes and whole seconds,
+        // and between 3 and 6 digits for the fractional part of the seconds...
+        String result = new DecimalFormat(""######00"").format(hours) + ':' + new DecimalFormat(""00"").format(minutes) + ':'
+                + new DecimalFormat(""00.0##"").format(seconds);
+        return result;
+    }
+
     private Strings() {
     }
 }",2016-06-02T19:05:06Z,102
"@@ -25,12 +25,6 @@
             <groupId>org.apache.kafka</groupId>
             <artifactId>connect-api</artifactId>
         </dependency>
-
-        <dependency>
-            <groupId>io.confluent</groupId>
-            <artifactId>kafka-connect-avro-converter</artifactId>
-        </dependency>
-
         <dependency>
             <groupId>org.slf4j</groupId>
             <artifactId>slf4j-api</artifactId>
@@ -74,6 +68,10 @@
             <groupId>org.easytesting</groupId>
             <artifactId>fest-assert</artifactId>
         </dependency>
+        <dependency>
+            <groupId>io.confluent</groupId>
+            <artifactId>kafka-connect-avro-converter</artifactId>
+        </dependency>
     </dependencies>
     <properties>
         <!-- ",2016-06-02T21:54:21Z,83
"@@ -59,20 +59,23 @@ public class MySqlSchema {
     private final DatabaseHistory dbHistory;
     private final TableSchemaBuilder schemaBuilder;
     private final DdlChanges ddlChanges;
+    private final String serverName;
     private Tables tables;
 
     /**
      * Create a schema component given the supplied {@link MySqlConnectorConfig MySQL connector configuration}.
      * 
      * @param config the connector configuration, which is presumed to be valid
+     * @param serverName the name of the server
      */
-    public MySqlSchema(Configuration config) {
+    public MySqlSchema(Configuration config, String serverName) {
         this.filters = new Filters(config);
         this.ddlParser = new MySqlDdlParser(false);
         this.tables = new Tables();
         this.ddlChanges = new DdlChanges(this.ddlParser.terminator());
         this.ddlParser.addListener(ddlChanges);
         this.schemaBuilder = new TableSchemaBuilder();
+        this.serverName = serverName;
 
         // Create and configure the database history ...
         this.dbHistory = config.getInstance(MySqlConnectorConfig.DATABASE_HISTORY, DatabaseHistory.class);
@@ -247,7 +250,7 @@ protected void refreshSchemas() {
         // Create TableSchema instances for any existing table ...
         this.tables.tableIds().forEach(id -> {
             Table table = this.tables.forTable(id);
-            TableSchema schema = schemaBuilder.create(table, filters.columnFilter(), filters.columnMappers());
+            TableSchema schema = schemaBuilder.create(serverName,table, filters.columnFilter(), filters.columnMappers());
             tableSchemaByTableId.put(id, schema);
         });
     }
@@ -317,7 +320,7 @@ public boolean applyDdl(SourceInfo source, String databaseName, String ddlStatem
             if (table == null) { // removed
                 tableSchemaByTableId.remove(tableId);
             } else {
-                TableSchema schema = schemaBuilder.create(table, filters.columnFilter(), filters.columnMappers());
+                TableSchema schema = schemaBuilder.create(serverName, table, filters.columnFilter(), filters.columnMappers());
                 tableSchemaByTableId.put(tableId, schema);
             }
         });",2016-06-02T21:54:21Z,19
"@@ -48,7 +48,7 @@ public MySqlTaskContext(Configuration config) {
         this.source.setServerName(serverName());
 
         // Set up the MySQL schema ...
-        this.dbSchema = new MySqlSchema(config);
+        this.dbSchema = new MySqlSchema(config, serverName());
         this.dbSchema.start();
 
         // Set up the record processor ...
@@ -108,7 +108,11 @@ public long serverId() {
     }
 
     public String serverName() {
-        return config.getString(MySqlConnectorConfig.SERVER_NAME);
+        String serverName = config.getString(MySqlConnectorConfig.SERVER_NAME);
+        if ( serverName == null ) {
+            serverName = hostname() + "":"" + port();
+        }
+        return serverName;
     }
 
     public String username() {",2016-06-02T21:54:21Z,69
"@@ -161,7 +161,7 @@ public boolean assign(long tableNumber, TableId id) {
 
         String topicName = topicSelector.getTopic(id);
         Envelope envelope = Envelope.defineSchema()
-                                    .withName(topicName)
+                                    .withName(topicName + "".Envelope"")
                                     .withRecord(schema.schemaFor(id).valueSchema())
                                     .withSource(SourceInfo.SCHEMA)
                                     .build();",2016-06-02T21:54:21Z,122
"@@ -77,7 +77,8 @@ public Filters createFilters() {
     }
 
     public MySqlSchema createSchemas() {
-        return new MySqlSchema(configBuilder.build());
+        Configuration config = configBuilder.build();
+        return new MySqlSchema(config,config.getString(MySqlConnectorConfig.SERVER_NAME));
     }
 
 }
\ No newline at end of file",2016-06-02T21:54:21Z,110
"@@ -71,6 +71,11 @@
             <artifactId>connect-file</artifactId>
             <scope>test</scope>
         </dependency>
+        <dependency>
+            <groupId>io.confluent</groupId>
+            <artifactId>kafka-connect-avro-converter</artifactId>
+            <scope>test</scope>
+        </dependency>
     </dependencies>
     <build>
         <resources>",2016-06-02T21:54:21Z,135
"@@ -24,6 +24,16 @@ public class SchemaUtil {
     private SchemaUtil() {
     }
 
+    /**
+     * Obtain a JSON string representation of the specified field.
+     * 
+     * @param field the field; may not be null
+     * @return the JSON string representation
+     */
+    public static String asString(Object field) {
+        return new RecordWriter().append(field).toString();
+    }
+
     /**
      * Obtain a JSON string representation of the specified field.
      * ",2016-06-02T21:54:21Z,127
"@@ -105,11 +105,12 @@ public TableSchema create(ResultSet resultSet, String name) throws SQLException
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
      * @param table the table definition; may not be null
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
-    public TableSchema create(Table table) {
-        return create(table, null, null);
+    public TableSchema create(String schemaPrefix, Table table) {
+        return create(schemaPrefix, table, null, null);
     }
 
     /**
@@ -120,18 +121,20 @@ public TableSchema create(Table table) {
      * <p>
      * This is equivalent to calling {@code create(table,false)}.
      * 
+     * @param schemaPrefix the prefix added to the table identifier to construct the schema names; may be null if there is no prefix
      * @param table the table definition; may not be null
      * @param filter the filter that specifies whether columns in the table should be included; may be null if all columns
      *            are to be included
      * @param mappers the mapping functions for columns; may be null if none of the columns are to be mapped to different values
      * @return the table schema that can be used for sending rows of data for this table to Kafka Connect; never null
      */
-    public TableSchema create(Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
+    public TableSchema create(String schemaPrefix, Table table, Predicate<ColumnId> filter, ColumnMappers mappers) {
+        if ( schemaPrefix == null ) schemaPrefix = """";
         // Build the schemas ...
         final TableId tableId = table.id();
         final String tableIdStr = tableId.toString();
-        SchemaBuilder valSchemaBuilder = SchemaBuilder.struct().name(tableIdStr);
-        SchemaBuilder keySchemaBuilder = SchemaBuilder.struct().name(tableIdStr + ""/pk"");
+        SchemaBuilder valSchemaBuilder = SchemaBuilder.struct().name(schemaPrefix + tableIdStr + "".Value"");
+        SchemaBuilder keySchemaBuilder = SchemaBuilder.struct().name(schemaPrefix + tableIdStr + "".Key"");
         AtomicBoolean hasPrimaryKey = new AtomicBoolean(false);
         table.columns().forEach(column -> {
             if (table.isPrimaryKeyColumn(column.name())) {",2016-06-02T21:54:21Z,129
"@@ -26,6 +26,8 @@
 
 import static org.fest.assertions.Assertions.assertThat;
 
+import io.confluent.connect.avro.AvroConverter;
+import io.confluent.kafka.schemaregistry.client.MockSchemaRegistryClient;
 import io.debezium.data.Envelope.FieldName;
 import io.debezium.data.Envelope.Operation;
 import io.debezium.util.Testing;
@@ -42,6 +44,10 @@ public class VerifyRecord {
     private static final JsonDeserializer keyJsonDeserializer = new JsonDeserializer();
     private static final JsonDeserializer valueJsonDeserializer = new JsonDeserializer();
 
+    private static final MockSchemaRegistryClient schemaRegistry = new MockSchemaRegistryClient();
+    private static final AvroConverter avroKeyConverter = new AvroConverter(schemaRegistry);
+    private static final AvroConverter avroValueConverter = new AvroConverter(schemaRegistry);
+    
     static {
         Map<String,Object> config = new HashMap<>();
         config.put(""schemas.enable"",Boolean.TRUE.toString());
@@ -50,6 +56,11 @@ public class VerifyRecord {
         keyJsonDeserializer.configure(config, true);
         valueJsonConverter.configure(config, false);
         valueJsonDeserializer.configure(config, false);
+
+        config = new HashMap<>();
+        config.put(""schema.registry.url"",""http://fake-url"");
+        avroKeyConverter.configure(config, false);
+        avroValueConverter.configure(config, false);
     }
 
     /**
@@ -263,6 +274,8 @@ public static void isValid(SourceRecord record) {
         JsonNode valueJson = null;
         SchemaAndValue keyWithSchema = null;
         SchemaAndValue valueWithSchema = null;
+        SchemaAndValue avroKeyWithSchema = null;
+        SchemaAndValue avroValueWithSchema = null;
         try {
             // The key should never be null ...
             assertThat(record.key()).isNotNull();
@@ -290,22 +303,43 @@ public static void isValid(SourceRecord record) {
             assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
             assertThat(valueWithSchema.value()).isEqualTo(record.value());
             schemaMatchesStruct(valueWithSchema);
+            
+            // Serialize and deserialize the key using the Avro converter, and check that we got the same result ...
+            byte[] avroKeyBytes = avroValueConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            avroKeyWithSchema = avroValueConverter.toConnectData(record.topic(), avroKeyBytes);
+            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            schemaMatchesStruct(keyWithSchema);
+
+            // Serialize and deserialize the value using the Avro converter, and check that we got the same result ...
+            byte[] avroValueBytes = avroValueConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            avroValueWithSchema = avroValueConverter.toConnectData(record.topic(), avroValueBytes);
+            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
+            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            schemaMatchesStruct(valueWithSchema);
+            
         } catch (Throwable t) {
-            Testing.printError(t);
+            Testing.Print.enable();
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
-            if (keyJson != null) {
-                Testing.print(""valid key = "" + prettyJson(keyJson));
-            } else if (keyWithSchema != null) {
-                Testing.print(""valid key with schema = "" + keyWithSchema);
+            Testing.printError(t);
+            if (keyJson == null ){
+                Testing.print(""error deserializing key from JSON: "" + SchemaUtil.asString(record.key()));
+            } else if (keyWithSchema == null ){
+                Testing.print(""error using JSON converter on key: "" + prettyJson(keyJson));
+            } else if (avroKeyWithSchema == null ){
+                Testing.print(""error using Avro converter on key: "" + prettyJson(keyJson));
             } else {
-                Testing.print(""invalid key"");
+                Testing.print(""valid key = "" + prettyJson(keyJson));
             }
-            if (valueJson != null) {
-                Testing.print(""valid value = "" + prettyJson(valueJson));
-            } else if (valueWithSchema != null) {
-                Testing.print(""valid value with schema = "" + valueWithSchema);
+
+            if (valueJson == null ){
+                Testing.print(""error deserializing value from JSON: "" + SchemaUtil.asString(record.value()));
+            } else if (valueWithSchema == null ){
+                Testing.print(""error using JSON converter on value: "" + prettyJson(valueJson));
+            } else if (avroValueWithSchema == null ){
+                Testing.print(""error using Avro converter on value: "" + prettyJson(valueJson));
             } else {
-                Testing.print(""invalid value"");
+                Testing.print(""valid key = "" + prettyJson(keyJson));
             }
             if (t instanceof AssertionError) throw t;
             fail(t.getMessage());",2016-06-02T21:54:21Z,77
"@@ -19,6 +19,7 @@
 
 public class TableSchemaBuilderTest {
 
+    private final String prefix = """";
     private final TableId id = new TableId(""catalog"", ""schema"", ""table"");
     private final Object[] data = new Object[] { ""c1value"", 3.142d, java.sql.Date.valueOf(""2001-10-31""), 4 };
     private Table table;
@@ -68,19 +69,19 @@ public void checkPreconditions() {
 
     @Test(expected = NullPointerException.class)
     public void shouldFailToBuildTableSchemaFromNullTable() {
-        new TableSchemaBuilder().create(null);
+        new TableSchemaBuilder().create(prefix,null);
     }
 
     @Test
     public void shouldBuildTableSchemaFromTable() {
-        schema = new TableSchemaBuilder().create(table);
+        schema = new TableSchemaBuilder().create(prefix,table);
         assertThat(schema).isNotNull();
     }
 
     @Test
     public void shouldBuildTableSchemaFromTableWithoutPrimaryKey() {
         table = table.edit().setPrimaryKeyNames().create();
-        schema = new TableSchemaBuilder().create(table);
+        schema = new TableSchemaBuilder().create(prefix,table);
         assertThat(schema).isNotNull();
         // Check the keys ...
         assertThat(schema.keySchema()).isNull();",2016-06-02T21:54:21Z,136
"@@ -50,13 +50,14 @@
         <!-- Kafka and it's dependencies MUST reflect what the Kafka version uses -->
         <version.kafka>0.9.0.1</version.kafka>
         <version.kafka.scala>2.11</version.kafka.scala>
-        <version.confluent.platform>2.1.0-alpha1</version.confluent.platform>
         <version.scala>2.11.7</version.scala>
         <version.curator>2.4.0</version.curator>
         <version.zookeeper>3.4.6</version.zookeeper>
         <version.jackson>2.5.4</version.jackson>
         <version.org.slf4j>1.7.6</version.org.slf4j>
         <version.log4j>1.2.17</version.log4j>
+        <!-- check new release version at https://github.com/confluentinc/schema-registry/releases -->
+        <version.confluent.platform>3.0.0</version.confluent.platform>
 
         <!-- Databases -->
         <version.postgresql.driver>9.4-1205-jdbc42</version.postgresql.driver>
@@ -185,7 +186,6 @@
             <dependency>
                 <groupId>io.confluent</groupId>
                 <artifactId>kafka-connect-avro-converter</artifactId>
-                <!-- check new release version at https://github.com/confluentinc/schema-registry/releases -->
                 <version>${version.confluent.platform}</version>
                 <scope>test</scope>
             </dependency>",2016-06-02T21:54:21Z,60
"@@ -16,6 +16,9 @@
           <excludes>
               <!-- Exclude all Kafka APIs and their dependencies, since they will be available in the runtime -->
               <exclude>org.apache.kafka:*</exclude>
+              <exclude>org.slf4j:*</exclude>
+              <exclude>log4j:log4j:*</exclude>
+              <exclude>com.fasterxml.jackson.core:jackson-core:*</exclude>
               <exclude>org.xerial.snappy:*</exclude>
               <exclude>net.jpountz.lz4:*</exclude>
           </excludes>",2016-03-17T16:03:27Z,82
"@@ -16,10 +16,6 @@
             <groupId>io.debezium</groupId>
             <artifactId>debezium-core</artifactId>
         </dependency>
-        <dependency>
-            <groupId>mysql</groupId>
-            <artifactId>mysql-connector-java</artifactId>
-        </dependency>
         <dependency>
             <groupId>com.github.shyiko</groupId>
             <artifactId>mysql-binlog-connector-java</artifactId>
@@ -50,6 +46,11 @@
             <artifactId>debezium-embedded</artifactId>
             <scope>test</scope>
         </dependency>
+        <dependency>
+            <groupId>mysql</groupId>
+            <artifactId>mysql-connector-java</artifactId>
+            <scope>test</scope>
+        </dependency>
         <dependency>
             <groupId>org.slf4j</groupId>
             <artifactId>slf4j-log4j12</artifactId>",2016-03-17T16:03:27Z,83
"@@ -45,18 +45,21 @@
         <maven.compiler.source>1.8</maven.compiler.source>
         <maven.compiler.target>1.8</maven.compiler.target>
 
-        <!-- Major dependencies -->
-        <version.jackson>2.4.0</version.jackson>
-
-        <!-- Logging -->
-        <version.org.slf4j>1.7.2</version.org.slf4j>
+        <!-- Kafka and it's dependencies MUST reflect what the Kafka version uses -->
+        <version.kafka>0.9.0.1</version.kafka>
+        <version.kafka.scala>2.11</version.kafka.scala>
+        <version.scala>2.11.7</version.scala>
+        <version.curator>2.4.0</version.curator>
+        <version.zookeeper>3.4.6</version.zookeeper>
+        <version.jackson>2.5.4</version.jackson>
+        <version.org.slf4j>1.7.6</version.org.slf4j>
         <version.log4j>1.2.17</version.log4j>
 
         <!-- Databases -->
         <version.postgresql.driver>9.4-1205-jdbc42</version.postgresql.driver>
         <version.postgresql.server>9.4</version.postgresql.server>
         <version.mysql.server>5.7</version.mysql.server>
-        <version.mysql.driver>5.1.37</version.mysql.driver>
+        <version.mysql.driver>5.1.38</version.mysql.driver>
         <version.mysql.binlog>0.2.4</version.mysql.binlog>
 
         <!-- Testing -->
@@ -75,13 +78,6 @@
         <!-- Dockerfiles -->
         <docker.maintainer>Debezium community</docker.maintainer>
 
-        <!-- Kafka -->
-        <version.kafka>0.9.0.1</version.kafka>
-        <version.kafka.scala>2.11</version.kafka.scala>
-        <version.scala>2.11.7</version.scala>
-        <version.curator>2.4.0</version.curator>
-        <version.zookeeper>3.4.6</version.zookeeper>
-
         <!--Skip long running tests by default-->
         <skipLongRunningTests>true</skipLongRunningTests>
 ",2016-03-17T16:03:27Z,60
"@@ -102,7 +102,9 @@ public void shouldConsumeAllEventsFromDatabase() throws SQLException, Interrupte
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         
         records.ddlRecordsForDatabase(""connector_test"").forEach(this::print);
-        //records.ddlRecordsForDatabase(""readbinlog_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
         
         // Make sure there are no more ...
         Testing.Print.disable();
@@ -200,6 +202,9 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
         assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.customers"").size()).isEqualTo(4);
         assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.orders"").size()).isEqualTo(5);
         assertThat(records.topics().size()).isEqualTo(4);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
         
         // More records may have been written (if this method were run after the others), but we don't care ...
         stopConnector();",2016-05-19T22:06:22Z,71
"@@ -24,7 +24,9 @@
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.DataException;
 import org.apache.kafka.connect.json.JsonConverter;
 import org.apache.kafka.connect.json.JsonDeserializer;
 import org.apache.kafka.connect.source.SourceConnector;
@@ -202,13 +204,13 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
-                               .notifying((record)->{
+                               .notifying((record) -> {
                                    try {
                                        consumedLines.put(record);
                                    } catch ( InterruptedException e ) {
                                        Thread.interrupted();
                                    }
-                                })
+                               })
                                .using(this.getClass().getClassLoader())
                                .using(wrapperCallback)
                                .build();
@@ -320,6 +322,12 @@ public List<SourceRecord> recordsForTopic( String topicName ) {
         public Set<String> topics() {
             return recordsByTopic.keySet();
         }
+        public void forEachInTopic(String topic, Consumer<SourceRecord> consumer) {
+            recordsForTopic(topic).forEach(consumer);
+        }
+        public void forEach( Consumer<SourceRecord> consumer) {
+            records.forEach(consumer);
+        }
         public void print() {
             Testing.print("""" + topics().size() + "" topics: "" + topics());
             recordsByTopic.forEach((k,v)->{
@@ -436,6 +444,100 @@ protected void assertTombstone(SourceRecord record) {
         assertThat(record.valueSchema()).isNull();
     }
 
+    /**
+     * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
+     * matches that of the supplied {@code schema}.
+     * 
+     * @param value the value with a schema; may not be null
+     */
+    protected void assertSchemaMatchesStruct(SchemaAndValue value) {
+        Object val = value.value();
+        assertThat(val).isInstanceOf(Struct.class);
+        assertSchemaMatchesStruct((Struct) val, value.schema());
+    }
+
+    /**
+     * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
+     * matches that of the supplied {@code schema}.
+     * 
+     * @param struct the {@link Struct} to validate; may not be null
+     * @param schema the expected schema of the {@link Struct}; may not be null
+     */
+    protected void assertSchemaMatchesStruct(Struct struct, Schema schema) {
+        // First validate the struct itself ...
+        try {
+            struct.validate();
+        } catch (DataException e) {
+            throw new AssertionError(""The struct '"" + struct + ""' failed to validate"", e);
+        }
+
+        Schema actualSchema = struct.schema();
+        assertThat(actualSchema).isEqualTo(schema);
+        assertFieldsInSchema(struct,schema);
+    }
+
+    private void assertFieldsInSchema(Struct struct, Schema schema ) {
+        schema.fields().forEach(field->{
+            Object val1 = struct.get(field);
+            Object val2 = struct.get(field.name());
+            assertThat(val1).isSameAs(val2);
+            if ( val1 instanceof Struct ) {
+                assertFieldsInSchema((Struct)val1,field.schema());
+            }
+        });
+    }
+    
+    /**
+     * Validate that a {@link SourceRecord}'s key and value can each be converted to a byte[] and then back to an equivalent
+     * {@link SourceRecord}.
+     * 
+     * @param record the record to validate; may not be null
+     */
+    protected void validate(SourceRecord record) {
+        print(record);
+
+        JsonNode keyJson = null;
+        JsonNode valueJson = null;
+        SchemaAndValue keyWithSchema = null;
+        SchemaAndValue valueWithSchema = null;
+        try {
+            // First serialize and deserialize the key ...
+            byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            assertSchemaMatchesStruct(keyWithSchema);
+
+            // then the value ...
+            byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
+            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
+            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            assertSchemaMatchesStruct(valueWithSchema);
+        } catch (Throwable t) {
+            Testing.printError(t);
+            Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
+            if (keyJson != null) {
+                Testing.print(""valid key = "" + prettyJson(keyJson));
+            } else if (keyWithSchema != null) {
+                Testing.print(""valid key with schema = "" + keyWithSchema);
+            } else {
+                Testing.print(""invalid key"");
+            }
+            if (valueJson != null) {
+                Testing.print(""valid value = "" + prettyJson(valueJson));
+            } else if (valueWithSchema != null) {
+                Testing.print(""valid value with schema = "" + valueWithSchema);
+            } else {
+                Testing.print(""invalid value"");
+            }
+            if (t instanceof AssertionError) throw t;
+            fail(t.getMessage());
+        }
+    }
+
     protected void print(SourceRecord record) {
         StringBuilder sb = new StringBuilder(""SourceRecord{"");
         sb.append(""sourcePartition="").append(record.sourcePartition());
@@ -470,12 +572,12 @@ protected void printJson(SourceRecord record) {
         } catch (Throwable t) {
             Testing.printError(t);
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
-            if ( keyJson != null ) {
+            if (keyJson != null) {
                 Testing.print(""valid key = "" + prettyJson(keyJson));
             } else {
                 Testing.print(""invalid key"");
             }
-            if ( valueJson != null ) {
+            if (valueJson != null) {
                 Testing.print(""valid value = "" + prettyJson(valueJson));
             } else {
                 Testing.print(""invalid value"");",2016-05-19T22:06:22Z,35
"@@ -102,7 +102,9 @@ public void shouldConsumeAllEventsFromDatabase() throws SQLException, Interrupte
         assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
         
         records.ddlRecordsForDatabase(""connector_test"").forEach(this::print);
-        //records.ddlRecordsForDatabase(""readbinlog_test"").forEach(this::print);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
         
         // Make sure there are no more ...
         Testing.Print.disable();
@@ -200,6 +202,9 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
         assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.customers"").size()).isEqualTo(4);
         assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.orders"").size()).isEqualTo(5);
         assertThat(records.topics().size()).isEqualTo(4);
+
+        // Check that all records are valid, can be serialized and deserialized ...
+        records.forEach(this::validate);
         
         // More records may have been written (if this method were run after the others), but we don't care ...
         stopConnector();",2016-05-19T22:06:22Z,71
"@@ -24,7 +24,9 @@
 
 import org.apache.kafka.connect.data.Field;
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.SchemaAndValue;
 import org.apache.kafka.connect.data.Struct;
+import org.apache.kafka.connect.errors.DataException;
 import org.apache.kafka.connect.json.JsonConverter;
 import org.apache.kafka.connect.json.JsonDeserializer;
 import org.apache.kafka.connect.source.SourceConnector;
@@ -202,13 +204,13 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
-                               .notifying((record)->{
+                               .notifying((record) -> {
                                    try {
                                        consumedLines.put(record);
                                    } catch ( InterruptedException e ) {
                                        Thread.interrupted();
                                    }
-                                })
+                               })
                                .using(this.getClass().getClassLoader())
                                .using(wrapperCallback)
                                .build();
@@ -320,6 +322,12 @@ public List<SourceRecord> recordsForTopic( String topicName ) {
         public Set<String> topics() {
             return recordsByTopic.keySet();
         }
+        public void forEachInTopic(String topic, Consumer<SourceRecord> consumer) {
+            recordsForTopic(topic).forEach(consumer);
+        }
+        public void forEach( Consumer<SourceRecord> consumer) {
+            records.forEach(consumer);
+        }
         public void print() {
             Testing.print("""" + topics().size() + "" topics: "" + topics());
             recordsByTopic.forEach((k,v)->{
@@ -436,6 +444,100 @@ protected void assertTombstone(SourceRecord record) {
         assertThat(record.valueSchema()).isNull();
     }
 
+    /**
+     * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
+     * matches that of the supplied {@code schema}.
+     * 
+     * @param value the value with a schema; may not be null
+     */
+    protected void assertSchemaMatchesStruct(SchemaAndValue value) {
+        Object val = value.value();
+        assertThat(val).isInstanceOf(Struct.class);
+        assertSchemaMatchesStruct((Struct) val, value.schema());
+    }
+
+    /**
+     * Assert that the supplied {@link Struct} is {@link Struct#validate() valid} and its {@link Struct#schema() schema}
+     * matches that of the supplied {@code schema}.
+     * 
+     * @param struct the {@link Struct} to validate; may not be null
+     * @param schema the expected schema of the {@link Struct}; may not be null
+     */
+    protected void assertSchemaMatchesStruct(Struct struct, Schema schema) {
+        // First validate the struct itself ...
+        try {
+            struct.validate();
+        } catch (DataException e) {
+            throw new AssertionError(""The struct '"" + struct + ""' failed to validate"", e);
+        }
+
+        Schema actualSchema = struct.schema();
+        assertThat(actualSchema).isEqualTo(schema);
+        assertFieldsInSchema(struct,schema);
+    }
+
+    private void assertFieldsInSchema(Struct struct, Schema schema ) {
+        schema.fields().forEach(field->{
+            Object val1 = struct.get(field);
+            Object val2 = struct.get(field.name());
+            assertThat(val1).isSameAs(val2);
+            if ( val1 instanceof Struct ) {
+                assertFieldsInSchema((Struct)val1,field.schema());
+            }
+        });
+    }
+    
+    /**
+     * Validate that a {@link SourceRecord}'s key and value can each be converted to a byte[] and then back to an equivalent
+     * {@link SourceRecord}.
+     * 
+     * @param record the record to validate; may not be null
+     */
+    protected void validate(SourceRecord record) {
+        print(record);
+
+        JsonNode keyJson = null;
+        JsonNode valueJson = null;
+        SchemaAndValue keyWithSchema = null;
+        SchemaAndValue valueWithSchema = null;
+        try {
+            // First serialize and deserialize the key ...
+            byte[] keyBytes = keyJsonConverter.fromConnectData(record.topic(), record.keySchema(), record.key());
+            keyJson = keyJsonDeserializer.deserialize(record.topic(), keyBytes);
+            keyWithSchema = keyJsonConverter.toConnectData(record.topic(), keyBytes);
+            assertThat(keyWithSchema.schema()).isEqualTo(record.keySchema());
+            assertThat(keyWithSchema.value()).isEqualTo(record.key());
+            assertSchemaMatchesStruct(keyWithSchema);
+
+            // then the value ...
+            byte[] valueBytes = valueJsonConverter.fromConnectData(record.topic(), record.valueSchema(), record.value());
+            valueJson = valueJsonDeserializer.deserialize(record.topic(), valueBytes);
+            valueWithSchema = valueJsonConverter.toConnectData(record.topic(), valueBytes);
+            assertThat(valueWithSchema.schema()).isEqualTo(record.valueSchema());
+            assertThat(valueWithSchema.value()).isEqualTo(record.value());
+            assertSchemaMatchesStruct(valueWithSchema);
+        } catch (Throwable t) {
+            Testing.printError(t);
+            Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
+            if (keyJson != null) {
+                Testing.print(""valid key = "" + prettyJson(keyJson));
+            } else if (keyWithSchema != null) {
+                Testing.print(""valid key with schema = "" + keyWithSchema);
+            } else {
+                Testing.print(""invalid key"");
+            }
+            if (valueJson != null) {
+                Testing.print(""valid value = "" + prettyJson(valueJson));
+            } else if (valueWithSchema != null) {
+                Testing.print(""valid value with schema = "" + valueWithSchema);
+            } else {
+                Testing.print(""invalid value"");
+            }
+            if (t instanceof AssertionError) throw t;
+            fail(t.getMessage());
+        }
+    }
+
     protected void print(SourceRecord record) {
         StringBuilder sb = new StringBuilder(""SourceRecord{"");
         sb.append(""sourcePartition="").append(record.sourcePartition());
@@ -470,12 +572,12 @@ protected void printJson(SourceRecord record) {
         } catch (Throwable t) {
             Testing.printError(t);
             Testing.print(""Problem with message on topic '"" + record.topic() + ""':"");
-            if ( keyJson != null ) {
+            if (keyJson != null) {
                 Testing.print(""valid key = "" + prettyJson(keyJson));
             } else {
                 Testing.print(""invalid key"");
             }
-            if ( valueJson != null ) {
+            if (valueJson != null) {
                 Testing.print(""valid value = "" + prettyJson(valueJson));
             } else {
                 Testing.print(""invalid value"");",2016-05-19T22:06:22Z,35
"@@ -578,7 +578,9 @@ protected void parseIndexOptions(Marker start) {
     protected void parseReferenceDefinition(Marker start) {
         tokens.consume(""REFERENCES"");
         parseSchemaQualifiedName(start); // table name
-        parseColumnNameList(start);
+        if (tokens.matches('(')) {
+            parseColumnNameList(start);
+        }
         if (tokens.canConsume(""MATCH"")) {
             tokens.consumeAnyOf(""FULL"", ""PARTIAL"", ""SIMPLE"");
             if (tokens.canConsume(""ON"")) {",2016-05-13T14:32:47Z,49
"@@ -150,28 +150,44 @@ public void shouldParseGrantStatement() {
     public void shouldParseCreateStatements() {
         parser.parse(readFile(""ddl/mysql-test-create.ddl""), tables);
         Testing.print(tables);
-        assertThat(tables.size()).isEqualTo(57); // no tables
+        assertThat(tables.size()).isEqualTo(57);
         assertThat(listener.total()).isEqualTo(144);
     }
 
     @Test
     public void shouldParseTestStatements() {
         parser.parse(readFile(""ddl/mysql-test-statements.ddl""), tables);
         Testing.print(tables);
-        assertThat(tables.size()).isEqualTo(6); // no tables
+        assertThat(tables.size()).isEqualTo(6);
         assertThat(listener.total()).isEqualTo(49);
         //listener.forEach(this::printEvent);
     }
 
     @Test
     public void shouldParseSomeLinesFromCreateStatements() {
         parser.parse(readLines(189,""ddl/mysql-test-create.ddl""), tables);
-        assertThat(tables.size()).isEqualTo(39); // no tables
+        assertThat(tables.size()).isEqualTo(39);
         assertThat(listener.total()).isEqualTo(120);
     }
+
+    @Test
+    public void shouldParseMySql56InitializationStatements() {
+        parser.parse(readLines(1,""ddl/mysql-test-init-5.6.ddl""), tables);
+        assertThat(tables.size()).isEqualTo(85); // 1 table
+        assertThat(listener.total()).isEqualTo(112);
+        listener.forEach(this::printEvent);
+    }
+
+    @Test
+    public void shouldParseMySql57InitializationStatements() {
+        parser.parse(readLines(1,""ddl/mysql-test-init-5.7.ddl""), tables);
+        assertThat(tables.size()).isEqualTo(123);
+        assertThat(listener.total()).isEqualTo(125);
+        listener.forEach(this::printEvent);
+    }
     
     protected void printEvent( Event event ) {
-        System.out.println(event);
+        Testing.print(event);
     }
 
     protected String readFile( String classpathResource ) {",2016-05-13T14:32:47Z,26
"@@ -239,6 +239,29 @@ public void shouldCaptureMultipleWriteUpdateDeletesInSingleEvents() throws Excep
                                                  .removedRow(""Jamie"", 19, any(), any()));
     }
 
+    /**
+     * Test case that is normally commented out since it is only useful to print out the DDL statements recorded by
+     * the binlog during a MySQL server initialization and startup.
+     * 
+     * @throws Exception if there are problems
+     */
+    @Ignore
+    @Test
+    public void shouldCaptureQueryEventData() throws Exception {
+        // Testing.Print.enable();
+        startClient(client -> {
+            client.setBinlogFilename(""mysql-bin.000001"");
+            client.setBinlogPosition(4);
+        });
+        counters.consumeAll(5, TimeUnit.SECONDS);
+        List<QueryEventData> allQueryEvents = recordedEventData(QueryEventData.class, -1);
+        allQueryEvents.forEach(event -> {
+            String sql = event.getSql();
+            if (sql.equalsIgnoreCase(""BEGIN"") || sql.equalsIgnoreCase(""COMMIT"")) return;
+            System.out.println(event.getSql());
+        });
+    }
+
     @Test
     public void shouldQueryInformationSchema() throws Exception {
         // long tableId = writeRows.getTableId();",2016-05-13T14:32:47Z,118
"@@ -0,0 +1,227 @@
+--
+-- Statements recorded by binlog during MySQL 5.6 initialization with Debezium scripts.
+--
+CREATE TABLE IF NOT EXISTS db (   Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Select_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Insert_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Update_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Delete_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Drop_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Grant_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, References_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Index_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tmp_table_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Lock_tables_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Execute_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Event_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Trigger_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY Host (Host,Db,User), KEY User (User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='Database privileges';
+CREATE TABLE IF NOT EXISTS user (   Host char(60) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Password char(41) character set latin1 collate latin1_bin DEFAULT '' NOT NULL, Select_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Insert_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Update_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Delete_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Drop_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Reload_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Shutdown_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Process_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, File_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Grant_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, References_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Index_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_db_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Super_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tmp_table_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Lock_tables_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Execute_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Repl_slave_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Repl_client_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_user_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Event_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Trigger_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tablespace_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, ssl_type enum('','ANY','X509', 'SPECIFIED') COLLATE utf8_general_ci DEFAULT '' NOT NULL, ssl_cipher BLOB NOT NULL, x509_issuer BLOB NOT NULL, x509_subject BLOB NOT NULL, max_questions int(11) unsigned DEFAULT 0  NOT NULL, max_updates int(11) unsigned DEFAULT 0  NOT NULL, max_connections int(11) unsigned DEFAULT 0  NOT NULL, max_user_connections int(11) unsigned DEFAULT 0  NOT NULL, plugin char(64) DEFAULT 'mysql_native_password', authentication_string TEXT, password_expired ENUM('N', 'Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY Host (Host,User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='Users and global privileges';
+CREATE TABLE IF NOT EXISTS func (  name char(64) binary DEFAULT '' NOT NULL, ret tinyint(1) DEFAULT '0' NOT NULL, dl char(128) DEFAULT '' NOT NULL, type enum ('function','aggregate') COLLATE utf8_general_ci NOT NULL, PRIMARY KEY (name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='User defined functions';
+CREATE TABLE IF NOT EXISTS plugin ( name varchar(64) DEFAULT '' NOT NULL, dl varchar(128) DEFAULT '' NOT NULL, PRIMARY KEY (name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_general_ci comment='MySQL plugins';
+CREATE TABLE IF NOT EXISTS servers ( Server_name char(64) NOT NULL DEFAULT '', Host char(64) NOT NULL DEFAULT '', Db char(64) NOT NULL DEFAULT '', Username char(64) NOT NULL DEFAULT '', Password char(64) NOT NULL DEFAULT '', Port INT(4) NOT NULL DEFAULT '0', Socket char(64) NOT NULL DEFAULT '', Wrapper char(64) NOT NULL DEFAULT '', Owner char(64) NOT NULL DEFAULT '', PRIMARY KEY (Server_name)) CHARACTER SET utf8 comment='MySQL Foreign Servers table';
+CREATE TABLE IF NOT EXISTS tables_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Table_name char(64) binary DEFAULT '' NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, Table_priv set('Select','Insert','Update','Delete','Create','Drop','Grant','References','Index','Alter','Create View','Show view','Trigger') COLLATE utf8_general_ci DEFAULT '' NOT NULL, Column_priv set('Select','Insert','Update','References') COLLATE utf8_general_ci DEFAULT '' NOT NULL, PRIMARY KEY (Host,Db,User,Table_name), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Table privileges';
+CREATE TABLE IF NOT EXISTS columns_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Table_name char(64) binary DEFAULT '' NOT NULL, Column_name char(64) binary DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, Column_priv set('Select','Insert','Update','References') COLLATE utf8_general_ci DEFAULT '' NOT NULL, PRIMARY KEY (Host,Db,User,Table_name,Column_name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Column privileges';
+CREATE TABLE IF NOT EXISTS help_topic ( help_topic_id int unsigned not null, name char(64) not null, help_category_id smallint unsigned not null, description text not null, example text not null, url text not null, primary key (help_topic_id), unique index (name) ) engine=MyISAM CHARACTER SET utf8 comment='help topics';
+CREATE TABLE IF NOT EXISTS help_category ( help_category_id smallint unsigned not null, name  char(64) not null, parent_category_id smallint unsigned null, url text not null, primary key (help_category_id), unique index (name) ) engine=MyISAM CHARACTER SET utf8 comment='help categories';
+CREATE TABLE IF NOT EXISTS help_relation ( help_topic_id int unsigned not null references help_topic, help_keyword_id  int unsigned not null references help_keyword, primary key (help_keyword_id, help_topic_id) ) engine=MyISAM CHARACTER SET utf8 comment='keyword-topic relation';
+CREATE TABLE IF NOT EXISTS help_keyword (   help_keyword_id  int unsigned not null, name char(64) not null, primary key (help_keyword_id), unique index (name) ) engine=MyISAM CHARACTER SET utf8 comment='help keywords';
+CREATE TABLE IF NOT EXISTS time_zone_name (   Name char(64) NOT NULL, Time_zone_id int unsigned NOT NULL, PRIMARY KEY Name (Name) ) engine=MyISAM CHARACTER SET utf8   comment='Time zone names';
+CREATE TABLE IF NOT EXISTS time_zone (   Time_zone_id int unsigned NOT NULL auto_increment, Use_leap_seconds enum('Y','N') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY TzId (Time_zone_id) ) engine=MyISAM CHARACTER SET utf8   comment='Time zones';
+CREATE TABLE IF NOT EXISTS time_zone_transition (   Time_zone_id int unsigned NOT NULL, Transition_time bigint signed NOT NULL, Transition_type_id int unsigned NOT NULL, PRIMARY KEY TzIdTranTime (Time_zone_id, Transition_time) ) engine=MyISAM CHARACTER SET utf8   comment='Time zone transitions';
+CREATE TABLE IF NOT EXISTS time_zone_transition_type (   Time_zone_id int unsigned NOT NULL, Transition_type_id int unsigned NOT NULL, Offset int signed DEFAULT 0 NOT NULL, Is_DST tinyint unsigned DEFAULT 0 NOT NULL, Abbreviation char(8) DEFAULT '' NOT NULL, PRIMARY KEY TzIdTrTId (Time_zone_id, Transition_type_id) ) engine=MyISAM CHARACTER SET utf8   comment='Time zone transition types';
+CREATE TABLE IF NOT EXISTS time_zone_leap_second (   Transition_time bigint signed NOT NULL, Correction int signed NOT NULL, PRIMARY KEY TranTime (Transition_time) ) engine=MyISAM CHARACTER SET utf8   comment='Leap seconds information for time zones';
+CREATE TABLE IF NOT EXISTS proc (db char(64) collate utf8_bin DEFAULT '' NOT NULL, name char(64) DEFAULT '' NOT NULL, type enum('FUNCTION','PROCEDURE') NOT NULL, specific_name char(64) DEFAULT '' NOT NULL, language enum('SQL') DEFAULT 'SQL' NOT NULL, sql_data_access enum( 'CONTAINS_SQL', 'NO_SQL', 'READS_SQL_DATA', 'MODIFIES_SQL_DATA') DEFAULT 'CONTAINS_SQL' NOT NULL, is_deterministic enum('YES','NO') DEFAULT 'NO' NOT NULL, security_type enum('INVOKER','DEFINER') DEFAULT 'DEFINER' NOT NULL, param_list blob NOT NULL, returns longblob DEFAULT '' NOT NULL, body longblob NOT NULL, definer char(77) collate utf8_bin DEFAULT '' NOT NULL, created timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, modified timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', sql_mode set( 'REAL_AS_FLOAT', 'PIPES_AS_CONCAT', 'ANSI_QUOTES', 'IGNORE_SPACE', 'NOT_USED', 'ONLY_FULL_GROUP_BY', 'NO_UNSIGNED_SUBTRACTION', 'NO_DIR_IN_CREATE', 'POSTGRESQL', 'ORACLE', 'MSSQL', 'DB2', 'MAXDB', 'NO_KEY_OPTIONS', 'NO_TABLE_OPTIONS', 'NO_FIELD_OPTIONS', 'MYSQL323', 'MYSQL40', 'ANSI', 'NO_AUTO_VALUE_ON_ZERO', 'NO_BACKSLASH_ESCAPES', 'STRICT_TRANS_TABLES', 'STRICT_ALL_TABLES', 'NO_ZERO_IN_DATE', 'NO_ZERO_DATE', 'INVALID_DATES', 'ERROR_FOR_DIVISION_BY_ZERO', 'TRADITIONAL', 'NO_AUTO_CREATE_USER', 'HIGH_NOT_PRECEDENCE', 'NO_ENGINE_SUBSTITUTION', 'PAD_CHAR_TO_FULL_LENGTH') DEFAULT '' NOT NULL, comment text collate utf8_bin NOT NULL, character_set_client char(32) collate utf8_bin, collation_connection char(32) collate utf8_bin, db_collation char(32) collate utf8_bin, body_utf8 longblob, PRIMARY KEY (db,name,type)) engine=MyISAM character set utf8 comment='Stored Procedures';
+CREATE TABLE IF NOT EXISTS procs_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Routine_name char(64) COLLATE utf8_general_ci DEFAULT '' NOT NULL, Routine_type enum('FUNCTION','PROCEDURE') NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Proc_priv set('Execute','Alter Routine','Grant') COLLATE utf8_general_ci DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (Host,Db,User,Routine_name,Routine_type), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Procedure privileges';
+CREATE TABLE IF NOT EXISTS general_log (event_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, user_host MEDIUMTEXT NOT NULL, thread_id BIGINT(21) UNSIGNED NOT NULL, server_id INTEGER UNSIGNED NOT NULL, command_type VARCHAR(64) NOT NULL, argument MEDIUMTEXT NOT NULL) engine=CSV CHARACTER SET utf8 comment=""General log""
+CREATE TABLE IF NOT EXISTS slow_log (start_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, user_host MEDIUMTEXT NOT NULL, query_time TIME NOT NULL, lock_time TIME NOT NULL, rows_sent INTEGER NOT NULL, rows_examined INTEGER NOT NULL, db VARCHAR(512) NOT NULL, last_insert_id INTEGER NOT NULL, insert_id INTEGER NOT NULL, server_id INTEGER UNSIGNED NOT NULL, sql_text MEDIUMTEXT NOT NULL, thread_id BIGINT(21) UNSIGNED NOT NULL) engine=CSV CHARACTER SET utf8 comment=""Slow log""
+CREATE TABLE IF NOT EXISTS event ( db char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', name char(64) CHARACTER SET utf8 NOT NULL default '', body longblob NOT NULL, definer char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', execute_at DATETIME default NULL, interval_value int(11) default NULL, interval_field ENUM('YEAR','QUARTER','MONTH','DAY','HOUR','MINUTE','WEEK','SECOND','MICROSECOND','YEAR_MONTH','DAY_HOUR','DAY_MINUTE','DAY_SECOND','HOUR_MINUTE','HOUR_SECOND','MINUTE_SECOND','DAY_MICROSECOND','HOUR_MICROSECOND','MINUTE_MICROSECOND','SECOND_MICROSECOND') default NULL, created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, modified TIMESTAMP NOT NULL DEFAULT '0000-00-00 00:00:00', last_executed DATETIME default NULL, starts DATETIME default NULL, ends DATETIME default NULL, status ENUM('ENABLED','DISABLED','SLAVESIDE_DISABLED') NOT NULL default 'ENABLED', on_completion ENUM('DROP','PRESERVE') NOT NULL default 'DROP', sql_mode  set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','NOT_USED','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_ALL_TABLES','NO_ZERO_IN_DATE','NO_ZERO_DATE','INVALID_DATES','ERROR_FOR_DIVISION_BY_ZERO','TRADITIONAL','NO_AUTO_CREATE_USER','HIGH_NOT_PRECEDENCE','NO_ENGINE_SUBSTITUTION','PAD_CHAR_TO_FULL_LENGTH') DEFAULT '' NOT NULL, comment char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', originator INTEGER UNSIGNED NOT NULL, time_zone char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM', character_set_client char(32) collate utf8_bin, collation_connection char(32) collate utf8_bin, db_collation char(32) collate utf8_bin, body_utf8 longblob, PRIMARY KEY (db, name) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT 'Events';
+CREATE TABLE IF NOT EXISTS ndb_binlog_index (Position BIGINT UNSIGNED NOT NULL, File VARCHAR(255) NOT NULL, epoch BIGINT UNSIGNED NOT NULL, inserts INT UNSIGNED NOT NULL, updates INT UNSIGNED NOT NULL, deletes INT UNSIGNED NOT NULL, schemaops INT UNSIGNED NOT NULL, orig_server_id INT UNSIGNED NOT NULL, orig_epoch BIGINT UNSIGNED NOT NULL, gci INT UNSIGNED NOT NULL, PRIMARY KEY(epoch, orig_server_id, orig_epoch)) ENGINE=MYISAM;
+CREATE TABLE IF NOT EXISTS innodb_table_stats (
+    database_name           VARCHAR(64) NOT NULL,
+    table_name          VARCHAR(64) NOT NULL,
+    last_update         TIMESTAMP NOT NULL NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+    n_rows              BIGINT UNSIGNED NOT NULL,
+    clustered_index_size        BIGINT UNSIGNED NOT NULL,
+    sum_of_other_index_sizes    BIGINT UNSIGNED NOT NULL,
+    PRIMARY KEY (database_name, table_name)
+) ENGINE=INNODB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0
+CREATE TABLE IF NOT EXISTS innodb_index_stats (
+    database_name           VARCHAR(64) NOT NULL,
+    table_name          VARCHAR(64) NOT NULL,
+    index_name          VARCHAR(64) NOT NULL,
+    last_update         TIMESTAMP NOT NULL NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+    /* there are at least:
+    stat_name='size'
+    stat_name='n_leaf_pages'
+    stat_name='n_diff_pfx%' */
+    stat_name           VARCHAR(64) NOT NULL,
+    stat_value          BIGINT UNSIGNED NOT NULL,
+    sample_size         BIGINT UNSIGNED,
+    stat_description        VARCHAR(1024) NOT NULL,
+    PRIMARY KEY (database_name, table_name, index_name, stat_name)
+) ENGINE=INNODB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0
+CREATE TABLE IF NOT EXISTS slave_relay_log_info (
+  Number_of_lines INTEGER UNSIGNED NOT NULL COMMENT 'Number of lines in the file or rows in the table. Used to version table definitions.',
+  Relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the current relay log file.',
+  Relay_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The relay log position of the last executed event.',
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log file from which the events in the relay log file were read.',
+  Master_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The master log position of the last executed event.',
+  Sql_delay INTEGER NOT NULL COMMENT 'The number of seconds that the slave must lag behind the master.',
+  Number_of_workers INTEGER UNSIGNED NOT NULL,
+  Id INTEGER UNSIGNED NOT NULL COMMENT 'Internal Id that uniquely identifies this record.',
+  PRIMARY KEY(Id)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Relay Log Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS slave_master_info (
+  Number_of_lines INTEGER UNSIGNED NOT NULL COMMENT 'Number of lines in the file.',
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log currently being read from the master.',
+  Master_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The master log position of the last read event.',
+  Host CHAR(64) CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The host name of the master.',
+  User_name TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The user name used to connect to the master.',
+  User_password TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The password used to connect to the master.',
+  Port INTEGER UNSIGNED NOT NULL COMMENT 'The network port used to connect to the master.',
+  Connect_retry INTEGER UNSIGNED NOT NULL COMMENT 'The period (in seconds) that the slave will wait before trying to reconnect to the master.',
+  Enabled_ssl BOOLEAN NOT NULL COMMENT 'Indicates whether the server supports SSL connections.',
+  Ssl_ca TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Authority (CA) certificate.',
+  Ssl_capath TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path to the Certificate Authority (CA) certificates.',
+  Ssl_cert TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL certificate file.',
+  Ssl_cipher TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the cipher in use for the SSL connection.',
+  Ssl_key TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL key file.',
+  Ssl_verify_server_cert BOOLEAN NOT NULL COMMENT 'Whether to verify the server certificate.',
+  Heartbeat FLOAT NOT NULL COMMENT '',
+  Bind TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'Displays which interface is employed when connecting to the MySQL server',
+  Ignored_server_ids TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The number of server IDs to be ignored, followed by the actual server IDs',
+  Uuid TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The master server uuid.',
+  Retry_count BIGINT UNSIGNED NOT NULL COMMENT 'Number of reconnect attempts, to the master, before giving up.',
+  Ssl_crl TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Revocation List (CRL)',
+  Ssl_crlpath TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path used for Certificate Revocation List (CRL) files',
+  Enabled_auto_position BOOLEAN NOT NULL COMMENT 'Indicates whether GTIDs will be used to retrieve events from the master.',
+  PRIMARY KEY(Host, Port)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Master Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS slave_worker_info (
+  Id INTEGER UNSIGNED NOT NULL,
+  Relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Relay_log_pos BIGINT UNSIGNED NOT NULL,
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Master_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Checkpoint_relay_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Checkpoint_master_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_seqno INT UNSIGNED NOT NULL,
+  Checkpoint_group_size INTEGER UNSIGNED NOT NULL,
+  Checkpoint_group_bitmap BLOB NOT NULL,
+  PRIMARY KEY(Id)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Worker Information' ENGINE= INNODB
+DROP DATABASE IF EXISTS performance_schema
+CREATE DATABASE performance_schema character set utf8
+CREATE TABLE performance_schema.cond_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_instance(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_user_by_event_name(USER CHAR(16) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_account_by_event_name(USER CHAR(16) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_instances(FILE_NAME VARCHAR(512) not null,EVENT_NAME VARCHAR(128) not null,OPEN_COUNT INTEGER unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_summary_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_summary_by_instance(FILE_NAME VARCHAR(512) not null,EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_instances(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,THREAD_ID BIGINT unsigned,SOCKET_ID INTEGER not null,IP VARCHAR(64) not null,PORT INTEGER not null,STATE ENUM('IDLE','ACTIVE') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_summary_by_instance(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT unsigned not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_summary_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT unsigned not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.host_cache(IP VARCHAR(64) not null,HOST VARCHAR(255) collate utf8_bin,HOST_VALIDATED ENUM ('YES', 'NO') not null,SUM_CONNECT_ERRORS BIGINT not null,COUNT_HOST_BLOCKED_ERRORS BIGINT not null,COUNT_NAMEINFO_TRANSIENT_ERRORS BIGINT not null,COUNT_NAMEINFO_PERMANENT_ERRORS BIGINT not null,COUNT_FORMAT_ERRORS BIGINT not null,COUNT_ADDRINFO_TRANSIENT_ERRORS BIGINT not null,COUNT_ADDRINFO_PERMANENT_ERRORS BIGINT not null,COUNT_FCRDNS_ERRORS BIGINT not null,COUNT_HOST_ACL_ERRORS BIGINT not null,COUNT_NO_AUTH_PLUGIN_ERRORS BIGINT not null,COUNT_AUTH_PLUGIN_ERRORS BIGINT not null,COUNT_HANDSHAKE_ERRORS BIGINT not null,COUNT_PROXY_USER_ERRORS BIGINT not null,COUNT_PROXY_USER_ACL_ERRORS BIGINT not null,COUNT_AUTHENTICATION_ERRORS BIGINT not null,COUNT_SSL_ERRORS BIGINT not null,COUNT_MAX_USER_CONNECTIONS_ERRORS BIGINT not null,COUNT_MAX_USER_CONNECTIONS_PER_HOUR_ERRORS BIGINT not null,COUNT_DEFAULT_DATABASE_ERRORS BIGINT not null,COUNT_INIT_CONNECT_ERRORS BIGINT not null,COUNT_LOCAL_ERRORS BIGINT not null,COUNT_UNKNOWN_ERRORS BIGINT not null,FIRST_SEEN TIMESTAMP(0) NOT NULL default 0,LAST_SEEN TIMESTAMP(0) NOT NULL default 0,FIRST_ERROR_SEEN TIMESTAMP(0) null default 0,LAST_ERROR_SEEN TIMESTAMP(0) null default 0)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.mutex_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,LOCKED_BY_THREAD_ID BIGINT unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.objects_summary_global_by_type(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.performance_timers(TIMER_NAME ENUM ('CYCLE', 'NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'TICK') not null,TIMER_FREQUENCY BIGINT,TIMER_RESOLUTION BIGINT,TIMER_OVERHEAD BIGINT) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.rwlock_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,WRITE_LOCKED_BY_THREAD_ID BIGINT unsigned,READ_LOCKED_BY_COUNT INTEGER unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_actors(HOST CHAR(60) collate utf8_bin default '%' not null,USER CHAR(16) collate utf8_bin default '%' not null,ROLE CHAR(16) collate utf8_bin default '%' not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_consumers(NAME VARCHAR(64) not null,ENABLED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_instruments(NAME VARCHAR(128) not null,ENABLED ENUM ('YES', 'NO') not null,TIMED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_objects(OBJECT_TYPE ENUM ('TABLE') not null default 'TABLE',OBJECT_SCHEMA VARCHAR(64) default '%',OBJECT_NAME VARCHAR(64) not null default '%',ENABLED ENUM ('YES', 'NO') not null default 'YES',TIMED ENUM ('YES', 'NO') not null default 'YES')ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_timers(NAME VARCHAR(64) not null,TIMER_NAME ENUM ('CYCLE', 'NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'TICK') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_io_waits_summary_by_index_usage(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),INDEX_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_FETCH BIGINT unsigned not null,SUM_TIMER_FETCH BIGINT unsigned not null,MIN_TIMER_FETCH BIGINT unsigned not null,AVG_TIMER_FETCH BIGINT unsigned not null,MAX_TIMER_FETCH BIGINT unsigned not null,COUNT_INSERT BIGINT unsigned not null,SUM_TIMER_INSERT BIGINT unsigned not null,MIN_TIMER_INSERT BIGINT unsigned not null,AVG_TIMER_INSERT BIGINT unsigned not null,MAX_TIMER_INSERT BIGINT unsigned not null,COUNT_UPDATE BIGINT unsigned not null,SUM_TIMER_UPDATE BIGINT unsigned not null,MIN_TIMER_UPDATE BIGINT unsigned not null,AVG_TIMER_UPDATE BIGINT unsigned not null,MAX_TIMER_UPDATE BIGINT unsigned not null,COUNT_DELETE BIGINT unsigned not null,SUM_TIMER_DELETE BIGINT unsigned not null,MIN_TIMER_DELETE BIGINT unsigned not null,AVG_TIMER_DELETE BIGINT unsigned not null,MAX_TIMER_DELETE BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_io_waits_summary_by_table(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_FETCH BIGINT unsigned not null,SUM_TIMER_FETCH BIGINT unsigned not null,MIN_TIMER_FETCH BIGINT unsigned not null,AVG_TIMER_FETCH BIGINT unsigned not null,MAX_TIMER_FETCH BIGINT unsigned not null,COUNT_INSERT BIGINT unsigned not null,SUM_TIMER_INSERT BIGINT unsigned not null,MIN_TIMER_INSERT BIGINT unsigned not null,AVG_TIMER_INSERT BIGINT unsigned not null,MAX_TIMER_INSERT BIGINT unsigned not null,COUNT_UPDATE BIGINT unsigned not null,SUM_TIMER_UPDATE BIGINT unsigned not null,MIN_TIMER_UPDATE BIGINT unsigned not null,AVG_TIMER_UPDATE BIGINT unsigned not null,MAX_TIMER_UPDATE BIGINT unsigned not null,COUNT_DELETE BIGINT unsigned not null,SUM_TIMER_DELETE BIGINT unsigned not null,MIN_TIMER_DELETE BIGINT unsigned not null,AVG_TIMER_DELETE BIGINT unsigned not null,MAX_TIMER_DELETE BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_lock_waits_summary_by_table(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_READ_NORMAL BIGINT unsigned not null,SUM_TIMER_READ_NORMAL BIGINT unsigned not null,MIN_TIMER_READ_NORMAL BIGINT unsigned not null,AVG_TIMER_READ_NORMAL BIGINT unsigned not null,MAX_TIMER_READ_NORMAL BIGINT unsigned not null,COUNT_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,SUM_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,MIN_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,AVG_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,MAX_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,COUNT_READ_HIGH_PRIORITY BIGINT unsigned not null,SUM_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,MIN_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,AVG_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,MAX_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,COUNT_READ_NO_INSERT BIGINT unsigned not null,SUM_TIMER_READ_NO_INSERT BIGINT unsigned not null,MIN_TIMER_READ_NO_INSERT BIGINT unsigned not null,AVG_TIMER_READ_NO_INSERT BIGINT unsigned not null,MAX_TIMER_READ_NO_INSERT BIGINT unsigned not null,COUNT_READ_EXTERNAL BIGINT unsigned not null,SUM_TIMER_READ_EXTERNAL BIGINT unsigned not null,MIN_TIMER_READ_EXTERNAL BIGINT unsigned not null,AVG_TIMER_READ_EXTERNAL BIGINT unsigned not null,MAX_TIMER_READ_EXTERNAL BIGINT unsigned not null,COUNT_WRITE_ALLOW_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,COUNT_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,SUM_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,MIN_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,AVG_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,MAX_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,COUNT_WRITE_DELAYED BIGINT unsigned not null,SUM_TIMER_WRITE_DELAYED BIGINT unsigned not null,MIN_TIMER_WRITE_DELAYED BIGINT unsigned not null,AVG_TIMER_WRITE_DELAYED BIGINT unsigned not null,MAX_TIMER_WRITE_DELAYED BIGINT unsigned not null,COUNT_WRITE_LOW_PRIORITY BIGINT unsigned not null,SUM_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,MIN_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,AVG_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,MAX_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,COUNT_WRITE_NORMAL BIGINT unsigned not null,SUM_TIMER_WRITE_NORMAL BIGINT unsigned not null,MIN_TIMER_WRITE_NORMAL BIGINT unsigned not null,AVG_TIMER_WRITE_NORMAL BIGINT unsigned not null,MAX_TIMER_WRITE_NORMAL BIGINT unsigned not null,COUNT_WRITE_EXTERNAL BIGINT unsigned not null,SUM_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,MIN_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,AVG_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,MAX_TIMER_WRITE_EXTERNAL BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.threads(THREAD_ID BIGINT unsigned not null,NAME VARCHAR(128) not null,TYPE VARCHAR(10) not null,PROCESSLIST_ID BIGINT unsigned,PROCESSLIST_USER VARCHAR(16),PROCESSLIST_HOST VARCHAR(60),PROCESSLIST_DB VARCHAR(64),PROCESSLIST_COMMAND VARCHAR(16),PROCESSLIST_TIME BIGINT,PROCESSLIST_STATE VARCHAR(64),PROCESSLIST_INFO LONGTEXT,PARENT_THREAD_ID BIGINT unsigned,ROLE VARCHAR(64),INSTRUMENTED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_user_by_event_name(USER CHAR(16) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_account_by_event_name(USER CHAR(16) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_user_by_event_name(USER CHAR(16) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_account_by_event_name(USER CHAR(16) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.hosts(HOST CHAR(60) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.users(USER CHAR(16) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.accounts(USER CHAR(16) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_digest(SCHEMA_NAME VARCHAR(64),DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null,FIRST_SEEN TIMESTAMP(0) NOT NULL default 0,LAST_SEEN TIMESTAMP(0) NOT NULL default 0)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.session_connect_attrs(PROCESSLIST_ID INT NOT NULL,ATTR_NAME VARCHAR(32) NOT NULL,ATTR_VALUE VARCHAR(1024),ORDINAL_POSITION INT)ENGINE=PERFORMANCE_SCHEMA CHARACTER SET utf8 COLLATE utf8_bin
+CREATE TABLE performance_schema.session_account_connect_attrs  LIKE performance_schema.session_connect_attrs
+CREATE TABLE IF NOT EXISTS proxies_priv (Host char(60) binary DEFAULT '' NOT NULL, User char(16) binary DEFAULT '' NOT NULL, Proxied_host char(60) binary DEFAULT '' NOT NULL, Proxied_user char(16) binary DEFAULT '' NOT NULL, With_grant BOOL DEFAULT 0 NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY Host (Host,User,Proxied_host,Proxied_user), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='User proxy privileges';
+DROP TEMPORARY TABLE IF EXISTS `tmp_db` /* generated by server */
+DROP TEMPORARY TABLE IF EXISTS `tmp_user` /* generated by server */
+DROP TEMPORARY TABLE IF EXISTS `tmp_proxies_priv` /* generated by server */
+TRUNCATE TABLE help_topic;
+TRUNCATE TABLE help_category;
+TRUNCATE TABLE help_keyword;
+TRUNCATE TABLE help_relation;
+TRUNCATE TABLE time_zone
+TRUNCATE TABLE time_zone_name
+TRUNCATE TABLE time_zone_transition
+TRUNCATE TABLE time_zone_transition_type
+ALTER TABLE time_zone_transition ORDER BY Time_zone_id, Transition_time
+ALTER TABLE time_zone_transition_type ORDER BY Time_zone_id, Transition_type_id
+CREATE DATABASE IF NOT EXISTS `mysql`
+CREATE USER 'mysqluser'@'%' IDENTIFIED BY PASSWORD '*FBC02A898D66B9181D6F8826C045C11FD2B364A4'
+GRANT ALL ON `mysql`.* TO 'mysqluser'@'%'
+FLUSH PRIVILEGES
+GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replicator'@'%' IDENTIFIED BY PASSWORD '*D98280F03D0F78162EBDBB9C883FC01395DEA2BF'
+CREATE DATABASE connector_test
+GRANT ALL PRIVILEGES ON connector_test.* TO 'mysqluser'@'%'
+CREATE TABLE products (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  name VARCHAR(255) NOT NULL,
+  description VARCHAR(512),
+  weight FLOAT
+)
+ALTER TABLE products AUTO_INCREMENT = 101
+CREATE TABLE products_on_hand (
+  product_id INTEGER NOT NULL PRIMARY KEY,
+  quantity INTEGER NOT NULL,
+  FOREIGN KEY (product_id) REFERENCES products(id)
+)
+CREATE TABLE customers (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  first_name VARCHAR(255) NOT NULL,
+  last_name VARCHAR(255) NOT NULL,
+  email VARCHAR(255) NOT NULL UNIQUE KEY
+) AUTO_INCREMENT=1001
+CREATE TABLE orders (
+  order_number INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  order_date DATE NOT NULL,
+  purchaser INTEGER NOT NULL,
+  quantity INTEGER NOT NULL,
+  product_id INTEGER NOT NULL,
+  FOREIGN KEY order_customer (purchaser) REFERENCES customers(id),
+  FOREIGN KEY ordered_product (product_id) REFERENCES products(id)
+) AUTO_INCREMENT = 10001
+CREATE DATABASE emptydb
+GRANT ALL PRIVILEGES ON emptydb.* TO 'mysqluser'@'%'
+GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replicator'@'%' IDENTIFIED BY PASSWORD '*D98280F03D0F78162EBDBB9C883FC01395DEA2BF'
+CREATE DATABASE readbinlog_test
+GRANT ALL PRIVILEGES ON readbinlog_test.* TO 'mysqluser'@'%'
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)",2016-05-13T14:32:47Z,137
"@@ -0,0 +1,289 @@
+--
+-- Statements recorded by binlog during MySQL 5.6 initialization with Debezium scripts.
+--
+CREATE DATABASE mysql;
+
+CREATE TABLE IF NOT EXISTS db (   Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Select_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Insert_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Update_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Delete_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Drop_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Grant_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, References_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Index_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tmp_table_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Lock_tables_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Execute_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Event_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Trigger_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY Host (Host,Db,User), KEY User (User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='Database privileges';
+
+CREATE TABLE IF NOT EXISTS user (   Host char(60) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Select_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Insert_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Update_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Delete_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Drop_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Reload_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Shutdown_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Process_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, File_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Grant_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, References_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Index_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_db_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Super_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tmp_table_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Lock_tables_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Execute_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Repl_slave_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Repl_client_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Show_view_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Alter_routine_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_user_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Event_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Trigger_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, Create_tablespace_priv enum('N','Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, ssl_type enum('','ANY','X509', 'SPECIFIED') COLLATE utf8_general_ci DEFAULT '' NOT NULL, ssl_cipher BLOB NOT NULL, x509_issuer BLOB NOT NULL, x509_subject BLOB NOT NULL, max_questions int(11) unsigned DEFAULT 0  NOT NULL, max_updates int(11) unsigned DEFAULT 0  NOT NULL, max_connections int(11) unsigned DEFAULT 0  NOT NULL, max_user_connections int(11) unsigned DEFAULT 0  NOT NULL, plugin char(64) DEFAULT 'mysql_native_password' NOT NULL, authentication_string TEXT, password_expired ENUM('N', 'Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, password_last_changed timestamp NULL DEFAULT NULL, password_lifetime smallint unsigned NULL DEFAULT NULL, account_locked ENUM('N', 'Y') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY Host (Host,User) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='Users and global privileges';
+
+CREATE TABLE IF NOT EXISTS func (  name char(64) binary DEFAULT '' NOT NULL, ret tinyint(1) DEFAULT '0' NOT NULL, dl char(128) DEFAULT '' NOT NULL, type enum ('function','aggregate') COLLATE utf8_general_ci NOT NULL, PRIMARY KEY (name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='User defined functions';
+
+CREATE TABLE IF NOT EXISTS plugin ( name varchar(64) DEFAULT '' NOT NULL, dl varchar(128) DEFAULT '' NOT NULL, PRIMARY KEY (name) ) engine=InnoDB STATS_PERSISTENT=0 CHARACTER SET utf8 COLLATE utf8_general_ci comment='MySQL plugins';
+
+CREATE TABLE IF NOT EXISTS servers ( Server_name char(64) NOT NULL DEFAULT '', Host char(64) NOT NULL DEFAULT '', Db char(64) NOT NULL DEFAULT '', Username char(64) NOT NULL DEFAULT '', Password char(64) NOT NULL DEFAULT '', Port INT(4) NOT NULL DEFAULT '0', Socket char(64) NOT NULL DEFAULT '', Wrapper char(64) NOT NULL DEFAULT '', Owner char(64) NOT NULL DEFAULT '', PRIMARY KEY (Server_name)) engine=InnoDB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='MySQL Foreign Servers table';
+
+CREATE TABLE IF NOT EXISTS tables_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Table_name char(64) binary DEFAULT '' NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, Table_priv set('Select','Insert','Update','Delete','Create','Drop','Grant','References','Index','Alter','Create View','Show view','Trigger') COLLATE utf8_general_ci DEFAULT '' NOT NULL, Column_priv set('Select','Insert','Update','References') COLLATE utf8_general_ci DEFAULT '' NOT NULL, PRIMARY KEY (Host,Db,User,Table_name), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Table privileges';
+
+CREATE TABLE IF NOT EXISTS columns_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Table_name char(64) binary DEFAULT '' NOT NULL, Column_name char(64) binary DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, Column_priv set('Select','Insert','Update','References') COLLATE utf8_general_ci DEFAULT '' NOT NULL, PRIMARY KEY (Host,Db,User,Table_name,Column_name) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Column privileges';
+
+CREATE TABLE IF NOT EXISTS help_topic ( help_topic_id int unsigned not null, name char(64) not null, help_category_id smallint unsigned not null, description text not null, example text not null, url text not null, primary key (help_topic_id), unique index (name) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='help topics';
+
+CREATE TABLE IF NOT EXISTS help_category ( help_category_id smallint unsigned not null, name  char(64) not null, parent_category_id smallint unsigned null, url text not null, primary key (help_category_id), unique index (name) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='help categories';
+
+CREATE TABLE IF NOT EXISTS help_relation ( help_topic_id int unsigned not null, help_keyword_id  int unsigned not null, primary key (help_keyword_id, help_topic_id) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='keyword-topic relation';
+
+CREATE TABLE IF NOT EXISTS help_keyword (   help_keyword_id  int unsigned not null, name char(64) not null, primary key (help_keyword_id), unique index (name) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8 comment='help keywords';
+
+CREATE TABLE IF NOT EXISTS time_zone_name (   Name char(64) NOT NULL, Time_zone_id int unsigned NOT NULL, PRIMARY KEY Name (Name) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Time zone names';
+
+CREATE TABLE IF NOT EXISTS time_zone (   Time_zone_id int unsigned NOT NULL auto_increment, Use_leap_seconds enum('Y','N') COLLATE utf8_general_ci DEFAULT 'N' NOT NULL, PRIMARY KEY TzId (Time_zone_id) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Time zones';
+
+CREATE TABLE IF NOT EXISTS time_zone_transition (   Time_zone_id int unsigned NOT NULL, Transition_time bigint signed NOT NULL, Transition_type_id int unsigned NOT NULL, PRIMARY KEY TzIdTranTime (Time_zone_id, Transition_time) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Time zone transitions';
+
+CREATE TABLE IF NOT EXISTS time_zone_transition_type (   Time_zone_id int unsigned NOT NULL, Transition_type_id int unsigned NOT NULL, Offset int signed DEFAULT 0 NOT NULL, Is_DST tinyint unsigned DEFAULT 0 NOT NULL, Abbreviation char(8) DEFAULT '' NOT NULL, PRIMARY KEY TzIdTrTId (Time_zone_id, Transition_type_id) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Time zone transition types';
+
+CREATE TABLE IF NOT EXISTS time_zone_leap_second (   Transition_time bigint signed NOT NULL, Correction int signed NOT NULL, PRIMARY KEY TranTime (Transition_time) ) engine=INNODB STATS_PERSISTENT=0 CHARACTER SET utf8   comment='Leap seconds information for time zones';
+
+CREATE TABLE IF NOT EXISTS proc (db char(64) collate utf8_bin DEFAULT '' NOT NULL, name char(64) DEFAULT '' NOT NULL, type enum('FUNCTION','PROCEDURE') NOT NULL, specific_name char(64) DEFAULT '' NOT NULL, language enum('SQL') DEFAULT 'SQL' NOT NULL, sql_data_access enum( 'CONTAINS_SQL', 'NO_SQL', 'READS_SQL_DATA', 'MODIFIES_SQL_DATA') DEFAULT 'CONTAINS_SQL' NOT NULL, is_deterministic enum('YES','NO') DEFAULT 'NO' NOT NULL, security_type enum('INVOKER','DEFINER') DEFAULT 'DEFINER' NOT NULL, param_list blob NOT NULL, returns longblob DEFAULT '' NOT NULL, body longblob NOT NULL, definer char(77) collate utf8_bin DEFAULT '' NOT NULL, created timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, modified timestamp NOT NULL DEFAULT '0000-00-00 00:00:00', sql_mode set( 'REAL_AS_FLOAT', 'PIPES_AS_CONCAT', 'ANSI_QUOTES', 'IGNORE_SPACE', 'NOT_USED', 'ONLY_FULL_GROUP_BY', 'NO_UNSIGNED_SUBTRACTION', 'NO_DIR_IN_CREATE', 'POSTGRESQL', 'ORACLE', 'MSSQL', 'DB2', 'MAXDB', 'NO_KEY_OPTIONS', 'NO_TABLE_OPTIONS', 'NO_FIELD_OPTIONS', 'MYSQL323', 'MYSQL40', 'ANSI', 'NO_AUTO_VALUE_ON_ZERO', 'NO_BACKSLASH_ESCAPES', 'STRICT_TRANS_TABLES', 'STRICT_ALL_TABLES', 'NO_ZERO_IN_DATE', 'NO_ZERO_DATE', 'INVALID_DATES', 'ERROR_FOR_DIVISION_BY_ZERO', 'TRADITIONAL', 'NO_AUTO_CREATE_USER', 'HIGH_NOT_PRECEDENCE', 'NO_ENGINE_SUBSTITUTION', 'PAD_CHAR_TO_FULL_LENGTH') DEFAULT '' NOT NULL, comment text collate utf8_bin NOT NULL, character_set_client char(32) collate utf8_bin, collation_connection char(32) collate utf8_bin, db_collation char(32) collate utf8_bin, body_utf8 longblob, PRIMARY KEY (db,name,type)) engine=MyISAM character set utf8 comment='Stored Procedures';
+
+CREATE TABLE IF NOT EXISTS procs_priv ( Host char(60) binary DEFAULT '' NOT NULL, Db char(64) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Routine_name char(64) COLLATE utf8_general_ci DEFAULT '' NOT NULL, Routine_type enum('FUNCTION','PROCEDURE') NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Proc_priv set('Execute','Alter Routine','Grant') COLLATE utf8_general_ci DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (Host,Db,User,Routine_name,Routine_type), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin   comment='Procedure privileges';
+
+CREATE TABLE IF NOT EXISTS general_log (event_time TIMESTAMP(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), user_host MEDIUMTEXT NOT NULL, thread_id BIGINT(21) UNSIGNED NOT NULL, server_id INTEGER UNSIGNED NOT NULL, command_type VARCHAR(64) NOT NULL, argument MEDIUMBLOB NOT NULL) engine=CSV CHARACTER SET utf8 comment=""General log"";
+
+CREATE TABLE IF NOT EXISTS slow_log (start_time TIMESTAMP(6) NOT NULL DEFAULT CURRENT_TIMESTAMP(6) ON UPDATE CURRENT_TIMESTAMP(6), user_host MEDIUMTEXT NOT NULL, query_time TIME(6) NOT NULL, lock_time TIME(6) NOT NULL, rows_sent INTEGER NOT NULL, rows_examined INTEGER NOT NULL, db VARCHAR(512) NOT NULL, last_insert_id INTEGER NOT NULL, insert_id INTEGER NOT NULL, server_id INTEGER UNSIGNED NOT NULL, sql_text MEDIUMBLOB NOT NULL, thread_id BIGINT(21) UNSIGNED NOT NULL) engine=CSV CHARACTER SET utf8 comment=""Slow log"";
+
+CREATE TABLE IF NOT EXISTS event ( db char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', name char(64) CHARACTER SET utf8 NOT NULL default '', body longblob NOT NULL, definer char(77) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', execute_at DATETIME default NULL, interval_value int(11) default NULL, interval_field ENUM('YEAR','QUARTER','MONTH','DAY','HOUR','MINUTE','WEEK','SECOND','MICROSECOND','YEAR_MONTH','DAY_HOUR','DAY_MINUTE','DAY_SECOND','HOUR_MINUTE','HOUR_SECOND','MINUTE_SECOND','DAY_MICROSECOND','HOUR_MICROSECOND','MINUTE_MICROSECOND','SECOND_MICROSECOND') default NULL, created TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, modified TIMESTAMP NOT NULL DEFAULT '0000-00-00 00:00:00', last_executed DATETIME default NULL, starts DATETIME default NULL, ends DATETIME default NULL, status ENUM('ENABLED','DISABLED','SLAVESIDE_DISABLED') NOT NULL default 'ENABLED', on_completion ENUM('DROP','PRESERVE') NOT NULL default 'DROP', sql_mode  set('REAL_AS_FLOAT','PIPES_AS_CONCAT','ANSI_QUOTES','IGNORE_SPACE','NOT_USED','ONLY_FULL_GROUP_BY','NO_UNSIGNED_SUBTRACTION','NO_DIR_IN_CREATE','POSTGRESQL','ORACLE','MSSQL','DB2','MAXDB','NO_KEY_OPTIONS','NO_TABLE_OPTIONS','NO_FIELD_OPTIONS','MYSQL323','MYSQL40','ANSI','NO_AUTO_VALUE_ON_ZERO','NO_BACKSLASH_ESCAPES','STRICT_TRANS_TABLES','STRICT_ALL_TABLES','NO_ZERO_IN_DATE','NO_ZERO_DATE','INVALID_DATES','ERROR_FOR_DIVISION_BY_ZERO','TRADITIONAL','NO_AUTO_CREATE_USER','HIGH_NOT_PRECEDENCE','NO_ENGINE_SUBSTITUTION','PAD_CHAR_TO_FULL_LENGTH') DEFAULT '' NOT NULL, comment char(64) CHARACTER SET utf8 COLLATE utf8_bin NOT NULL default '', originator INTEGER UNSIGNED NOT NULL, time_zone char(64) CHARACTER SET latin1 NOT NULL DEFAULT 'SYSTEM', character_set_client char(32) collate utf8_bin, collation_connection char(32) collate utf8_bin, db_collation char(32) collate utf8_bin, body_utf8 longblob, PRIMARY KEY (db, name) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 COMMENT 'Events';
+
+CREATE TABLE IF NOT EXISTS ndb_binlog_index (Position BIGINT UNSIGNED NOT NULL, File VARCHAR(255) NOT NULL, epoch BIGINT UNSIGNED NOT NULL, inserts INT UNSIGNED NOT NULL, updates INT UNSIGNED NOT NULL, deletes INT UNSIGNED NOT NULL, schemaops INT UNSIGNED NOT NULL, orig_server_id INT UNSIGNED NOT NULL, orig_epoch BIGINT UNSIGNED NOT NULL, gci INT UNSIGNED NOT NULL, next_position BIGINT UNSIGNED NOT NULL, next_file VARCHAR(255) NOT NULL, PRIMARY KEY(epoch, orig_server_id, orig_epoch)) ENGINE=MYISAM;
+
+CREATE TABLE IF NOT EXISTS innodb_table_stats (
+	database_name			VARCHAR(64) NOT NULL,
+	table_name			VARCHAR(64) NOT NULL,
+	last_update			TIMESTAMP NOT NULL NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+	n_rows				BIGINT UNSIGNED NOT NULL,
+	clustered_index_size		BIGINT UNSIGNED NOT NULL,
+	sum_of_other_index_sizes	BIGINT UNSIGNED NOT NULL,
+	PRIMARY KEY (database_name, table_name)
+) ENGINE=INNODB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0
+CREATE TABLE IF NOT EXISTS innodb_index_stats (
+	database_name			VARCHAR(64) NOT NULL,
+	table_name			VARCHAR(64) NOT NULL,
+	index_name			VARCHAR(64) NOT NULL,
+	last_update			TIMESTAMP NOT NULL NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+	/* there are at least:
+	stat_name='size'
+	stat_name='n_leaf_pages'
+	stat_name='n_diff_pfx%' */
+	stat_name			VARCHAR(64) NOT NULL,
+	stat_value			BIGINT UNSIGNED NOT NULL,
+	sample_size			BIGINT UNSIGNED,
+	stat_description		VARCHAR(1024) NOT NULL,
+	PRIMARY KEY (database_name, table_name, index_name, stat_name)
+) ENGINE=INNODB DEFAULT CHARSET=utf8 COLLATE=utf8_bin STATS_PERSISTENT=0
+CREATE TABLE IF NOT EXISTS slave_relay_log_info (
+  Number_of_lines INTEGER UNSIGNED NOT NULL COMMENT 'Number of lines in the file or rows in the table. Used to version table definitions.',
+  Relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the current relay log file.',
+  Relay_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The relay log position of the last executed event.',
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log file from which the events in the relay log file were read.',
+  Master_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The master log position of the last executed event.',
+  Sql_delay INTEGER NOT NULL COMMENT 'The number of seconds that the slave must lag behind the master.',
+  Number_of_workers INTEGER UNSIGNED NOT NULL,
+  Id INTEGER UNSIGNED NOT NULL COMMENT 'Internal Id that uniquely identifies this record.',
+  Channel_name CHAR(64) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT 'The channel on which the slave is connected to a source. Used in Multisource Replication',
+  PRIMARY KEY(Channel_name)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Relay Log Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS slave_master_info (
+  Number_of_lines INTEGER UNSIGNED NOT NULL COMMENT 'Number of lines in the file.',
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL COMMENT 'The name of the master binary log currently being read from the master.',
+  Master_log_pos BIGINT UNSIGNED NOT NULL COMMENT 'The master log position of the last read event.',
+  Host CHAR(64) CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The host name of the master.',
+  User_name TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The user name used to connect to the master.',
+  User_password TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The password used to connect to the master.',
+  Port INTEGER UNSIGNED NOT NULL COMMENT 'The network port used to connect to the master.',
+  Connect_retry INTEGER UNSIGNED NOT NULL COMMENT 'The period (in seconds) that the slave will wait before trying to reconnect to the master.',
+  Enabled_ssl BOOLEAN NOT NULL COMMENT 'Indicates whether the server supports SSL connections.',
+  Ssl_ca TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Authority (CA) certificate.',
+  Ssl_capath TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path to the Certificate Authority (CA) certificates.',
+  Ssl_cert TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL certificate file.',
+  Ssl_cipher TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the cipher in use for the SSL connection.',
+  Ssl_key TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The name of the SSL key file.',
+  Ssl_verify_server_cert BOOLEAN NOT NULL COMMENT 'Whether to verify the server certificate.',
+  Heartbeat FLOAT NOT NULL COMMENT '',
+  Bind TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'Displays which interface is employed when connecting to the MySQL server',
+  Ignored_server_ids TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The number of server IDs to be ignored, followed by the actual server IDs',
+  Uuid TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The master server uuid.',
+  Retry_count BIGINT UNSIGNED NOT NULL COMMENT 'Number of reconnect attempts, to the master, before giving up.',
+  Ssl_crl TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The file used for the Certificate Revocation List (CRL)',
+  Ssl_crlpath TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'The path used for Certificate Revocation List (CRL) files',
+  Enabled_auto_position BOOLEAN NOT NULL COMMENT 'Indicates whether GTIDs will be used to retrieve events from the master.',
+  Channel_name CHAR(64) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT 'The channel on which the slave is connected to a source. Used in Multisource Replication',
+  Tls_version TEXT CHARACTER SET utf8 COLLATE utf8_bin COMMENT 'Tls version',
+  PRIMARY KEY(Channel_name)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Master Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS slave_worker_info (
+  Id INTEGER UNSIGNED NOT NULL,
+  Relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Relay_log_pos BIGINT UNSIGNED NOT NULL,
+  Master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Master_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_relay_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Checkpoint_relay_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_master_log_name TEXT CHARACTER SET utf8 COLLATE utf8_bin NOT NULL,
+  Checkpoint_master_log_pos BIGINT UNSIGNED NOT NULL,
+  Checkpoint_seqno INT UNSIGNED NOT NULL,
+  Checkpoint_group_size INTEGER UNSIGNED NOT NULL,
+  Checkpoint_group_bitmap BLOB NOT NULL,
+  Channel_name CHAR(64) CHARACTER SET utf8 COLLATE utf8_general_ci NOT NULL COMMENT 'The channel on which the slave is connected to a source. Used in Multisource Replication',
+  PRIMARY KEY(Channel_name, Id)) DEFAULT CHARSET=utf8 STATS_PERSISTENT=0 COMMENT 'Worker Information' ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS gtid_executed (
+    source_uuid CHAR(36) NOT NULL COMMENT 'uuid of the source where the transaction was originally executed.',
+    interval_start BIGINT NOT NULL COMMENT 'First number of interval.',
+    interval_end BIGINT NOT NULL COMMENT 'Last number of interval.',
+    PRIMARY KEY(source_uuid, interval_start)) ENGINE= INNODB
+CREATE TABLE IF NOT EXISTS server_cost (
+  cost_name   VARCHAR(64) NOT NULL,
+  cost_value  FLOAT DEFAULT NULL,
+  last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+  comment     VARCHAR(1024) DEFAULT NULL,
+  PRIMARY KEY (cost_name)
+) ENGINE=InnoDB CHARACTER SET=utf8 COLLATE=utf8_general_ci STATS_PERSISTENT=0;
+
+CREATE TABLE IF NOT EXISTS engine_cost (
+  engine_name VARCHAR(64) NOT NULL,
+  device_type INTEGER NOT NULL,
+  cost_name   VARCHAR(64) NOT NULL,
+  cost_value  FLOAT DEFAULT NULL,
+  last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
+  comment     VARCHAR(1024) DEFAULT NULL,
+  PRIMARY KEY (cost_name, engine_name, device_type)
+) ENGINE=InnoDB CHARACTER SET=utf8 COLLATE=utf8_general_ci STATS_PERSISTENT=0;
+
+DROP DATABASE IF EXISTS performance_schema
+CREATE DATABASE performance_schema character set utf8
+CREATE TABLE performance_schema.cond_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,SPINS INTEGER unsigned,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(512),INDEX_NAME VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),OPERATION VARCHAR(32) not null,NUMBER_OF_BYTES BIGINT,FLAGS INTEGER unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_instance(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_waits_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_instances(FILE_NAME VARCHAR(512) not null,EVENT_NAME VARCHAR(128) not null,OPEN_COUNT INTEGER unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_summary_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.file_summary_by_instance(FILE_NAME VARCHAR(512) not null,EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_instances(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,THREAD_ID BIGINT unsigned,SOCKET_ID INTEGER not null,IP VARCHAR(64) not null,PORT INTEGER not null,STATE ENUM('IDLE','ACTIVE') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_summary_by_instance(EVENT_NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT unsigned not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.socket_summary_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_WRITE BIGINT unsigned not null,COUNT_MISC BIGINT unsigned not null,SUM_TIMER_MISC BIGINT unsigned not null,MIN_TIMER_MISC BIGINT unsigned not null,AVG_TIMER_MISC BIGINT unsigned not null,MAX_TIMER_MISC BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.host_cache(IP VARCHAR(64) not null,HOST VARCHAR(255) collate utf8_bin,HOST_VALIDATED ENUM ('YES', 'NO') not null,SUM_CONNECT_ERRORS BIGINT not null,COUNT_HOST_BLOCKED_ERRORS BIGINT not null,COUNT_NAMEINFO_TRANSIENT_ERRORS BIGINT not null,COUNT_NAMEINFO_PERMANENT_ERRORS BIGINT not null,COUNT_FORMAT_ERRORS BIGINT not null,COUNT_ADDRINFO_TRANSIENT_ERRORS BIGINT not null,COUNT_ADDRINFO_PERMANENT_ERRORS BIGINT not null,COUNT_FCRDNS_ERRORS BIGINT not null,COUNT_HOST_ACL_ERRORS BIGINT not null,COUNT_NO_AUTH_PLUGIN_ERRORS BIGINT not null,COUNT_AUTH_PLUGIN_ERRORS BIGINT not null,COUNT_HANDSHAKE_ERRORS BIGINT not null,COUNT_PROXY_USER_ERRORS BIGINT not null,COUNT_PROXY_USER_ACL_ERRORS BIGINT not null,COUNT_AUTHENTICATION_ERRORS BIGINT not null,COUNT_SSL_ERRORS BIGINT not null,COUNT_MAX_USER_CONNECTIONS_ERRORS BIGINT not null,COUNT_MAX_USER_CONNECTIONS_PER_HOUR_ERRORS BIGINT not null,COUNT_DEFAULT_DATABASE_ERRORS BIGINT not null,COUNT_INIT_CONNECT_ERRORS BIGINT not null,COUNT_LOCAL_ERRORS BIGINT not null,COUNT_UNKNOWN_ERRORS BIGINT not null,FIRST_SEEN TIMESTAMP(0) NOT NULL default 0,LAST_SEEN TIMESTAMP(0) NOT NULL default 0,FIRST_ERROR_SEEN TIMESTAMP(0) null default 0,LAST_ERROR_SEEN TIMESTAMP(0) null default 0)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.mutex_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,LOCKED_BY_THREAD_ID BIGINT unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.objects_summary_global_by_type(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.performance_timers(TIMER_NAME ENUM ('CYCLE', 'NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'TICK') not null,TIMER_FREQUENCY BIGINT,TIMER_RESOLUTION BIGINT,TIMER_OVERHEAD BIGINT) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.rwlock_instances(NAME VARCHAR(128) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,WRITE_LOCKED_BY_THREAD_ID BIGINT unsigned,READ_LOCKED_BY_COUNT INTEGER unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_actors(HOST CHAR(60) collate utf8_bin default '%' not null,USER CHAR(32) collate utf8_bin default '%' not null,ROLE CHAR(16) collate utf8_bin default '%' not null,ENABLED ENUM ('YES', 'NO') not null default 'YES',HISTORY ENUM ('YES', 'NO') not null default 'YES')ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_consumers(NAME VARCHAR(64) not null,ENABLED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_instruments(NAME VARCHAR(128) not null,ENABLED ENUM ('YES', 'NO') not null,TIMED ENUM ('YES', 'NO') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_objects(OBJECT_TYPE ENUM ('EVENT', 'FUNCTION', 'PROCEDURE', 'TABLE', 'TRIGGER') not null default 'TABLE',OBJECT_SCHEMA VARCHAR(64) default '%',OBJECT_NAME VARCHAR(64) not null default '%',ENABLED ENUM ('YES', 'NO') not null default 'YES',TIMED ENUM ('YES', 'NO') not null default 'YES')ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.setup_timers(NAME VARCHAR(64) not null,TIMER_NAME ENUM ('CYCLE', 'NANOSECOND', 'MICROSECOND', 'MILLISECOND', 'TICK') not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_io_waits_summary_by_index_usage(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),INDEX_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_FETCH BIGINT unsigned not null,SUM_TIMER_FETCH BIGINT unsigned not null,MIN_TIMER_FETCH BIGINT unsigned not null,AVG_TIMER_FETCH BIGINT unsigned not null,MAX_TIMER_FETCH BIGINT unsigned not null,COUNT_INSERT BIGINT unsigned not null,SUM_TIMER_INSERT BIGINT unsigned not null,MIN_TIMER_INSERT BIGINT unsigned not null,AVG_TIMER_INSERT BIGINT unsigned not null,MAX_TIMER_INSERT BIGINT unsigned not null,COUNT_UPDATE BIGINT unsigned not null,SUM_TIMER_UPDATE BIGINT unsigned not null,MIN_TIMER_UPDATE BIGINT unsigned not null,AVG_TIMER_UPDATE BIGINT unsigned not null,MAX_TIMER_UPDATE BIGINT unsigned not null,COUNT_DELETE BIGINT unsigned not null,SUM_TIMER_DELETE BIGINT unsigned not null,MIN_TIMER_DELETE BIGINT unsigned not null,AVG_TIMER_DELETE BIGINT unsigned not null,MAX_TIMER_DELETE BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_io_waits_summary_by_table(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_FETCH BIGINT unsigned not null,SUM_TIMER_FETCH BIGINT unsigned not null,MIN_TIMER_FETCH BIGINT unsigned not null,AVG_TIMER_FETCH BIGINT unsigned not null,MAX_TIMER_FETCH BIGINT unsigned not null,COUNT_INSERT BIGINT unsigned not null,SUM_TIMER_INSERT BIGINT unsigned not null,MIN_TIMER_INSERT BIGINT unsigned not null,AVG_TIMER_INSERT BIGINT unsigned not null,MAX_TIMER_INSERT BIGINT unsigned not null,COUNT_UPDATE BIGINT unsigned not null,SUM_TIMER_UPDATE BIGINT unsigned not null,MIN_TIMER_UPDATE BIGINT unsigned not null,AVG_TIMER_UPDATE BIGINT unsigned not null,MAX_TIMER_UPDATE BIGINT unsigned not null,COUNT_DELETE BIGINT unsigned not null,SUM_TIMER_DELETE BIGINT unsigned not null,MIN_TIMER_DELETE BIGINT unsigned not null,AVG_TIMER_DELETE BIGINT unsigned not null,MAX_TIMER_DELETE BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.table_lock_waits_summary_by_table(OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ BIGINT unsigned not null,SUM_TIMER_READ BIGINT unsigned not null,MIN_TIMER_READ BIGINT unsigned not null,AVG_TIMER_READ BIGINT unsigned not null,MAX_TIMER_READ BIGINT unsigned not null,COUNT_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE BIGINT unsigned not null,COUNT_READ_NORMAL BIGINT unsigned not null,SUM_TIMER_READ_NORMAL BIGINT unsigned not null,MIN_TIMER_READ_NORMAL BIGINT unsigned not null,AVG_TIMER_READ_NORMAL BIGINT unsigned not null,MAX_TIMER_READ_NORMAL BIGINT unsigned not null,COUNT_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,SUM_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,MIN_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,AVG_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,MAX_TIMER_READ_WITH_SHARED_LOCKS BIGINT unsigned not null,COUNT_READ_HIGH_PRIORITY BIGINT unsigned not null,SUM_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,MIN_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,AVG_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,MAX_TIMER_READ_HIGH_PRIORITY BIGINT unsigned not null,COUNT_READ_NO_INSERT BIGINT unsigned not null,SUM_TIMER_READ_NO_INSERT BIGINT unsigned not null,MIN_TIMER_READ_NO_INSERT BIGINT unsigned not null,AVG_TIMER_READ_NO_INSERT BIGINT unsigned not null,MAX_TIMER_READ_NO_INSERT BIGINT unsigned not null,COUNT_READ_EXTERNAL BIGINT unsigned not null,SUM_TIMER_READ_EXTERNAL BIGINT unsigned not null,MIN_TIMER_READ_EXTERNAL BIGINT unsigned not null,AVG_TIMER_READ_EXTERNAL BIGINT unsigned not null,MAX_TIMER_READ_EXTERNAL BIGINT unsigned not null,COUNT_WRITE_ALLOW_WRITE BIGINT unsigned not null,SUM_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,MIN_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,AVG_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,MAX_TIMER_WRITE_ALLOW_WRITE BIGINT unsigned not null,COUNT_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,SUM_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,MIN_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,AVG_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,MAX_TIMER_WRITE_CONCURRENT_INSERT BIGINT unsigned not null,COUNT_WRITE_LOW_PRIORITY BIGINT unsigned not null,SUM_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,MIN_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,AVG_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,MAX_TIMER_WRITE_LOW_PRIORITY BIGINT unsigned not null,COUNT_WRITE_NORMAL BIGINT unsigned not null,SUM_TIMER_WRITE_NORMAL BIGINT unsigned not null,MIN_TIMER_WRITE_NORMAL BIGINT unsigned not null,AVG_TIMER_WRITE_NORMAL BIGINT unsigned not null,MAX_TIMER_WRITE_NORMAL BIGINT unsigned not null,COUNT_WRITE_EXTERNAL BIGINT unsigned not null,SUM_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,MIN_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,AVG_TIMER_WRITE_EXTERNAL BIGINT unsigned not null,MAX_TIMER_WRITE_EXTERNAL BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.threads(THREAD_ID BIGINT unsigned not null,NAME VARCHAR(128) not null,TYPE VARCHAR(10) not null,PROCESSLIST_ID BIGINT unsigned,PROCESSLIST_USER VARCHAR(32),PROCESSLIST_HOST VARCHAR(60),PROCESSLIST_DB VARCHAR(64),PROCESSLIST_COMMAND VARCHAR(16),PROCESSLIST_TIME BIGINT,PROCESSLIST_STATE VARCHAR(64),PROCESSLIST_INFO LONGTEXT,PARENT_THREAD_ID BIGINT unsigned,ROLE VARCHAR(64),INSTRUMENTED ENUM ('YES', 'NO') not null,HISTORY ENUM ('YES', 'NO') not null,CONNECTION_TYPE VARCHAR(16),THREAD_OS_ID BIGINT unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,WORK_COMPLETED BIGINT unsigned,WORK_ESTIMATED BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,WORK_COMPLETED BIGINT unsigned,WORK_ESTIMATED BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,WORK_COMPLETED BIGINT unsigned,WORK_ESTIMATED BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_stages_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),NESTING_EVENT_LEVEL INTEGER)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),NESTING_EVENT_LEVEL INTEGER)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,LOCK_TIME bigint unsigned not null,SQL_TEXT LONGTEXT,DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,CURRENT_SCHEMA VARCHAR(64),OBJECT_TYPE VARCHAR(64),OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned,MYSQL_ERRNO INTEGER,RETURNED_SQLSTATE VARCHAR(5),MESSAGE_TEXT VARCHAR(128),ERRORS BIGINT unsigned not null,WARNINGS BIGINT unsigned not null,ROWS_AFFECTED BIGINT unsigned not null,ROWS_SENT BIGINT unsigned not null,ROWS_EXAMINED BIGINT unsigned not null,CREATED_TMP_DISK_TABLES BIGINT unsigned not null,CREATED_TMP_TABLES BIGINT unsigned not null,SELECT_FULL_JOIN BIGINT unsigned not null,SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SELECT_RANGE BIGINT unsigned not null,SELECT_RANGE_CHECK BIGINT unsigned not null,SELECT_SCAN BIGINT unsigned not null,SORT_MERGE_PASSES BIGINT unsigned not null,SORT_RANGE BIGINT unsigned not null,SORT_ROWS BIGINT unsigned not null,SORT_SCAN BIGINT unsigned not null,NO_INDEX_USED BIGINT unsigned not null,NO_GOOD_INDEX_USED BIGINT unsigned not null,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'),NESTING_EVENT_LEVEL INTEGER)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_current(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,STATE ENUM('ACTIVE', 'COMMITTED', 'ROLLED BACK'),TRX_ID BIGINT unsigned,GTID VARCHAR(64),XID_FORMAT_ID INTEGER,XID_GTRID VARCHAR(130),XID_BQUAL VARCHAR(130),XA_STATE VARCHAR(64),SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,ACCESS_MODE ENUM('READ ONLY', 'READ WRITE'),ISOLATION_LEVEL VARCHAR(64),AUTOCOMMIT ENUM('YES','NO') not null,NUMBER_OF_SAVEPOINTS BIGINT unsigned,NUMBER_OF_ROLLBACK_TO_SAVEPOINT BIGINT unsigned,NUMBER_OF_RELEASE_SAVEPOINT BIGINT unsigned,OBJECT_INSTANCE_BEGIN BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_history(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,STATE ENUM('ACTIVE', 'COMMITTED', 'ROLLED BACK'),TRX_ID BIGINT unsigned,GTID VARCHAR(64),XID_FORMAT_ID INTEGER,XID_GTRID VARCHAR(130),XID_BQUAL VARCHAR(130),XA_STATE VARCHAR(64),SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,ACCESS_MODE ENUM('READ ONLY', 'READ WRITE'),ISOLATION_LEVEL VARCHAR(64),AUTOCOMMIT ENUM('YES','NO') not null,NUMBER_OF_SAVEPOINTS BIGINT unsigned,NUMBER_OF_ROLLBACK_TO_SAVEPOINT BIGINT unsigned,NUMBER_OF_RELEASE_SAVEPOINT BIGINT unsigned,OBJECT_INSTANCE_BEGIN BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_history_long(THREAD_ID BIGINT unsigned not null,EVENT_ID BIGINT unsigned not null,END_EVENT_ID BIGINT unsigned,EVENT_NAME VARCHAR(128) not null,STATE ENUM('ACTIVE', 'COMMITTED', 'ROLLED BACK'),TRX_ID BIGINT unsigned,GTID VARCHAR(64),XID_FORMAT_ID INTEGER,XID_GTRID VARCHAR(130),XID_BQUAL VARCHAR(130),XA_STATE VARCHAR(64),SOURCE VARCHAR(64),TIMER_START BIGINT unsigned,TIMER_END BIGINT unsigned,TIMER_WAIT BIGINT unsigned,ACCESS_MODE ENUM('READ ONLY', 'READ WRITE'),ISOLATION_LEVEL VARCHAR(64),AUTOCOMMIT ENUM('YES','NO') not null,NUMBER_OF_SAVEPOINTS BIGINT unsigned,NUMBER_OF_ROLLBACK_TO_SAVEPOINT BIGINT unsigned,NUMBER_OF_RELEASE_SAVEPOINT BIGINT unsigned,OBJECT_INSTANCE_BEGIN BIGINT unsigned,NESTING_EVENT_ID BIGINT unsigned,NESTING_EVENT_TYPE ENUM('TRANSACTION', 'STATEMENT', 'STAGE', 'WAIT'))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_transactions_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,COUNT_READ_WRITE BIGINT unsigned not null,SUM_TIMER_READ_WRITE BIGINT unsigned not null,MIN_TIMER_READ_WRITE BIGINT unsigned not null,AVG_TIMER_READ_WRITE BIGINT unsigned not null,MAX_TIMER_READ_WRITE BIGINT unsigned not null,COUNT_READ_ONLY BIGINT unsigned not null,SUM_TIMER_READ_ONLY BIGINT unsigned not null,MIN_TIMER_READ_ONLY BIGINT unsigned not null,AVG_TIMER_READ_ONLY BIGINT unsigned not null,MAX_TIMER_READ_ONLY BIGINT unsigned not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.hosts(HOST CHAR(60) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.users(USER CHAR(32) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.accounts(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,CURRENT_CONNECTIONS bigint not null,TOTAL_CONNECTIONS bigint not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_global_by_event_name(EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_by_thread_by_event_name(THREAD_ID BIGINT unsigned not null,EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_by_account_by_event_name(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_by_host_by_event_name(HOST CHAR(60) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.memory_summary_by_user_by_event_name(USER CHAR(32) collate utf8_bin default null,EVENT_NAME VARCHAR(128) not null,COUNT_ALLOC BIGINT unsigned not null,COUNT_FREE BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_ALLOC BIGINT unsigned not null,SUM_NUMBER_OF_BYTES_FREE BIGINT unsigned not null,LOW_COUNT_USED BIGINT not null,CURRENT_COUNT_USED BIGINT not null,HIGH_COUNT_USED BIGINT not null,LOW_NUMBER_OF_BYTES_USED BIGINT not null,CURRENT_NUMBER_OF_BYTES_USED BIGINT not null,HIGH_NUMBER_OF_BYTES_USED BIGINT not null)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_digest(SCHEMA_NAME VARCHAR(64),DIGEST VARCHAR(32),DIGEST_TEXT LONGTEXT,COUNT_STAR BIGINT unsigned not null,SUM_TIMER_WAIT BIGINT unsigned not null,MIN_TIMER_WAIT BIGINT unsigned not null,AVG_TIMER_WAIT BIGINT unsigned not null,MAX_TIMER_WAIT BIGINT unsigned not null,SUM_LOCK_TIME BIGINT unsigned not null,SUM_ERRORS BIGINT unsigned not null,SUM_WARNINGS BIGINT unsigned not null,SUM_ROWS_AFFECTED BIGINT unsigned not null,SUM_ROWS_SENT BIGINT unsigned not null,SUM_ROWS_EXAMINED BIGINT unsigned not null,SUM_CREATED_TMP_DISK_TABLES BIGINT unsigned not null,SUM_CREATED_TMP_TABLES BIGINT unsigned not null,SUM_SELECT_FULL_JOIN BIGINT unsigned not null,SUM_SELECT_FULL_RANGE_JOIN BIGINT unsigned not null,SUM_SELECT_RANGE BIGINT unsigned not null,SUM_SELECT_RANGE_CHECK BIGINT unsigned not null,SUM_SELECT_SCAN BIGINT unsigned not null,SUM_SORT_MERGE_PASSES BIGINT unsigned not null,SUM_SORT_RANGE BIGINT unsigned not null,SUM_SORT_ROWS BIGINT unsigned not null,SUM_SORT_SCAN BIGINT unsigned not null,SUM_NO_INDEX_USED BIGINT unsigned not null,SUM_NO_GOOD_INDEX_USED BIGINT unsigned not null,FIRST_SEEN TIMESTAMP(0) NOT NULL default 0,LAST_SEEN TIMESTAMP(0) NOT NULL default 0)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.events_statements_summary_by_program(OBJECT_TYPE enum('EVENT', 'FUNCTION', 'PROCEDURE', 'TABLE', 'TRIGGER'),OBJECT_SCHEMA varchar(64) NOT NULL,OBJECT_NAME varchar(64) NOT NULL,COUNT_STAR bigint(20) unsigned NOT NULL,SUM_TIMER_WAIT bigint(20) unsigned NOT NULL,MIN_TIMER_WAIT bigint(20) unsigned NOT NULL,AVG_TIMER_WAIT bigint(20) unsigned NOT NULL,MAX_TIMER_WAIT bigint(20) unsigned NOT NULL,COUNT_STATEMENTS bigint(20) unsigned NOT NULL,SUM_STATEMENTS_WAIT bigint(20) unsigned NOT NULL,MIN_STATEMENTS_WAIT bigint(20) unsigned NOT NULL,AVG_STATEMENTS_WAIT bigint(20) unsigned NOT NULL,MAX_STATEMENTS_WAIT bigint(20) unsigned NOT NULL,SUM_LOCK_TIME bigint(20) unsigned NOT NULL,SUM_ERRORS bigint(20) unsigned NOT NULL,SUM_WARNINGS bigint(20) unsigned NOT NULL,SUM_ROWS_AFFECTED bigint(20) unsigned NOT NULL,SUM_ROWS_SENT bigint(20) unsigned NOT NULL,SUM_ROWS_EXAMINED bigint(20) unsigned NOT NULL,SUM_CREATED_TMP_DISK_TABLES bigint(20) unsigned NOT NULL,SUM_CREATED_TMP_TABLES bigint(20) unsigned NOT NULL,SUM_SELECT_FULL_JOIN bigint(20) unsigned NOT NULL,SUM_SELECT_FULL_RANGE_JOIN bigint(20) unsigned NOT NULL,SUM_SELECT_RANGE bigint(20) unsigned NOT NULL,SUM_SELECT_RANGE_CHECK bigint(20) unsigned NOT NULL,SUM_SELECT_SCAN bigint(20) unsigned NOT NULL,SUM_SORT_MERGE_PASSES bigint(20) unsigned NOT NULL,SUM_SORT_RANGE bigint(20) unsigned NOT NULL,SUM_SORT_ROWS bigint(20) unsigned NOT NULL,SUM_SORT_SCAN bigint(20) unsigned NOT NULL,SUM_NO_INDEX_USED bigint(20) unsigned NOT NULL,SUM_NO_GOOD_INDEX_USED bigint(20) unsigned NOT NULL)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.prepared_statements_instances(OBJECT_INSTANCE_BEGIN bigint(20) unsigned NOT NULL,STATEMENT_ID bigint(20) unsigned NOT NULL,STATEMENT_NAME varchar(64) default NULL,SQL_TEXT longtext NOT NULL,OWNER_THREAD_ID bigint(20) unsigned NOT NULL,OWNER_EVENT_ID bigint(20) unsigned NOT NULL,OWNER_OBJECT_TYPE enum('EVENT','FUNCTION','PROCEDURE','TABLE','TRIGGER') DEFAULT NULL,OWNER_OBJECT_SCHEMA varchar(64) DEFAULT NULL,OWNER_OBJECT_NAME varchar(64) DEFAULT NULL,TIMER_PREPARE bigint(20) unsigned NOT NULL,COUNT_REPREPARE bigint(20) unsigned NOT NULL,COUNT_EXECUTE bigint(20) unsigned NOT NULL,SUM_TIMER_EXECUTE bigint(20) unsigned NOT NULL,MIN_TIMER_EXECUTE bigint(20) unsigned NOT NULL,AVG_TIMER_EXECUTE bigint(20) unsigned NOT NULL,MAX_TIMER_EXECUTE bigint(20) unsigned NOT NULL,SUM_LOCK_TIME bigint(20) unsigned NOT NULL,SUM_ERRORS bigint(20) unsigned NOT NULL,SUM_WARNINGS bigint(20) unsigned NOT NULL,SUM_ROWS_AFFECTED bigint(20) unsigned NOT NULL,SUM_ROWS_SENT bigint(20) unsigned NOT NULL,SUM_ROWS_EXAMINED bigint(20) unsigned NOT NULL,SUM_CREATED_TMP_DISK_TABLES bigint(20) unsigned NOT NULL,SUM_CREATED_TMP_TABLES bigint(20) unsigned NOT NULL,SUM_SELECT_FULL_JOIN bigint(20) unsigned NOT NULL,SUM_SELECT_FULL_RANGE_JOIN bigint(20) unsigned NOT NULL,SUM_SELECT_RANGE bigint(20) unsigned NOT NULL,SUM_SELECT_RANGE_CHECK bigint(20) unsigned NOT NULL,SUM_SELECT_SCAN bigint(20) unsigned NOT NULL,SUM_SORT_MERGE_PASSES bigint(20) unsigned NOT NULL,SUM_SORT_RANGE bigint(20) unsigned NOT NULL,SUM_SORT_ROWS bigint(20) unsigned NOT NULL,SUM_SORT_SCAN bigint(20) unsigned NOT NULL,SUM_NO_INDEX_USED bigint(20) unsigned NOT NULL,SUM_NO_GOOD_INDEX_USED bigint(20) unsigned NOT NULL)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_connection_configuration(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,HOST CHAR(60) collate utf8_bin not null,PORT INTEGER not null,USER CHAR(32) collate utf8_bin not null,NETWORK_INTERFACE CHAR(60) collate utf8_bin not null,AUTO_POSITION ENUM('1','0') not null,SSL_ALLOWED ENUM('YES','NO','IGNORED') not null,SSL_CA_FILE VARCHAR(512) not null,SSL_CA_PATH VARCHAR(512) not null,SSL_CERTIFICATE VARCHAR(512) not null,SSL_CIPHER VARCHAR(512) not null,SSL_KEY VARCHAR(512) not null,SSL_VERIFY_SERVER_CERTIFICATE ENUM('YES','NO') not null,SSL_CRL_FILE VARCHAR(255) not null,SSL_CRL_PATH VARCHAR(255) not null,CONNECTION_RETRY_INTERVAL INTEGER not null,CONNECTION_RETRY_COUNT BIGINT unsigned not null,HEARTBEAT_INTERVAL DOUBLE(10,3) unsigned not null COMMENT 'Number of seconds after which a heartbeat will be sent .',TLS_VERSION VARCHAR(255) not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_group_member_stats(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,VIEW_ID CHAR(60) collate utf8_bin not null,MEMBER_ID CHAR(36) collate utf8_bin not null,COUNT_TRANSACTIONS_IN_QUEUE BIGINT unsigned not null,COUNT_TRANSACTIONS_CHECKED BIGINT unsigned not null,COUNT_CONFLICTS_DETECTED BIGINT unsigned not null,COUNT_TRANSACTIONS_VALIDATING BIGINT unsigned not null,TRANSACTIONS_COMMITTED_ALL_MEMBERS LONGTEXT not null,LAST_CONFLICT_FREE_TRANSACTION TEXT not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_group_members(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,MEMBER_ID CHAR(36) collate utf8_bin not null,MEMBER_HOST CHAR(60) collate utf8_bin not null,MEMBER_PORT INTEGER,MEMBER_STATE CHAR(64) collate utf8_bin not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_connection_status(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,GROUP_NAME CHAR(36) collate utf8_bin not null,SOURCE_UUID CHAR(36) collate utf8_bin not null,THREAD_ID BIGINT unsigned,SERVICE_STATE ENUM('ON','OFF','CONNECTING') not null,COUNT_RECEIVED_HEARTBEATS bigint unsigned NOT NULL DEFAULT 0,LAST_HEARTBEAT_TIMESTAMP TIMESTAMP(0) not null COMMENT 'Shows when the most recent heartbeat signal was received.',RECEIVED_TRANSACTION_SET TEXT not null,LAST_ERROR_NUMBER INTEGER not null,LAST_ERROR_MESSAGE VARCHAR(1024) not null,LAST_ERROR_TIMESTAMP TIMESTAMP(0) not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_applier_configuration(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,DESIRED_DELAY INTEGER not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_applier_status(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,SERVICE_STATE ENUM('ON','OFF') not null,REMAINING_DELAY INTEGER unsigned,COUNT_TRANSACTIONS_RETRIES BIGINT unsigned not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_applier_status_by_coordinator(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,THREAD_ID BIGINT UNSIGNED,SERVICE_STATE ENUM('ON','OFF') not null,LAST_ERROR_NUMBER INTEGER not null,LAST_ERROR_MESSAGE VARCHAR(1024) not null,LAST_ERROR_TIMESTAMP TIMESTAMP(0) not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.replication_applier_status_by_worker(CHANNEL_NAME CHAR(64) collate utf8_general_ci not null,WORKER_ID BIGINT UNSIGNED not null,THREAD_ID BIGINT UNSIGNED,SERVICE_STATE ENUM('ON','OFF') not null,LAST_SEEN_TRANSACTION CHAR(57) not null,LAST_ERROR_NUMBER INTEGER not null,LAST_ERROR_MESSAGE VARCHAR(1024) not null,LAST_ERROR_TIMESTAMP TIMESTAMP(0) not null) ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.session_connect_attrs(PROCESSLIST_ID INT NOT NULL,ATTR_NAME VARCHAR(32) NOT NULL,ATTR_VALUE VARCHAR(1024),ORDINAL_POSITION INT)ENGINE=PERFORMANCE_SCHEMA CHARACTER SET utf8 COLLATE utf8_bin
+CREATE TABLE performance_schema.session_account_connect_attrs  LIKE performance_schema.session_connect_attrs
+CREATE TABLE performance_schema.table_handles(OBJECT_TYPE VARCHAR(64) not null,OBJECT_SCHEMA VARCHAR(64) not null,OBJECT_NAME VARCHAR(64) not null,OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,OWNER_THREAD_ID BIGINT unsigned,OWNER_EVENT_ID BIGINT unsigned,INTERNAL_LOCK VARCHAR(64),EXTERNAL_LOCK VARCHAR(64))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.metadata_locks(OBJECT_TYPE VARCHAR(64) not null,OBJECT_SCHEMA VARCHAR(64),OBJECT_NAME VARCHAR(64),OBJECT_INSTANCE_BEGIN BIGINT unsigned not null,LOCK_TYPE VARCHAR(32) not null,LOCK_DURATION VARCHAR(32) not null,LOCK_STATUS VARCHAR(32) not null,SOURCE VARCHAR(64),OWNER_THREAD_ID BIGINT unsigned,OWNER_EVENT_ID BIGINT unsigned)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.user_variables_by_thread(THREAD_ID BIGINT unsigned not null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE LONGBLOB)ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.variables_by_thread(THREAD_ID BIGINT unsigned not null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.global_variables(VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.session_variables(VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.status_by_thread(THREAD_ID BIGINT unsigned not null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.status_by_user(USER CHAR(32) collate utf8_bin default null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.status_by_host(HOST CHAR(60) collate utf8_bin default null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.status_by_account(USER CHAR(32) collate utf8_bin default null,HOST CHAR(60) collate utf8_bin default null,VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.global_status(VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE performance_schema.session_status(VARIABLE_NAME VARCHAR(64) not null,VARIABLE_VALUE VARCHAR(1024))ENGINE=PERFORMANCE_SCHEMA
+CREATE TABLE IF NOT EXISTS proxies_priv (Host char(60) binary DEFAULT '' NOT NULL, User char(32) binary DEFAULT '' NOT NULL, Proxied_host char(60) binary DEFAULT '' NOT NULL, Proxied_user char(32) binary DEFAULT '' NOT NULL, With_grant BOOL DEFAULT 0 NOT NULL, Grantor char(77) DEFAULT '' NOT NULL, Timestamp timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY Host (Host,User,Proxied_host,Proxied_user), KEY Grantor (Grantor) ) engine=MyISAM CHARACTER SET utf8 COLLATE utf8_bin comment='User proxy privileges';
+
+FLUSH PRIVILEGES
+CREATE USER 'root'@'localhost' IDENTIFIED WITH 'mysql_native_password'
+GRANT ALL PRIVILEGES ON *.* TO 'root'@'localhost' WITH GRANT OPTION
+GRANT USAGE ON *.* TO ''@'','root'@'localhost' WITH GRANT OPTION
+TRUNCATE TABLE time_zone
+TRUNCATE TABLE time_zone_name
+TRUNCATE TABLE time_zone_transition
+TRUNCATE TABLE time_zone_transition_type
+CREATE DATABASE IF NOT EXISTS `mysql`
+CREATE USER 'mysqluser'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*FBC02A898D66B9181D6F8826C045C11FD2B364A4'
+GRANT ALL PRIVILEGES ON `mysql`.* TO 'mysqluser'@'%'
+FLUSH PRIVILEGES
+GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replicator'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*D98280F03D0F78162EBDBB9C883FC01395DEA2BF'
+CREATE DATABASE connector_test
+GRANT ALL PRIVILEGES ON `connector_test`.* TO 'mysqluser'@'%'
+CREATE TABLE products (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  name VARCHAR(255) NOT NULL,
+  description VARCHAR(512),
+  weight FLOAT
+)
+ALTER TABLE products AUTO_INCREMENT = 101
+CREATE TABLE products_on_hand (
+  product_id INTEGER NOT NULL PRIMARY KEY,
+  quantity INTEGER NOT NULL,
+  FOREIGN KEY (product_id) REFERENCES products(id)
+)
+CREATE TABLE customers (
+  id INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  first_name VARCHAR(255) NOT NULL,
+  last_name VARCHAR(255) NOT NULL,
+  email VARCHAR(255) NOT NULL UNIQUE KEY
+) AUTO_INCREMENT=1001
+CREATE TABLE orders (
+  order_number INTEGER NOT NULL AUTO_INCREMENT PRIMARY KEY,
+  order_date DATE NOT NULL,
+  purchaser INTEGER NOT NULL,
+  quantity INTEGER NOT NULL,
+  product_id INTEGER NOT NULL,
+  FOREIGN KEY order_customer (purchaser) REFERENCES customers(id),
+  FOREIGN KEY ordered_product (product_id) REFERENCES products(id)
+) AUTO_INCREMENT = 10001
+CREATE DATABASE emptydb
+GRANT ALL PRIVILEGES ON `emptydb`.* TO 'mysqluser'@'%'
+GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'replicator'@'%' IDENTIFIED WITH 'mysql_native_password' AS '*D98280F03D0F78162EBDBB9C883FC01395DEA2BF'
+CREATE DATABASE readbinlog_test
+GRANT ALL PRIVILEGES ON `readbinlog_test`.* TO 'mysqluser'@'%'
+DROP TABLE IF EXISTS `person` /* generated by server */
+CREATE TABLE person (  name VARCHAR(255) primary key,  age INTEGER NULL DEFAULT 10,  createdAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP,  updatedAt DATETIME NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP)",2016-05-13T14:32:47Z,138
"@@ -148,6 +148,12 @@ public void start(Map<String, String> props) {
         maxBatchSize = config.getInteger(MySqlConnectorConfig.MAX_BATCH_SIZE);
         metronome = Metronome.parker(pollIntervalMs, TimeUnit.MILLISECONDS, Clock.SYSTEM);
 
+        // Define the filter used for database names ...
+        Predicate<String> dbFilter = Selectors.databaseSelector()
+                                              .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
+                                              .excludeDatabases(config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST))
+                                              .build();
+
         // Define the filter using the whitelists and blacklists for tables and database names ...
         Predicate<TableId> tableFilter = Selectors.tableSelector()
                                                   .includeDatabases(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST))
@@ -178,7 +184,7 @@ public void start(Map<String, String> props) {
         // Set up our handlers for specific kinds of events ...
         tables = new Tables();
         tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, clock,
-                                              tables, tableFilter, columnFilter, columnMappers);
+                                              dbFilter, tables, tableFilter, columnFilter, columnMappers);
         eventHandlers.put(EventType.ROTATE, tableConverters::rotateLogs);
         eventHandlers.put(EventType.TABLE_MAP, tableConverters::updateTableMetadata);
         eventHandlers.put(EventType.QUERY, tableConverters::updateTableCommand);",2016-05-19T21:54:22Z,68
"@@ -143,7 +143,7 @@ public Struct struct() {
         setRowInEvent(eventRowNumber);
         return offset();
     }
-
+    
     /**
      * Set the name of the MySQL binary log file.
      * ",2016-05-19T21:54:22Z,70
"@@ -18,6 +18,7 @@
 import java.util.function.Predicate;
 
 import org.apache.kafka.connect.data.Schema;
+import org.apache.kafka.connect.data.SchemaBuilder;
 import org.apache.kafka.connect.data.Struct;
 import org.apache.kafka.connect.source.SourceRecord;
 import org.slf4j.Logger;
@@ -40,7 +41,7 @@
 import io.debezium.relational.TableSchemaBuilder;
 import io.debezium.relational.Tables;
 import io.debezium.relational.history.DatabaseHistory;
-import io.debezium.relational.history.HistoryRecord;
+import io.debezium.relational.history.HistoryRecord.Fields;
 import io.debezium.relational.mapping.ColumnMappers;
 import io.debezium.text.ParsingException;
 import io.debezium.util.Clock;
@@ -53,6 +54,32 @@
 @NotThreadSafe
 final class TableConverters {
 
+    protected static final Schema SCHEMA_CHANGE_RECORD_KEY_SCHEMA = SchemaBuilder.struct()
+                                                                                 .name(""io.debezium.connector.mysql.SchemaRecordKey"")
+                                                                                 .field(Fields.DATABASE_NAME, Schema.STRING_SCHEMA)
+                                                                                 .build();
+
+    protected static final Schema SCHEMA_CHANGE_RECORD_VALUE_SCHEMA = SchemaBuilder.struct()
+                                                                                   .name(""io.debezium.connector.mysql.SchemaRecordKey"")
+                                                                                   .field(Fields.SOURCE, SourceInfo.SCHEMA)
+                                                                                   .field(Fields.DATABASE_NAME, Schema.STRING_SCHEMA)
+                                                                                   .field(Fields.DDL_STATEMENTS, Schema.STRING_SCHEMA)
+                                                                                   .build();
+
+    public Struct schemaChangeRecordKey(String databaseName) {
+        Struct result = new Struct(SCHEMA_CHANGE_RECORD_KEY_SCHEMA);
+        result.put(Fields.DATABASE_NAME, databaseName);
+        return result;
+    }
+
+    public Struct schemaChangeRecordValue(SourceInfo source, String databaseName, String ddlStatements) {
+        Struct result = new Struct(SCHEMA_CHANGE_RECORD_VALUE_SCHEMA);
+        result.put(Fields.SOURCE, source.struct());
+        result.put(Fields.DATABASE_NAME, databaseName);
+        result.put(Fields.DDL_STATEMENTS, ddlStatements);
+        return result;
+    }
+
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final DatabaseHistory dbHistory;
     private final TopicSelector topicSelector;
@@ -63,6 +90,7 @@ final class TableConverters {
     private final Map<Long, Converter> convertersByTableId = new HashMap<>();
     private final Map<String, Long> tableNumbersByTableName = new HashMap<>();
     private final boolean recordSchemaChangesInSourceRecords;
+    private final Predicate<String> dbFilter;
     private final Predicate<TableId> tableFilter;
     private final Predicate<ColumnId> columnFilter;
     private final ColumnMappers columnMappers;
@@ -71,15 +99,17 @@ final class TableConverters {
     private final Clock clock;
 
     public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
-            boolean recordSchemaChangesInSourceRecords, Clock clock, Tables tables,
+            boolean recordSchemaChangesInSourceRecords, Clock clock, Predicate<String> dbFilter, Tables tables,
             Predicate<TableId> tableFilter, Predicate<ColumnId> columnFilter, ColumnMappers columnSelectors) {
         Objects.requireNonNull(topicSelector, ""A topic selector is required"");
         Objects.requireNonNull(dbHistory, ""Database history storage is required"");
         Objects.requireNonNull(tables, ""A Tables object is required"");
         Objects.requireNonNull(clock, ""A Clock object is required"");
+        Objects.requireNonNull(dbFilter, ""A database filter object is required"");
         this.topicSelector = topicSelector;
         this.dbHistory = dbHistory;
         this.clock = clock;
+        this.dbFilter = dbFilter;
         this.tables = tables;
         this.columnFilter = columnFilter;
         this.columnMappers = columnSelectors;
@@ -124,12 +154,17 @@ public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRe
             // Record the DDL statement so that we can later recover them if needed ...
             dbHistory.record(source.partition(), source.offset(), databaseName, tables, ddlStatements);
 
-            if (recordSchemaChangesInSourceRecords) {
+            if (recordSchemaChangesInSourceRecords && dbFilter.test(databaseName)) {
                 String serverName = source.serverName();
                 String topicName = topicSelector.getTopic(serverName);
-                HistoryRecord historyRecord = new HistoryRecord(source.partition(), source.offset(), databaseName, ddlStatements);
-                recorder.accept(new SourceRecord(source.partition(), source.offset(), topicName, 0,
-                        Schema.STRING_SCHEMA, databaseName, Schema.STRING_SCHEMA, historyRecord.document().toString()));
+                Integer partition = 0;
+                Struct key = schemaChangeRecordKey(databaseName);
+                Struct value = schemaChangeRecordValue(source, databaseName, ddlStatements);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(),
+                        topicName, partition,
+                        SCHEMA_CHANGE_RECORD_KEY_SCHEMA, key,
+                        SCHEMA_CHANGE_RECORD_VALUE_SCHEMA, value);
+                recorder.accept(record);
             }
         }
 ",2016-05-19T21:54:22Z,109
"@@ -9,7 +9,6 @@
 
 import java.nio.file.Path;
 import java.sql.SQLException;
-import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.TimeUnit;
 
@@ -49,7 +48,7 @@ public void afterEach() {
         stopConnector();
         Testing.Files.delete(DB_HISTORY_PATH);
     }
-    
+
     /**
      * Verifies that the connector doesn't run with an invalid configuration. This does not actually connect to the MySQL server.
      */
@@ -70,15 +69,7 @@ public void shouldNotStartWithInvalidConfiguration() {
     }
 
     @Test
-    public void shouldStartAndPollShouldReturnSourceRecordsFromDatabase() throws SQLException {
-        try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
-            try (JdbcConnection connection = db.connect()) {
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
-                connection.execute(""INSERT INTO products VALUES (default,'robot','Toy robot',1.304);"");
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
-            }
-        }
-
+    public void shouldConsumeAllEventsFromDatabase() throws SQLException, InterruptedException {
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()
                               .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
@@ -90,70 +81,97 @@ public void shouldStartAndPollShouldReturnSourceRecordsFromDatabase() throws SQL
                               .with(MySqlConnectorConfig.INITIAL_BINLOG_FILENAME, ""mysql-bin.000001"")
                               .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test"")
                               .with(MySqlConnectorConfig.DATABASE_HISTORY, FileDatabaseHistory.class)
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, true)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
                               .build();
         // Start the connector ...
         start(MySqlConnector.class, config);
+        //waitForAvailableRecords(10, TimeUnit.SECONDS);
 
-        waitForAvailableRecords(10, TimeUnit.SECONDS);
-
+        // Consume the first records due to startup and initialization of the database ...
+        //Testing.Print.enable();
+        SourceRecords records = consumeRecordsByTopic(6+9+9+4+5);
+        assertThat(records.recordsForTopic(""kafka-connect"").size()).isEqualTo(6);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.products"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.products_on_hand"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.customers"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.orders"").size()).isEqualTo(5);
+        assertThat(records.topics().size()).isEqualTo(5);
+        assertThat(records.databaseNames().size()).isEqualTo(1);
+        assertThat(records.ddlRecordsForDatabase(""connector_test"").size()).isEqualTo(6);
+        assertThat(records.ddlRecordsForDatabase(""readbinlog_test"")).isNull();
+        
+        records.ddlRecordsForDatabase(""connector_test"").forEach(this::print);
+        //records.ddlRecordsForDatabase(""readbinlog_test"").forEach(this::print);
+        
+        // Make sure there are no more ...
+        Testing.Print.disable();
+        waitForAvailableRecords(3, TimeUnit.SECONDS);
+        int totalConsumed = consumeAvailableRecords(this::print);
+        assertThat(totalConsumed).isEqualTo(0);
+        stopConnector();
+        
+        // Make some changes to data only ...
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
-                connection.execute(""INSERT INTO products VALUES (default,'harrison','real robot',134.82);"");
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
+                connection.execute(""INSERT INTO products VALUES (default,'robot','Toy robot',1.304);"");
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
             }
         }
-        
-        //Testing.Print.enable();
-        int totalConsumed = consumeAvailableRecords(this::print);  // expecting at least 1
-        stopConnector();
-        
-        // Restart the connector and wait for a few seconds (at most) for records that will never arrive ...
+
+        // Restart the connector and read the insert record ...
         start(MySqlConnector.class, config);
-        waitForAvailableRecords(2, TimeUnit.SECONDS);
-        totalConsumed += consumeAvailableRecords(this::print);
-        stopConnector();
+        records = consumeRecordsByTopic(1);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.products"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(1);
         
         // Create an additional few records ...
-        Testing.Print.disable();
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
                 connection.execute(""INSERT INTO products VALUES (1001,'roy','old robot',1234.56);"");
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
             }
         }
-
-        // Restart the connector and wait for a few seconds (at most) for the new record ...
-        //Testing.Print.enable();
-        start(MySqlConnector.class, config);
-        waitForAvailableRecords(5, TimeUnit.SECONDS);
-        totalConsumed += consumeAvailableRecords(this::print);
-
+        
+        // And consume the one insert ...
+        records = consumeRecordsByTopic(1);
+        assertThat(records.recordsForTopic(""kafka-connect.connector_test.products"").size()).isEqualTo(1);
+        assertThat(records.topics().size()).isEqualTo(1);
+        List<SourceRecord> inserts = records.recordsForTopic(""kafka-connect.connector_test.products"");
+        assertInsert(inserts.get(0), ""id"", 1001);
+
+        // Update one of the records by changing its primary key ...
         try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
             try (JdbcConnection connection = db.connect()) {
                 connection.execute(""UPDATE products SET id=2001, description='really old robot' WHERE id=1001"");
-                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
+                connection.query(""SELECT * FROM products"", rs -> {
+                    if (Testing.Print.isEnabled()) connection.print(rs);
+                });
             }
         }
-        waitForAvailableRecords(5, TimeUnit.SECONDS);
-        List<SourceRecord> deletes = new ArrayList<>();
-        totalConsumed += consumeAvailableRecords(deletes::add);
+        // And consume the update of the PK, which is one insert followed by a delete followed by a tombstone ...
+        records = consumeRecordsByTopic(3);
+        List<SourceRecord> updates = records.recordsForTopic(""kafka-connect.connector_test.products"");
+        assertThat(updates.size()).isEqualTo(3);
+        assertInsert(updates.get(0), ""id"", 2001);
+        assertDelete(updates.get(1), ""id"", 1001);
+        assertTombstone(updates.get(2), ""id"", 1001);
+
+        // Stop the connector ...
         stopConnector();
-        
-        // Verify that the update of a record where the pk changes results in
-        // 1 update, 1 delete, and 1 tombstone event ...
-        assertThat(deletes.size()).isEqualTo(3);
-        assertInsert(deletes.get(0),""id"",2001);
-        assertDelete(deletes.get(1),""id"",1001);
-        assertTombstone(deletes.get(2),""id"",1001);
-
-        // We should have seen a total of 33 events, though when they appear may vary ...
-        assertThat(totalConsumed).isEqualTo(33);
     }
-    
+
     @Test
-    public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLException {
+    public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLException, InterruptedException {
         Testing.Files.delete(DB_HISTORY_PATH);
+        
         // Use the DB configuration to define the connector's configuration ...
         config = Configuration.create()
                               .with(MySqlConnectorConfig.HOSTNAME, System.getProperty(""database.hostname""))
@@ -167,42 +185,49 @@ public void shouldConsumeEventsWithMaskedAndBlacklistedColumns() throws SQLExcep
                               .with(MySqlConnectorConfig.DATABASE_WHITELIST, ""connector_test"")
                               .with(MySqlConnectorConfig.COLUMN_BLACKLIST, ""connector_test.orders.order_number"")
                               .with(MySqlConnectorConfig.MASK_COLUMN(12), ""connector_test.customers.email"")
+                              .with(MySqlConnectorConfig.INCLUDE_SCHEMA_CHANGES, false)
                               .with(FileDatabaseHistory.FILE_PATH, DB_HISTORY_PATH)
                               .build();
+
         // Start the connector ...
         start(MySqlConnector.class, config);
 
-        // Wait for records to become available ...
-        //Testing.Print.enable();
-        waitForAvailableRecords(15, TimeUnit.SECONDS);
+        // Consume the first records due to startup and initialization of the database ...
+        // Testing.Print.enable();
+        SourceRecords records = consumeRecordsByTopic(9+9+4+5);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.products"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.products_on_hand"").size()).isEqualTo(9);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.customers"").size()).isEqualTo(4);
+        assertThat(records.recordsForTopic(""kafka-connect-2.connector_test.orders"").size()).isEqualTo(5);
+        assertThat(records.topics().size()).isEqualTo(4);
         
-        // Now consume the records ...
-        int totalConsumed = consumeAvailableRecords((record)->{
+        // More records may have been written (if this method were run after the others), but we don't care ...
+        stopConnector();
+
+        // Check that the orders.order_number is not present ...
+        records.recordsForTopic(""kafka-connect-2.connector_test.orders"").forEach(record->{
             print(record);
-            if ( record.topic().endsWith("".orders"")) {
-                Struct value = (Struct) record.value();
-                try {
-                    value.get(""order_number"");
-                    fail(""The 'order_number' field was found but should not exist"");
-                } catch ( DataException e ) {
-                    // expected
-                    printJson(record);
-                }
-            } else if ( record.topic().endsWith("".customers"")) {
-                Struct value = (Struct) record.value();
-                if ( value.getStruct(""after"") != null ) {
-                    assertThat(value.getStruct(""after"").getString(""email"")).isEqualTo(""************"");
-                }
-                if ( value.getStruct(""before"") != null ) {
-                    assertThat(value.getStruct(""before"").getString(""email"")).isEqualTo(""************"");
-                }
+            Struct value = (Struct) record.value();
+            try {
+                value.get(""order_number"");
+                fail(""The 'order_number' field was found but should not exist"");
+            } catch (DataException e) {
+                // expected
                 printJson(record);
             }
         });
-        stopConnector();
-
-        // We should have seen a total of 27 events, though when they appear may vary ...
-        assertThat(totalConsumed).isEqualTo(27);
+        
+        // Check that the customer.email is masked ...
+        records.recordsForTopic(""kafka-connect-2.connector_test.customers"").forEach(record->{
+            Struct value = (Struct) record.value();
+            if (value.getStruct(""after"") != null) {
+                assertThat(value.getStruct(""after"").getString(""email"")).isEqualTo(""************"");
+            }
+            if (value.getStruct(""before"") != null) {
+                assertThat(value.getStruct(""before"").getString(""email"")).isEqualTo(""************"");
+            }
+            printJson(record);
+        });
     }
-    
+
 }",2016-05-19T21:54:22Z,71
"@@ -28,6 +28,68 @@
 @Immutable
 public class Selectors {
     
+    /**
+     * Obtain a new {@link TableSelectionPredicateBuilder builder} for a table selection predicate.
+     * 
+     * @return the builder; never null
+     */
+    public static DatabaseSelectionPredicateBuilder databaseSelector() {
+        return new DatabaseSelectionPredicateBuilder();
+    }
+
+    /**
+     * A builder of a database predicate.
+     */
+    public static class DatabaseSelectionPredicateBuilder {
+        private Predicate<String> dbInclusions;
+        private Predicate<String> dbExclusions;
+
+        /**
+         * Specify the names of the databases that should be included. This method will override previously included and
+         * {@link #excludeDatabases(String) excluded} databases.
+         * 
+         * @param databaseNames the comma-separated list of database names to include; may be null or empty
+         * @return this builder so that methods can be chained together; never null
+         */
+        public DatabaseSelectionPredicateBuilder includeDatabases(String databaseNames) {
+            if (databaseNames == null || databaseNames.trim().isEmpty()) {
+                dbInclusions = null;
+            } else {
+                dbInclusions = Predicates.includes(databaseNames);
+            }
+            return this;
+        }
+
+        /**
+         * Specify the names of the databases that should be excluded. This method will override previously {@link
+         * #excludeDatabases(String) excluded} databases, although {@link #includeDatabases(String) including databases} overrides
+         * exclusions.
+         * 
+         * @param databaseNames the comma-separated list of database names to exclude; may be null or empty
+         * @return this builder so that methods can be chained together; never null
+         */
+        public DatabaseSelectionPredicateBuilder excludeDatabases(String databaseNames) {
+            if (databaseNames == null || databaseNames.trim().isEmpty()) {
+                dbExclusions = null;
+            } else {
+                dbExclusions = Predicates.excludes(databaseNames);
+            }
+            return this;
+        }
+
+        /**
+         * Build the {@link Predicate} that determines whether a database identified by its name is to be included.
+         * 
+         * @return the table selection predicate; never null
+         * @see #includeDatabases(String)
+         * @see #excludeDatabases(String)
+         */
+        public Predicate<String> build() {
+            Predicate<String> dbFilter = dbInclusions != null ? dbInclusions : dbExclusions;
+            return dbFilter != null ? dbFilter : (id) -> true;
+        }
+    }
+
     /**
      * Obtain a new {@link TableSelectionPredicateBuilder builder} for a table selection predicate.
      * 
@@ -38,9 +100,7 @@ public static TableSelectionPredicateBuilder tableSelector() {
     }
 
     /**
-     * A builder of {@link Selectors}.
-     * 
-     * @author Randall Hauch
+     * A builder of a table predicate.
      */
     public static class TableSelectionPredicateBuilder {
         private Predicate<String> dbInclusions;",2016-05-19T21:54:22Z,139
"@@ -10,20 +10,28 @@
 import io.debezium.document.Document;
 
 public class HistoryRecord {
+
+    public static final class Fields {
+        public static final String SOURCE = ""source"";
+        public static final String POSITION = ""position"";
+        public static final String DATABASE_NAME = ""databaseName"";
+        public static final String DDL_STATEMENTS = ""ddl"";
+    }
+
     private final Document doc;
-    
+
     public HistoryRecord(Document document) {
         this.doc = document;
     }
 
     public HistoryRecord(Map<String, ?> source, Map<String, ?> position, String databaseName, String ddl) {
         this.doc = Document.create();
-        Document src = doc.setDocument(""source"");
+        Document src = doc.setDocument(Fields.SOURCE);
         if (source != null) source.forEach(src::set);
-        Document pos = doc.setDocument(""position"");
+        Document pos = doc.setDocument(Fields.POSITION);
         if (position != null) position.forEach(pos::set);
-        if (databaseName != null) doc.setString(""databaseName"", databaseName);
-        if (ddl != null) doc.setString(""ddl"", ddl);
+        if (databaseName != null) doc.setString(Fields.DATABASE_NAME, databaseName);
+        if (ddl != null) doc.setString(Fields.DDL_STATEMENTS, ddl);
     }
 
     public Document document() {
@@ -61,7 +69,7 @@ protected boolean hasSameDatabase(HistoryRecord other) {
         if (this == other) return true;
         return other != null && databaseName().equals(other.databaseName());
     }
-    
+
     @Override
     public String toString() {
         return doc.toString();",2016-05-19T21:54:22Z,140
"@@ -8,9 +8,12 @@
 import static org.junit.Assert.fail;
 
 import java.nio.file.Path;
+import java.util.ArrayList;
+import java.util.HashMap;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
+import java.util.Set;
 import java.util.concurrent.ArrayBlockingQueue;
 import java.util.concurrent.BlockingQueue;
 import java.util.concurrent.CountDownLatch;
@@ -199,7 +202,13 @@ protected void start(Class<? extends SourceConnector> connectorClass, Configurat
         // Create the connector ...
         engine = EmbeddedEngine.create()
                                .using(config)
-                               .notifying(consumedLines::add)
+                               .notifying((record)->{
+                                   try {
+                                       consumedLines.put(record);
+                                   } catch ( InterruptedException e ) {
+                                       Thread.interrupted();
+                                   }
+                                })
                                .using(this.getClass().getClassLoader())
                                .using(wrapperCallback)
                                .build();
@@ -255,7 +264,7 @@ protected int consumeRecords(int numberOfRecords) throws InterruptedException {
      */
     protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordConsumer) throws InterruptedException {
         int recordsConsumed = 0;
-        for (int i = 0; i != numberOfRecords; ++i) {
+        while ( recordsConsumed < numberOfRecords ) {
             SourceRecord record = consumedLines.poll(pollTimeoutInMs, TimeUnit.MILLISECONDS);
             if (record != null) {
                 ++recordsConsumed;
@@ -266,6 +275,60 @@ protected int consumeRecords(int numberOfRecords, Consumer<SourceRecord> recordC
         }
         return recordsConsumed;
     }
+    
+    protected SourceRecords consumeRecordsByTopic(int numRecords) throws InterruptedException {
+        return consumeRecordsByTopic(numRecords, new SourceRecords());
+    }
+    
+    protected SourceRecords consumeRecordsByTopic(int numRecords, SourceRecords records) throws InterruptedException {
+        consumeRecords(numRecords,records::add);
+        return records;
+    }
+    
+    protected class SourceRecords {
+        private final List<SourceRecord> records = new ArrayList<>();
+        private final Map<String,List<SourceRecord>> recordsByTopic = new HashMap<>();
+        private final Map<String,List<SourceRecord>> ddlRecordsByDbName = new HashMap<>();
+        public void add( SourceRecord record ) {
+            records.add(record);
+            recordsByTopic.compute(record.topic(), (topicName,list)->{
+                if ( list == null ) list = new ArrayList<SourceRecord>();
+                list.add(record);
+                return list;
+            });
+            if ( record.key() instanceof Struct ) {
+                Struct key = (Struct)record.key();
+                if ( key.schema().field(""databaseName"") != null) {
+                    String dbName = key.getString(""databaseName"");
+                    ddlRecordsByDbName.compute(dbName, (databaseName,list)->{
+                        if ( list == null ) list = new ArrayList<SourceRecord>();
+                        list.add(record);
+                        return list;
+                    });
+                }
+            }
+        }
+        public List<SourceRecord> ddlRecordsForDatabase( String dbName ) {
+            return ddlRecordsByDbName.get(dbName);
+        }
+        public Set<String> databaseNames() {
+            return ddlRecordsByDbName.keySet();
+        }
+        public List<SourceRecord> recordsForTopic( String topicName ) {
+            return recordsByTopic.get(topicName);
+        }
+        public Set<String> topics() {
+            return recordsByTopic.keySet();
+        }
+        public void print() {
+            Testing.print("""" + topics().size() + "" topics: "" + topics());
+            recordsByTopic.forEach((k,v)->{
+                Testing.print("" - topic:'"" + k + ""'; # of events = "" + v.size());
+            });
+            Testing.print(""Records:"" );
+            records.forEach(record->AbstractConnectorTest.this.print(record));
+        }
+    }
 
     /**
      * Try to consume all of the messages that have already been returned by the connector.
@@ -281,7 +344,7 @@ protected int consumeAvailableRecords(Consumer<SourceRecord> recordConsumer) {
         }
         return records.size();
     }
-
+    
     /**
      * Wait for a maximum amount of time until the first record is available.
      * ",2016-05-19T21:54:22Z,35
"@@ -969,7 +969,9 @@ protected void parseRenameTable(Marker start) {
         tokens.consume(""TO"");
         TableId to = parseQualifiedTableName(start);
         databaseTables.renameTable(from, to);
-        signalAlterTable(from, to, start);
+        // Signal a separate statement for this table rename action, even though multiple renames might be
+        // performed by a single DDL statement on the token stream ...
+        signalAlterTable(from,to,""RENAME TABLE "" + from + "" TO "" + to);
     }
 
     protected List<String> parseColumnNameList(Marker start) {",2016-04-12T22:58:07Z,49
"@@ -22,6 +22,7 @@
 import io.debezium.relational.TableId;
 import io.debezium.relational.Tables;
 import io.debezium.relational.ddl.DdlParser;
+import io.debezium.relational.ddl.DdlParserListener.Event;
 import io.debezium.relational.ddl.SimpleDdlParserListener;
 import io.debezium.util.IoUtil;
 import io.debezium.util.Testing;
@@ -159,6 +160,7 @@ public void shouldParseTestStatements() {
         Testing.print(tables);
         assertThat(tables.size()).isEqualTo(6); // no tables
         assertThat(listener.total()).isEqualTo(49);
+        // listener.forEach(this::printEvent);
     }
 
     @Test
@@ -167,6 +169,10 @@ public void shouldParseSomeLinesFromCreateStatements() {
         assertThat(tables.size()).isEqualTo(39); // no tables
         assertThat(listener.total()).isEqualTo(120);
     }
+    
+    protected void printEvent( Event event ) {
+        System.out.println(event);
+    }
 
     protected String readFile( String classpathResource ) {
         try ( InputStream stream = getClass().getClassLoader().getResourceAsStream(classpathResource); ) {",2016-04-12T22:58:07Z,26
"@@ -391,6 +391,17 @@ protected void signalAlterTable(TableId id, TableId previousId, Marker statement
         signalEvent(new TableAlteredEvent(id, previousId, statement(statementStart), false));
     }
 
+    /**
+     * Signal an alter table event to all listeners.
+     * 
+     * @param id the table identifier; may not be null
+     * @param previousId the previous name of the view if it was renamed, or null if it was not renamed
+     * @param statement the DDL statement; may not be null
+     */
+    protected void signalAlterTable(TableId id, TableId previousId, String statement) {
+        signalEvent(new TableAlteredEvent(id, previousId, statement, false));
+    }
+
     /**
      * Signal a drop table event to all listeners.
      * ",2016-04-12T22:58:07Z,65
"@@ -8,6 +8,7 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicLong;
+import java.util.function.Consumer;
 
 import static org.fest.assertions.Assertions.assertThat;
 
@@ -167,4 +168,12 @@ public EventAssert assertNext() {
         assertThat( events.isEmpty()).isFalse();
         return new EventAssert(events.remove(0));
     }
+
+    /**
+     * Perform an operation on each of the events.
+     * @param eventConsumer the event consumer function; may not be null
+     */
+    public void forEach( Consumer<Event> eventConsumer ) {
+        events.forEach(eventConsumer);
+    }
 }",2016-04-12T22:58:07Z,141
"@@ -17,6 +17,7 @@
 import java.util.concurrent.LinkedBlockingDeque;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.function.Consumer;
 import java.util.function.Predicate;
 
@@ -67,6 +68,8 @@ public final class MySqlConnectorTask extends SourceTask {
                                                                              ""slave_relay_log_info"", ""slave_master_info"",
                                                                              ""slave_worker_info"", ""gtid_executed"",
                                                                              ""server_cost"", ""engine_cost"");
+    private final Set<String> BUILT_IN_DB_NAMES = Collect.unmodifiableSet(""mysql"", ""performance_schema"");
+
     private final Logger logger = LoggerFactory.getLogger(getClass());
     private final TopicSelector topicSelector;
 
@@ -81,6 +84,7 @@ public final class MySqlConnectorTask extends SourceTask {
     private int maxBatchSize;
     private String serverName;
     private Metronome metronome;
+    private final AtomicBoolean running = new AtomicBoolean(false);
 
     // Used in the methods that process events ...
     private final SourceInfo source = new SourceInfo();
@@ -122,6 +126,7 @@ public void start(Map<String, String> props) {
                                                                                                                  // prefix
         this.dbHistory.configure(dbHistoryConfig); // validates
         this.dbHistory.start();
+        this.running.set(true);
 
         // Read the configuration ...
         final String user = config.getString(MySqlConnectorConfig.USER);
@@ -145,7 +150,9 @@ public void start(Map<String, String> props) {
                                                         config.getString(MySqlConnectorConfig.TABLE_WHITELIST),
                                                         config.getString(MySqlConnectorConfig.TABLE_BLACKLIST));
         if (config.getBoolean(MySqlConnectorConfig.TABLES_IGNORE_BUILTIN)) {
-            Predicate<TableId> ignoreBuiltins = (id) -> !BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase());
+            Predicate<TableId> ignoreBuiltins = (id) -> {
+                return !BUILT_IN_TABLE_NAMES.contains(id.table().toLowerCase()) && !BUILT_IN_DB_NAMES.contains(id.catalog().toLowerCase());
+            };
             tableFilter = ignoreBuiltins.or(tableFilter);
         }
 
@@ -197,7 +204,7 @@ public void start(Map<String, String> props) {
                 logger.info(""Recovering MySQL connector '{}' database schemas from history stored in {}"", serverName, dbHistory);
                 DdlParser ddlParser = new MySqlDdlParser();
                 dbHistory.recover(source.partition(), source.offset(), tables, ddlParser);
-                logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, tables);
+                logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, tables.subset(tableFilter));
             } catch (Throwable t) {
                 throw new ConnectException(""Failure while recovering database schemas"", t);
             }
@@ -229,7 +236,7 @@ public void start(Map<String, String> props) {
     @Override
     public List<SourceRecord> poll() throws InterruptedException {
         logger.trace(""Polling for events from MySQL server '{}'"", serverName);
-        while (events.drainTo(batchEvents, maxBatchSize - batchEvents.size()) == 0 || batchEvents.isEmpty()) {
+        while (running.get() && (events.drainTo(batchEvents, maxBatchSize - batchEvents.size()) == 0 || batchEvents.isEmpty())) {
             // No events to process, so sleep for a bit ...
             metronome.pause();
         }
@@ -263,6 +270,8 @@ public List<SourceRecord> poll() throws InterruptedException {
                     source.setRowInEvent(0);
                 }
             }
+            
+            if ( !running.get()) break;
 
             // If there is a handler for this event, forward the event to it ...
             EventHandler handler = eventHandlers.get(eventType);
@@ -272,6 +281,12 @@ public List<SourceRecord> poll() throws InterruptedException {
         }
         logger.trace(""Completed processing {} events from MySQL server '{}'"", serverName);
 
+        if (!this.running.get()) {
+            // We're supposed to stop, so return nothing that we might have already processed
+            // so that no records get persisted if DB history has already been stopped ...
+            return null;
+        }
+
         // We've processed them all, so clear the batch and return the records ...
         assert batchEvents.isEmpty();
         return records;
@@ -280,6 +295,10 @@ public List<SourceRecord> poll() throws InterruptedException {
     @Override
     public void stop() {
         try {
+            // Signal to the 'poll()' method that it should stop what its doing ...
+            this.running.set(false);
+
+            // Flush and stop the database history ...
             logger.debug(""Stopping database history for MySQL server '{}'"", serverName);
             dbHistory.stop();
         } catch (Throwable e) {",2016-03-03T21:27:11Z,68
"@@ -95,7 +95,32 @@ public void shouldStartAndPollShouldReturnSourceRecordsFromDatabase() throws SQL
         }
         
         Testing.Print.enable();
-        assertThat(consumeAvailableRecords(this::print)).isGreaterThan(0); // expecting at least 1
+        int totalConsumed = consumeAvailableRecords(this::print);  // expecting at least 1
         stopConnector();
+        
+        // Restart the connector and wait for a few seconds (at most) for records that will never arrive ...
+        start(MySqlConnector.class, config);
+        waitForAvailableRecords(2, TimeUnit.SECONDS);
+        totalConsumed += consumeAvailableRecords(this::print);
+        stopConnector();
+        
+        // Create an additional few records ...
+        Testing.Print.disable();
+        try (MySQLConnection db = MySQLConnection.forTestDatabase(""connector_test"");) {
+            try (JdbcConnection connection = db.connect()) {
+                connection.execute(""INSERT INTO products VALUES (default,'roy','old robot',1234.56);"");
+                connection.query(""SELECT * FROM products"", rs->{if (Testing.Print.isEnabled()) connection.print(rs);});
+            }
+        }
+
+        // Restart the connector and wait for a few seconds (at most) for the new record ...
+        Testing.Print.enable();
+        start(MySqlConnector.class, config);
+        waitForAvailableRecords(5, TimeUnit.SECONDS);
+        totalConsumed += consumeAvailableRecords(this::print);
+        stopConnector();
+
+        // We should have seen a total of 30 events, though when they appear may vary ...
+        assertThat(totalConsumed).isEqualTo(30);
     }
 }",2016-03-03T21:27:11Z,71
"@@ -12,6 +12,7 @@
 import java.util.Set;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.function.Function;
+import java.util.function.Predicate;
 
 import org.apache.kafka.connect.data.Schema;
 
@@ -314,6 +315,19 @@ public boolean equals(Object obj) {
         return false;
     }
 
+    public Tables subset(Predicate<TableId> filter) {
+        if (filter == null) return this;
+        return lock.read(() -> {
+            Tables result = new Tables();
+            tablesByTableId.forEach((tableId, table) -> {
+                if (filter.test(tableId)) {
+                    result.overwriteTable(table);
+                }
+            });
+            return result;
+        });
+    }
+
     @Override
     public String toString() {
         return lock.read(() -> {",2016-03-03T21:27:11Z,98
"@@ -47,7 +47,7 @@ public static void enable() {
         }
 
         public static void disable() {
-            enabled = true;
+            enabled = false;
         }
 
         public static boolean isEnabled() {",2016-03-03T21:27:11Z,142
"@@ -469,7 +469,7 @@ protected void maybeFlush(OffsetStorageWriter offsetWriter, OffsetCommitPolicy p
 
             long started = clock.currentTimeInMillis();
             long timeout = started + commitTimeoutMs;
-            offsetWriter.beginFlush();
+            if ( !offsetWriter.beginFlush() ) return;
             Future<Void> flush = offsetWriter.doFlush(this::completedFlush);
             if (flush == null) return; // no offsets to commit ...
 ",2016-03-03T21:27:11Z,78
"@@ -83,9 +83,7 @@ public void stopConnector(BooleanConsumer callback) {
             if (engine != null && engine.isRunning()) {
                 engine.stop();
                 try {
-                    while (!engine.await(5, TimeUnit.SECONDS)) {
-                        // Wait for connector to stop completely ...
-                    }
+                    engine.await(5, TimeUnit.SECONDS);
                 } catch (InterruptedException e) {
                     Thread.interrupted();
                 }
@@ -101,6 +99,15 @@ public void stopConnector(BooleanConsumer callback) {
                     Thread.interrupted();
                 }
             }
+            if (engine != null && engine.isRunning()) {
+                try {
+                    while (!engine.await(5, TimeUnit.SECONDS)) {
+                        // Wait for connector to stop completely ...
+                    }
+                } catch (InterruptedException e) {
+                    Thread.interrupted();
+                }
+            }
             if (callback != null) callback.accept(engine != null ? engine.isRunning() : false);
         } finally {
             engine = null;",2016-03-03T21:27:11Z,35
"@@ -204,6 +204,7 @@ public void start(Map<String, String> props) {
                 logger.info(""Recovering MySQL connector '{}' database schemas from history stored in {}"", serverName, dbHistory);
                 DdlParser ddlParser = new MySqlDdlParser();
                 dbHistory.recover(source.partition(), source.offset(), tables, ddlParser);
+                tableConverters.loadTables();
                 logger.debug(""Recovered MySQL connector '{}' database schemas: {}"", serverName, tables.subset(tableFilter));
             } catch (Throwable t) {
                 throw new ConnectException(""Failure while recovering database schemas"", t);",2016-03-03T21:27:39Z,68
"@@ -76,6 +76,15 @@ public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
         Predicate<TableId> knownTables = (id) -> !unknownTableIds.contains(id); // known if not unknown
         this.tableFilter = tableFilter != null ? tableFilter.and(knownTables) : knownTables;
     }
+    
+    public void loadTables() {
+        // Create TableSchema instances for any existing table ...
+        this.tables.tableIds().forEach(id->{
+            Table table = this.tables.forTable(id);
+            TableSchema schema = schemaBuilder.create(table, false);
+            tableSchemaByTableId.put(id, schema);
+        });
+    }
 
     public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
         QueryEventData command = event.getData();",2016-03-03T21:27:39Z,109
"@@ -16,6 +16,7 @@
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
 import java.util.function.Consumer;
+import java.util.function.Predicate;
 
 import org.apache.kafka.connect.errors.ConnectException;
 import org.apache.kafka.connect.source.SourceRecord;
@@ -36,6 +37,7 @@
 
 import io.debezium.annotation.NotThreadSafe;
 import io.debezium.config.Configuration;
+import io.debezium.relational.TableId;
 import io.debezium.relational.Tables;
 import io.debezium.relational.ddl.DdlParser;
 import io.debezium.relational.history.DatabaseHistory;
@@ -133,13 +135,19 @@ public void start(Map<String, String> props) {
                          MySqlConnectorConfig.MAX_BATCH_SIZE, MySqlConnectorConfig.MAX_QUEUE_SIZE, maxBatchSize);
         }
 
+        // Define the filter using the whitelists and blacklists for tables and database names ...
+        Predicate<TableId> tableFilter = TableId.filter(config.getString(MySqlConnectorConfig.DATABASE_WHITELIST),
+                                                        config.getString(MySqlConnectorConfig.DATABASE_BLACKLIST),
+                                                        config.getString(MySqlConnectorConfig.TABLE_WHITELIST),
+                                                        config.getString(MySqlConnectorConfig.TABLE_BLACKLIST));
+
         // Create the queue ...
         events = new LinkedBlockingDeque<>(maxQueueSize);
         batchEvents = new ArrayList<>(maxBatchSize);
 
-        // Set up our handlers ...
+        // Set up our handlers for specific kinds of events ...
         tables = new Tables();
-        tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, tables);
+        tableConverters = new TableConverters(topicSelector, dbHistory, includeSchemaChanges, tables, tableFilter);
         eventHandlers.put(EventType.TABLE_MAP, tableConverters::updateTableMetadata);
         eventHandlers.put(EventType.QUERY, tableConverters::updateTableCommand);
         eventHandlers.put(EventType.EXT_WRITE_ROWS, tableConverters::handleInsert);",2016-02-04T13:56:13Z,143
"@@ -84,10 +84,26 @@ public class MySqlConnectorConfig {
                                                             .withDefault(false)
                                                             .withValidation(Field::isBoolean);
 
+    public static final Field TABLE_WHITELIST = Field.create(""table.whitelist"")
+                                                     .withDescription(""A comma-separated list of table identifiers to be monitored, where each identifer consists ""
+                                                             + ""of the '<databaseName>.<tableName>'. A whitelist takes precedence over any blacklist."");
+
+    public static final Field TABLE_BLACKLIST = Field.create(""table.blacklist"")
+                                                     .withDescription(""A comma-separated list of table identifiers to not be monitored, where each identifer consists ""
+                                                             + ""of the '<databaseName>.<tableName>'. Any whitelist takes precedence over this blacklist."");
+
+    public static final Field DATABASE_WHITELIST = Field.create(""database.whitelist"")
+                                                        .withDescription(""A comma-separated list of database names to be monitored. A database whitelist takes precedence over any database blacklist and supersedes a table whitelist or table blacklist."");
+
+    public static final Field DATABASE_BLACKLIST = Field.create(""database.blacklist"")
+                                                        .withDescription(""A comma-separated list of database names to not be monitored. Any database whitelist takes precedence over this blacklist, and supersedes a table whitelist or table blacklist."");
+
     public static Collection<Field> ALL_FIELDS = Collect.arrayListOf(USER, PASSWORD, HOSTNAME, PORT, SERVER_ID,
                                                                      SERVER_NAME, CONNECTION_TIMEOUT_MS, KEEP_ALIVE,
                                                                      MAX_QUEUE_SIZE, MAX_BATCH_SIZE, POLL_INTERVAL_MS,
-                                                                     DATABASE_HISTORY, INCLUDE_SCHEMA_CHANGES);
+                                                                     DATABASE_HISTORY, INCLUDE_SCHEMA_CHANGES,
+                                                                     TABLE_WHITELIST, TABLE_BLACKLIST,
+                                                                     DATABASE_WHITELIST, DATABASE_BLACKLIST);
 
     private static int validateMaxQueueSize(Configuration config, Field field, Consumer<String> problems) {
         int maxQueueSize = config.getInteger(field);",2016-02-04T13:56:13Z,40
"@@ -12,6 +12,7 @@
 import java.util.Objects;
 import java.util.Set;
 import java.util.function.Consumer;
+import java.util.function.Predicate;
 
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.Struct;
@@ -53,9 +54,11 @@ final class TableConverters {
     private final Map<Long, Converter> convertersByTableId = new HashMap<>();
     private final Map<String, Long> tableNumbersByTableName = new HashMap<>();
     private final boolean recordSchemaChangesInSourceRecords;
+    private final Predicate<TableId> tableFilter;
 
     public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
-            boolean recordSchemaChangesInSourceRecords, Tables tables) {
+            boolean recordSchemaChangesInSourceRecords, Tables tables,
+            Predicate<TableId> tableFilter) {
         Objects.requireNonNull(topicSelector, ""A topic selector is required"");
         Objects.requireNonNull(dbHistory, ""Database history storage is required"");
         Objects.requireNonNull(tables, ""A Tables object is required"");
@@ -64,6 +67,7 @@ public TableConverters(TopicSelector topicSelector, DatabaseHistory dbHistory,
         this.tables = tables;
         this.ddlParser = new MySqlDdlParser(false); // don't include views
         this.recordSchemaChangesInSourceRecords = recordSchemaChangesInSourceRecords;
+        this.tableFilter = tableFilter != null ? tableFilter : (id) -> true;
     }
 
     public void updateTableCommand(Event event, SourceInfo source, Consumer<SourceRecord> recorder) {
@@ -133,6 +137,11 @@ public void updateTableMetadata(Event event, SourceInfo source, Consumer<SourceR
 
             // Generate this table's insert, update, and delete converters ...
             Converter converter = new Converter() {
+                @Override
+                public TableId tableId() {
+                    return tableId;
+                }
+
                 @Override
                 public String topic() {
                     return topicName;
@@ -191,17 +200,19 @@ public void handleInsert(Event event, SourceInfo source, Consumer<SourceRecord>
         long tableNumber = write.getTableId();
         BitSet includedColumns = write.getIncludedColumns();
         Converter converter = convertersByTableId.get(tableNumber);
-        String topic = converter.topic();
-        Integer partition = converter.partition();
-        for (int row = 0; row <= source.eventRowNumber(); ++row) {
-            Serializable[] values = write.getRows().get(row);
-            Schema keySchema = converter.keySchema();
-            Object key = converter.createKey(values, includedColumns);
-            Schema valueSchema = converter.valueSchema();
-            Struct value = converter.inserted(values, includedColumns);
-            SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                    keySchema, key, valueSchema, value);
-            recorder.accept(record);
+        if (tableFilter.test(converter.tableId())) {
+            String topic = converter.topic();
+            Integer partition = converter.partition();
+            for (int row = 0; row <= source.eventRowNumber(); ++row) {
+                Serializable[] values = write.getRows().get(row);
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(values, includedColumns);
+                Schema valueSchema = converter.valueSchema();
+                Struct value = converter.inserted(values, includedColumns);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                        keySchema, key, valueSchema, value);
+                recorder.accept(record);
+            }
         }
     }
 
@@ -218,19 +229,21 @@ public void handleUpdate(Event event, SourceInfo source, Consumer<SourceRecord>
         BitSet includedColumns = update.getIncludedColumns();
         BitSet includedColumnsBefore = update.getIncludedColumnsBeforeUpdate();
         Converter converter = convertersByTableId.get(tableNumber);
-        String topic = converter.topic();
-        Integer partition = converter.partition();
-        for (int row = 0; row <= source.eventRowNumber(); ++row) {
-            Map.Entry<Serializable[], Serializable[]> changes = update.getRows().get(row);
-            Serializable[] before = changes.getKey();
-            Serializable[] after = changes.getValue();
-            Schema keySchema = converter.keySchema();
-            Object key = converter.createKey(after, includedColumns);
-            Schema valueSchema = converter.valueSchema();
-            Struct value = converter.updated(before, includedColumnsBefore, after, includedColumns);
-            SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                    keySchema, key, valueSchema, value);
-            recorder.accept(record);
+        if (tableFilter.test(converter.tableId())) {
+            String topic = converter.topic();
+            Integer partition = converter.partition();
+            for (int row = 0; row <= source.eventRowNumber(); ++row) {
+                Map.Entry<Serializable[], Serializable[]> changes = update.getRows().get(row);
+                Serializable[] before = changes.getKey();
+                Serializable[] after = changes.getValue();
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(after, includedColumns);
+                Schema valueSchema = converter.valueSchema();
+                Struct value = converter.updated(before, includedColumnsBefore, after, includedColumns);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                        keySchema, key, valueSchema, value);
+                recorder.accept(record);
+            }
         }
     }
 
@@ -239,21 +252,25 @@ public void handleDelete(Event event, SourceInfo source, Consumer<SourceRecord>
         long tableNumber = deleted.getTableId();
         BitSet includedColumns = deleted.getIncludedColumns();
         Converter converter = convertersByTableId.get(tableNumber);
-        String topic = converter.topic();
-        Integer partition = converter.partition();
-        for (int row = 0; row <= source.eventRowNumber(); ++row) {
-            Serializable[] values = deleted.getRows().get(row);
-            Schema keySchema = converter.keySchema();
-            Object key = converter.createKey(values, includedColumns);
-            Schema valueSchema = converter.valueSchema();
-            Struct value = converter.inserted(values, includedColumns);
-            SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
-                    keySchema, key, valueSchema, value);
-            recorder.accept(record);
+        if (tableFilter.test(converter.tableId())) {
+            String topic = converter.topic();
+            Integer partition = converter.partition();
+            for (int row = 0; row <= source.eventRowNumber(); ++row) {
+                Serializable[] values = deleted.getRows().get(row);
+                Schema keySchema = converter.keySchema();
+                Object key = converter.createKey(values, includedColumns);
+                Schema valueSchema = converter.valueSchema();
+                Struct value = converter.inserted(values, includedColumns);
+                SourceRecord record = new SourceRecord(source.partition(), source.offset(row), topic, partition,
+                        keySchema, key, valueSchema, value);
+                recorder.accept(record);
+            }
         }
     }
 
     protected static interface Converter {
+        TableId tableId();
+
         String topic();
 
         Integer partition();",2016-02-04T13:56:13Z,109
"@@ -5,6 +5,9 @@
  */
 package io.debezium.function;
 
+import java.util.HashSet;
+import java.util.Set;
+import java.util.function.Function;
 import java.util.function.Predicate;
 
 /**
@@ -13,6 +16,112 @@
  */
 public class Predicates {
 
+
+    /**
+     * Generate a whitelist filter/predicate that allows only those values that <em>are</em> included in the supplied input.
+     * 
+     * @param input the input string
+     * @param splitter the function that splits the input into multiple items; may not be null
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> whitelist(String input, Function<String, String[]> splitter, Function<String, T> factory) {
+        if ( input == null ) return (str)->false;
+        Set<T> matches = new HashSet<>();
+        for (String item : splitter.apply(input)) {
+            T obj = factory.apply(item);
+            if ( obj != null ) matches.add(obj);
+        }
+        return matches::contains;
+    }
+
+    /**
+     * Generate a whitelist filter/predicate that allows only those values that <em>are</em> included in the supplied input.
+     * 
+     * @param input the input string
+     * @param delimiter the character used to delimit the items in the input
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> whitelist(String input, char delimiter, Function<String, T> factory) {
+        return whitelist(input, (str) -> str.split(""["" + delimiter + ""]""), factory);
+    }
+
+    /**
+     * Generate a whitelist filter/predicate that allows only those values that <em>are</em> included in the supplied
+     * comma-separated input.
+     * 
+     * @param input the input string
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> whitelist(String input, Function<String, T> factory) {
+        return whitelist(input, (str) -> str.split(""[\\,]""), factory);
+    }
+
+    /**
+     * Generate a whitelist filter/predicate that allows only those values that <em>are</em> included in the supplied
+     * comma-separated input.
+     * 
+     * @param input the input string
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static Predicate<String> whitelist(String input) {
+        return whitelist(input, (str) -> str);
+    }
+
+    /**
+     * Generate a blacklist filter/predicate that allows only those values that are <em>not</em> included in the supplied input.
+     * 
+     * @param input the input string
+     * @param splitter the function that splits the input into multiple items; may not be null
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> blacklist(String input, Function<String, String[]> splitter, Function<String, T> factory) {
+        return whitelist(input, splitter, factory).negate();
+    }
+
+    /**
+     * Generate a blacklist filter/predicate that allows only those values that are <em>not</em> included in the supplied input.
+     * 
+     * @param input the input string
+     * @param delimiter the character used to delimit the items in the input
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> blacklist(String input, char delimiter, Function<String, T> factory) {
+        return whitelist(input, delimiter, factory).negate();
+    }
+
+    /**
+     * Generate a blacklist filter/predicate that allows only those values that are <em>not</em> included in the supplied comma-separated input.
+     * 
+     * @param input the input string
+     * @param factory the factory for creating string items into filter matches; may not be null
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static <T> Predicate<T> blacklist(String input, Function<String, T> factory) {
+        return whitelist(input, factory).negate();
+    }
+    /**
+     * Generate a blacklist filter/predicate that allows only those values that are <em>not</em> included in the supplied comma-separated input.
+     * 
+     * @param input the input string
+     * @return the predicate that returns {@code true} if and only if the argument to the predicate matches (with
+     *         {@link Object#equals(Object) equals(...)} one of the objects parsed from the input; never null
+     */
+    public static Predicate<String> blacklist(String input) {
+        return whitelist(input).negate();
+    }
+
     public static <R> Predicate<R> not(Predicate<R> predicate) {
         return predicate.negate();
     }",2016-02-04T13:56:13Z,112
"@@ -5,89 +5,167 @@
  */
 package io.debezium.relational;
 
+import java.util.function.Predicate;
+
 import io.debezium.annotation.Immutable;
+import io.debezium.function.Predicates;
 
 /**
  * Unique identifier for a database table.
+ * 
  * @author Randall Hauch
  */
 @Immutable
 public final class TableId implements Comparable<TableId> {
 
+    /**
+     * Create a predicate function that allows only those {@link TableId}s that are allowed by the database whitelist (or
+     * not disallowed by the database blacklist) and allowed by the table whitelist (or not disallowed by the table blacklist).
+     * Therefore, blacklists are only used if there is no corresponding whitelist.
+     * <p>
+     * Qualified table names are comma-separated strings that are each {@link #parse(String) parsed} into {@link TableId} objects.
+     * 
+     * @param dbWhitelist the comma-separated string listing the names of the databases to be explicitly allowed;
+     *            may be null
+     * @param dbBlacklist the comma-separated string listing the names of the databases to be explicitly disallowed;
+     *            may be null
+     * @param tableWhitelist the comma-separated string listing the qualified names of the tables to be explicitly allowed;
+     *            may be null
+     * @param tableBlacklist the comma-separated string listing the qualified names of the tables to be explicitly disallowed;
+     *            may be null
+     * @return the predicate function; never null
+     */
+    public static Predicate<TableId> filter(String dbWhitelist, String dbBlacklist, String tableWhitelist, String tableBlacklist) {
+        Predicate<TableId> tableExclusions = tableBlacklist == null ? null : Predicates.blacklist(tableBlacklist, TableId::parse);
+        Predicate<TableId> tableInclusions = tableWhitelist == null ? null : Predicates.whitelist(tableWhitelist, TableId::parse);
+        Predicate<TableId> tableFilter = tableInclusions != null ? tableInclusions : tableExclusions;
+        Predicate<String> dbExclusions = dbBlacklist == null ? null : Predicates.blacklist(dbBlacklist);
+        Predicate<String> dbInclusions = dbWhitelist == null ? null : Predicates.whitelist(dbWhitelist);
+        Predicate<String> dbFilter = dbInclusions != null ? dbInclusions : dbExclusions;
+        if (dbFilter != null) {
+            if (tableFilter != null) {
+                return (id) -> dbFilter.test(id.catalog()) && tableFilter.test(id);
+            }
+            return (id) -> dbFilter.test(id.catalog());
+        }
+        if (tableFilter != null) {
+            return tableFilter;
+        }
+        return (id) -> true;
+    }
+
+    /**
+     * Parse the supplied string delimited with a period ({@code .}) character, extracting up to the first 3 parts into a TableID.
+     * If the input contains only two parts, then the first part will be used as the catalog name and the second as the table
+     * name.
+     * 
+     * @param str the input string
+     * @return the table ID, or null if it could not be parsed
+     */
+    public static TableId parse(String str) {
+        return parse(str, '.', true);
+    }
+
+    /**
+     * Parse the supplied string, extracting up to the first 3 parts into a TableID.
+     * 
+     * @param str the input string
+     * @param delimiter the delimiter between parts
+     * @param useCatalogBeforeSchema {@code true} if the parsed string contains only 2 items and the first should be used as
+     *            the catalog and the second as the table name, or {@code false} if the first should be used as the schema and the
+     *            second
+     *            as the table name
+     * @return the table ID, or null if it could not be parsed
+     */
+    public static TableId parse(String str, char delimiter, boolean useCatalogBeforeSchema) {
+        String[] parts = str.split(""[\\"" + delimiter + ""]"");
+        if (parts.length == 0) return null;
+        if (parts.length == 1) return new TableId(null, null, parts[0]); // table only
+        if (parts.length == 2) {
+            if (useCatalogBeforeSchema) return new TableId(parts[0], null, parts[1]); // catalog & table only
+            return new TableId(null, parts[0], parts[1]); // catalog & table only
+        }
+        return new TableId(parts[0], parts[1], parts[2]); // catalog & table only
+    }
+
     private final String catalogName;
     private final String schemaName;
     private final String tableName;
     private final String id;
-    
+
     /**
      * Create a new table identifier.
+     * 
      * @param catalogName the name of the database catalog that contains the table; may be null if the JDBC driver does not
      *            show a schema for this table
      * @param schemaName the name of the database schema that contains the table; may be null if the JDBC driver does not
      *            show a schema for this table
      * @param tableName the name of the table; may not be null
      */
-    public TableId( String catalogName, String schemaName, String tableName ) {
+    public TableId(String catalogName, String schemaName, String tableName) {
         this.catalogName = catalogName;
         this.schemaName = schemaName;
         this.tableName = tableName;
         assert this.tableName != null;
         this.id = tableId(this.catalogName, this.schemaName, this.tableName);
     }
-    
+
     /**
      * Get the name of the JDBC catalog.
+     * 
      * @return the catalog name, or null if the table does not belong to a catalog
      */
     public String catalog() {
         return catalogName;
     }
-    
+
     /**
      * Get the name of the JDBC schema.
+     * 
      * @return the JDBC schema name, or null if the table does not belong to a JDBC schema
      */
     public String schema() {
         return schemaName;
     }
-    
+
     /**
      * Get the name of the table.
+     * 
      * @return the table name; never null
      */
     public String table() {
         return tableName;
     }
-    
+
     @Override
     public int compareTo(TableId that) {
-        if ( this == that ) return 0;
+        if (this == that) return 0;
         return this.id.compareTo(that.id);
     }
-    
+
     public int compareToIgnoreCase(TableId that) {
-        if ( this == that ) return 0;
+        if (this == that) return 0;
         return this.id.compareToIgnoreCase(that.id);
     }
-    
+
     @Override
     public int hashCode() {
         return id.hashCode();
     }
-    
+
     @Override
     public boolean equals(Object obj) {
-        if ( obj instanceof TableId ) {
-            return this.compareTo((TableId)obj) == 0;
+        if (obj instanceof TableId) {
+            return this.compareTo((TableId) obj) == 0;
         }
         return false;
     }
-    
+
     @Override
     public String toString() {
         return id;
     }
-    
+
     private static String tableId(String catalog, String schema, String table) {
         if (catalog == null || catalog.length() == 0) {
             if (schema == null || schema.length() == 0) {",2016-02-04T13:56:13Z,113
"@@ -0,0 +1,45 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.function;
+
+import java.util.function.Predicate;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author Randall Hauch
+ */
+public class PredicatesTest {
+
+    @Test
+    public void shouldWhitelistCommaSeparatedIntegers() {
+        Predicate<Integer> p = Predicates.whitelist(""1,2,3,4,5"",',', Integer::parseInt);
+        assertThat(p.test(1)).isTrue();
+        assertThat(p.test(2)).isTrue();
+        assertThat(p.test(3)).isTrue();
+        assertThat(p.test(4)).isTrue();
+        assertThat(p.test(5)).isTrue();
+        assertThat(p.test(0)).isFalse();
+        assertThat(p.test(6)).isFalse();
+        assertThat(p.test(-1)).isFalse();
+    }
+
+    @Test
+    public void shouldBlacklistCommaSeparatedIntegers() {
+        Predicate<Integer> p = Predicates.blacklist(""1,2,3,4,5"",',', Integer::parseInt);
+        assertThat(p.test(1)).isFalse();
+        assertThat(p.test(2)).isFalse();
+        assertThat(p.test(3)).isFalse();
+        assertThat(p.test(4)).isFalse();
+        assertThat(p.test(5)).isFalse();
+        assertThat(p.test(0)).isTrue();
+        assertThat(p.test(6)).isTrue();
+        assertThat(p.test(-1)).isTrue();
+    }
+
+}",2016-02-04T13:56:13Z,144
"@@ -0,0 +1,159 @@
+/*
+ * Copyright Debezium Authors.
+ * 
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+package io.debezium.relational;
+
+import java.util.function.Predicate;
+
+import org.junit.Test;
+
+import static org.fest.assertions.Assertions.assertThat;
+
+/**
+ * @author Randall Hauch
+ */
+public class TableIdTest {
+
+    private Predicate<TableId> filter;
+
+    @Test
+    public void shouldCreateFilterWithDatabaseWhitelistAndTableWhitelist() {
+        filter = TableId.filter(""db1,db2"", null, ""db1.A,db1.B,db2.C"", null);
+
+        assertAllowed(filter, ""db1"", ""A"");
+        assertAllowed(filter, ""db1"", ""B"");
+        assertNotAllowed(filter, ""db1"", ""D"");
+        assertNotAllowed(filter, ""db1"", ""E"");
+        assertNotAllowed(filter, ""db1"", ""F"");
+
+        assertAllowed(filter, ""db2"", ""C"");
+        assertNotAllowed(filter, ""db2"", ""G"");
+        assertNotAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseWhitelistAndTableBlacklist() {
+        filter = TableId.filter(""db1,db2"", null, null, ""db1.A,db1.B,db2.C"");
+
+        assertNotAllowed(filter, ""db1"", ""A"");
+        assertNotAllowed(filter, ""db1"", ""B"");
+        assertAllowed(filter, ""db1"", ""D"");
+        assertAllowed(filter, ""db1"", ""E"");
+        assertAllowed(filter, ""db1"", ""F"");
+
+        assertNotAllowed(filter, ""db2"", ""C"");
+        assertAllowed(filter, ""db2"", ""G"");
+        assertAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseBlacklistAndTableWhitelist() {
+        filter = TableId.filter(null,""db3,db4"", ""db1.A,db1.B,db2.C"", null);
+
+        assertAllowed(filter, ""db1"", ""A"");
+        assertAllowed(filter, ""db1"", ""B"");
+        assertNotAllowed(filter, ""db1"", ""D"");
+        assertNotAllowed(filter, ""db1"", ""E"");
+        assertNotAllowed(filter, ""db1"", ""F"");
+
+        assertAllowed(filter, ""db2"", ""C"");
+        assertNotAllowed(filter, ""db2"", ""G"");
+        assertNotAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseBlacklistAndTableBlacklist() {
+        filter = TableId.filter(null,""db3,db4"", null, ""db1.A,db1.B,db2.C"");
+
+        assertNotAllowed(filter, ""db1"", ""A"");
+        assertNotAllowed(filter, ""db1"", ""B"");
+        assertAllowed(filter, ""db1"", ""D"");
+        assertAllowed(filter, ""db1"", ""E"");
+        assertAllowed(filter, ""db1"", ""F"");
+
+        assertNotAllowed(filter, ""db2"", ""C"");
+        assertAllowed(filter, ""db2"", ""G"");
+        assertAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithNoDatabaseFilterAndTableWhitelist() {
+        filter = TableId.filter(null, null, ""db1.A,db1.B,db2.C"", null);
+
+        assertAllowed(filter, ""db1"", ""A"");
+        assertAllowed(filter, ""db1"", ""B"");
+        assertNotAllowed(filter, ""db1"", ""D"");
+        assertNotAllowed(filter, ""db1"", ""E"");
+        assertNotAllowed(filter, ""db1"", ""F"");
+
+        assertAllowed(filter, ""db2"", ""C"");
+        assertNotAllowed(filter, ""db2"", ""G"");
+        assertNotAllowed(filter, ""db2"", ""H"");
+
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithNoDatabaseFilterAndTableBlacklist() {
+        filter = TableId.filter(null, null, null, ""db1.A,db1.B,db2.C"");
+
+        assertNotAllowed(filter, ""db1"", ""A"");
+        assertNotAllowed(filter, ""db1"", ""B"");
+        assertAllowed(filter, ""db1"", ""D"");
+        assertAllowed(filter, ""db1"", ""E"");
+        assertAllowed(filter, ""db1"", ""F"");
+
+        assertNotAllowed(filter, ""db2"", ""C"");
+        assertAllowed(filter, ""db2"", ""G"");
+        assertAllowed(filter, ""db2"", ""H"");
+
+        assertAllowed(filter, ""db3"", ""A"");
+        assertAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseWhitelistAndNoTableFilter() {
+        filter = TableId.filter(""db1,db2"", null, null, null);
+
+        assertAllowed(filter, ""db1"", ""A"");
+        assertAllowed(filter, ""db2"", ""A"");
+        assertNotAllowed(filter, ""db3"", ""A"");
+        assertNotAllowed(filter, ""db4"", ""A"");
+    }
+
+    @Test
+    public void shouldCreateFilterWithDatabaseBlacklistAndNoTableFilter() {
+        filter = TableId.filter(null, ""db1,db2"", null, null);
+
+        assertNotAllowed(filter, ""db1"", ""A"");
+        assertNotAllowed(filter, ""db2"", ""A"");
+        assertAllowed(filter, ""db3"", ""A"");
+        assertAllowed(filter, ""db4"", ""A"");
+    }
+
+    protected void assertAllowed(Predicate<TableId> filter, String dbName, String tableName) {
+        TableId id = new TableId(dbName, null, tableName);
+        assertThat(filter.test(id)).isTrue();
+    }
+
+    protected void assertNotAllowed(Predicate<TableId> filter, String dbName, String tableName) {
+        TableId id = new TableId(dbName, null, tableName);
+        assertThat(filter.test(id)).isFalse();
+    }
+
+}",2016-02-04T13:56:13Z,145
"@@ -102,11 +102,14 @@ public static boolean isPerconaServer() {
         return comment.startsWith(""Percona"");
     }
 
-    protected static void addDefaults(Configuration.Builder builder) {
-        builder.withDefault(JdbcConfiguration.HOSTNAME, ""localhost"")
+    private static Configuration addDefaultSettings(Configuration configuration) {
+        return configuration.edit()
+                .withDefault(JdbcConfiguration.HOSTNAME, ""localhost"")
                 .withDefault(JdbcConfiguration.PORT, 3306)
                 .withDefault(JdbcConfiguration.USER, ""mysqluser"")
-                .withDefault(JdbcConfiguration.PASSWORD, ""mysqlpw"");
+                .withDefault(JdbcConfiguration.PASSWORD, ""mysqlpw"")
+                .build();
+
     }
 
     protected static ConnectionFactory FACTORY = JdbcConnection.patternBasedFactory(""jdbc:mysql://${hostname}:${port}/${dbname}"");
@@ -117,7 +120,7 @@ protected static void addDefaults(Configuration.Builder builder) {
      * @param config the configuration; may not be null
      */
     public MySqlTestConnection(Configuration config) {
-        super(config, FACTORY, null, MySqlTestConnection::addDefaults, ""`"", ""`"");
+        super(addDefaultSettings(config), FACTORY, null, null, ""`"", ""`"");
     }
 
     public MySqlVersion getMySqlVersion() {",2022-03-07T16:53:45Z,146
"@@ -84,7 +84,7 @@ public class PostgresConnection extends JdbcConnection {
      * @param valueConverterBuilder supplies a configured {@link PostgresValueConverter} for a given {@link TypeRegistry}
      */
     public PostgresConnection(Configuration config, PostgresValueConverterBuilder valueConverterBuilder) {
-        super(config, FACTORY, PostgresConnection::validateServerVersion, PostgresConnection::defaultSettings, ""\"""", ""\"""");
+        super(addDefaultSettings(config), FACTORY, PostgresConnection::validateServerVersion, null, ""\"""", ""\"""");
 
         if (Objects.isNull(valueConverterBuilder)) {
             this.typeRegistry = null;
@@ -104,7 +104,7 @@ public PostgresConnection(Configuration config, PostgresValueConverterBuilder va
      * @param typeRegistry an existing/already-primed {@link TypeRegistry} instance
      */
     public PostgresConnection(PostgresConnectorConfig config, TypeRegistry typeRegistry) {
-        super(config.getJdbcConfig(), FACTORY, PostgresConnection::validateServerVersion, PostgresConnection::defaultSettings, ""\"""", ""\"""");
+        super(addDefaultSettings(config.getJdbcConfig()), FACTORY, PostgresConnection::validateServerVersion, null, ""\"""", ""\"""");
         if (Objects.isNull(typeRegistry)) {
             this.typeRegistry = null;
             this.defaultValueConverter = null;
@@ -126,6 +126,13 @@ public PostgresConnection(Configuration config) {
         this(config, null);
     }
 
+    static Configuration addDefaultSettings(Configuration configuration) {
+        // we require Postgres 9.4 as the minimum server version since that's where logical replication was first introduced
+        return configuration.edit()
+                .with(""assumeMinServerVersion"", ""9.4"")
+                .build();
+    }
+
     /**
      * Returns a JDBC connection string for the current configuration.
      *
@@ -479,11 +486,6 @@ public TimestampUtils getTimestampUtils() {
         }
     }
 
-    protected static void defaultSettings(Configuration.Builder builder) {
-        // we require Postgres 9.4 as the minimum server version since that's where logical replication was first introduced
-        builder.with(""assumeMinServerVersion"", ""9.4"");
-    }
-
     private static void validateServerVersion(Statement statement) throws SQLException {
         DatabaseMetaData metaData = statement.getConnection().getMetaData();
         int majorVersion = metaData.getDatabaseMajorVersion();",2022-03-07T16:53:45Z,55
"@@ -105,7 +105,7 @@ private PostgresReplicationConnection(PostgresConnectorConfig config,
                                           TypeRegistry typeRegistry,
                                           Properties streamParams,
                                           PostgresSchema schema) {
-        super(config.getJdbcConfig(), PostgresConnection.FACTORY, null, PostgresReplicationConnection::defaultSettings, ""\"""", ""\"""");
+        super(addDefaultSettings(config.getJdbcConfig()), PostgresConnection.FACTORY, null, null, ""\"""", ""\"""");
 
         this.connectorConfig = config;
         this.slotName = slotName;
@@ -122,6 +122,16 @@ private PostgresReplicationConnection(PostgresConnectorConfig config,
         this.hasInitedSlot = false;
     }
 
+    private static Configuration addDefaultSettings(Configuration configuration) {
+        // first copy the parent's default settings...
+        // then set some additional replication specific settings
+        return PostgresConnection.addDefaultSettings(configuration)
+                .edit()
+                .with(""replication"", ""database"")
+                .with(""preferQueryMode"", ""simple"") // replication protocol only supports simple query mode
+                .build();
+    }
+
     private ServerInfo.ReplicationSlot getSlotInfo() throws SQLException, InterruptedException {
         try (PostgresConnection connection = new PostgresConnection(connectorConfig.getJdbcConfig())) {
             return connection.readReplicationSlotInfo(slotName, plugin.getPostgresPluginName());
@@ -646,14 +656,6 @@ public void reconnect() throws SQLException {
         connection(false);
     }
 
-    protected static void defaultSettings(Configuration.Builder builder) {
-        // first copy the parent's default settings...
-        PostgresConnection.defaultSettings(builder);
-        // then set some additional replication specific settings
-        builder.with(""replication"", ""database"")
-                .with(""preferQueryMode"", ""simple""); // replication protocol only supports simple query mode
-    }
-
     protected static class ReplicationConnectionBuilder implements Builder {
 
         private final PostgresConnectorConfig config;",2022-03-07T16:53:45Z,147
"@@ -321,7 +321,7 @@ private static String findAndReplace(String url, String name, Properties props,
      * @param connectionFactory the connection factory; may not be null
      */
     public JdbcConnection(Configuration config, ConnectionFactory connectionFactory, String openingQuoteCharacter, String closingQuoteCharacter) {
-        this(config, connectionFactory, null, null, null, openingQuoteCharacter, closingQuoteCharacter);
+        this(config, connectionFactory, null, null, openingQuoteCharacter, closingQuoteCharacter);
     }
 
     /**
@@ -332,7 +332,7 @@ public JdbcConnection(Configuration config, ConnectionFactory connectionFactory,
      */
     public JdbcConnection(Configuration config, ConnectionFactory connectionFactory, Supplier<ClassLoader> classLoaderSupplier, String openingQuoteCharacter,
                           String closingQuoteCharacter) {
-        this(config, connectionFactory, null, null, classLoaderSupplier, openingQuoteCharacter, closingQuoteCharacter);
+        this(config, connectionFactory, null, classLoaderSupplier, openingQuoteCharacter, closingQuoteCharacter);
     }
 
     /**
@@ -342,28 +342,13 @@ public JdbcConnection(Configuration config, ConnectionFactory connectionFactory,
      * @param config the configuration; may not be null
      * @param connectionFactory the connection factory; may not be null
      * @param initialOperations the initial operations that should be run on each new connection; may be null
-     * @param adapter the function that can be called to update the configuration with defaults
-     */
-    protected JdbcConnection(Configuration config, ConnectionFactory connectionFactory, Operations initialOperations,
-                             Consumer<Configuration.Builder> adapter, String openingQuotingChar, String closingQuotingChar) {
-        this(config, connectionFactory, initialOperations, adapter, null, openingQuotingChar, closingQuotingChar);
-    }
-
-    /**
-     * Create a new instance with the given configuration and connection factory, and specify the operations that should be
-     * run against each newly-established connection.
-     *
-     * @param config the configuration; may not be null
-     * @param connectionFactory the connection factory; may not be null
-     * @param initialOperations the initial operations that should be run on each new connection; may be null
-     * @param adapter the function that can be called to update the configuration with defaults
      * @param classLoaderSupplier class loader supplier
      * @param openingQuotingChar the opening quoting character
      * @param closingQuotingChar the closing quoting character
      */
     protected JdbcConnection(Configuration config, ConnectionFactory connectionFactory, Operations initialOperations,
-                             Consumer<Configuration.Builder> adapter, Supplier<ClassLoader> classLoaderSupplier, String openingQuotingChar, String closingQuotingChar) {
-        this.config = adapter == null ? config : config.edit().apply(adapter).build();
+                             Supplier<ClassLoader> classLoaderSupplier, String openingQuotingChar, String closingQuotingChar) {
+        this.config = config;
         this.factory = classLoaderSupplier == null ? connectionFactory : new ConnectionFactoryDecorator(connectionFactory, classLoaderSupplier);
         this.initialOps = initialOperations;
         this.openingQuoteCharacter = openingQuotingChar;",2022-03-07T16:53:45Z,9
"@@ -0,0 +1,34 @@
+/*
+ * Copyright Debezium Authors.
+ *
+ * Licensed under the Apache Software License version 2.0, available at http://www.apache.org/licenses/LICENSE-2.0
+ */
+
+package io.debezium.connector.postgresql;
+
+import java.util.function.Predicate;
+
+import io.debezium.function.Predicates;
+
+public class LogicalDecodingMessageFilter {
+    private final Predicate<String> filter;
+
+    public LogicalDecodingMessageFilter(String inclusionString, String exclusionString) {
+        Predicate<String> inclusions = notEmpty(inclusionString) ? Predicates.includes(inclusionString) : null;
+        Predicate<String> exclusions = notEmpty(exclusionString) ? Predicates.excludes(exclusionString) : null;
+        Predicate<String> filter = inclusions != null ? inclusions : exclusions;
+        this.filter = filter != null ? filter : (id) -> true;
+    }
+
+    public Predicate<String> getFilter() {
+        return filter;
+    }
+
+    public boolean isIncluded(String prefix) {
+        return filter.test(prefix);
+    }
+
+    private boolean notEmpty(String value) {
+        return value != null && !value.trim().isEmpty();
+    }
+}",2021-11-26T13:29:15Z,148
"@@ -43,7 +43,6 @@ public class LogicalDecodingMessageMonitor {
     public static final String DEBEZIUM_LOGICAL_DECODING_MESSAGE_KEY = ""message"";
     public static final String DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY = ""prefix"";
     public static final String DEBEZIUM_LOGICAL_DECODING_MESSAGE_CONTENT_KEY = ""content"";
-    public static final String DEBEZIUM_LOGICAL_DECODING_MESSAGE_TRANSACTIONAL_KEY = ""transactional"";
 
     private final BlockingConsumer<SourceRecord> sender;
     private final String topicName;
@@ -59,7 +58,6 @@ public LogicalDecodingMessageMonitor(PostgresConnectorConfig connectorConfig, Bl
         this.base64Encoder = Base64.getEncoder();
 
         this.blockSchema = SchemaBuilder.struct().optional()
-                .field(DEBEZIUM_LOGICAL_DECODING_MESSAGE_TRANSACTIONAL_KEY, Schema.BOOLEAN_SCHEMA)
                 .field(DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY, Schema.OPTIONAL_STRING_SCHEMA)
                 .field(DEBEZIUM_LOGICAL_DECODING_MESSAGE_CONTENT_KEY, binaryMode.getSchema().build())
                 .build();
@@ -76,7 +74,6 @@ public void logicalDecodingMessageEvent(Partition partition, OffsetContext offse
                                             LogicalDecodingMessage message)
             throws InterruptedException {
         final Struct logicalMsgStruct = new Struct(blockSchema);
-        logicalMsgStruct.put(DEBEZIUM_LOGICAL_DECODING_MESSAGE_TRANSACTIONAL_KEY, message.isTransactional());
         logicalMsgStruct.put(DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY, message.getPrefix());
         logicalMsgStruct.put(DEBEZIUM_LOGICAL_DECODING_MESSAGE_CONTENT_KEY, convertContent(message.getContent()));
 ",2021-11-26T13:29:15Z,149
"@@ -607,46 +607,6 @@ public static TruncateHandlingMode parse(String value) {
         }
     }
 
-    /**
-     * The set of predefined LogicalDecodingMessageHandlingMode options or aliases
-     */
-    public enum LogicalDecodingMessageHandlingMode implements EnumeratedValue {
-
-        /**
-         * Skip MESSAGE messages
-         */
-        SKIP(""skip""),
-
-        /**
-         * Handle & Include MESSAGE messages
-         */
-        INCLUDE(""include"");
-
-        private final String value;
-
-        LogicalDecodingMessageHandlingMode(String value) {
-            this.value = value;
-        }
-
-        @Override
-        public String getValue() {
-            return value;
-        }
-
-        public static LogicalDecodingMessageHandlingMode parse(String value) {
-            if (value == null) {
-                return null;
-            }
-            value = value.trim();
-            for (LogicalDecodingMessageHandlingMode option : LogicalDecodingMessageHandlingMode.values()) {
-                if (option.getValue().equalsIgnoreCase(value)) {
-                    return option;
-                }
-            }
-            return null;
-        }
-    }
-
     /**
      * The set of predefined SchemaRefreshMode options or aliases.
      */
@@ -965,16 +925,32 @@ public static AutoCreateMode parse(String value, String defaultValue) {
                     ""When 'snapshot.mode' is set as custom, this setting must be set to specify a fully qualified class name to load (via the default class loader).""
                             + ""This class must implement the 'Snapshotter' interface and is called on each app boot to determine whether to do a snapshot and how to build queries."");
 
-    public static final Field LOGICAL_DECODING_MESSAGE_HANDLING_MODE = Field.create(""logical_decoding_message.handling.mode"")
-            .withDisplayName(""Logical Decoding Message handling mode"")
+    /**
+     * A comma-separated list of regular expressions that match the prefix of logical decoding messages to be excluded
+     * from monitoring. Must not be used with {@link #LOGICAL_DECODING_MESSAGE_PREFIX_INCLUDE_LIST}
+     */
+    public static final Field LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST = Field.create(""message.prefix.exclude.list"")
+            .withDisplayName(""Exclude Logical Decoding Message Prefixes"")
+            .withType(Type.LIST)
+            .withGroup(Field.createGroupEntry(Field.Group.CONNECTOR, 25))
+            .withWidth(Width.LONG)
+            .withImportance(Importance.MEDIUM)
+            .withValidation(Field::isListOfRegex, PostgresConnectorConfig::validateLogicalDecodingMessageExcludeList)
+            .withDescription(""A comma-separated list of regular expressions that match the logical decoding message prefixes to be excluded from monitoring."");
+
+    /**
+     * A comma-separated list of regular expressions that match the prefix of logical decoding messages to be monitored.
+     * Must not be used with {@link #LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST}
+     */
+    public static final Field LOGICAL_DECODING_MESSAGE_PREFIX_INCLUDE_LIST = Field.create(""message.prefix.include.list"")
+            .withDisplayName(""Include Logical Decoding Message Prefixes"")
+            .withType(Type.LIST)
             .withGroup(Field.createGroupEntry(Field.Group.CONNECTOR, 24))
-            .withEnum(LogicalDecodingMessageHandlingMode.class, LogicalDecodingMessageHandlingMode.SKIP)
-            .withWidth(Width.MEDIUM)
+            .withWidth(Width.LONG)
             .withImportance(Importance.MEDIUM)
-            .withValidation(PostgresConnectorConfig::logicalDecodingMessageHandlingMode)
-            .withDescription(""Specify how MESSAGE operations are handled (supported only on pg14+ pgoutput plugin), including: "" +
-                    ""'skip' to skip / ignore MESSAGE events (default), "" +
-                    ""'include' to handle and include MESSAGE events"");
+            .withValidation(Field::isListOfRegex)
+            .withDescription(
+                    ""A comma-separated list of regular expressions that match the logical decoding message prefixes to be monitored. All prefixes are monitored by default."");
 
     public static final Field TRUNCATE_HANDLING_MODE = Field.create(""truncate.handling.mode"")
             .withDisplayName(""Truncate handling mode"")
@@ -1096,7 +1072,7 @@ public static AutoCreateMode parse(String value, String defaultValue) {
             .withDescription(""Number of fractional digits when money type is converted to 'precise' decimal number."");
 
     private final TruncateHandlingMode truncateHandlingMode;
-    private final LogicalDecodingMessageHandlingMode logicalDecodingMessageHandlingMode;
+    private final LogicalDecodingMessageFilter logicalDecodingMessageFilter;
     private final HStoreHandlingMode hStoreHandlingMode;
     private final IntervalHandlingMode intervalHandlingMode;
     private final SnapshotMode snapshotMode;
@@ -1112,8 +1088,8 @@ public PostgresConnectorConfig(Configuration config) {
                 ColumnFilterMode.SCHEMA);
 
         this.truncateHandlingMode = TruncateHandlingMode.parse(config.getString(PostgresConnectorConfig.TRUNCATE_HANDLING_MODE));
-        this.logicalDecodingMessageHandlingMode = LogicalDecodingMessageHandlingMode
-                .parse(config.getString(PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_HANDLING_MODE));
+        this.logicalDecodingMessageFilter = new LogicalDecodingMessageFilter(config.getString(LOGICAL_DECODING_MESSAGE_PREFIX_INCLUDE_LIST),
+                config.getString(LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST));
         String hstoreHandlingModeStr = config.getString(PostgresConnectorConfig.HSTORE_HANDLING_MODE);
         this.hStoreHandlingMode = HStoreHandlingMode.parse(hstoreHandlingModeStr);
         this.intervalHandlingMode = IntervalHandlingMode.parse(config.getString(PostgresConnectorConfig.INTERVAL_HANDLING_MODE));
@@ -1177,8 +1153,8 @@ public TruncateHandlingMode truncateHandlingMode() {
         return truncateHandlingMode;
     }
 
-    public LogicalDecodingMessageHandlingMode logicalDecodingMessageHandlingMode() {
-        return logicalDecodingMessageHandlingMode;
+    public LogicalDecodingMessageFilter getMessageFilter() {
+        return logicalDecodingMessageFilter;
     }
 
     protected HStoreHandlingMode hStoreHandlingMode() {
@@ -1274,7 +1250,8 @@ protected SourceInfoStructMaker<? extends AbstractSourceInfo> getSourceInfoStruc
                     TRUNCATE_HANDLING_MODE,
                     INCREMENTAL_SNAPSHOT_CHUNK_SIZE,
                     UNAVAILABLE_VALUE_PLACEHOLDER,
-                    LOGICAL_DECODING_MESSAGE_HANDLING_MODE)
+                    LOGICAL_DECODING_MESSAGE_PREFIX_INCLUDE_LIST,
+                    LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST)
             .excluding(INCLUDE_SCHEMA_CHANGES)
             .create();
 
@@ -1336,31 +1313,16 @@ private static int validateToastedValuePlaceholder(Configuration config, Field f
         return 0;
     }
 
-    private static int logicalDecodingMessageHandlingMode(Configuration config, Field field, Field.ValidationOutput problems) {
-        final String value = config.getString(field);
-        int errors = 0;
-        if (value != null) {
-            LogicalDecodingMessageHandlingMode mode = LogicalDecodingMessageHandlingMode.parse(value);
-            if (mode == null) {
-                List<String> validModes = Arrays.stream(LogicalDecodingMessageHandlingMode.values()).map(LogicalDecodingMessageHandlingMode::getValue)
-                        .collect(Collectors.toList());
-                String message = String.format(""Valid values for %s are %s, but got '%s'"", field.name(), validModes, value);
-                problems.accept(field, value, message);
-                errors++;
-                return errors;
-            }
-            if (mode == LogicalDecodingMessageHandlingMode.INCLUDE) {
-                final LogicalDecoder logicalDecoder = LogicalDecoder.parse(config.getString(PLUGIN_NAME));
-                if (!logicalDecoder.supportsLogicalDecodingMessage()) {
-                    String message = String.format(
-                            ""%s '%s' is not supported with configuration %s '%s'"",
-                            field.name(), mode.getValue(), PLUGIN_NAME.name(), logicalDecoder.getValue());
-                    problems.accept(field, value, message);
-                    errors++;
-                }
-            }
+    private static int validateLogicalDecodingMessageExcludeList(Configuration config, Field field, Field.ValidationOutput problems) {
+        String includeList = config.getString(LOGICAL_DECODING_MESSAGE_PREFIX_INCLUDE_LIST);
+        String excludeList = config.getString(LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST);
+
+        if (includeList != null && excludeList != null) {
+            problems.accept(LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST, excludeList,
+                    ""\""logical_decoding_message.prefix.include.list\"" is already specified"");
+            return 1;
         }
-        return errors;
+        return 0;
     }
 
     @Override",2021-11-26T13:29:15Z,42
"@@ -7,6 +7,8 @@
 package io.debezium.connector.postgresql;
 
 import org.apache.kafka.connect.source.SourceRecord;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
 
 import io.debezium.connector.base.ChangeEventQueue;
 import io.debezium.connector.postgresql.connection.LogicalDecodingMessage;
@@ -30,8 +32,10 @@
  * @author Lairen Hightower
  */
 public class PostgresEventDispatcher<T extends DataCollectionId> extends EventDispatcher<T> {
+    private static final Logger LOGGER = LoggerFactory.getLogger(PostgresEventDispatcher.class);
     private final ChangeEventQueue<DataChangeEvent> queue;
     private final LogicalDecodingMessageMonitor logicalDecodingMessageMonitor;
+    private final LogicalDecodingMessageFilter messageFilter;
 
     public PostgresEventDispatcher(PostgresConnectorConfig connectorConfig, TopicSelector<T> topicSelector,
                                    DatabaseSchema<T> schema, ChangeEventQueue<DataChangeEvent> queue, DataCollectionFilters.DataCollectionFilter<T> filter,
@@ -57,12 +61,18 @@ public PostgresEventDispatcher(PostgresConnectorConfig connectorConfig, TopicSel
                 customHeartbeat, schemaNameAdjuster, jdbcConnection);
         this.queue = queue;
         this.logicalDecodingMessageMonitor = new LogicalDecodingMessageMonitor(connectorConfig, this::enqueueLogicalDecodingMessage);
+        this.messageFilter = connectorConfig.getMessageFilter();
     }
 
     public void dispatchLogicalDecodingMessage(Partition partition, OffsetContext offset, Long decodeTimestamp,
                                                LogicalDecodingMessage message)
             throws InterruptedException {
-        logicalDecodingMessageMonitor.logicalDecodingMessageEvent(partition, offset, decodeTimestamp, message);
+        if (messageFilter.isIncluded(message.getPrefix())) {
+            logicalDecodingMessageMonitor.logicalDecodingMessageEvent(partition, offset, decodeTimestamp, message);
+        }
+        else {
+            LOGGER.trace(""Filtered data change event for logical decoding message with prefix{}"", message.getPrefix());
+        }
     }
 
     private void enqueueLogicalDecodingMessage(SourceRecord record) throws InterruptedException {",2021-11-26T13:29:15Z,150
"@@ -9,6 +9,7 @@
 import java.time.Instant;
 import java.util.HashMap;
 import java.util.Map;
+import java.util.OptionalLong;
 
 import org.apache.kafka.connect.data.Schema;
 import org.apache.kafka.connect.data.Struct;
@@ -126,17 +127,19 @@ public void postSnapshotCompletion() {
         sourceInfo.setSnapshot(SnapshotRecord.FALSE);
     }
 
-    public void updateWalPosition(Lsn lsn, Lsn lastCompletelyProcessedLsn, Instant commitTime, Long txId, TableId tableId, Long xmin) {
+    public void updateWalPosition(Lsn lsn, Lsn lastCompletelyProcessedLsn, Instant commitTime, OptionalLong txId, TableId tableId, Long xmin) {
         this.lastCompletelyProcessedLsn = lastCompletelyProcessedLsn;
-        sourceInfo.update(lsn, commitTime, txId, xmin, tableId);
+        Long transactionId = txId.isPresent() ? txId.getAsLong() : null;
+        sourceInfo.update(lsn, commitTime, transactionId, xmin, tableId);
     }
 
     /**
      * update wal position for lsn events that do not have an associated table or schema
      */
-    public void updateWalPosition(Lsn lsn, Lsn lastCompletelyProcessedLsn, Instant commitTime, Long txId, Long xmin) {
+    public void updateWalPosition(Lsn lsn, Lsn lastCompletelyProcessedLsn, Instant commitTime, OptionalLong txId, Long xmin) {
         this.lastCompletelyProcessedLsn = lastCompletelyProcessedLsn;
-        sourceInfo.update(lsn, commitTime, txId, xmin);
+        Long transactionId = txId.isPresent() ? txId.getAsLong() : null;
+        sourceInfo.update(lsn, commitTime, transactionId, xmin);
     }
 
     public void updateCommitPosition(Lsn lsn, Lsn lastCompletelyProcessedLsn) {",2021-11-26T13:29:15Z,151
"@@ -9,6 +9,7 @@
 import java.time.Duration;
 import java.util.List;
 import java.util.Optional;
+import java.util.OptionalLong;
 import java.util.Set;
 import java.util.stream.Collectors;
 
@@ -140,7 +141,7 @@ protected void determineSnapshotOffset(RelationalSnapshotContext<PostgresPartiti
 
     private void updateOffsetForSnapshot(PostgresOffsetContext offset) throws SQLException {
         final Lsn xlogStart = getTransactionStartLsn();
-        final long txId = jdbcConnection.currentTransactionId().longValue();
+        final OptionalLong txId = OptionalLong.of(jdbcConnection.currentTransactionId().longValue());
         LOGGER.info(""Read xlogStart at '{}' from transaction '{}'"", xlogStart, txId);
 
         // use the old xmin, as we don't want to update it if in xmin recovery",2021-11-26T13:29:15Z,152
"@@ -224,7 +224,7 @@ private void processMessages(ChangeEventSourceContext context, PostgresPartition
                     offsetContext.updateWalPosition(lsn, lastCompletelyProcessedLsn, message.getCommitTime(), message.getTransactionId(), null,
                             taskContext.getSlotXmin(connection));
                     if (message.getOperation() == Operation.BEGIN) {
-                        dispatcher.dispatchTransactionStartedEvent(partition, Long.toString(message.getTransactionId()), offsetContext);
+                        dispatcher.dispatchTransactionStartedEvent(partition, message.getTransactionId().toString(), offsetContext);
                     }
                     else if (message.getOperation() == Operation.COMMIT) {
                         commitMessage(partition, offsetContext, lsn);
@@ -233,24 +233,13 @@ else if (message.getOperation() == Operation.COMMIT) {
                     maybeWarnAboutGrowingWalBacklog(true);
                 }
                 else if (message.getOperation() == Operation.MESSAGE) {
-                    if (connectorConfig.logicalDecodingMessageHandlingMode() != PostgresConnectorConfig.LogicalDecodingMessageHandlingMode.INCLUDE) {
-                        LOGGER.debug(""Received logical decoding message {}"", message);
-                        if (message.isLastEventForLsn()) {
-                            commitMessage(partition, offsetContext, lsn);
-                        }
-                        return;
-                    }
+                    offsetContext.updateWalPosition(lsn, lastCompletelyProcessedLsn, message.getCommitTime(), message.getTransactionId(),
+                            taskContext.getSlotXmin(connection));
 
-                    // non-transactional messages will not have a commitTime or txId
+                    // non-transactional message that will not be followed by a COMMIT message
                     if (message.isLastEventForLsn()) {
-                        offsetContext.updateWalPosition(lsn, lastCompletelyProcessedLsn, null, null,
-                                taskContext.getSlotXmin(connection));
                         commitMessage(partition, offsetContext, lsn);
                     }
-                    else {
-                        offsetContext.updateWalPosition(lsn, lastCompletelyProcessedLsn, message.getCommitTime(), message.getTransactionId(),
-                                taskContext.getSlotXmin(connection));
-                    }
 
                     dispatcher.dispatchLogicalDecodingMessage(
                             partition,",2021-11-26T13:29:15Z,153
"@@ -111,7 +111,7 @@ protected ReplicationConnection createReplicationConnection(boolean doSnapshot)
                 .withPublicationAutocreateMode(config.publicationAutocreateMode())
                 .withPlugin(config.plugin())
                 .withTruncateHandlingMode(config.truncateHandlingMode())
-                .withLogicalDecodingMessageHandlingMode(config.logicalDecodingMessageHandlingMode())
+                .withLogicalDecodingMessageFilter(config.getMessageFilter())
                 .dropSlotOnClose(dropSlotOnStop)
                 .streamParams(config.streamParams())
                 .statusUpdateInterval(config.statusUpdateInterval())",2021-11-26T13:29:15Z,53
"@@ -8,6 +8,7 @@
 
 import java.time.Instant;
 import java.util.List;
+import java.util.OptionalLong;
 
 /**
  * Replication message instance representing a generic logical decoding message
@@ -17,12 +18,12 @@
 public class LogicalDecodingMessage implements ReplicationMessage {
     private final Operation operation;
     private final Instant commitTime;
-    private final long transactionId;
+    private final Long transactionId;
     private final boolean isTransactional;
     private final String prefix;
     private final byte[] content;
 
-    public LogicalDecodingMessage(Operation op, Instant commitTimestamp, long transactionId, boolean isTransactional,
+    public LogicalDecodingMessage(Operation op, Instant commitTimestamp, Long transactionId, boolean isTransactional,
                                   String prefix, byte[] content) {
         this.operation = op;
         this.commitTime = commitTimestamp;
@@ -48,8 +49,8 @@ public Instant getCommitTime() {
     }
 
     @Override
-    public long getTransactionId() {
-        return transactionId;
+    public OptionalLong getTransactionId() {
+        return transactionId == null ? OptionalLong.empty() : OptionalLong.of(transactionId);
     }
 
     @Override
@@ -79,8 +80,4 @@ public String getPrefix() {
     public byte[] getContent() {
         return content;
     }
-
-    public boolean isTransactional() {
-        return isTransactional;
-    }
 }",2021-11-26T13:29:15Z,154
"@@ -36,6 +36,7 @@
 
 import io.debezium.DebeziumException;
 import io.debezium.config.Configuration;
+import io.debezium.connector.postgresql.LogicalDecodingMessageFilter;
 import io.debezium.connector.postgresql.PostgresConnectorConfig;
 import io.debezium.connector.postgresql.PostgresSchema;
 import io.debezium.connector.postgresql.TypeRegistry;
@@ -60,6 +61,7 @@ public class PostgresReplicationConnection extends JdbcConnection implements Rep
     private final String slotName;
     private final String publicationName;
     private final RelationalTableFilters tableFilter;
+    private final LogicalDecodingMessageFilter logicalDecodingMessageFilter;
     private final PostgresConnectorConfig.AutoCreateMode publicationAutocreateMode;
     private final PostgresConnectorConfig.LogicalDecoder plugin;
     private final boolean dropSlotOnClose;
@@ -80,6 +82,7 @@ public class PostgresReplicationConnection extends JdbcConnection implements Rep
      * @param slotName                  the name of the DB slot for logical replication; may not be null
      * @param publicationName           the name of the DB publication for logical replication; may not be null
      * @param tableFilter               the tables to watch of the DB publication for logical replication; may not be null
+     * @param logicalDecodingMessageFilter the type of logical decoding messages to watch for; may not be bull
      * @param publicationAutocreateMode the mode for publication autocreation; may not be null
      * @param plugin                    decoder matching the server side plug-in used for streaming changes; may not be null
      * @param dropSlotOnClose           whether the replication slot should be dropped once the connection is closed
@@ -95,6 +98,7 @@ private PostgresReplicationConnection(PostgresConnectorConfig config,
                                           String slotName,
                                           String publicationName,
                                           RelationalTableFilters tableFilter,
+                                          LogicalDecodingMessageFilter logicalDecodingMessageFilter,
                                           PostgresConnectorConfig.AutoCreateMode publicationAutocreateMode,
                                           PostgresConnectorConfig.LogicalDecoder plugin,
                                           boolean dropSlotOnClose,
@@ -109,6 +113,7 @@ private PostgresReplicationConnection(PostgresConnectorConfig config,
         this.slotName = slotName;
         this.publicationName = publicationName;
         this.tableFilter = tableFilter;
+        this.logicalDecodingMessageFilter = logicalDecodingMessageFilter;
         this.publicationAutocreateMode = publicationAutocreateMode;
         this.plugin = plugin;
         this.dropSlotOnClose = dropSlotOnClose;
@@ -182,7 +187,7 @@ protected void initPublication() {
             // This is what ties the publication definition to the replication stream
             streamParams.put(""proto_version"", 1);
             streamParams.put(""publication_names"", publicationName);
-            streamParams.put(""messages"", originalConfig.logicalDecodingMessageHandlingMode() == PostgresConnectorConfig.LogicalDecodingMessageHandlingMode.INCLUDE);
+            streamParams.put(""messages"", true);
         }
     }
 
@@ -646,10 +651,10 @@ protected static class ReplicationConnectionBuilder implements Builder {
         private String slotName = DEFAULT_SLOT_NAME;
         private String publicationName = DEFAULT_PUBLICATION_NAME;
         private RelationalTableFilters tableFilter;
+        private LogicalDecodingMessageFilter logicalDecodingMessageFilter;
         private PostgresConnectorConfig.AutoCreateMode publicationAutocreateMode = PostgresConnectorConfig.AutoCreateMode.ALL_TABLES;
         private PostgresConnectorConfig.LogicalDecoder plugin = PostgresConnectorConfig.LogicalDecoder.DECODERBUFS;
         private PostgresConnectorConfig.TruncateHandlingMode truncateHandlingMode;
-        private PostgresConnectorConfig.LogicalDecodingMessageHandlingMode logicalDecodingMessageHandlingMode;
         private boolean dropSlotOnClose = DEFAULT_DROP_SLOT_ON_CLOSE;
         private Duration statusUpdateIntervalVal;
         private boolean doSnapshot;
@@ -705,9 +710,9 @@ public Builder withTruncateHandlingMode(PostgresConnectorConfig.TruncateHandling
         }
 
         @Override
-        public Builder withLogicalDecodingMessageHandlingMode(PostgresConnectorConfig.LogicalDecodingMessageHandlingMode logicalDecodingMessageHandlingMode) {
-            assert logicalDecodingMessageHandlingMode != null;
-            this.logicalDecodingMessageHandlingMode = logicalDecodingMessageHandlingMode;
+        public Builder withLogicalDecodingMessageFilter(LogicalDecodingMessageFilter logicalDecodingMessageFilter) {
+            assert logicalDecodingMessageFilter != null;
+            this.logicalDecodingMessageFilter = logicalDecodingMessageFilter;
             return this;
         }
 
@@ -750,8 +755,8 @@ public Builder doSnapshot(boolean doSnapshot) {
         @Override
         public ReplicationConnection build() {
             assert plugin != null : ""Decoding plugin name is not set"";
-            return new PostgresReplicationConnection(config, slotName, publicationName, tableFilter, publicationAutocreateMode, plugin,
-                    dropSlotOnClose, doSnapshot, statusUpdateIntervalVal, typeRegistry, slotStreamParams, schema);
+            return new PostgresReplicationConnection(config, slotName, publicationName, tableFilter, logicalDecodingMessageFilter,
+                    publicationAutocreateMode, plugin, dropSlotOnClose, doSnapshot, statusUpdateIntervalVal, typeRegistry, slotStreamParams, schema);
         }
 
         @Override",2021-11-26T13:29:15Z,147
"@@ -13,6 +13,7 @@
 import org.postgresql.replication.PGReplicationStream;
 
 import io.debezium.annotation.NotThreadSafe;
+import io.debezium.connector.postgresql.LogicalDecodingMessageFilter;
 import io.debezium.connector.postgresql.PostgresConnectorConfig;
 import io.debezium.connector.postgresql.PostgresSchema;
 import io.debezium.connector.postgresql.TypeRegistry;
@@ -160,13 +161,13 @@ interface Builder {
         Builder withTruncateHandlingMode(final PostgresConnectorConfig.TruncateHandlingMode truncateHandlingMode);
 
         /**
-         * Sets the instance for the Logical Decoding Message handling mode
+         * Sets the prefixes to watch for logical decoding messages
          *
-         * @param logicalDecodingMessageHandlingMode MESSAGE handling mode, may not be null.
+         * @param  logicalDecodingMessageFilter
          * @return this instance
-         * @see io.debezium.connector.postgresql.PostgresConnectorConfig.LogicalDecodingMessageHandlingMode
+         * @see #config.getMessageFilter
          */
-        Builder withLogicalDecodingMessageHandlingMode(final PostgresConnectorConfig.LogicalDecodingMessageHandlingMode logicalDecodingMessageHandlingMode);
+        Builder withLogicalDecodingMessageFilter(final LogicalDecodingMessageFilter logicalDecodingMessageFilter);
 
         /**
          * Whether or not to drop the replication slot once the replication connection closes",2021-11-26T13:29:15Z,155
"@@ -11,6 +11,7 @@
 import java.time.OffsetDateTime;
 import java.time.OffsetTime;
 import java.util.List;
+import java.util.OptionalLong;
 
 import org.postgresql.geometric.PGbox;
 import org.postgresql.geometric.PGcircle;
@@ -150,7 +151,7 @@ public interface ColumnValue<T> {
     /**
      * @return An id of transaction to which this change belongs
      */
-    public long getTransactionId();
+    public OptionalLong getTransactionId();
 
     /**
      * @return Table changed
@@ -193,11 +194,11 @@ default boolean isTransactionalMessage() {
 
     public class TransactionMessage implements ReplicationMessage {
 
-        private final long transationId;
+        private final OptionalLong transationId;
         private final Instant commitTime;
         private final Operation operation;
 
-        public TransactionMessage(Operation operation, long transactionId, Instant commitTime) {
+        public TransactionMessage(Operation operation, OptionalLong transactionId, Instant commitTime) {
             this.operation = operation;
             this.transationId = transactionId;
             this.commitTime = commitTime;
@@ -214,7 +215,7 @@ public boolean hasTypeMetadata() {
         }
 
         @Override
-        public long getTransactionId() {
+        public OptionalLong getTransactionId() {
             return transationId;
         }
 
@@ -250,11 +251,11 @@ public Instant getCommitTime() {
      */
     public class NoopMessage implements ReplicationMessage {
 
-        private final long transactionId;
+        private final Long transactionId;
         private final Instant commitTime;
         private final Operation operation;
 
-        public NoopMessage(long transactionId, Instant commitTime) {
+        public NoopMessage(Long transactionId, Instant commitTime) {
             this.operation = Operation.NOOP;
             this.transactionId = transactionId;
             this.commitTime = commitTime;
@@ -271,8 +272,8 @@ public boolean hasTypeMetadata() {
         }
 
         @Override
-        public long getTransactionId() {
-            return transactionId;
+        public OptionalLong getTransactionId() {
+            return transactionId == null ? OptionalLong.empty() : OptionalLong.of(transactionId);
         }
 
         @Override",2021-11-26T13:29:15Z,156
"@@ -8,6 +8,7 @@
 
 import java.time.Instant;
 import java.util.List;
+import java.util.OptionalLong;
 
 /**
  * Replication message instance representing transaction demarcation events.
@@ -17,11 +18,11 @@
  */
 public class TransactionMessage implements ReplicationMessage {
 
-    private final long transationId;
+    private final Long transationId;
     private final Instant commitTime;
     private final Operation operation;
 
-    public TransactionMessage(Operation operation, long transactionId, Instant commitTime) {
+    public TransactionMessage(Operation operation, Long transactionId, Instant commitTime) {
         this.operation = operation;
         this.transationId = transactionId;
         this.commitTime = commitTime;
@@ -38,8 +39,8 @@ public boolean hasTypeMetadata() {
     }
 
     @Override
-    public long getTransactionId() {
-        return transationId;
+    public OptionalLong getTransactionId() {
+        return transationId == null ? OptionalLong.empty() : OptionalLong.of(transationId);
     }
 
     @Override",2021-11-26T13:29:15Z,157
"@@ -70,7 +70,7 @@ public class PgOutputMessageDecoder extends AbstractMessageDecoder {
     private final PostgresConnection connection;
 
     private Instant commitTimestamp;
-    private int transactionId;
+    private Long transactionId;
 
     public enum MessageType {
         RELATION,
@@ -180,12 +180,7 @@ public void processNotEmptyMessage(ByteBuffer buffer, ReplicationMessageProcesso
                 handleRelationMessage(buffer, typeRegistry);
                 break;
             case LOGICAL_DECODING_MESSAGE:
-                if (decoderContext.getConfig().logicalDecodingMessageHandlingMode() == PostgresConnectorConfig.LogicalDecodingMessageHandlingMode.INCLUDE) {
-                    handleLogicalDecodingMessage(buffer, processor);
-                }
-                else {
-                    LOGGER.debug(""Message Type {} skipped, not processed."", messageType);
-                }
+                handleLogicalDecodingMessage(buffer, processor);
                 break;
             case INSERT:
                 decodeInsert(buffer, typeRegistry, processor);
@@ -214,8 +209,7 @@ public void processNotEmptyMessage(ByteBuffer buffer, ReplicationMessageProcesso
     public ChainedLogicalStreamBuilder optionsWithMetadata(ChainedLogicalStreamBuilder builder) {
         return builder.withSlotOption(""proto_version"", 1)
                 .withSlotOption(""publication_names"", decoderContext.getConfig().publicationName())
-                .withSlotOption(""messages"",
-                        decoderContext.getConfig().logicalDecodingMessageHandlingMode() == PostgresConnectorConfig.LogicalDecodingMessageHandlingMode.INCLUDE);
+                .withSlotOption(""messages"", true);
     }
 
     @Override
@@ -232,7 +226,7 @@ public ChainedLogicalStreamBuilder optionsWithoutMetadata(ChainedLogicalStreamBu
     private void handleBeginMessage(ByteBuffer buffer, ReplicationMessageProcessor processor) throws SQLException, InterruptedException {
         final Lsn lsn = Lsn.valueOf(buffer.getLong()); // LSN
         this.commitTimestamp = PG_EPOCH.plus(buffer.getLong(), ChronoUnit.MICROS);
-        this.transactionId = buffer.getInt();
+        this.transactionId = (long) buffer.getInt();
         LOGGER.trace(""Event: {}"", MessageType.BEGIN);
         LOGGER.trace(""Final LSN of transaction: {}"", lsn);
         LOGGER.trace(""Commit timestamp of transaction: {}"", commitTimestamp);
@@ -577,20 +571,26 @@ private void handleLogicalDecodingMessage(ByteBuffer buffer, ReplicationMessageP
             throws SQLException, InterruptedException {
         // As of PG14, the MESSAGE message format is as described:
         // Byte1 Always 'M'
-        // Int32 Xid of the transaction (only present for streamed transactions). This field is available since protocol version 2.
+        // Int32 Xid of the transaction (only present for streamed transactions in protocol version 2).
         // Int8 flags; Either 0 for no flags or 1 if the logical decoding message is transactional.
         // Int64 The LSN of the logical decoding message
         // String The prefix of the logical decoding message.
         // Int32 Length of the content.
         // Byten The content of the logical decoding message.
 
         boolean isTransactional = buffer.get() == 1;
-        final Lsn lsn = Lsn.valueOf(buffer.getLong()); // LSN
+        final Lsn lsn = Lsn.valueOf(buffer.getLong());
         String prefix = readString(buffer);
         int contentLength = buffer.getInt();
         byte[] content = new byte[contentLength];
         buffer.get(content);
 
+        // non-transactional messages do not have xids or commitTimestamps
+        if (!isTransactional) {
+            transactionId = null;
+            commitTimestamp = null;
+        }
+
         LOGGER.trace(""Event: {}"", MessageType.LOGICAL_DECODING_MESSAGE);
         LOGGER.trace(""Commit LSN: {}"", lsn);
         LOGGER.trace(""Commit timestamp of transaction: {}"", commitTimestamp);",2021-11-26T13:29:15Z,158
"@@ -7,6 +7,7 @@
 
 import java.time.Instant;
 import java.util.List;
+import java.util.OptionalLong;
 
 import io.debezium.connector.postgresql.PostgresStreamingChangeEventSource.PgConnectionSupplier;
 import io.debezium.connector.postgresql.PostgresType;
@@ -22,12 +23,12 @@ public class PgOutputReplicationMessage implements ReplicationMessage {
 
     private Operation op;
     private Instant commitTimestamp;
-    private long transactionId;
+    private Long transactionId;
     private String table;
     private List<Column> oldColumns;
     private List<Column> newColumns;
 
-    public PgOutputReplicationMessage(Operation op, String table, Instant commitTimestamp, long transactionId, List<Column> oldColumns, List<Column> newColumns) {
+    public PgOutputReplicationMessage(Operation op, String table, Instant commitTimestamp, Long transactionId, List<Column> oldColumns, List<Column> newColumns) {
         this.op = op;
         this.commitTimestamp = commitTimestamp;
         this.transactionId = transactionId;
@@ -47,8 +48,8 @@ public Instant getCommitTime() {
     }
 
     @Override
-    public long getTransactionId() {
-        return transactionId;
+    public OptionalLong getTransactionId() {
+        return transactionId == null ? OptionalLong.empty() : OptionalLong.of(transactionId);
     }
 
     @Override",2021-11-26T13:29:15Z,159
"@@ -9,6 +9,7 @@
 import java.time.Instant;
 import java.util.List;
 import java.util.Optional;
+import java.util.OptionalLong;
 import java.util.stream.Collectors;
 import java.util.stream.IntStream;
 
@@ -67,8 +68,8 @@ public Instant getCommitTime() {
     }
 
     @Override
-    public long getTransactionId() {
-        return Integer.toUnsignedLong(rawMessage.getTransactionId());
+    public OptionalLong getTransactionId() {
+        return OptionalLong.of(Integer.toUnsignedLong(rawMessage.getTransactionId()));
     }
 
     @Override",2021-11-26T13:29:15Z,160
"@@ -104,7 +104,7 @@ public class StreamingWal2JsonMessageDecoder extends AbstractMessageDecoder {
      */
     private byte[] currentChunk;
 
-    private long txId;
+    private Long txId;
 
     private Instant commitTime;
 ",2021-11-26T13:29:15Z,161
"@@ -9,6 +9,7 @@
 import java.time.Instant;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.OptionalLong;
 import java.util.regex.Matcher;
 
 import org.apache.kafka.connect.data.Field;
@@ -36,14 +37,14 @@ class Wal2JsonReplicationMessage implements ReplicationMessage {
 
     private static final Logger LOGGER = LoggerFactory.getLogger(Wal2JsonReplicationMessage.class);
 
-    private final long txId;
+    private final Long txId;
     private final Instant commitTime;
     private final Document rawMessage;
     private final boolean hasMetadata;
     private final boolean lastEventForLsn;
     private final TypeRegistry typeRegistry;
 
-    public Wal2JsonReplicationMessage(long txId, Instant commitTime, Document rawMessage, boolean hasMetadata, boolean lastEventForLsn, TypeRegistry typeRegistry) {
+    public Wal2JsonReplicationMessage(Long txId, Instant commitTime, Document rawMessage, boolean hasMetadata, boolean lastEventForLsn, TypeRegistry typeRegistry) {
         this.txId = txId;
         this.commitTime = commitTime;
         this.rawMessage = rawMessage;
@@ -73,8 +74,8 @@ public Instant getCommitTime() {
     }
 
     @Override
-    public long getTransactionId() {
-        return txId;
+    public OptionalLong getTransactionId() {
+        return txId == null ? OptionalLong.empty() : OptionalLong.of(txId);
     }
 
     @Override",2021-11-26T13:29:15Z,162
"@@ -11,12 +11,7 @@
 import static io.debezium.junit.EqualityCheck.LESS_THAN;
 import static junit.framework.TestCase.assertEquals;
 import static org.fest.assertions.Assertions.assertThat;
-import static org.junit.Assert.assertFalse;
-import static org.junit.Assert.assertNotEquals;
-import static org.junit.Assert.assertNotNull;
-import static org.junit.Assert.assertNull;
-import static org.junit.Assert.assertTrue;
-import static org.junit.Assert.fail;
+import static org.junit.Assert.*;
 
 import java.io.IOException;
 import java.lang.management.ManagementFactory;
@@ -247,8 +242,8 @@ public void shouldValidateConfiguration() throws Exception {
         validateConfigField(validatedConfig, PostgresConnectorConfig.DECIMAL_HANDLING_MODE, PostgresConnectorConfig.DecimalHandlingMode.PRECISE);
         validateConfigField(validatedConfig, PostgresConnectorConfig.SSL_SOCKET_FACTORY, null);
         validateConfigField(validatedConfig, PostgresConnectorConfig.TCP_KEEPALIVE, true);
-        validateConfigField(validatedConfig, PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_HANDLING_MODE,
-                PostgresConnectorConfig.LogicalDecodingMessageHandlingMode.SKIP);
+        validateConfigField(validatedConfig, PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST, null);
+        validateConfigField(validatedConfig, PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_PREFIX_INCLUDE_LIST, null);
     }
 
     @Test
@@ -2891,38 +2886,44 @@ public void testStreamingWithNumericReplicationSlotName() throws Exception {
     @FixFor(""DBZ-2363"")
     @SkipWhenDecoderPluginNameIsNot(value = SkipWhenDecoderPluginNameIsNot.DecoderPluginName.PGOUTPUT, reason = ""Only supported on PgOutput"")
     @SkipWhenDatabaseVersion(check = LESS_THAN, major = 14, minor = 0, reason = ""Database Version less than 14"")
-    public void shouldNotConsumeLogicalDecodingMessagesWhenConfigIsSetToSkip() throws Exception {
+    public void shouldNotConsumeLogicalDecodingMessagesWhenAllPrefixesAreInTheExcludedList() throws Exception {
         TestHelper.execute(SETUP_TABLES_STMT);
         Configuration.Builder configBuilder = TestHelper.defaultConfig()
-                .with(PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_HANDLING_MODE, ""skip"");
+                .with(PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST, "".*"");
         start(PostgresConnector.class, configBuilder.build());
         assertConnectorIsRunning();
         waitForSnapshotToBeCompleted();
 
         // emit logical decoding message
         TestHelper.execute(""SELECT pg_logical_emit_message(false, 'prefix', 'content');"");
+        TestHelper.execute(""INSERT into s1.a VALUES(201, 1);"");
 
-        assertNoRecordsToConsume();
+        SourceRecords records = consumeRecordsByTopic(2);
+        List<SourceRecord> insertRecords = records.recordsForTopic(topicName(""s1.a""));
+        List<SourceRecord> logicalMessageRecords = records.recordsForTopic(topicName(""message""));
+        assertThat(insertRecords.size()).isEqualTo(1);
+        assertNull(logicalMessageRecords);
     }
 
     @Test
     @FixFor(""DBZ-2363"")
     @SkipWhenDecoderPluginNameIsNot(value = SkipWhenDecoderPluginNameIsNot.DecoderPluginName.PGOUTPUT, reason = ""Only supported on PgOutput"")
-    @SkipWhenDatabaseVersion(check = LESS_THAN, major = 14, minor = 0, reason = ""Database Version less than 14"")
-    public void shouldConsumeLogicalDecodingMessagesWhenConfigIsSetToInclude() throws Exception {
+    @SkipWhenDatabaseVersion(check = LESS_THAN, major = 14, minor = 0, reason = ""Message not supported for PG version < 14"")
+    public void shouldConsumeNonTransactionalLogicalDecodingMessages() throws Exception {
         TestHelper.execute(SETUP_TABLES_STMT);
-        Configuration.Builder configBuilder = TestHelper.defaultConfig()
-                .with(PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_HANDLING_MODE, ""include"");
+        Configuration.Builder configBuilder = TestHelper.defaultConfig();
 
         start(PostgresConnector.class, configBuilder.build());
         assertConnectorIsRunning();
 
         assertRecordsFromSnapshot(2, 1, 1);
 
-        // emit non transactional logical decoding message
+        // emit non transactional logical decoding message with text
         TestHelper.execute(""SELECT pg_logical_emit_message(false, 'foo', 'bar');"");
+        // emit non transactional logical decoding message with binary
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'foo', E'bar'::bytea);"");
 
-        SourceRecords records = consumeRecordsByTopic(1);
+        SourceRecords records = consumeRecordsByTopic(2);
         List<SourceRecord> recordsForTopic = records.recordsForTopic(topicName(""message""));
         recordsForTopic.forEach(record -> {
             Struct value = (Struct) record.value();
@@ -2937,12 +2938,30 @@ public void shouldConsumeLogicalDecodingMessagesWhenConfigIsSetToInclude() throw
             assertEquals("""", source.getString(SourceInfo.SCHEMA_NAME_KEY));
 
             assertEquals(Envelope.Operation.MESSAGE.code(), op);
-            assertEquals(""foo"", message.getString(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY));
-            assertFalse(message.getBoolean(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_TRANSACTIONAL_KEY));
+            assertEquals(""foo"",
+                    message.getString(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY));
+            assertArrayEquals(""bar"".getBytes(),
+                    message.getBytes(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_CONTENT_KEY));
         });
+    }
 
-        // emit transactional logical decoding message
+    @Test
+    @FixFor(""DBZ-2363"")
+    @SkipWhenDecoderPluginNameIsNot(value = SkipWhenDecoderPluginNameIsNot.DecoderPluginName.PGOUTPUT, reason = ""Only supported on PgOutput"")
+    @SkipWhenDatabaseVersion(check = LESS_THAN, major = 14, minor = 0, reason = ""Message not supported for PG version < 14"")
+    public void shouldConsumeTransactionalLogicalDecodingMessages() throws Exception {
+        TestHelper.execute(SETUP_TABLES_STMT);
+        Configuration.Builder configBuilder = TestHelper.defaultConfig();
+
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+
+        assertRecordsFromSnapshot(2, 1, 1);
+
+        // emit transactional logical decoding message with text
         TestHelper.execute(""SELECT pg_logical_emit_message(true, 'txn_foo', 'txn_bar');"");
+        // emit transactional logical decoding message with binary
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'foo', E'txn_bar'::bytea);"");
 
         SourceRecords txnRecords = consumeRecordsByTopic(1);
         List<SourceRecord> txnRecordsForTopic = txnRecords.recordsForTopic(topicName(""message""));
@@ -2960,8 +2979,70 @@ public void shouldConsumeLogicalDecodingMessagesWhenConfigIsSetToInclude() throw
             assertEquals("""", source.getString(SourceInfo.SCHEMA_NAME_KEY));
 
             assertEquals(Envelope.Operation.MESSAGE.code(), op);
-            assertEquals(""txn_foo"", message.getString(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY));
-            assertTrue(message.getBoolean(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_TRANSACTIONAL_KEY));
+            assertEquals(""txn_foo"",
+                    message.getString(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY));
+            assertArrayEquals(""txn_bar"".getBytes(),
+                    message.getBytes(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_CONTENT_KEY));
+        });
+    }
+
+    @Test
+    @FixFor(""DBZ-2363"")
+    @SkipWhenDecoderPluginNameIsNot(value = SkipWhenDecoderPluginNameIsNot.DecoderPluginName.PGOUTPUT, reason = ""Only supported on PgOutput"")
+    @SkipWhenDatabaseVersion(check = LESS_THAN, major = 14, minor = 0, reason = ""Database Version less than 14"")
+    public void shouldNotConsumeLogicalDecodingMessagesWithExludedPrefixes() throws Exception {
+        TestHelper.execute(SETUP_TABLES_STMT);
+        Configuration.Builder configBuilder = TestHelper.defaultConfig()
+                .with(PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_PREFIX_EXCLUDE_LIST, ""excluded_prefix, prefix:excluded"");
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+        waitForSnapshotToBeCompleted();
+
+        // emit logical decoding message
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'included_prefix', 'content');"");
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'excluded_prefix', 'content');"");
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'prefix:excluded', 'content');"");
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'prefix:included', 'content');"");
+
+        SourceRecords records = consumeRecordsByTopic(4);
+        List<SourceRecord> recordsForTopic = records.recordsForTopic(topicName(""message""));
+        assertEquals(2, recordsForTopic.size());
+
+        recordsForTopic.forEach(record -> {
+            Struct message = ((Struct) record.value()).getStruct(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_KEY);
+            assertThat(message.getString(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY))
+                    .doesNotMatch(""excluded_prefix"");
+            assertThat(message.getString(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY))
+                    .doesNotMatch(""prefix:excluded"");
+        });
+    }
+
+    @Test
+    @FixFor(""DBZ-2363"")
+    @SkipWhenDecoderPluginNameIsNot(value = SkipWhenDecoderPluginNameIsNot.DecoderPluginName.PGOUTPUT, reason = ""Only supported on PgOutput"")
+    @SkipWhenDatabaseVersion(check = LESS_THAN, major = 14, minor = 0, reason = ""Database Version less than 14"")
+    public void shouldOnlyConsumeLogicalDecodingMessagesWithIncludedPrefixes() throws Exception {
+        TestHelper.execute(SETUP_TABLES_STMT);
+        Configuration.Builder configBuilder = TestHelper.defaultConfig()
+                .with(PostgresConnectorConfig.LOGICAL_DECODING_MESSAGE_PREFIX_INCLUDE_LIST, ""included_prefix, prefix:included"");
+        start(PostgresConnector.class, configBuilder.build());
+        assertConnectorIsRunning();
+        waitForSnapshotToBeCompleted();
+
+        // emit logical decoding message
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'included_prefix', 'content');"");
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'excluded_prefix', 'content');"");
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'prefix:excluded', 'content');"");
+        TestHelper.execute(""SELECT pg_logical_emit_message(false, 'prefix:included', 'content');"");
+
+        SourceRecords records = consumeRecordsByTopic(4);
+        List<SourceRecord> recordsForTopic = records.recordsForTopic(topicName(""message""));
+        assertEquals(2, recordsForTopic.size());
+
+        recordsForTopic.forEach(record -> {
+            Struct message = ((Struct) record.value()).getStruct(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_KEY);
+            assertThat(message.getString(LogicalDecodingMessageMonitor.DEBEZIUM_LOGICAL_DECODING_MESSAGE_PREFIX_KEY))
+                    .matches(""included_prefix|prefix:included"");
         });
     }
 ",2021-11-26T13:29:15Z,45
"@@ -1211,7 +1211,6 @@ The message value looks like this for transactional messages:
         ""op"": ""m"", // <2>
         ""ts_ms"": 1559033904961 // <3>
         ""message"": { // <4>
-            ""transactional"": true,
             ""prefix"": ""foo"",
             ""content"": ""Ymfy""
     }
@@ -1241,7 +1240,6 @@ The message value looks like this for non-transactional messages:
         ""op"": ""m"", // <2>
         ""ts_ms"": 1559033904961 // <3>
         ""message"": { // <4>
-            ""transactional"": false,
             ""prefix"": ""foo"",
             ""content"": ""Ymfy""
     }
@@ -1262,7 +1260,7 @@ a|Mandatory field that describes the source metadata for the event. In a _messag
 * Database name
 * Schema name (always `""""` for _mesage_ events)
 * Table name (always `""""` for _message_ events)
-* If the event was part of a snapshot
+* If the event was part of a snapshot (always `false` for _message_ events)
 * ID of the transaction in which the operation was performed (`null` for non-transactional _message_ events)
 * Offset of the operation in the database log
 * Transactional messages: Timestamp for when the message was inserted into the WAL
@@ -1276,15 +1274,14 @@ a|Mandatory string that describes the type of operation. The `op` field value is
 |`ts_ms`
 a|Optional field that displays the time at which the connector processed the event. The time is based on the system clock in the JVM running the Kafka Connect task.  +
  +
-For transaction _message_ events, In the `source` object, `ts_ms` indicates the time that the change was made in the database for transactional _message_ events. By comparing the value for `payload.source.ts_ms` with the value for `payload.ts_ms`, you can determine the lag between the source database update and {prodname}.
+For transactional _message_ events, the `ts_ms` attribute of the `source` object indicates the time that the change was made in the database for transactional _message_ events. By comparing the value for `payload.source.ts_ms` with the value for `payload.ts_ms`, you can determine the lag between the source database update and {prodname}.
 
 For non-transactional _message_ events, the `source` object's `ts_ms` indicates time at which the connector encounters the _message_ event, while the `payload.ts_ms` indicates the time at which the connector processed the event. This difference is due to the fact that the commit timestamp is not present in Postgres's generic logical message format and non-transactional logical messages are not preceded by a `BEGIN` event (which has timestamp information).
 
 |4
 |`message`
 a|Field that contains the message metadata
 
-* Boolean that indicates if the message was part of current transaction or if it was written immediately)
 * Prefix (text)
 * Content (byte array that is encoded based on the {link-prefix}:{link-postgresql-connector}#postgresql-property-binary-handling-mode[binary handling mode] setting)
 
@@ -2908,14 +2905,16 @@ For information about the structure of _truncate_ events and about their orderin
 |Specifies how many decimal digits should be used when converting Postgres `money` type to `java.math.BigDecimal`, which represents the values in change events.
 Applicable only when `decimal.handling.mode` is set to `precise`.
 
-|[[postgresql-property-logical-decoding-message-handling-mode]]<<postgresql-property-logical-decoding-message-handling-mode, `+logical_decoding_message.handling.mode+`>>
-|skip
-|Specifies whether `MESSSAGE` events should be propagated or not (only available when using the `pgoutput` plug-in with Postgres 14 or later): +
- +
-`skip` causes those event to be omitted (the default). +
- +
-`include` causes those events to be included. +
- +
+|[[postgresql-property-logical-decoding-message-handling-mode]]<<postgresql-property-logical-decoding-message-prefix-include-list, `+message.prefix.include.list+`>>
+|No default
+|An optional, comma-separated list of regular expressions that match names of logical decoding message prefixes for which you *want* to capture. Any logical decoding message with a prefix not included in `message.prefix.include.list` is excluded By default, all logical decoding messages are captured. Do not also set the `message.prefix.exclude.list` property.
+
+For information about the structure of _message_ events and about their ordering semantics, see xref:postgresql-message-events[].
+
+|[[postgresql-property-logical-decoding-message-handling-mode]]<<postgresql-property-logical-decoding-message-prefix-exclude-list, `+message.prefix.exclude.list+`>>
+|No default
+|An optional, comma-separated list of regular expressions that match names of logical decoding message prefixes for which you *do not* to capture. Any logical decoding message with a prefix that is not included in `message.prefix.exclude.list` is included. Do not also set the `message.prefix.include.list` property. To exclude all logiacl decoding messages pass `.*` into this config.
+
 For information about the structure of _message_ events and about their ordering semantics, see xref:postgresql-message-events[].
 
 |===",2021-11-26T13:29:15Z,11
